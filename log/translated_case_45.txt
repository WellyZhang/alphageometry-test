I0123 12:31:09.389933 140256880439296 inference_utils.py:69] Parsing gin configuration.
I0123 12:31:09.390036 140256880439296 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0123 12:31:09.390244 140256880439296 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0123 12:31:09.390276 140256880439296 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0123 12:31:09.390304 140256880439296 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0123 12:31:09.390330 140256880439296 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0123 12:31:09.390356 140256880439296 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0123 12:31:09.390383 140256880439296 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0123 12:31:09.390409 140256880439296 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0123 12:31:09.390436 140256880439296 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=32
I0123 12:31:09.390462 140256880439296 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0123 12:31:09.390488 140256880439296 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0123 12:31:09.390533 140256880439296 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0123 12:31:09.390672 140256880439296 resource_reader.py:55] Path not found: base_htrans.gin
I0123 12:31:09.390885 140256880439296 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0123 12:31:09.390982 140256880439296 resource_reader.py:55] Path not found: trainer_configuration.gin
I0123 12:31:09.397271 140256880439296 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0123 12:31:09.397389 140256880439296 resource_reader.py:55] Path not found: size/medium_150M.gin
I0123 12:31:09.397744 140256880439296 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0123 12:31:09.397849 140256880439296 resource_reader.py:55] Path not found: options/positions_t5.gin
I0123 12:31:09.398125 140256880439296 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0123 12:31:09.398224 140256880439296 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0123 12:31:09.398632 140256880439296 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0123 12:31:09.398730 140256880439296 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0123 12:31:09.402382 140256880439296 training_loop.py:334] ==== Training loop: initializing model ====
I0123 12:31:09.492531 140256880439296 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0123 12:31:09.493240 140256880439296 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0123 12:31:09.499783 140256880439296 training_loop.py:335] Process 0 of 1
I0123 12:31:09.499838 140256880439296 training_loop.py:336] Local device count = 1
I0123 12:31:09.499878 140256880439296 training_loop.py:337] Number of replicas = 1
I0123 12:31:09.499910 140256880439296 training_loop.py:339] Using random number seed 42
I0123 12:31:09.964791 140256880439296 training_loop.py:359] Initializing the model.
I0123 12:31:10.386263 140256880439296 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.386555 140256880439296 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 12:31:10.386666 140256880439296 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:31:10.386747 140256880439296 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:31:10.386825 140256880439296 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:31:10.386909 140256880439296 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:31:10.386983 140256880439296 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:31:10.387055 140256880439296 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:31:10.387128 140256880439296 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:31:10.387201 140256880439296 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:31:10.387272 140256880439296 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:31:10.387341 140256880439296 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:31:10.387413 140256880439296 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:31:10.387482 140256880439296 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:31:10.387523 140256880439296 attention.py:418] Single window, no scan.
I0123 12:31:10.387570 140256880439296 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 12:31:10.387688 140256880439296 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:10.387728 140256880439296 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:10.387760 140256880439296 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:10.389836 140256880439296 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.395310 140256880439296 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:10.406433 140256880439296 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.406717 140256880439296 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:10.411202 140256880439296 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:31:10.422101 140256880439296 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:10.422159 140256880439296 attention.py:418] Single window, no scan.
I0123 12:31:10.422198 140256880439296 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:10.422232 140256880439296 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.422297 140256880439296 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.423496 140256880439296 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.423574 140256880439296 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.424302 140256880439296 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.426814 140256880439296 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.432723 140256880439296 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.434483 140256880439296 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.434574 140256880439296 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:10.434610 140256880439296 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:10.434675 140256880439296 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.434804 140256880439296 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:10.435146 140256880439296 nn_components.py:325] mlp: activation = None
I0123 12:31:10.435195 140256880439296 nn_components.py:329] mlp: final activation = None
I0123 12:31:10.437152 140256880439296 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.437254 140256880439296 nn_components.py:261] mlp: residual
I0123 12:31:10.440229 140256880439296 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.440311 140256880439296 transformer_base.py:443] tbase: final FFN
I0123 12:31:10.440812 140256880439296 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:10.451192 140256880439296 nn_components.py:329] mlp: final activation = None
I0123 12:31:10.460407 140256880439296 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.460507 140256880439296 nn_components.py:261] mlp: residual
I0123 12:31:10.460814 140256880439296 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.460895 140256880439296 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 12:31:10.461007 140256880439296 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:10.461046 140256880439296 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:10.461078 140256880439296 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:10.462962 140256880439296 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.465592 140256880439296 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:10.471327 140256880439296 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.471592 140256880439296 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:10.474286 140256880439296 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:31:10.478723 140256880439296 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:10.478832 140256880439296 attention.py:418] Single window, no scan.
I0123 12:31:10.478871 140256880439296 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:10.478904 140256880439296 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.478978 140256880439296 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.479607 140256880439296 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.479684 140256880439296 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.480072 140256880439296 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.480880 140256880439296 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.483460 140256880439296 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.484094 140256880439296 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.484173 140256880439296 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:10.484210 140256880439296 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:10.484270 140256880439296 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.484397 140256880439296 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:10.484752 140256880439296 nn_components.py:325] mlp: activation = None
I0123 12:31:10.484797 140256880439296 nn_components.py:329] mlp: final activation = None
I0123 12:31:10.486815 140256880439296 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.486911 140256880439296 nn_components.py:261] mlp: residual
I0123 12:31:10.489477 140256880439296 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.489557 140256880439296 transformer_base.py:443] tbase: final FFN
I0123 12:31:10.490004 140256880439296 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:10.492368 140256880439296 nn_components.py:329] mlp: final activation = None
I0123 12:31:10.494312 140256880439296 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.494408 140256880439296 nn_components.py:261] mlp: residual
I0123 12:31:10.494704 140256880439296 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.494784 140256880439296 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 12:31:10.494896 140256880439296 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:10.494935 140256880439296 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:10.494967 140256880439296 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:10.496882 140256880439296 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.499302 140256880439296 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:10.505363 140256880439296 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.505626 140256880439296 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:10.508328 140256880439296 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:31:10.512272 140256880439296 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:10.512327 140256880439296 attention.py:418] Single window, no scan.
I0123 12:31:10.512364 140256880439296 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:10.512396 140256880439296 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.512459 140256880439296 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.513027 140256880439296 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.513103 140256880439296 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.513474 140256880439296 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.514250 140256880439296 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.516819 140256880439296 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.517499 140256880439296 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.517577 140256880439296 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:10.517614 140256880439296 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:10.517685 140256880439296 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.517816 140256880439296 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:10.518143 140256880439296 nn_components.py:325] mlp: activation = None
I0123 12:31:10.518187 140256880439296 nn_components.py:329] mlp: final activation = None
I0123 12:31:10.520131 140256880439296 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.520229 140256880439296 nn_components.py:261] mlp: residual
I0123 12:31:10.522782 140256880439296 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.523269 140256880439296 transformer_base.py:443] tbase: final FFN
I0123 12:31:10.523756 140256880439296 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:10.526082 140256880439296 nn_components.py:329] mlp: final activation = None
I0123 12:31:10.528024 140256880439296 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.528119 140256880439296 nn_components.py:261] mlp: residual
I0123 12:31:10.528415 140256880439296 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.528495 140256880439296 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 12:31:10.528605 140256880439296 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:10.528644 140256880439296 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:10.528675 140256880439296 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:10.530573 140256880439296 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.532992 140256880439296 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:10.538723 140256880439296 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.538998 140256880439296 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:10.541723 140256880439296 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:31:10.545560 140256880439296 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:10.545615 140256880439296 attention.py:418] Single window, no scan.
I0123 12:31:10.545659 140256880439296 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:10.545695 140256880439296 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.545758 140256880439296 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.546338 140256880439296 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.546416 140256880439296 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.546795 140256880439296 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.547631 140256880439296 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.550249 140256880439296 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.550901 140256880439296 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.550980 140256880439296 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:10.551018 140256880439296 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:10.551081 140256880439296 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.551218 140256880439296 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:10.551580 140256880439296 nn_components.py:325] mlp: activation = None
I0123 12:31:10.551633 140256880439296 nn_components.py:329] mlp: final activation = None
I0123 12:31:10.553567 140256880439296 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.553668 140256880439296 nn_components.py:261] mlp: residual
I0123 12:31:10.556255 140256880439296 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.556340 140256880439296 transformer_base.py:443] tbase: final FFN
I0123 12:31:10.556774 140256880439296 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:10.559074 140256880439296 nn_components.py:329] mlp: final activation = None
I0123 12:31:10.560992 140256880439296 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.561086 140256880439296 nn_components.py:261] mlp: residual
I0123 12:31:10.561384 140256880439296 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.561464 140256880439296 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 12:31:10.561573 140256880439296 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:10.561612 140256880439296 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:10.561650 140256880439296 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:10.563569 140256880439296 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.566140 140256880439296 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:10.572019 140256880439296 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.572278 140256880439296 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:10.575036 140256880439296 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:31:10.578840 140256880439296 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:10.578897 140256880439296 attention.py:418] Single window, no scan.
I0123 12:31:10.578933 140256880439296 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:10.578965 140256880439296 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.579027 140256880439296 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.579604 140256880439296 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.579681 140256880439296 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.580048 140256880439296 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.580827 140256880439296 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.583818 140256880439296 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.584457 140256880439296 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.584536 140256880439296 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:10.584572 140256880439296 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:10.584633 140256880439296 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.584772 140256880439296 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:10.585101 140256880439296 nn_components.py:325] mlp: activation = None
I0123 12:31:10.585146 140256880439296 nn_components.py:329] mlp: final activation = None
I0123 12:31:10.587094 140256880439296 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.587191 140256880439296 nn_components.py:261] mlp: residual
I0123 12:31:10.589791 140256880439296 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.589870 140256880439296 transformer_base.py:443] tbase: final FFN
I0123 12:31:10.590307 140256880439296 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:10.592611 140256880439296 nn_components.py:329] mlp: final activation = None
I0123 12:31:10.594606 140256880439296 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.594702 140256880439296 nn_components.py:261] mlp: residual
I0123 12:31:10.594995 140256880439296 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.595076 140256880439296 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 12:31:10.595186 140256880439296 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:10.595225 140256880439296 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:10.595256 140256880439296 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:10.597120 140256880439296 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.599565 140256880439296 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:10.605276 140256880439296 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.605531 140256880439296 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:10.608286 140256880439296 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:31:10.612196 140256880439296 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:10.612252 140256880439296 attention.py:418] Single window, no scan.
I0123 12:31:10.612289 140256880439296 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:10.612321 140256880439296 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.612384 140256880439296 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.612997 140256880439296 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.613075 140256880439296 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.613445 140256880439296 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.614246 140256880439296 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.616798 140256880439296 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.617432 140256880439296 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.617511 140256880439296 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:10.617548 140256880439296 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:10.617609 140256880439296 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.617751 140256880439296 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:10.618082 140256880439296 nn_components.py:325] mlp: activation = None
I0123 12:31:10.618128 140256880439296 nn_components.py:329] mlp: final activation = None
I0123 12:31:10.620086 140256880439296 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.620187 140256880439296 nn_components.py:261] mlp: residual
I0123 12:31:10.622833 140256880439296 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.622915 140256880439296 transformer_base.py:443] tbase: final FFN
I0123 12:31:10.623354 140256880439296 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:10.625739 140256880439296 nn_components.py:329] mlp: final activation = None
I0123 12:31:10.627702 140256880439296 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.627799 140256880439296 nn_components.py:261] mlp: residual
I0123 12:31:10.628095 140256880439296 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.628176 140256880439296 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 12:31:10.628287 140256880439296 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:10.628326 140256880439296 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:10.628358 140256880439296 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:10.630241 140256880439296 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.632758 140256880439296 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:10.638572 140256880439296 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.638842 140256880439296 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:10.641535 140256880439296 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:31:10.645401 140256880439296 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:10.645458 140256880439296 attention.py:418] Single window, no scan.
I0123 12:31:10.645494 140256880439296 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:10.645527 140256880439296 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.645589 140256880439296 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.646165 140256880439296 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.646244 140256880439296 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.646610 140256880439296 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.647395 140256880439296 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.649915 140256880439296 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.650538 140256880439296 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.650619 140256880439296 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:10.650655 140256880439296 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:10.650714 140256880439296 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.650841 140256880439296 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:10.651167 140256880439296 nn_components.py:325] mlp: activation = None
I0123 12:31:10.651212 140256880439296 nn_components.py:329] mlp: final activation = None
I0123 12:31:10.653219 140256880439296 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.653314 140256880439296 nn_components.py:261] mlp: residual
I0123 12:31:10.655880 140256880439296 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.655965 140256880439296 transformer_base.py:443] tbase: final FFN
I0123 12:31:10.656399 140256880439296 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:10.659126 140256880439296 nn_components.py:329] mlp: final activation = None
I0123 12:31:10.661054 140256880439296 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.661156 140256880439296 nn_components.py:261] mlp: residual
I0123 12:31:10.661458 140256880439296 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.661541 140256880439296 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 12:31:10.661661 140256880439296 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:10.661703 140256880439296 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:10.661735 140256880439296 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:10.802231 140256880439296 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.805407 140256880439296 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:10.811428 140256880439296 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.811727 140256880439296 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:10.814488 140256880439296 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:31:10.818481 140256880439296 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:10.818540 140256880439296 attention.py:418] Single window, no scan.
I0123 12:31:10.818579 140256880439296 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:10.818613 140256880439296 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.818682 140256880439296 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.819306 140256880439296 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.819386 140256880439296 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.819762 140256880439296 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.820559 140256880439296 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.823221 140256880439296 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.823863 140256880439296 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.823942 140256880439296 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:10.823978 140256880439296 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:10.824041 140256880439296 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.824174 140256880439296 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:10.824520 140256880439296 nn_components.py:325] mlp: activation = None
I0123 12:31:10.824566 140256880439296 nn_components.py:329] mlp: final activation = None
I0123 12:31:10.826515 140256880439296 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.826612 140256880439296 nn_components.py:261] mlp: residual
I0123 12:31:10.829246 140256880439296 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.829325 140256880439296 transformer_base.py:443] tbase: final FFN
I0123 12:31:10.829782 140256880439296 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:10.832160 140256880439296 nn_components.py:329] mlp: final activation = None
I0123 12:31:10.834132 140256880439296 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.834238 140256880439296 nn_components.py:261] mlp: residual
I0123 12:31:10.834542 140256880439296 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.834625 140256880439296 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 12:31:10.834738 140256880439296 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:10.834777 140256880439296 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:10.834809 140256880439296 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:10.836766 140256880439296 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.839208 140256880439296 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:10.844984 140256880439296 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.845247 140256880439296 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:10.848008 140256880439296 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:31:10.851835 140256880439296 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:10.851889 140256880439296 attention.py:418] Single window, no scan.
I0123 12:31:10.851926 140256880439296 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:10.851957 140256880439296 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.852020 140256880439296 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.852590 140256880439296 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.852665 140256880439296 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.853036 140256880439296 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.853835 140256880439296 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.856455 140256880439296 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.857082 140256880439296 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.857159 140256880439296 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:10.857194 140256880439296 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:10.857254 140256880439296 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.857380 140256880439296 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:10.857710 140256880439296 nn_components.py:325] mlp: activation = None
I0123 12:31:10.857755 140256880439296 nn_components.py:329] mlp: final activation = None
I0123 12:31:10.859682 140256880439296 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.859778 140256880439296 nn_components.py:261] mlp: residual
I0123 12:31:10.862483 140256880439296 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.862568 140256880439296 transformer_base.py:443] tbase: final FFN
I0123 12:31:10.863026 140256880439296 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:10.865408 140256880439296 nn_components.py:329] mlp: final activation = None
I0123 12:31:10.867467 140256880439296 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.867567 140256880439296 nn_components.py:261] mlp: residual
I0123 12:31:10.867864 140256880439296 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.867952 140256880439296 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 12:31:10.868067 140256880439296 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:10.868108 140256880439296 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:10.868139 140256880439296 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:10.870188 140256880439296 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.872773 140256880439296 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:10.878555 140256880439296 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.878838 140256880439296 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:10.881963 140256880439296 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:31:10.885786 140256880439296 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:10.885843 140256880439296 attention.py:418] Single window, no scan.
I0123 12:31:10.885880 140256880439296 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:10.885911 140256880439296 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.885976 140256880439296 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.886602 140256880439296 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.886683 140256880439296 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.887068 140256880439296 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.887898 140256880439296 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.890478 140256880439296 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.891237 140256880439296 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.891317 140256880439296 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:10.891354 140256880439296 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:10.891421 140256880439296 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.891559 140256880439296 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:10.891891 140256880439296 nn_components.py:325] mlp: activation = None
I0123 12:31:10.891935 140256880439296 nn_components.py:329] mlp: final activation = None
I0123 12:31:10.893869 140256880439296 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.893963 140256880439296 nn_components.py:261] mlp: residual
I0123 12:31:10.896655 140256880439296 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.896735 140256880439296 transformer_base.py:443] tbase: final FFN
I0123 12:31:10.897172 140256880439296 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:10.899619 140256880439296 nn_components.py:329] mlp: final activation = None
I0123 12:31:10.901576 140256880439296 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.901677 140256880439296 nn_components.py:261] mlp: residual
I0123 12:31:10.901980 140256880439296 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.902072 140256880439296 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 12:31:10.902190 140256880439296 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:10.902231 140256880439296 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:10.902264 140256880439296 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:10.904173 140256880439296 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.906715 140256880439296 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:10.912525 140256880439296 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.912797 140256880439296 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:10.915577 140256880439296 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:31:10.919472 140256880439296 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:10.919531 140256880439296 attention.py:418] Single window, no scan.
I0123 12:31:10.919567 140256880439296 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:10.919599 140256880439296 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.919661 140256880439296 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.920233 140256880439296 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.920309 140256880439296 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.920675 140256880439296 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.921467 140256880439296 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.924071 140256880439296 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.924696 140256880439296 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.924773 140256880439296 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:10.924809 140256880439296 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:10.924870 140256880439296 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.925000 140256880439296 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:10.925322 140256880439296 nn_components.py:325] mlp: activation = None
I0123 12:31:10.925366 140256880439296 nn_components.py:329] mlp: final activation = None
I0123 12:31:10.927412 140256880439296 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.927520 140256880439296 nn_components.py:261] mlp: residual
I0123 12:31:10.930387 140256880439296 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.930473 140256880439296 transformer_base.py:443] tbase: final FFN
I0123 12:31:10.930926 140256880439296 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:10.933326 140256880439296 nn_components.py:329] mlp: final activation = None
I0123 12:31:10.935322 140256880439296 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.935431 140256880439296 nn_components.py:261] mlp: residual
I0123 12:31:10.935773 140256880439296 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.935856 140256880439296 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 12:31:10.935972 140256880439296 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:10.936012 140256880439296 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:10.936044 140256880439296 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:10.937966 140256880439296 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.940447 140256880439296 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:10.946209 140256880439296 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.946474 140256880439296 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:10.949201 140256880439296 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:31:10.953120 140256880439296 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:10.953176 140256880439296 attention.py:418] Single window, no scan.
I0123 12:31:10.953213 140256880439296 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:10.953245 140256880439296 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.953311 140256880439296 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.953884 140256880439296 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.953961 140256880439296 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.954344 140256880439296 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.955158 140256880439296 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.957726 140256880439296 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.958744 140256880439296 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.958826 140256880439296 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:10.958863 140256880439296 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:10.958928 140256880439296 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.959061 140256880439296 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:10.959406 140256880439296 nn_components.py:325] mlp: activation = None
I0123 12:31:10.959452 140256880439296 nn_components.py:329] mlp: final activation = None
I0123 12:31:10.961413 140256880439296 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.961507 140256880439296 nn_components.py:261] mlp: residual
I0123 12:31:10.964144 140256880439296 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.964226 140256880439296 transformer_base.py:443] tbase: final FFN
I0123 12:31:10.964717 140256880439296 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:10.967058 140256880439296 nn_components.py:329] mlp: final activation = None
I0123 12:31:10.968977 140256880439296 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.969072 140256880439296 nn_components.py:261] mlp: residual
I0123 12:31:10.969367 140256880439296 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:10.969657 140256880439296 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:31:10.969729 140256880439296 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:31:10.969795 140256880439296 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:31:10.969853 140256880439296 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:31:10.969909 140256880439296 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:31:10.969965 140256880439296 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:31:10.970021 140256880439296 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:31:10.970077 140256880439296 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:31:10.970132 140256880439296 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:31:10.970186 140256880439296 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:31:10.970241 140256880439296 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:31:10.970296 140256880439296 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:31:10.970335 140256880439296 decoder_stack.py:344] dstack: Final layernorm.
I0123 12:31:10.973951 140256880439296 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:11.022186 140256880439296 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.022272 140256880439296 decoder_stack.py:333] dstack: autoregressive generator.
I0123 12:31:11.022327 140256880439296 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 12:31:11.022433 140256880439296 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:11.022473 140256880439296 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:11.022504 140256880439296 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:11.022573 140256880439296 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.025070 140256880439296 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:11.030708 140256880439296 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.030971 140256880439296 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:11.033663 140256880439296 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:31:11.050263 140256880439296 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:11.050320 140256880439296 attention.py:418] Single window, no scan.
I0123 12:31:11.050356 140256880439296 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:11.050389 140256880439296 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.050452 140256880439296 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.051600 140256880439296 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.051682 140256880439296 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.052402 140256880439296 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.054445 140256880439296 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.059278 140256880439296 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.060607 140256880439296 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.060693 140256880439296 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:11.060731 140256880439296 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:11.060793 140256880439296 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.060927 140256880439296 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:11.061039 140256880439296 nn_components.py:325] mlp: activation = None
I0123 12:31:11.061079 140256880439296 nn_components.py:329] mlp: final activation = None
I0123 12:31:11.063042 140256880439296 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.063140 140256880439296 nn_components.py:261] mlp: residual
I0123 12:31:11.065629 140256880439296 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.065717 140256880439296 transformer_base.py:443] tbase: final FFN
I0123 12:31:11.065829 140256880439296 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:11.068119 140256880439296 nn_components.py:329] mlp: final activation = None
I0123 12:31:11.070130 140256880439296 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.070229 140256880439296 nn_components.py:261] mlp: residual
I0123 12:31:11.070525 140256880439296 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.070605 140256880439296 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 12:31:11.070715 140256880439296 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:11.070755 140256880439296 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:11.070787 140256880439296 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:11.070854 140256880439296 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.073157 140256880439296 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:11.078767 140256880439296 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.079032 140256880439296 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:11.081781 140256880439296 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:31:11.095036 140256880439296 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:11.095092 140256880439296 attention.py:418] Single window, no scan.
I0123 12:31:11.095129 140256880439296 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:11.095161 140256880439296 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.095222 140256880439296 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.095791 140256880439296 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.095868 140256880439296 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.096230 140256880439296 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.096937 140256880439296 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.099489 140256880439296 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.100111 140256880439296 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.100188 140256880439296 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:11.100230 140256880439296 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:11.100292 140256880439296 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.100426 140256880439296 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:11.100539 140256880439296 nn_components.py:325] mlp: activation = None
I0123 12:31:11.100579 140256880439296 nn_components.py:329] mlp: final activation = None
I0123 12:31:11.102737 140256880439296 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.102834 140256880439296 nn_components.py:261] mlp: residual
I0123 12:31:11.105295 140256880439296 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.105374 140256880439296 transformer_base.py:443] tbase: final FFN
I0123 12:31:11.105484 140256880439296 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:11.107758 140256880439296 nn_components.py:329] mlp: final activation = None
I0123 12:31:11.109716 140256880439296 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.109815 140256880439296 nn_components.py:261] mlp: residual
I0123 12:31:11.110110 140256880439296 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.110191 140256880439296 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 12:31:11.110300 140256880439296 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:11.110339 140256880439296 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:11.110371 140256880439296 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:11.110435 140256880439296 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.112734 140256880439296 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:11.118302 140256880439296 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.118567 140256880439296 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:11.121288 140256880439296 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:31:11.134209 140256880439296 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:11.134265 140256880439296 attention.py:418] Single window, no scan.
I0123 12:31:11.134302 140256880439296 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:11.134334 140256880439296 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.134398 140256880439296 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.134961 140256880439296 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.135037 140256880439296 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.135401 140256880439296 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.136105 140256880439296 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.142126 140256880439296 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.142869 140256880439296 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.142952 140256880439296 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:11.142989 140256880439296 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:11.143067 140256880439296 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.143208 140256880439296 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:11.143335 140256880439296 nn_components.py:325] mlp: activation = None
I0123 12:31:11.143375 140256880439296 nn_components.py:329] mlp: final activation = None
I0123 12:31:11.145518 140256880439296 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.145615 140256880439296 nn_components.py:261] mlp: residual
I0123 12:31:11.148210 140256880439296 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.148290 140256880439296 transformer_base.py:443] tbase: final FFN
I0123 12:31:11.148404 140256880439296 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:11.150742 140256880439296 nn_components.py:329] mlp: final activation = None
I0123 12:31:11.152739 140256880439296 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.152835 140256880439296 nn_components.py:261] mlp: residual
I0123 12:31:11.153128 140256880439296 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.153212 140256880439296 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 12:31:11.153328 140256880439296 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:11.153372 140256880439296 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:11.153405 140256880439296 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:11.153475 140256880439296 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.155807 140256880439296 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:11.161409 140256880439296 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.161682 140256880439296 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:11.164456 140256880439296 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:31:11.177441 140256880439296 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:11.177499 140256880439296 attention.py:418] Single window, no scan.
I0123 12:31:11.177537 140256880439296 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:11.177570 140256880439296 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.177636 140256880439296 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.178225 140256880439296 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.178301 140256880439296 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.178666 140256880439296 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.179376 140256880439296 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.181929 140256880439296 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.182550 140256880439296 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.182626 140256880439296 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:11.182662 140256880439296 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:11.182721 140256880439296 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.182861 140256880439296 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:11.182972 140256880439296 nn_components.py:325] mlp: activation = None
I0123 12:31:11.183012 140256880439296 nn_components.py:329] mlp: final activation = None
I0123 12:31:11.185010 140256880439296 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.185106 140256880439296 nn_components.py:261] mlp: residual
I0123 12:31:11.187593 140256880439296 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.187673 140256880439296 transformer_base.py:443] tbase: final FFN
I0123 12:31:11.187782 140256880439296 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:11.190056 140256880439296 nn_components.py:329] mlp: final activation = None
I0123 12:31:11.191968 140256880439296 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.192063 140256880439296 nn_components.py:261] mlp: residual
I0123 12:31:11.192356 140256880439296 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.192436 140256880439296 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 12:31:11.192547 140256880439296 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:11.192585 140256880439296 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:11.192618 140256880439296 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:11.192683 140256880439296 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.195332 140256880439296 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:11.200927 140256880439296 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.201195 140256880439296 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:11.203865 140256880439296 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:31:11.216715 140256880439296 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:11.216770 140256880439296 attention.py:418] Single window, no scan.
I0123 12:31:11.216807 140256880439296 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:11.216840 140256880439296 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.216902 140256880439296 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.217464 140256880439296 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.217545 140256880439296 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.217921 140256880439296 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.218631 140256880439296 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.221235 140256880439296 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.221879 140256880439296 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.221958 140256880439296 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:11.221994 140256880439296 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:11.222055 140256880439296 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.222190 140256880439296 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:11.222302 140256880439296 nn_components.py:325] mlp: activation = None
I0123 12:31:11.222342 140256880439296 nn_components.py:329] mlp: final activation = None
I0123 12:31:11.224285 140256880439296 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.224380 140256880439296 nn_components.py:261] mlp: residual
I0123 12:31:11.226866 140256880439296 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.226946 140256880439296 transformer_base.py:443] tbase: final FFN
I0123 12:31:11.227055 140256880439296 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:11.229401 140256880439296 nn_components.py:329] mlp: final activation = None
I0123 12:31:11.231322 140256880439296 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.231419 140256880439296 nn_components.py:261] mlp: residual
I0123 12:31:11.231714 140256880439296 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.231796 140256880439296 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 12:31:11.231906 140256880439296 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:11.231946 140256880439296 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:11.231978 140256880439296 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:11.232044 140256880439296 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.234366 140256880439296 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:11.239957 140256880439296 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.240219 140256880439296 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:11.242972 140256880439296 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:31:11.255807 140256880439296 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:11.255864 140256880439296 attention.py:418] Single window, no scan.
I0123 12:31:11.255901 140256880439296 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:11.255933 140256880439296 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.255996 140256880439296 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.256563 140256880439296 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.256640 140256880439296 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.257009 140256880439296 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.257730 140256880439296 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.260262 140256880439296 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.260879 140256880439296 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.260956 140256880439296 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:11.260992 140256880439296 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:11.261052 140256880439296 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.261186 140256880439296 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:11.261301 140256880439296 nn_components.py:325] mlp: activation = None
I0123 12:31:11.261340 140256880439296 nn_components.py:329] mlp: final activation = None
I0123 12:31:11.263326 140256880439296 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.263421 140256880439296 nn_components.py:261] mlp: residual
I0123 12:31:11.265879 140256880439296 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.265959 140256880439296 transformer_base.py:443] tbase: final FFN
I0123 12:31:11.266068 140256880439296 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:11.268323 140256880439296 nn_components.py:329] mlp: final activation = None
I0123 12:31:11.270237 140256880439296 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.270334 140256880439296 nn_components.py:261] mlp: residual
I0123 12:31:11.270627 140256880439296 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.270708 140256880439296 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 12:31:11.270819 140256880439296 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:11.270859 140256880439296 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:11.270892 140256880439296 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:11.270960 140256880439296 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.273251 140256880439296 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:11.278925 140256880439296 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.279188 140256880439296 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:11.281877 140256880439296 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:31:11.294798 140256880439296 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:11.294855 140256880439296 attention.py:418] Single window, no scan.
I0123 12:31:11.294893 140256880439296 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:11.294926 140256880439296 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.294990 140256880439296 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.295561 140256880439296 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.295639 140256880439296 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.296009 140256880439296 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.296710 140256880439296 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.299263 140256880439296 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.300251 140256880439296 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.300330 140256880439296 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:11.300366 140256880439296 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:11.300426 140256880439296 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.300556 140256880439296 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:11.300670 140256880439296 nn_components.py:325] mlp: activation = None
I0123 12:31:11.300715 140256880439296 nn_components.py:329] mlp: final activation = None
I0123 12:31:11.302685 140256880439296 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.302783 140256880439296 nn_components.py:261] mlp: residual
I0123 12:31:11.305252 140256880439296 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.305334 140256880439296 transformer_base.py:443] tbase: final FFN
I0123 12:31:11.305444 140256880439296 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:11.307729 140256880439296 nn_components.py:329] mlp: final activation = None
I0123 12:31:11.309726 140256880439296 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.309824 140256880439296 nn_components.py:261] mlp: residual
I0123 12:31:11.310120 140256880439296 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.310201 140256880439296 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 12:31:11.310312 140256880439296 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:11.310351 140256880439296 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:11.310383 140256880439296 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:11.310448 140256880439296 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.312757 140256880439296 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:11.318383 140256880439296 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.318657 140256880439296 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:11.321387 140256880439296 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:31:11.334267 140256880439296 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:11.334322 140256880439296 attention.py:418] Single window, no scan.
I0123 12:31:11.334360 140256880439296 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:11.334392 140256880439296 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.334455 140256880439296 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.335062 140256880439296 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.335139 140256880439296 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.335507 140256880439296 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.336201 140256880439296 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.338742 140256880439296 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.339372 140256880439296 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.339451 140256880439296 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:11.339487 140256880439296 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:11.339548 140256880439296 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.339678 140256880439296 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:11.339787 140256880439296 nn_components.py:325] mlp: activation = None
I0123 12:31:11.339832 140256880439296 nn_components.py:329] mlp: final activation = None
I0123 12:31:11.341759 140256880439296 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.341856 140256880439296 nn_components.py:261] mlp: residual
I0123 12:31:11.344369 140256880439296 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.344450 140256880439296 transformer_base.py:443] tbase: final FFN
I0123 12:31:11.344560 140256880439296 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:11.346846 140256880439296 nn_components.py:329] mlp: final activation = None
I0123 12:31:11.348749 140256880439296 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.348844 140256880439296 nn_components.py:261] mlp: residual
I0123 12:31:11.349136 140256880439296 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.349217 140256880439296 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 12:31:11.349326 140256880439296 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:11.349366 140256880439296 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:11.349398 140256880439296 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:11.349463 140256880439296 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.351752 140256880439296 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:11.357387 140256880439296 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.357663 140256880439296 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:11.360336 140256880439296 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:31:11.373205 140256880439296 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:11.373262 140256880439296 attention.py:418] Single window, no scan.
I0123 12:31:11.373299 140256880439296 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:11.373330 140256880439296 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.373394 140256880439296 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.373971 140256880439296 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.374048 140256880439296 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.374418 140256880439296 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.375129 140256880439296 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.377708 140256880439296 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.378391 140256880439296 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.378471 140256880439296 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:11.378508 140256880439296 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:11.378574 140256880439296 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.378706 140256880439296 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:11.378818 140256880439296 nn_components.py:325] mlp: activation = None
I0123 12:31:11.378858 140256880439296 nn_components.py:329] mlp: final activation = None
I0123 12:31:11.380794 140256880439296 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.380890 140256880439296 nn_components.py:261] mlp: residual
I0123 12:31:11.383372 140256880439296 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.383453 140256880439296 transformer_base.py:443] tbase: final FFN
I0123 12:31:11.383564 140256880439296 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:11.385861 140256880439296 nn_components.py:329] mlp: final activation = None
I0123 12:31:11.387853 140256880439296 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.387950 140256880439296 nn_components.py:261] mlp: residual
I0123 12:31:11.388246 140256880439296 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.388328 140256880439296 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 12:31:11.388440 140256880439296 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:11.388479 140256880439296 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:11.388511 140256880439296 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:11.388576 140256880439296 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.390889 140256880439296 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:11.396498 140256880439296 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.396763 140256880439296 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:11.399542 140256880439296 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:31:11.412735 140256880439296 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:11.412791 140256880439296 attention.py:418] Single window, no scan.
I0123 12:31:11.412828 140256880439296 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:11.412859 140256880439296 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.412922 140256880439296 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.413538 140256880439296 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.413614 140256880439296 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.413984 140256880439296 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.414690 140256880439296 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.417215 140256880439296 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.417855 140256880439296 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.417932 140256880439296 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:11.417969 140256880439296 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:11.418028 140256880439296 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.418154 140256880439296 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:11.418264 140256880439296 nn_components.py:325] mlp: activation = None
I0123 12:31:11.418303 140256880439296 nn_components.py:329] mlp: final activation = None
I0123 12:31:11.420235 140256880439296 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.420335 140256880439296 nn_components.py:261] mlp: residual
I0123 12:31:11.422842 140256880439296 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.422924 140256880439296 transformer_base.py:443] tbase: final FFN
I0123 12:31:11.423034 140256880439296 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:11.425335 140256880439296 nn_components.py:329] mlp: final activation = None
I0123 12:31:11.427247 140256880439296 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.427345 140256880439296 nn_components.py:261] mlp: residual
I0123 12:31:11.427639 140256880439296 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.427721 140256880439296 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 12:31:11.427832 140256880439296 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:11.427871 140256880439296 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:11.427903 140256880439296 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:11.427968 140256880439296 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.430258 140256880439296 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:11.435897 140256880439296 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.436160 140256880439296 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:11.438864 140256880439296 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:31:11.451732 140256880439296 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:11.451790 140256880439296 attention.py:418] Single window, no scan.
I0123 12:31:11.451827 140256880439296 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:11.451858 140256880439296 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.451921 140256880439296 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.452489 140256880439296 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.452567 140256880439296 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.452932 140256880439296 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.453630 140256880439296 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.456153 140256880439296 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.456826 140256880439296 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.456904 140256880439296 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:11.456939 140256880439296 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:11.456998 140256880439296 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.457130 140256880439296 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:11.457242 140256880439296 nn_components.py:325] mlp: activation = None
I0123 12:31:11.457281 140256880439296 nn_components.py:329] mlp: final activation = None
I0123 12:31:11.459243 140256880439296 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.459345 140256880439296 nn_components.py:261] mlp: residual
I0123 12:31:11.461837 140256880439296 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.461923 140256880439296 transformer_base.py:443] tbase: final FFN
I0123 12:31:11.462033 140256880439296 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:11.464296 140256880439296 nn_components.py:329] mlp: final activation = None
I0123 12:31:11.466263 140256880439296 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.466360 140256880439296 nn_components.py:261] mlp: residual
I0123 12:31:11.466654 140256880439296 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.466735 140256880439296 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 12:31:11.466846 140256880439296 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:11.466885 140256880439296 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:11.466917 140256880439296 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:11.466982 140256880439296 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.469269 140256880439296 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:11.474928 140256880439296 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.475190 140256880439296 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:11.477952 140256880439296 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:31:11.490823 140256880439296 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:11.490879 140256880439296 attention.py:418] Single window, no scan.
I0123 12:31:11.490916 140256880439296 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:11.490948 140256880439296 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.491012 140256880439296 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.491579 140256880439296 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.491655 140256880439296 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.492013 140256880439296 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.492761 140256880439296 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.495344 140256880439296 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.495970 140256880439296 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.496046 140256880439296 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:11.496082 140256880439296 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:11.496142 140256880439296 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.496274 140256880439296 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:11.496386 140256880439296 nn_components.py:325] mlp: activation = None
I0123 12:31:11.496426 140256880439296 nn_components.py:329] mlp: final activation = None
I0123 12:31:11.498348 140256880439296 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.498444 140256880439296 nn_components.py:261] mlp: residual
I0123 12:31:11.500926 140256880439296 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.501005 140256880439296 transformer_base.py:443] tbase: final FFN
I0123 12:31:11.501115 140256880439296 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:11.503452 140256880439296 nn_components.py:329] mlp: final activation = None
I0123 12:31:11.505372 140256880439296 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.505468 140256880439296 nn_components.py:261] mlp: residual
I0123 12:31:11.505770 140256880439296 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.505861 140256880439296 decoder_stack.py:344] dstack: Final layernorm.
I0123 12:31:11.508774 140256880439296 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:31:11.564584 140256880439296 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.564671 140256880439296 decoder_stack.py:333] dstack: autoregressive generator.
I0123 12:31:11.564726 140256880439296 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 12:31:11.564835 140256880439296 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:11.564875 140256880439296 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:11.564907 140256880439296 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:11.564972 140256880439296 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.567670 140256880439296 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:11.573253 140256880439296 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.573515 140256880439296 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:11.576167 140256880439296 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:31:11.588821 140256880439296 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:11.588876 140256880439296 attention.py:418] Single window, no scan.
I0123 12:31:11.588914 140256880439296 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:11.588947 140256880439296 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.589009 140256880439296 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.589565 140256880439296 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.589650 140256880439296 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.590020 140256880439296 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.590706 140256880439296 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.593247 140256880439296 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.593870 140256880439296 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.593949 140256880439296 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:11.593985 140256880439296 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:11.594045 140256880439296 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.594175 140256880439296 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:11.594293 140256880439296 nn_components.py:325] mlp: activation = None
I0123 12:31:11.594333 140256880439296 nn_components.py:329] mlp: final activation = None
I0123 12:31:11.596215 140256880439296 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.596310 140256880439296 nn_components.py:261] mlp: residual
I0123 12:31:11.598739 140256880439296 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.598821 140256880439296 transformer_base.py:443] tbase: final FFN
I0123 12:31:11.598932 140256880439296 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:11.601209 140256880439296 nn_components.py:329] mlp: final activation = None
I0123 12:31:11.603137 140256880439296 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.603234 140256880439296 nn_components.py:261] mlp: residual
I0123 12:31:11.603526 140256880439296 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.603608 140256880439296 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 12:31:11.603717 140256880439296 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:11.603756 140256880439296 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:11.603788 140256880439296 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:11.603853 140256880439296 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.606115 140256880439296 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:11.611576 140256880439296 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.611837 140256880439296 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:11.614548 140256880439296 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:31:11.626953 140256880439296 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:11.627008 140256880439296 attention.py:418] Single window, no scan.
I0123 12:31:11.627045 140256880439296 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:11.627076 140256880439296 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.627138 140256880439296 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.627692 140256880439296 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.627770 140256880439296 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.628132 140256880439296 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.628828 140256880439296 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.631385 140256880439296 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.632000 140256880439296 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.632077 140256880439296 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:11.632113 140256880439296 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:11.632174 140256880439296 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.632299 140256880439296 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:11.632409 140256880439296 nn_components.py:325] mlp: activation = None
I0123 12:31:11.632453 140256880439296 nn_components.py:329] mlp: final activation = None
I0123 12:31:11.634343 140256880439296 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.634437 140256880439296 nn_components.py:261] mlp: residual
I0123 12:31:11.636858 140256880439296 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.636939 140256880439296 transformer_base.py:443] tbase: final FFN
I0123 12:31:11.637050 140256880439296 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:11.639341 140256880439296 nn_components.py:329] mlp: final activation = None
I0123 12:31:11.641215 140256880439296 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.641311 140256880439296 nn_components.py:261] mlp: residual
I0123 12:31:11.641602 140256880439296 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.641689 140256880439296 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 12:31:11.641798 140256880439296 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:11.641838 140256880439296 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:11.641870 140256880439296 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:11.641936 140256880439296 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.644188 140256880439296 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:11.649619 140256880439296 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.649886 140256880439296 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:11.652569 140256880439296 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:31:11.664927 140256880439296 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:11.664984 140256880439296 attention.py:418] Single window, no scan.
I0123 12:31:11.665021 140256880439296 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:11.665052 140256880439296 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.665115 140256880439296 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.665673 140256880439296 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.665749 140256880439296 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.666104 140256880439296 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.666782 140256880439296 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.669304 140256880439296 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.669923 140256880439296 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.670000 140256880439296 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:11.670036 140256880439296 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:11.670096 140256880439296 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.670221 140256880439296 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:11.670330 140256880439296 nn_components.py:325] mlp: activation = None
I0123 12:31:11.670370 140256880439296 nn_components.py:329] mlp: final activation = None
I0123 12:31:11.672240 140256880439296 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.672334 140256880439296 nn_components.py:261] mlp: residual
I0123 12:31:11.674767 140256880439296 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.674848 140256880439296 transformer_base.py:443] tbase: final FFN
I0123 12:31:11.674958 140256880439296 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:11.677681 140256880439296 nn_components.py:329] mlp: final activation = None
I0123 12:31:11.679540 140256880439296 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.679636 140256880439296 nn_components.py:261] mlp: residual
I0123 12:31:11.679927 140256880439296 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.680007 140256880439296 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 12:31:11.680115 140256880439296 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:11.680155 140256880439296 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:11.680185 140256880439296 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:11.680249 140256880439296 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.682505 140256880439296 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:11.688125 140256880439296 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.688387 140256880439296 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:11.691090 140256880439296 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:31:11.703503 140256880439296 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:11.703559 140256880439296 attention.py:418] Single window, no scan.
I0123 12:31:11.703599 140256880439296 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:11.703640 140256880439296 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.703705 140256880439296 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.704264 140256880439296 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.704338 140256880439296 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.704693 140256880439296 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.705382 140256880439296 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.707946 140256880439296 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.708564 140256880439296 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.708640 140256880439296 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:11.708675 140256880439296 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:11.708734 140256880439296 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.708859 140256880439296 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:11.708966 140256880439296 nn_components.py:325] mlp: activation = None
I0123 12:31:11.709006 140256880439296 nn_components.py:329] mlp: final activation = None
I0123 12:31:11.710904 140256880439296 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.710998 140256880439296 nn_components.py:261] mlp: residual
I0123 12:31:11.713408 140256880439296 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.713486 140256880439296 transformer_base.py:443] tbase: final FFN
I0123 12:31:11.713594 140256880439296 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:11.715898 140256880439296 nn_components.py:329] mlp: final activation = None
I0123 12:31:11.717789 140256880439296 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.717884 140256880439296 nn_components.py:261] mlp: residual
I0123 12:31:11.718173 140256880439296 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.718253 140256880439296 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 12:31:11.718361 140256880439296 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:11.718399 140256880439296 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:11.718430 140256880439296 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:11.718492 140256880439296 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.720751 140256880439296 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:11.726252 140256880439296 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.726513 140256880439296 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:11.729233 140256880439296 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:31:11.741853 140256880439296 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:11.741908 140256880439296 attention.py:418] Single window, no scan.
I0123 12:31:11.741944 140256880439296 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:11.741975 140256880439296 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.742037 140256880439296 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.742595 140256880439296 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.742670 140256880439296 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.743031 140256880439296 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.743728 140256880439296 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.746310 140256880439296 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.746941 140256880439296 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.747017 140256880439296 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:11.747052 140256880439296 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:11.747112 140256880439296 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.747238 140256880439296 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:11.747346 140256880439296 nn_components.py:325] mlp: activation = None
I0123 12:31:11.747384 140256880439296 nn_components.py:329] mlp: final activation = None
I0123 12:31:11.749289 140256880439296 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.749391 140256880439296 nn_components.py:261] mlp: residual
I0123 12:31:11.751855 140256880439296 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.751932 140256880439296 transformer_base.py:443] tbase: final FFN
I0123 12:31:11.752039 140256880439296 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:11.754351 140256880439296 nn_components.py:329] mlp: final activation = None
I0123 12:31:11.756244 140256880439296 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.756340 140256880439296 nn_components.py:261] mlp: residual
I0123 12:31:11.756630 140256880439296 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.756710 140256880439296 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 12:31:11.756818 140256880439296 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:11.756857 140256880439296 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:11.756887 140256880439296 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:11.756951 140256880439296 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.759240 140256880439296 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:11.764766 140256880439296 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.765027 140256880439296 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:11.767767 140256880439296 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:31:11.780586 140256880439296 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:11.780639 140256880439296 attention.py:418] Single window, no scan.
I0123 12:31:11.780675 140256880439296 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:11.780705 140256880439296 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.780766 140256880439296 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.781320 140256880439296 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.781395 140256880439296 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.781767 140256880439296 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.782474 140256880439296 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.785079 140256880439296 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.785707 140256880439296 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.785786 140256880439296 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:11.785820 140256880439296 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:11.785878 140256880439296 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.786004 140256880439296 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:11.786113 140256880439296 nn_components.py:325] mlp: activation = None
I0123 12:31:11.786152 140256880439296 nn_components.py:329] mlp: final activation = None
I0123 12:31:11.788050 140256880439296 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.788149 140256880439296 nn_components.py:261] mlp: residual
I0123 12:31:11.790604 140256880439296 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.790684 140256880439296 transformer_base.py:443] tbase: final FFN
I0123 12:31:11.790793 140256880439296 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:11.793516 140256880439296 nn_components.py:329] mlp: final activation = None
I0123 12:31:11.795430 140256880439296 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.795526 140256880439296 nn_components.py:261] mlp: residual
I0123 12:31:11.795819 140256880439296 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.795898 140256880439296 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 12:31:11.796005 140256880439296 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:11.796043 140256880439296 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:11.796074 140256880439296 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:11.796137 140256880439296 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.798406 140256880439296 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:11.803900 140256880439296 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.804163 140256880439296 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:11.806902 140256880439296 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:31:11.819519 140256880439296 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:11.819574 140256880439296 attention.py:418] Single window, no scan.
I0123 12:31:11.819609 140256880439296 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:11.819640 140256880439296 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.819701 140256880439296 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.820261 140256880439296 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.820335 140256880439296 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.820698 140256880439296 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.821392 140256880439296 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.823972 140256880439296 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.824592 140256880439296 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.824668 140256880439296 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:11.824702 140256880439296 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:11.824760 140256880439296 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.824885 140256880439296 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:11.824993 140256880439296 nn_components.py:325] mlp: activation = None
I0123 12:31:11.825031 140256880439296 nn_components.py:329] mlp: final activation = None
I0123 12:31:11.826975 140256880439296 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.827069 140256880439296 nn_components.py:261] mlp: residual
I0123 12:31:11.829504 140256880439296 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.829583 140256880439296 transformer_base.py:443] tbase: final FFN
I0123 12:31:11.829699 140256880439296 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:11.832015 140256880439296 nn_components.py:329] mlp: final activation = None
I0123 12:31:11.833895 140256880439296 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.833990 140256880439296 nn_components.py:261] mlp: residual
I0123 12:31:11.834280 140256880439296 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.834358 140256880439296 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 12:31:11.834465 140256880439296 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:11.834503 140256880439296 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:11.834533 140256880439296 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:11.834595 140256880439296 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.836853 140256880439296 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:11.842386 140256880439296 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.842644 140256880439296 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:11.845359 140256880439296 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:31:11.857976 140256880439296 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:11.858031 140256880439296 attention.py:418] Single window, no scan.
I0123 12:31:11.858065 140256880439296 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:11.858096 140256880439296 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.858158 140256880439296 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.858718 140256880439296 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.858793 140256880439296 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.859157 140256880439296 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.859854 140256880439296 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.862443 140256880439296 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.863064 140256880439296 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.863139 140256880439296 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:11.863172 140256880439296 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:11.863231 140256880439296 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.863356 140256880439296 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:11.863464 140256880439296 nn_components.py:325] mlp: activation = None
I0123 12:31:11.863502 140256880439296 nn_components.py:329] mlp: final activation = None
I0123 12:31:11.865389 140256880439296 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.865482 140256880439296 nn_components.py:261] mlp: residual
I0123 12:31:11.867936 140256880439296 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.868024 140256880439296 transformer_base.py:443] tbase: final FFN
I0123 12:31:11.868137 140256880439296 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:11.870464 140256880439296 nn_components.py:329] mlp: final activation = None
I0123 12:31:11.872343 140256880439296 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.872436 140256880439296 nn_components.py:261] mlp: residual
I0123 12:31:11.872725 140256880439296 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.872803 140256880439296 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 12:31:11.872911 140256880439296 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:11.872949 140256880439296 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:11.872979 140256880439296 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:11.873043 140256880439296 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.875312 140256880439296 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:11.880954 140256880439296 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.881218 140256880439296 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:11.883971 140256880439296 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:31:11.896655 140256880439296 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:11.896709 140256880439296 attention.py:418] Single window, no scan.
I0123 12:31:11.896744 140256880439296 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:11.896775 140256880439296 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.896835 140256880439296 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.897397 140256880439296 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.897473 140256880439296 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.897846 140256880439296 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.898543 140256880439296 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.901153 140256880439296 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.901776 140256880439296 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.901854 140256880439296 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:11.901887 140256880439296 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:11.901946 140256880439296 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.902170 140256880439296 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:11.902279 140256880439296 nn_components.py:325] mlp: activation = None
I0123 12:31:11.902317 140256880439296 nn_components.py:329] mlp: final activation = None
I0123 12:31:11.904235 140256880439296 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.904328 140256880439296 nn_components.py:261] mlp: residual
I0123 12:31:11.906791 140256880439296 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.906875 140256880439296 transformer_base.py:443] tbase: final FFN
I0123 12:31:11.906989 140256880439296 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:11.909689 140256880439296 nn_components.py:329] mlp: final activation = None
I0123 12:31:11.911585 140256880439296 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.911681 140256880439296 nn_components.py:261] mlp: residual
I0123 12:31:11.911976 140256880439296 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.912057 140256880439296 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 12:31:11.912166 140256880439296 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:11.912203 140256880439296 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:11.912234 140256880439296 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:11.912296 140256880439296 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.914579 140256880439296 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:11.920176 140256880439296 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.920437 140256880439296 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:11.923173 140256880439296 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:31:11.935827 140256880439296 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:11.935881 140256880439296 attention.py:418] Single window, no scan.
I0123 12:31:11.935917 140256880439296 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:11.935948 140256880439296 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.936017 140256880439296 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.936582 140256880439296 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.936657 140256880439296 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.937021 140256880439296 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.937721 140256880439296 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.940313 140256880439296 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.940943 140256880439296 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.941019 140256880439296 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:11.941054 140256880439296 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:11.941112 140256880439296 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.941238 140256880439296 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:11.941346 140256880439296 nn_components.py:325] mlp: activation = None
I0123 12:31:11.941384 140256880439296 nn_components.py:329] mlp: final activation = None
I0123 12:31:11.943748 140256880439296 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.943844 140256880439296 nn_components.py:261] mlp: residual
I0123 12:31:11.946295 140256880439296 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.946375 140256880439296 transformer_base.py:443] tbase: final FFN
I0123 12:31:11.946491 140256880439296 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:11.948761 140256880439296 nn_components.py:329] mlp: final activation = None
I0123 12:31:11.950642 140256880439296 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.950737 140256880439296 nn_components.py:261] mlp: residual
I0123 12:31:11.951025 140256880439296 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.951105 140256880439296 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 12:31:11.951213 140256880439296 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:11.951251 140256880439296 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:11.951282 140256880439296 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:11.951345 140256880439296 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.953588 140256880439296 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:11.959114 140256880439296 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.959375 140256880439296 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:11.962096 140256880439296 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:31:11.974740 140256880439296 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:11.974795 140256880439296 attention.py:418] Single window, no scan.
I0123 12:31:11.974830 140256880439296 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:11.974861 140256880439296 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.974921 140256880439296 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.975487 140256880439296 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.975563 140256880439296 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.975923 140256880439296 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.976621 140256880439296 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.979200 140256880439296 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.979837 140256880439296 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.979913 140256880439296 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:11.979947 140256880439296 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:11.980005 140256880439296 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.980132 140256880439296 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:11.980239 140256880439296 nn_components.py:325] mlp: activation = None
I0123 12:31:11.980277 140256880439296 nn_components.py:329] mlp: final activation = None
I0123 12:31:11.982185 140256880439296 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.982278 140256880439296 nn_components.py:261] mlp: residual
I0123 12:31:11.984710 140256880439296 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.984788 140256880439296 transformer_base.py:443] tbase: final FFN
I0123 12:31:11.984896 140256880439296 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:11.987211 140256880439296 nn_components.py:329] mlp: final activation = None
I0123 12:31:11.989099 140256880439296 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.989193 140256880439296 nn_components.py:261] mlp: residual
I0123 12:31:11.989482 140256880439296 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.989561 140256880439296 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 12:31:11.989712 140256880439296 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:11.989753 140256880439296 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:11.989785 140256880439296 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:11.989849 140256880439296 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.992126 140256880439296 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:11.997638 140256880439296 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:11.997903 140256880439296 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:12.000620 140256880439296 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:31:12.013467 140256880439296 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:12.013521 140256880439296 attention.py:418] Single window, no scan.
I0123 12:31:12.013557 140256880439296 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:12.013587 140256880439296 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:12.013654 140256880439296 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:12.014214 140256880439296 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:12.014289 140256880439296 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:12.014652 140256880439296 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:12.015361 140256880439296 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:12.017926 140256880439296 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:12.018546 140256880439296 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:12.018620 140256880439296 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:12.018654 140256880439296 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:12.018711 140256880439296 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:12.018839 140256880439296 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:12.018949 140256880439296 nn_components.py:325] mlp: activation = None
I0123 12:31:12.018989 140256880439296 nn_components.py:329] mlp: final activation = None
I0123 12:31:12.020949 140256880439296 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:12.021041 140256880439296 nn_components.py:261] mlp: residual
I0123 12:31:12.023488 140256880439296 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:12.023569 140256880439296 transformer_base.py:443] tbase: final FFN
I0123 12:31:12.023676 140256880439296 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:12.026354 140256880439296 nn_components.py:329] mlp: final activation = None
I0123 12:31:12.028238 140256880439296 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:12.028332 140256880439296 nn_components.py:261] mlp: residual
I0123 12:31:12.028622 140256880439296 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:12.028706 140256880439296 decoder_stack.py:344] dstack: Final layernorm.
I0123 12:31:12.031571 140256880439296 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:31:16.439976 140256880439296 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
I0123 12:31:16.963413 140256880439296 training_loop.py:409] No working directory specified.
I0123 12:31:16.963538 140256880439296 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0123 12:31:16.964314 140256880439296 checkpoints.py:1062] Restoring legacy Flax checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0123 12:31:20.156089 140256880439296 training_loop.py:447] Only restoring trainable parameters.
I0123 12:31:20.156790 140256880439296 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0123 12:31:20.156849 140256880439296 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0123 12:31:20.156895 140256880439296 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:31:20.156938 140256880439296 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:31:20.156978 140256880439296 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:20.157017 140256880439296 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:31:20.157057 140256880439296 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:20.157097 140256880439296 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:20.157135 140256880439296 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:31:20.157172 140256880439296 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:31:20.157211 140256880439296 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:20.157249 140256880439296 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:31:20.157287 140256880439296 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:31:20.157325 140256880439296 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:31:20.157361 140256880439296 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:20.157397 140256880439296 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:31:20.157433 140256880439296 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:20.157470 140256880439296 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:20.157506 140256880439296 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:31:20.157543 140256880439296 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:31:20.157593 140256880439296 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:20.157633 140256880439296 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:31:20.157688 140256880439296 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:31:20.157726 140256880439296 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:31:20.157764 140256880439296 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:20.157801 140256880439296 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:31:20.157837 140256880439296 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:20.157874 140256880439296 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:20.157911 140256880439296 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:31:20.157948 140256880439296 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:31:20.157984 140256880439296 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:20.158021 140256880439296 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:31:20.158057 140256880439296 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:31:20.158093 140256880439296 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:31:20.158129 140256880439296 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:20.158165 140256880439296 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:31:20.158200 140256880439296 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:20.158236 140256880439296 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:20.158271 140256880439296 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:31:20.158308 140256880439296 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:31:20.158344 140256880439296 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:20.158380 140256880439296 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:31:20.158417 140256880439296 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:31:20.158453 140256880439296 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:31:20.158489 140256880439296 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:20.158525 140256880439296 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:31:20.158569 140256880439296 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:20.158608 140256880439296 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:20.158644 140256880439296 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:31:20.158680 140256880439296 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:31:20.158716 140256880439296 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:20.158752 140256880439296 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:31:20.158788 140256880439296 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:31:20.158824 140256880439296 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:31:20.158861 140256880439296 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:20.158897 140256880439296 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:31:20.158932 140256880439296 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:20.158968 140256880439296 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:20.159003 140256880439296 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:31:20.159038 140256880439296 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:31:20.159073 140256880439296 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:20.159109 140256880439296 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:31:20.159144 140256880439296 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:31:20.159179 140256880439296 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:31:20.159214 140256880439296 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:20.159250 140256880439296 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:31:20.159286 140256880439296 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:20.159322 140256880439296 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:20.159358 140256880439296 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:31:20.159394 140256880439296 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:31:20.159430 140256880439296 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:20.159466 140256880439296 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:31:20.159503 140256880439296 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:31:20.159544 140256880439296 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:31:20.159583 140256880439296 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:20.159619 140256880439296 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:31:20.159656 140256880439296 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:20.159692 140256880439296 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:20.159729 140256880439296 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:31:20.159765 140256880439296 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:31:20.159801 140256880439296 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:20.159836 140256880439296 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:31:20.159872 140256880439296 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:31:20.159908 140256880439296 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:31:20.159944 140256880439296 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:20.159980 140256880439296 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:31:20.160016 140256880439296 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:20.160052 140256880439296 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:20.160089 140256880439296 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:31:20.160125 140256880439296 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:31:20.160161 140256880439296 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:20.160197 140256880439296 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:31:20.160234 140256880439296 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:31:20.160270 140256880439296 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:31:20.160306 140256880439296 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:20.160343 140256880439296 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:31:20.160379 140256880439296 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:20.160415 140256880439296 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:20.160452 140256880439296 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:31:20.160488 140256880439296 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:31:20.160529 140256880439296 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:20.160567 140256880439296 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:31:20.160602 140256880439296 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:31:20.160638 140256880439296 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:31:20.160673 140256880439296 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:20.160708 140256880439296 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:31:20.160743 140256880439296 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:20.160779 140256880439296 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:20.160815 140256880439296 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:31:20.160850 140256880439296 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:31:20.160885 140256880439296 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:20.160920 140256880439296 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:31:20.160956 140256880439296 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:31:20.160991 140256880439296 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:31:20.161026 140256880439296 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:20.161061 140256880439296 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:31:20.161095 140256880439296 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:20.161131 140256880439296 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:20.161167 140256880439296 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:31:20.161202 140256880439296 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:31:20.161237 140256880439296 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:31:20.161273 140256880439296 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:31:20.161301 140256880439296 training_loop.py:725] Total parameters: 152072288
I0123 12:31:20.161510 140256880439296 training_loop.py:739] Total state size: 0
I0123 12:31:20.181484 140256880439296 training_loop.py:492] Training loop: creating task for mode beam_search
I0123 12:31:20.181755 140256880439296 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0123 12:31:20.182106 140256880439296 training_loop.py:652] Compiling mode beam_search with jit.
I0123 12:31:20.182419 140256880439296 training_loop.py:89] registering functions: dict_keys([])
I0123 12:31:20.198354 140256880439296 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e c b a; f = foot f b c a; g = on_line g c e, on_line g b f; h = midpoint h c b; i = midpoint i c a; j = on_line j a h, on_line j b i ? coll j d g
I0123 12:31:20.882872 140256880439296 ddar.py:60] Depth 1/1000 time = 0.661935567855835
I0123 12:31:22.818425 140256880439296 ddar.py:60] Depth 2/1000 time = 1.93538498878479
I0123 12:31:26.479401 140256880439296 ddar.py:60] Depth 3/1000 time = 3.660799264907837
I0123 12:31:30.481940 140256880439296 ddar.py:60] Depth 4/1000 time = 4.002355098724365
I0123 12:31:34.549574 140256880439296 ddar.py:60] Depth 5/1000 time = 4.067398548126221
I0123 12:31:38.693849 140256880439296 ddar.py:60] Depth 6/1000 time = 4.144016981124878
I0123 12:31:38.702723 140256880439296 alphageometry.py:191] 
==========================
 * From theorem premises:
A B C D E F G H I J : Points
DC = DB [00]
DB = DA [01]
A,B,E are collinear [02]
CE  AB [03]
BF  AC [04]
C,A,F are collinear [05]
B,F,G are collinear [06]
C,E,G are collinear [07]
C,B,H are collinear [08]
HC = HB [09]
C,A,I are collinear [10]
IC = IA [11]
A,J,H are collinear [12]
I,B,J are collinear [13]

 * Auxiliary Constructions:
: Points


 * Proof steps:
001. C,A,I are collinear [10] & IC = IA [11]   I is midpoint of CA [14]
002. C,B,H are collinear [08] & HC = HB [09]   H is midpoint of CB [15]
003. I is midpoint of CA [14] & H is midpoint of CB [15]   IH  AB [16]
004. HI  AB [16] & A,J,H are collinear [12] & I,B,J are collinear [13]   AJ:AB = JH:IH [17]
005. DC = DB [00] & DB = DA [01]   DA = DC [18]
006. DC = DB [00] & DB = DA [01]   D is the circumcenter of \Delta ABC [19]
007. DA = DC [18] & IC = IA [11]   AC  DI [20]
008. B,F,G are collinear [06] & AC  DI [20] & BF  AC [04] & AB  HI [16]   ABG = HID [21]
009. DC = DB [00] & HC = HB [09]   BC  DH [22]
010. BF  AC [04] & BC  DH [22]   (BF-DH) = ACB [23]
011. A,E,B are collinear [02] & C,A,F are collinear [05] & BF  AC [04] & CE  AB [03]   CEB = CFB [24]
012. CEB = CFB [24]   C,B,E,F are concyclic [25]
013. C,B,E,F are concyclic [25]   CBE = CFE [26]
014. C,B,E,F are concyclic [25]   CBF = CEF [27]
015. C,E,G are collinear [07] & A,E,B are collinear [02] & B,F,G are collinear [06] & C,A,F are collinear [05] & BF  AC [04] & CE  AB [03]   GEA = GFA [28]
016. GEA = GFA [28]   A,E,F,G are concyclic [29]
017. A,E,F,G are concyclic [29]   AEF = AGF [30]
018. A,E,F,G are concyclic [29]   AFE = AGE [31]
019. B,F,G are collinear [06] & (BF-DH) = ACB [23] & AC  DI [20] & BF  AC [04] & CBE = CFE [26] & A,B,E are collinear [02] & C,A,F are collinear [05] & AEF = AGF [30]   AGB = HDI [32]
020. ABG = HID [21] & AGB = HDI [32] (Similar Triangles)  AB:AG = IH:HD [33]
021. AJ:AB = JH:IH [17] & AB:AG = IH:HD [33]   JH:HD = AJ:AG [34]
022. D is the circumcenter of \Delta ABC [19] & I is midpoint of CA [14]   BAD = (BC-DI) [35]
023. DB = DA [01]   BAD = DBA [36]
024. D is the circumcenter of \Delta ABC [19] & H is midpoint of CB [15]   DBA = (DH-AC) [37]
025. AFE = AGE [31] & C,A,F are collinear [05] & C,E,G are collinear [07] & CBF = CEF [27] & AC  DI [20] & BF  AC [04] & BAD = (BC-DI) [35] & BAD = DBA [36] & DBA = (DH-AC) [37]   (HD-CA) = GAC [38]
026. (HD-CA) = GAC [38]   HD  AG [39]
027. A,J,H are collinear [12] & DH  AG [39]   JHD = JAG [40]
028. JH:HD = AJ:AG [34] & JHD = JAG [40] (Similar Triangles)  HJD = AJG [41]
029. HJD = AJG [41] & A,J,H are collinear [12]   DJ  GJ [42]
030. DJ  GJ [42]   G,J,D are collinear
==========================

