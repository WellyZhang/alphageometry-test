I0123 10:58:02.622112 139630718992384 inference_utils.py:69] Parsing gin configuration.
I0123 10:58:02.622214 139630718992384 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0123 10:58:02.622449 139630718992384 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0123 10:58:02.622484 139630718992384 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0123 10:58:02.622513 139630718992384 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0123 10:58:02.622542 139630718992384 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0123 10:58:02.622570 139630718992384 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0123 10:58:02.622597 139630718992384 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0123 10:58:02.622624 139630718992384 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0123 10:58:02.622650 139630718992384 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=32
I0123 10:58:02.622677 139630718992384 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0123 10:58:02.622704 139630718992384 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0123 10:58:02.622752 139630718992384 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0123 10:58:02.622890 139630718992384 resource_reader.py:55] Path not found: base_htrans.gin
I0123 10:58:02.623127 139630718992384 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0123 10:58:02.623238 139630718992384 resource_reader.py:55] Path not found: trainer_configuration.gin
I0123 10:58:02.629526 139630718992384 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0123 10:58:02.629665 139630718992384 resource_reader.py:55] Path not found: size/medium_150M.gin
I0123 10:58:02.630004 139630718992384 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0123 10:58:02.630110 139630718992384 resource_reader.py:55] Path not found: options/positions_t5.gin
I0123 10:58:02.630396 139630718992384 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0123 10:58:02.630498 139630718992384 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0123 10:58:02.630915 139630718992384 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0123 10:58:02.631016 139630718992384 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0123 10:58:02.634649 139630718992384 training_loop.py:334] ==== Training loop: initializing model ====
I0123 10:58:02.737238 139630718992384 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0123 10:58:02.737984 139630718992384 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0123 10:58:02.744896 139630718992384 training_loop.py:335] Process 0 of 1
I0123 10:58:02.744954 139630718992384 training_loop.py:336] Local device count = 1
I0123 10:58:02.744995 139630718992384 training_loop.py:337] Number of replicas = 1
I0123 10:58:02.745028 139630718992384 training_loop.py:339] Using random number seed 42
I0123 10:58:03.208242 139630718992384 training_loop.py:359] Initializing the model.
I0123 10:58:03.641737 139630718992384 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.642014 139630718992384 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 10:58:03.642122 139630718992384 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 10:58:03.642203 139630718992384 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 10:58:03.642282 139630718992384 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 10:58:03.642368 139630718992384 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 10:58:03.642442 139630718992384 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 10:58:03.642515 139630718992384 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 10:58:03.642587 139630718992384 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 10:58:03.642659 139630718992384 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 10:58:03.642731 139630718992384 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 10:58:03.642806 139630718992384 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 10:58:03.642880 139630718992384 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 10:58:03.642954 139630718992384 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 10:58:03.642997 139630718992384 attention.py:418] Single window, no scan.
I0123 10:58:03.643049 139630718992384 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 10:58:03.643164 139630718992384 transformer_layer.py:154] tlayer: recurrent = False
I0123 10:58:03.643218 139630718992384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 10:58:03.643250 139630718992384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 10:58:03.645243 139630718992384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.650549 139630718992384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 10:58:03.661188 139630718992384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.661480 139630718992384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 10:58:03.665860 139630718992384 transformer_layer.py:213] tlayer: windowed attention.
I0123 10:58:03.676447 139630718992384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 10:58:03.676509 139630718992384 attention.py:418] Single window, no scan.
I0123 10:58:03.676549 139630718992384 transformer_layer.py:389] tlayer: self-attention.
I0123 10:58:03.676584 139630718992384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.676646 139630718992384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.677837 139630718992384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.677919 139630718992384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.678652 139630718992384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.681119 139630718992384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.686876 139630718992384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.688602 139630718992384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.688686 139630718992384 transformer_layer.py:468] tlayer: End windows.
I0123 10:58:03.688724 139630718992384 transformer_layer.py:472] tlayer: final FFN.
I0123 10:58:03.688786 139630718992384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.688927 139630718992384 transformer_base.py:410] tbase: post-attention MLP.
I0123 10:58:03.689281 139630718992384 nn_components.py:325] mlp: activation = None
I0123 10:58:03.689330 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 10:58:03.691282 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.691388 139630718992384 nn_components.py:261] mlp: residual
I0123 10:58:03.694307 139630718992384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.694391 139630718992384 transformer_base.py:443] tbase: final FFN
I0123 10:58:03.694899 139630718992384 nn_components.py:320] mlp: hidden 4096, relu
I0123 10:58:03.705072 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 10:58:03.713979 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.714083 139630718992384 nn_components.py:261] mlp: residual
I0123 10:58:03.714401 139630718992384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.714485 139630718992384 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 10:58:03.714598 139630718992384 transformer_layer.py:154] tlayer: recurrent = False
I0123 10:58:03.714639 139630718992384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 10:58:03.714672 139630718992384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 10:58:03.716526 139630718992384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.719026 139630718992384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 10:58:03.724663 139630718992384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.724945 139630718992384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 10:58:03.727609 139630718992384 transformer_layer.py:213] tlayer: windowed attention.
I0123 10:58:03.731445 139630718992384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 10:58:03.731505 139630718992384 attention.py:418] Single window, no scan.
I0123 10:58:03.731543 139630718992384 transformer_layer.py:389] tlayer: self-attention.
I0123 10:58:03.731577 139630718992384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.731641 139630718992384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.732221 139630718992384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.732306 139630718992384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.732691 139630718992384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.733477 139630718992384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.736022 139630718992384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.736666 139630718992384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.736746 139630718992384 transformer_layer.py:468] tlayer: End windows.
I0123 10:58:03.736783 139630718992384 transformer_layer.py:472] tlayer: final FFN.
I0123 10:58:03.736846 139630718992384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.736982 139630718992384 transformer_base.py:410] tbase: post-attention MLP.
I0123 10:58:03.737330 139630718992384 nn_components.py:325] mlp: activation = None
I0123 10:58:03.737376 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 10:58:03.739357 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.739459 139630718992384 nn_components.py:261] mlp: residual
I0123 10:58:03.742017 139630718992384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.742103 139630718992384 transformer_base.py:443] tbase: final FFN
I0123 10:58:03.742540 139630718992384 nn_components.py:320] mlp: hidden 4096, relu
I0123 10:58:03.744876 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 10:58:03.746806 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.746909 139630718992384 nn_components.py:261] mlp: residual
I0123 10:58:03.747221 139630718992384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.747304 139630718992384 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 10:58:03.747418 139630718992384 transformer_layer.py:154] tlayer: recurrent = False
I0123 10:58:03.747459 139630718992384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 10:58:03.747492 139630718992384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 10:58:03.749396 139630718992384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.751782 139630718992384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 10:58:03.757767 139630718992384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.758044 139630718992384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 10:58:03.760726 139630718992384 transformer_layer.py:213] tlayer: windowed attention.
I0123 10:58:03.764566 139630718992384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 10:58:03.764626 139630718992384 attention.py:418] Single window, no scan.
I0123 10:58:03.764666 139630718992384 transformer_layer.py:389] tlayer: self-attention.
I0123 10:58:03.764699 139630718992384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.764763 139630718992384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.765337 139630718992384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.765415 139630718992384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.765806 139630718992384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.766590 139630718992384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.769115 139630718992384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.769815 139630718992384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.769897 139630718992384 transformer_layer.py:468] tlayer: End windows.
I0123 10:58:03.769935 139630718992384 transformer_layer.py:472] tlayer: final FFN.
I0123 10:58:03.769996 139630718992384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.770132 139630718992384 transformer_base.py:410] tbase: post-attention MLP.
I0123 10:58:03.770459 139630718992384 nn_components.py:325] mlp: activation = None
I0123 10:58:03.770504 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 10:58:03.772423 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.772520 139630718992384 nn_components.py:261] mlp: residual
I0123 10:58:03.775068 139630718992384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.775156 139630718992384 transformer_base.py:443] tbase: final FFN
I0123 10:58:03.775646 139630718992384 nn_components.py:320] mlp: hidden 4096, relu
I0123 10:58:03.777925 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 10:58:03.779842 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.779940 139630718992384 nn_components.py:261] mlp: residual
I0123 10:58:03.780238 139630718992384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.780321 139630718992384 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 10:58:03.780434 139630718992384 transformer_layer.py:154] tlayer: recurrent = False
I0123 10:58:03.780476 139630718992384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 10:58:03.780510 139630718992384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 10:58:03.783075 139630718992384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.785611 139630718992384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 10:58:03.791292 139630718992384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.791566 139630718992384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 10:58:03.794206 139630718992384 transformer_layer.py:213] tlayer: windowed attention.
I0123 10:58:03.798031 139630718992384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 10:58:03.798092 139630718992384 attention.py:418] Single window, no scan.
I0123 10:58:03.798131 139630718992384 transformer_layer.py:389] tlayer: self-attention.
I0123 10:58:03.798163 139630718992384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.798228 139630718992384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.798804 139630718992384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.798882 139630718992384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.799250 139630718992384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.800025 139630718992384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.802597 139630718992384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.803235 139630718992384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.803315 139630718992384 transformer_layer.py:468] tlayer: End windows.
I0123 10:58:03.803353 139630718992384 transformer_layer.py:472] tlayer: final FFN.
I0123 10:58:03.803416 139630718992384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.803551 139630718992384 transformer_base.py:410] tbase: post-attention MLP.
I0123 10:58:03.803876 139630718992384 nn_components.py:325] mlp: activation = None
I0123 10:58:03.803921 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 10:58:03.805851 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.805949 139630718992384 nn_components.py:261] mlp: residual
I0123 10:58:03.808542 139630718992384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.808631 139630718992384 transformer_base.py:443] tbase: final FFN
I0123 10:58:03.809069 139630718992384 nn_components.py:320] mlp: hidden 4096, relu
I0123 10:58:03.811384 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 10:58:03.813313 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.813412 139630718992384 nn_components.py:261] mlp: residual
I0123 10:58:03.813719 139630718992384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.813807 139630718992384 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 10:58:03.813921 139630718992384 transformer_layer.py:154] tlayer: recurrent = False
I0123 10:58:03.813964 139630718992384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 10:58:03.813998 139630718992384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 10:58:03.815905 139630718992384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.818323 139630718992384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 10:58:03.823954 139630718992384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.824223 139630718992384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 10:58:03.826959 139630718992384 transformer_layer.py:213] tlayer: windowed attention.
I0123 10:58:03.830698 139630718992384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 10:58:03.830757 139630718992384 attention.py:418] Single window, no scan.
I0123 10:58:03.830795 139630718992384 transformer_layer.py:389] tlayer: self-attention.
I0123 10:58:03.830827 139630718992384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.830892 139630718992384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.831466 139630718992384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.831545 139630718992384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.831909 139630718992384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.832684 139630718992384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.835587 139630718992384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.836222 139630718992384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.836302 139630718992384 transformer_layer.py:468] tlayer: End windows.
I0123 10:58:03.836340 139630718992384 transformer_layer.py:472] tlayer: final FFN.
I0123 10:58:03.836411 139630718992384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.836551 139630718992384 transformer_base.py:410] tbase: post-attention MLP.
I0123 10:58:03.836882 139630718992384 nn_components.py:325] mlp: activation = None
I0123 10:58:03.836928 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 10:58:03.838839 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.838937 139630718992384 nn_components.py:261] mlp: residual
I0123 10:58:03.841521 139630718992384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.841603 139630718992384 transformer_base.py:443] tbase: final FFN
I0123 10:58:03.842057 139630718992384 nn_components.py:320] mlp: hidden 4096, relu
I0123 10:58:03.844345 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 10:58:03.846334 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.846435 139630718992384 nn_components.py:261] mlp: residual
I0123 10:58:03.846732 139630718992384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.846816 139630718992384 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 10:58:03.846930 139630718992384 transformer_layer.py:154] tlayer: recurrent = False
I0123 10:58:03.846971 139630718992384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 10:58:03.847005 139630718992384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 10:58:03.848854 139630718992384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.851264 139630718992384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 10:58:03.856953 139630718992384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.857217 139630718992384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 10:58:03.859940 139630718992384 transformer_layer.py:213] tlayer: windowed attention.
I0123 10:58:03.863740 139630718992384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 10:58:03.863799 139630718992384 attention.py:418] Single window, no scan.
I0123 10:58:03.863837 139630718992384 transformer_layer.py:389] tlayer: self-attention.
I0123 10:58:03.863869 139630718992384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.863933 139630718992384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.864545 139630718992384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.864626 139630718992384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.864995 139630718992384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.865795 139630718992384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.868375 139630718992384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.869001 139630718992384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.869082 139630718992384 transformer_layer.py:468] tlayer: End windows.
I0123 10:58:03.869120 139630718992384 transformer_layer.py:472] tlayer: final FFN.
I0123 10:58:03.869181 139630718992384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.869313 139630718992384 transformer_base.py:410] tbase: post-attention MLP.
I0123 10:58:03.869636 139630718992384 nn_components.py:325] mlp: activation = None
I0123 10:58:03.869688 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 10:58:03.871660 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.871761 139630718992384 nn_components.py:261] mlp: residual
I0123 10:58:03.874363 139630718992384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.874447 139630718992384 transformer_base.py:443] tbase: final FFN
I0123 10:58:03.874884 139630718992384 nn_components.py:320] mlp: hidden 4096, relu
I0123 10:58:03.877225 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 10:58:03.879206 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.879313 139630718992384 nn_components.py:261] mlp: residual
I0123 10:58:03.879644 139630718992384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.879733 139630718992384 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 10:58:03.879845 139630718992384 transformer_layer.py:154] tlayer: recurrent = False
I0123 10:58:03.879886 139630718992384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 10:58:03.879919 139630718992384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 10:58:03.881775 139630718992384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.884229 139630718992384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 10:58:03.889947 139630718992384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.890236 139630718992384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 10:58:03.892917 139630718992384 transformer_layer.py:213] tlayer: windowed attention.
I0123 10:58:03.896771 139630718992384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 10:58:03.896831 139630718992384 attention.py:418] Single window, no scan.
I0123 10:58:03.896872 139630718992384 transformer_layer.py:389] tlayer: self-attention.
I0123 10:58:03.896906 139630718992384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.896970 139630718992384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.897539 139630718992384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.897618 139630718992384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.897992 139630718992384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.898780 139630718992384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.901289 139630718992384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.901924 139630718992384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.902005 139630718992384 transformer_layer.py:468] tlayer: End windows.
I0123 10:58:03.902043 139630718992384 transformer_layer.py:472] tlayer: final FFN.
I0123 10:58:03.902106 139630718992384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.902245 139630718992384 transformer_base.py:410] tbase: post-attention MLP.
I0123 10:58:03.902566 139630718992384 nn_components.py:325] mlp: activation = None
I0123 10:58:03.902611 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 10:58:03.904582 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.904681 139630718992384 nn_components.py:261] mlp: residual
I0123 10:58:03.907212 139630718992384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.907299 139630718992384 transformer_base.py:443] tbase: final FFN
I0123 10:58:03.907730 139630718992384 nn_components.py:320] mlp: hidden 4096, relu
I0123 10:58:03.910399 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 10:58:03.912352 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.912458 139630718992384 nn_components.py:261] mlp: residual
I0123 10:58:03.912765 139630718992384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:03.912850 139630718992384 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 10:58:03.912965 139630718992384 transformer_layer.py:154] tlayer: recurrent = False
I0123 10:58:03.913008 139630718992384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 10:58:03.913041 139630718992384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 10:58:04.050983 139630718992384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.054064 139630718992384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 10:58:04.059949 139630718992384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.060258 139630718992384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 10:58:04.062998 139630718992384 transformer_layer.py:213] tlayer: windowed attention.
I0123 10:58:04.066962 139630718992384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 10:58:04.067024 139630718992384 attention.py:418] Single window, no scan.
I0123 10:58:04.067064 139630718992384 transformer_layer.py:389] tlayer: self-attention.
I0123 10:58:04.067099 139630718992384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.067172 139630718992384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.067805 139630718992384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.067886 139630718992384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.068282 139630718992384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.069092 139630718992384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.071723 139630718992384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.072390 139630718992384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.072471 139630718992384 transformer_layer.py:468] tlayer: End windows.
I0123 10:58:04.072508 139630718992384 transformer_layer.py:472] tlayer: final FFN.
I0123 10:58:04.072572 139630718992384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.072705 139630718992384 transformer_base.py:410] tbase: post-attention MLP.
I0123 10:58:04.073070 139630718992384 nn_components.py:325] mlp: activation = None
I0123 10:58:04.073116 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 10:58:04.075065 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.075162 139630718992384 nn_components.py:261] mlp: residual
I0123 10:58:04.077770 139630718992384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.077854 139630718992384 transformer_base.py:443] tbase: final FFN
I0123 10:58:04.078309 139630718992384 nn_components.py:320] mlp: hidden 4096, relu
I0123 10:58:04.080649 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 10:58:04.082607 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.082720 139630718992384 nn_components.py:261] mlp: residual
I0123 10:58:04.083036 139630718992384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.083123 139630718992384 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 10:58:04.083236 139630718992384 transformer_layer.py:154] tlayer: recurrent = False
I0123 10:58:04.083278 139630718992384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 10:58:04.083312 139630718992384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 10:58:04.085276 139630718992384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.087710 139630718992384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 10:58:04.093399 139630718992384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.093687 139630718992384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 10:58:04.096421 139630718992384 transformer_layer.py:213] tlayer: windowed attention.
I0123 10:58:04.100229 139630718992384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 10:58:04.100287 139630718992384 attention.py:418] Single window, no scan.
I0123 10:58:04.100325 139630718992384 transformer_layer.py:389] tlayer: self-attention.
I0123 10:58:04.100358 139630718992384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.100421 139630718992384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.101003 139630718992384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.101083 139630718992384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.101459 139630718992384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.102254 139630718992384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.104845 139630718992384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.105479 139630718992384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.105564 139630718992384 transformer_layer.py:468] tlayer: End windows.
I0123 10:58:04.105602 139630718992384 transformer_layer.py:472] tlayer: final FFN.
I0123 10:58:04.105672 139630718992384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.105808 139630718992384 transformer_base.py:410] tbase: post-attention MLP.
I0123 10:58:04.106139 139630718992384 nn_components.py:325] mlp: activation = None
I0123 10:58:04.106187 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 10:58:04.108100 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.108196 139630718992384 nn_components.py:261] mlp: residual
I0123 10:58:04.110771 139630718992384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.110855 139630718992384 transformer_base.py:443] tbase: final FFN
I0123 10:58:04.111289 139630718992384 nn_components.py:320] mlp: hidden 4096, relu
I0123 10:58:04.113604 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 10:58:04.115597 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.115697 139630718992384 nn_components.py:261] mlp: residual
I0123 10:58:04.115994 139630718992384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.117557 139630718992384 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 10:58:04.117687 139630718992384 transformer_layer.py:154] tlayer: recurrent = False
I0123 10:58:04.117732 139630718992384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 10:58:04.117766 139630718992384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 10:58:04.119616 139630718992384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.122226 139630718992384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 10:58:04.127854 139630718992384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.128131 139630718992384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 10:58:04.131275 139630718992384 transformer_layer.py:213] tlayer: windowed attention.
I0123 10:58:04.135090 139630718992384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 10:58:04.135150 139630718992384 attention.py:418] Single window, no scan.
I0123 10:58:04.135188 139630718992384 transformer_layer.py:389] tlayer: self-attention.
I0123 10:58:04.135221 139630718992384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.135285 139630718992384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.135899 139630718992384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.135984 139630718992384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.136361 139630718992384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.137155 139630718992384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.139702 139630718992384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.140331 139630718992384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.140417 139630718992384 transformer_layer.py:468] tlayer: End windows.
I0123 10:58:04.140455 139630718992384 transformer_layer.py:472] tlayer: final FFN.
I0123 10:58:04.140515 139630718992384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.140647 139630718992384 transformer_base.py:410] tbase: post-attention MLP.
I0123 10:58:04.140969 139630718992384 nn_components.py:325] mlp: activation = None
I0123 10:58:04.141015 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 10:58:04.142949 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.143048 139630718992384 nn_components.py:261] mlp: residual
I0123 10:58:04.145652 139630718992384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.145741 139630718992384 transformer_base.py:443] tbase: final FFN
I0123 10:58:04.146174 139630718992384 nn_components.py:320] mlp: hidden 4096, relu
I0123 10:58:04.148517 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 10:58:04.150454 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.150558 139630718992384 nn_components.py:261] mlp: residual
I0123 10:58:04.150859 139630718992384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.150951 139630718992384 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 10:58:04.151067 139630718992384 transformer_layer.py:154] tlayer: recurrent = False
I0123 10:58:04.151109 139630718992384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 10:58:04.151142 139630718992384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 10:58:04.152977 139630718992384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.155461 139630718992384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 10:58:04.161050 139630718992384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.161315 139630718992384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 10:58:04.163948 139630718992384 transformer_layer.py:213] tlayer: windowed attention.
I0123 10:58:04.167774 139630718992384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 10:58:04.167834 139630718992384 attention.py:418] Single window, no scan.
I0123 10:58:04.167873 139630718992384 transformer_layer.py:389] tlayer: self-attention.
I0123 10:58:04.167905 139630718992384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.167969 139630718992384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.168539 139630718992384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.168618 139630718992384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.168981 139630718992384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.169771 139630718992384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.172268 139630718992384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.172896 139630718992384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.172976 139630718992384 transformer_layer.py:468] tlayer: End windows.
I0123 10:58:04.173013 139630718992384 transformer_layer.py:472] tlayer: final FFN.
I0123 10:58:04.173075 139630718992384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.173204 139630718992384 transformer_base.py:410] tbase: post-attention MLP.
I0123 10:58:04.173521 139630718992384 nn_components.py:325] mlp: activation = None
I0123 10:58:04.173566 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 10:58:04.175526 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.175629 139630718992384 nn_components.py:261] mlp: residual
I0123 10:58:04.178450 139630718992384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.178533 139630718992384 transformer_base.py:443] tbase: final FFN
I0123 10:58:04.178951 139630718992384 nn_components.py:320] mlp: hidden 4096, relu
I0123 10:58:04.181284 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 10:58:04.183195 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.183295 139630718992384 nn_components.py:261] mlp: residual
I0123 10:58:04.183590 139630718992384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.183674 139630718992384 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 10:58:04.183794 139630718992384 transformer_layer.py:154] tlayer: recurrent = False
I0123 10:58:04.183837 139630718992384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 10:58:04.183871 139630718992384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 10:58:04.185774 139630718992384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.188167 139630718992384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 10:58:04.193819 139630718992384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.194086 139630718992384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 10:58:04.196733 139630718992384 transformer_layer.py:213] tlayer: windowed attention.
I0123 10:58:04.200569 139630718992384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 10:58:04.200628 139630718992384 attention.py:418] Single window, no scan.
I0123 10:58:04.200667 139630718992384 transformer_layer.py:389] tlayer: self-attention.
I0123 10:58:04.200701 139630718992384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.200766 139630718992384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.201331 139630718992384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.201410 139630718992384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.201788 139630718992384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.202583 139630718992384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.205075 139630718992384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.206075 139630718992384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.206158 139630718992384 transformer_layer.py:468] tlayer: End windows.
I0123 10:58:04.206196 139630718992384 transformer_layer.py:472] tlayer: final FFN.
I0123 10:58:04.206258 139630718992384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.206388 139630718992384 transformer_base.py:410] tbase: post-attention MLP.
I0123 10:58:04.206710 139630718992384 nn_components.py:325] mlp: activation = None
I0123 10:58:04.206755 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 10:58:04.208655 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.208752 139630718992384 nn_components.py:261] mlp: residual
I0123 10:58:04.211247 139630718992384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.211328 139630718992384 transformer_base.py:443] tbase: final FFN
I0123 10:58:04.211811 139630718992384 nn_components.py:320] mlp: hidden 4096, relu
I0123 10:58:04.214082 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 10:58:04.215981 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.216081 139630718992384 nn_components.py:261] mlp: residual
I0123 10:58:04.216379 139630718992384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.216668 139630718992384 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 10:58:04.216743 139630718992384 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 10:58:04.216817 139630718992384 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 10:58:04.216880 139630718992384 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 10:58:04.216940 139630718992384 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 10:58:04.216998 139630718992384 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 10:58:04.217056 139630718992384 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 10:58:04.217112 139630718992384 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 10:58:04.217171 139630718992384 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 10:58:04.217230 139630718992384 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 10:58:04.217286 139630718992384 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 10:58:04.217342 139630718992384 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 10:58:04.217382 139630718992384 decoder_stack.py:344] dstack: Final layernorm.
I0123 10:58:04.220941 139630718992384 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 10:58:04.268369 139630718992384 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.268459 139630718992384 decoder_stack.py:333] dstack: autoregressive generator.
I0123 10:58:04.268517 139630718992384 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 10:58:04.268626 139630718992384 transformer_layer.py:154] tlayer: recurrent = False
I0123 10:58:04.268677 139630718992384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 10:58:04.268709 139630718992384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 10:58:04.268777 139630718992384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.271237 139630718992384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 10:58:04.276733 139630718992384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.277007 139630718992384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 10:58:04.279691 139630718992384 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 10:58:04.296098 139630718992384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 10:58:04.296157 139630718992384 attention.py:418] Single window, no scan.
I0123 10:58:04.296195 139630718992384 transformer_layer.py:389] tlayer: self-attention.
I0123 10:58:04.296228 139630718992384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.296291 139630718992384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.297428 139630718992384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.297509 139630718992384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.298227 139630718992384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.300226 139630718992384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.304990 139630718992384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.306311 139630718992384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.306403 139630718992384 transformer_layer.py:468] tlayer: End windows.
I0123 10:58:04.306442 139630718992384 transformer_layer.py:472] tlayer: final FFN.
I0123 10:58:04.306505 139630718992384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.306642 139630718992384 transformer_base.py:410] tbase: post-attention MLP.
I0123 10:58:04.306764 139630718992384 nn_components.py:325] mlp: activation = None
I0123 10:58:04.306813 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 10:58:04.308734 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.308833 139630718992384 nn_components.py:261] mlp: residual
I0123 10:58:04.311310 139630718992384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.311395 139630718992384 transformer_base.py:443] tbase: final FFN
I0123 10:58:04.311507 139630718992384 nn_components.py:320] mlp: hidden 4096, relu
I0123 10:58:04.313784 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 10:58:04.315766 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.315866 139630718992384 nn_components.py:261] mlp: residual
I0123 10:58:04.316174 139630718992384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.316261 139630718992384 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 10:58:04.316374 139630718992384 transformer_layer.py:154] tlayer: recurrent = False
I0123 10:58:04.316415 139630718992384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 10:58:04.316448 139630718992384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 10:58:04.316516 139630718992384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.318813 139630718992384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 10:58:04.324304 139630718992384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.324575 139630718992384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 10:58:04.327281 139630718992384 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 10:58:04.340313 139630718992384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 10:58:04.340372 139630718992384 attention.py:418] Single window, no scan.
I0123 10:58:04.340409 139630718992384 transformer_layer.py:389] tlayer: self-attention.
I0123 10:58:04.340442 139630718992384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.340505 139630718992384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.341073 139630718992384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.341153 139630718992384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.341520 139630718992384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.342230 139630718992384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.344745 139630718992384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.345373 139630718992384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.345453 139630718992384 transformer_layer.py:468] tlayer: End windows.
I0123 10:58:04.345496 139630718992384 transformer_layer.py:472] tlayer: final FFN.
I0123 10:58:04.345559 139630718992384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.345702 139630718992384 transformer_base.py:410] tbase: post-attention MLP.
I0123 10:58:04.345817 139630718992384 nn_components.py:325] mlp: activation = None
I0123 10:58:04.345866 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 10:58:04.347807 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.347904 139630718992384 nn_components.py:261] mlp: residual
I0123 10:58:04.350339 139630718992384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.350426 139630718992384 transformer_base.py:443] tbase: final FFN
I0123 10:58:04.350537 139630718992384 nn_components.py:320] mlp: hidden 4096, relu
I0123 10:58:04.352773 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 10:58:04.354708 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.354807 139630718992384 nn_components.py:261] mlp: residual
I0123 10:58:04.355108 139630718992384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.355191 139630718992384 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 10:58:04.355304 139630718992384 transformer_layer.py:154] tlayer: recurrent = False
I0123 10:58:04.355344 139630718992384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 10:58:04.355378 139630718992384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 10:58:04.355445 139630718992384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.357713 139630718992384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 10:58:04.363185 139630718992384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.363458 139630718992384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 10:58:04.366153 139630718992384 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 10:58:04.378742 139630718992384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 10:58:04.378802 139630718992384 attention.py:418] Single window, no scan.
I0123 10:58:04.378839 139630718992384 transformer_layer.py:389] tlayer: self-attention.
I0123 10:58:04.378872 139630718992384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.378936 139630718992384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.379498 139630718992384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.379576 139630718992384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.379946 139630718992384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.380646 139630718992384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.383156 139630718992384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.383786 139630718992384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.383865 139630718992384 transformer_layer.py:468] tlayer: End windows.
I0123 10:58:04.383902 139630718992384 transformer_layer.py:472] tlayer: final FFN.
I0123 10:58:04.383968 139630718992384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.384099 139630718992384 transformer_base.py:410] tbase: post-attention MLP.
I0123 10:58:04.384226 139630718992384 nn_components.py:325] mlp: activation = None
I0123 10:58:04.384268 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 10:58:04.386216 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.386315 139630718992384 nn_components.py:261] mlp: residual
I0123 10:58:04.388754 139630718992384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.388836 139630718992384 transformer_base.py:443] tbase: final FFN
I0123 10:58:04.388947 139630718992384 nn_components.py:320] mlp: hidden 4096, relu
I0123 10:58:04.391191 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 10:58:04.393124 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.393224 139630718992384 nn_components.py:261] mlp: residual
I0123 10:58:04.393522 139630718992384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.393607 139630718992384 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 10:58:04.393724 139630718992384 transformer_layer.py:154] tlayer: recurrent = False
I0123 10:58:04.393766 139630718992384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 10:58:04.393800 139630718992384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 10:58:04.393866 139630718992384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.396114 139630718992384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 10:58:04.401559 139630718992384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.401841 139630718992384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 10:58:04.404516 139630718992384 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 10:58:04.417129 139630718992384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 10:58:04.417187 139630718992384 attention.py:418] Single window, no scan.
I0123 10:58:04.417225 139630718992384 transformer_layer.py:389] tlayer: self-attention.
I0123 10:58:04.417257 139630718992384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.417320 139630718992384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.417890 139630718992384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.417970 139630718992384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.418344 139630718992384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.419046 139630718992384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.421515 139630718992384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.422156 139630718992384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.422237 139630718992384 transformer_layer.py:468] tlayer: End windows.
I0123 10:58:04.422275 139630718992384 transformer_layer.py:472] tlayer: final FFN.
I0123 10:58:04.422337 139630718992384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.422483 139630718992384 transformer_base.py:410] tbase: post-attention MLP.
I0123 10:58:04.422607 139630718992384 nn_components.py:325] mlp: activation = None
I0123 10:58:04.422648 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 10:58:04.424589 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.424688 139630718992384 nn_components.py:261] mlp: residual
I0123 10:58:04.427118 139630718992384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.427201 139630718992384 transformer_base.py:443] tbase: final FFN
I0123 10:58:04.427313 139630718992384 nn_components.py:320] mlp: hidden 4096, relu
I0123 10:58:04.429549 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 10:58:04.431449 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.431550 139630718992384 nn_components.py:261] mlp: residual
I0123 10:58:04.431841 139630718992384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.431926 139630718992384 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 10:58:04.432037 139630718992384 transformer_layer.py:154] tlayer: recurrent = False
I0123 10:58:04.432079 139630718992384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 10:58:04.432111 139630718992384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 10:58:04.432179 139630718992384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.434794 139630718992384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 10:58:04.440288 139630718992384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.440561 139630718992384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 10:58:04.447545 139630718992384 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 10:58:04.460481 139630718992384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 10:58:04.460544 139630718992384 attention.py:418] Single window, no scan.
I0123 10:58:04.460584 139630718992384 transformer_layer.py:389] tlayer: self-attention.
I0123 10:58:04.460617 139630718992384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.460691 139630718992384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.461310 139630718992384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.461390 139630718992384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.461784 139630718992384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.462506 139630718992384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.465112 139630718992384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.465911 139630718992384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.465993 139630718992384 transformer_layer.py:468] tlayer: End windows.
I0123 10:58:04.466030 139630718992384 transformer_layer.py:472] tlayer: final FFN.
I0123 10:58:04.466090 139630718992384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.466231 139630718992384 transformer_base.py:410] tbase: post-attention MLP.
I0123 10:58:04.466359 139630718992384 nn_components.py:325] mlp: activation = None
I0123 10:58:04.466401 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 10:58:04.468389 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.468485 139630718992384 nn_components.py:261] mlp: residual
I0123 10:58:04.470974 139630718992384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.471057 139630718992384 transformer_base.py:443] tbase: final FFN
I0123 10:58:04.471171 139630718992384 nn_components.py:320] mlp: hidden 4096, relu
I0123 10:58:04.473500 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 10:58:04.475408 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.475509 139630718992384 nn_components.py:261] mlp: residual
I0123 10:58:04.475805 139630718992384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.475891 139630718992384 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 10:58:04.476006 139630718992384 transformer_layer.py:154] tlayer: recurrent = False
I0123 10:58:04.476050 139630718992384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 10:58:04.476084 139630718992384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 10:58:04.476151 139630718992384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.478475 139630718992384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 10:58:04.483973 139630718992384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.484236 139630718992384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 10:58:04.486922 139630718992384 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 10:58:04.499642 139630718992384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 10:58:04.499701 139630718992384 attention.py:418] Single window, no scan.
I0123 10:58:04.499739 139630718992384 transformer_layer.py:389] tlayer: self-attention.
I0123 10:58:04.499773 139630718992384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.499836 139630718992384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.500415 139630718992384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.500495 139630718992384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.500872 139630718992384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.501576 139630718992384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.504084 139630718992384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.504712 139630718992384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.504791 139630718992384 transformer_layer.py:468] tlayer: End windows.
I0123 10:58:04.504828 139630718992384 transformer_layer.py:472] tlayer: final FFN.
I0123 10:58:04.504888 139630718992384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.505024 139630718992384 transformer_base.py:410] tbase: post-attention MLP.
I0123 10:58:04.505143 139630718992384 nn_components.py:325] mlp: activation = None
I0123 10:58:04.505185 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 10:58:04.507163 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.507263 139630718992384 nn_components.py:261] mlp: residual
I0123 10:58:04.509733 139630718992384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.509817 139630718992384 transformer_base.py:443] tbase: final FFN
I0123 10:58:04.509929 139630718992384 nn_components.py:320] mlp: hidden 4096, relu
I0123 10:58:04.512173 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 10:58:04.514070 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.514173 139630718992384 nn_components.py:261] mlp: residual
I0123 10:58:04.514467 139630718992384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.514552 139630718992384 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 10:58:04.514663 139630718992384 transformer_layer.py:154] tlayer: recurrent = False
I0123 10:58:04.514704 139630718992384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 10:58:04.514736 139630718992384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 10:58:04.514802 139630718992384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.517076 139630718992384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 10:58:04.522659 139630718992384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.522922 139630718992384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 10:58:04.525551 139630718992384 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 10:58:04.538248 139630718992384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 10:58:04.538307 139630718992384 attention.py:418] Single window, no scan.
I0123 10:58:04.538345 139630718992384 transformer_layer.py:389] tlayer: self-attention.
I0123 10:58:04.538378 139630718992384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.538440 139630718992384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.539008 139630718992384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.539087 139630718992384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.539451 139630718992384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.540150 139630718992384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.542655 139630718992384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.543640 139630718992384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.543722 139630718992384 transformer_layer.py:468] tlayer: End windows.
I0123 10:58:04.543759 139630718992384 transformer_layer.py:472] tlayer: final FFN.
I0123 10:58:04.543819 139630718992384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.543950 139630718992384 transformer_base.py:410] tbase: post-attention MLP.
I0123 10:58:04.544068 139630718992384 nn_components.py:325] mlp: activation = None
I0123 10:58:04.544114 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 10:58:04.546050 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.546148 139630718992384 nn_components.py:261] mlp: residual
I0123 10:58:04.548577 139630718992384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.548658 139630718992384 transformer_base.py:443] tbase: final FFN
I0123 10:58:04.548769 139630718992384 nn_components.py:320] mlp: hidden 4096, relu
I0123 10:58:04.551015 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 10:58:04.552974 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.553073 139630718992384 nn_components.py:261] mlp: residual
I0123 10:58:04.553366 139630718992384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.553451 139630718992384 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 10:58:04.553563 139630718992384 transformer_layer.py:154] tlayer: recurrent = False
I0123 10:58:04.553605 139630718992384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 10:58:04.553638 139630718992384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 10:58:04.553716 139630718992384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.556022 139630718992384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 10:58:04.561490 139630718992384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.561774 139630718992384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 10:58:04.564504 139630718992384 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 10:58:04.577228 139630718992384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 10:58:04.577286 139630718992384 attention.py:418] Single window, no scan.
I0123 10:58:04.577323 139630718992384 transformer_layer.py:389] tlayer: self-attention.
I0123 10:58:04.577356 139630718992384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.577420 139630718992384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.578044 139630718992384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.578125 139630718992384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.578491 139630718992384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.579181 139630718992384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.581668 139630718992384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.582296 139630718992384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.582379 139630718992384 transformer_layer.py:468] tlayer: End windows.
I0123 10:58:04.582416 139630718992384 transformer_layer.py:472] tlayer: final FFN.
I0123 10:58:04.582477 139630718992384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.582612 139630718992384 transformer_base.py:410] tbase: post-attention MLP.
I0123 10:58:04.582724 139630718992384 nn_components.py:325] mlp: activation = None
I0123 10:58:04.582772 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 10:58:04.584676 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.584773 139630718992384 nn_components.py:261] mlp: residual
I0123 10:58:04.587260 139630718992384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.587343 139630718992384 transformer_base.py:443] tbase: final FFN
I0123 10:58:04.587455 139630718992384 nn_components.py:320] mlp: hidden 4096, relu
I0123 10:58:04.589695 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 10:58:04.591581 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.591680 139630718992384 nn_components.py:261] mlp: residual
I0123 10:58:04.591974 139630718992384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.592059 139630718992384 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 10:58:04.592169 139630718992384 transformer_layer.py:154] tlayer: recurrent = False
I0123 10:58:04.592210 139630718992384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 10:58:04.592243 139630718992384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 10:58:04.592311 139630718992384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.594575 139630718992384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 10:58:04.600095 139630718992384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.600359 139630718992384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 10:58:04.603011 139630718992384 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 10:58:04.615640 139630718992384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 10:58:04.615698 139630718992384 attention.py:418] Single window, no scan.
I0123 10:58:04.615735 139630718992384 transformer_layer.py:389] tlayer: self-attention.
I0123 10:58:04.615768 139630718992384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.615831 139630718992384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.616397 139630718992384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.616476 139630718992384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.616854 139630718992384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.617552 139630718992384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.620068 139630718992384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.620745 139630718992384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.620830 139630718992384 transformer_layer.py:468] tlayer: End windows.
I0123 10:58:04.620867 139630718992384 transformer_layer.py:472] tlayer: final FFN.
I0123 10:58:04.620932 139630718992384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.621064 139630718992384 transformer_base.py:410] tbase: post-attention MLP.
I0123 10:58:04.621177 139630718992384 nn_components.py:325] mlp: activation = None
I0123 10:58:04.621217 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 10:58:04.623149 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.623246 139630718992384 nn_components.py:261] mlp: residual
I0123 10:58:04.625688 139630718992384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.625771 139630718992384 transformer_base.py:443] tbase: final FFN
I0123 10:58:04.625883 139630718992384 nn_components.py:320] mlp: hidden 4096, relu
I0123 10:58:04.628189 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 10:58:04.630149 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.630250 139630718992384 nn_components.py:261] mlp: residual
I0123 10:58:04.630544 139630718992384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.630630 139630718992384 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 10:58:04.630745 139630718992384 transformer_layer.py:154] tlayer: recurrent = False
I0123 10:58:04.630787 139630718992384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 10:58:04.630820 139630718992384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 10:58:04.630888 139630718992384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.633161 139630718992384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 10:58:04.638667 139630718992384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.638934 139630718992384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 10:58:04.641654 139630718992384 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 10:58:04.654653 139630718992384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 10:58:04.654713 139630718992384 attention.py:418] Single window, no scan.
I0123 10:58:04.654751 139630718992384 transformer_layer.py:389] tlayer: self-attention.
I0123 10:58:04.654784 139630718992384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.654848 139630718992384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.655467 139630718992384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.655547 139630718992384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.655923 139630718992384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.656625 139630718992384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.659150 139630718992384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.659782 139630718992384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.659862 139630718992384 transformer_layer.py:468] tlayer: End windows.
I0123 10:58:04.659898 139630718992384 transformer_layer.py:472] tlayer: final FFN.
I0123 10:58:04.659959 139630718992384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.660092 139630718992384 transformer_base.py:410] tbase: post-attention MLP.
I0123 10:58:04.660204 139630718992384 nn_components.py:325] mlp: activation = None
I0123 10:58:04.660245 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 10:58:04.662145 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.662250 139630718992384 nn_components.py:261] mlp: residual
I0123 10:58:04.664736 139630718992384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.664821 139630718992384 transformer_base.py:443] tbase: final FFN
I0123 10:58:04.664934 139630718992384 nn_components.py:320] mlp: hidden 4096, relu
I0123 10:58:04.667184 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 10:58:04.669069 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.669168 139630718992384 nn_components.py:261] mlp: residual
I0123 10:58:04.669460 139630718992384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.669544 139630718992384 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 10:58:04.669664 139630718992384 transformer_layer.py:154] tlayer: recurrent = False
I0123 10:58:04.669708 139630718992384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 10:58:04.669741 139630718992384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 10:58:04.669808 139630718992384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.672076 139630718992384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 10:58:04.677638 139630718992384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.677907 139630718992384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 10:58:04.680560 139630718992384 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 10:58:04.693345 139630718992384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 10:58:04.693404 139630718992384 attention.py:418] Single window, no scan.
I0123 10:58:04.693442 139630718992384 transformer_layer.py:389] tlayer: self-attention.
I0123 10:58:04.693476 139630718992384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.693543 139630718992384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.694103 139630718992384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.694183 139630718992384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.694547 139630718992384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.695253 139630718992384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.697761 139630718992384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.698430 139630718992384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.698508 139630718992384 transformer_layer.py:468] tlayer: End windows.
I0123 10:58:04.698545 139630718992384 transformer_layer.py:472] tlayer: final FFN.
I0123 10:58:04.698607 139630718992384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.698741 139630718992384 transformer_base.py:410] tbase: post-attention MLP.
I0123 10:58:04.698855 139630718992384 nn_components.py:325] mlp: activation = None
I0123 10:58:04.698896 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 10:58:04.700803 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.700906 139630718992384 nn_components.py:261] mlp: residual
I0123 10:58:04.703356 139630718992384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.703438 139630718992384 transformer_base.py:443] tbase: final FFN
I0123 10:58:04.703548 139630718992384 nn_components.py:320] mlp: hidden 4096, relu
I0123 10:58:04.705777 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 10:58:04.707729 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.707828 139630718992384 nn_components.py:261] mlp: residual
I0123 10:58:04.708121 139630718992384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.708205 139630718992384 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 10:58:04.708318 139630718992384 transformer_layer.py:154] tlayer: recurrent = False
I0123 10:58:04.708360 139630718992384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 10:58:04.708394 139630718992384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 10:58:04.708460 139630718992384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.710721 139630718992384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 10:58:04.716237 139630718992384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.716502 139630718992384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 10:58:04.719231 139630718992384 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 10:58:04.731945 139630718992384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 10:58:04.732005 139630718992384 attention.py:418] Single window, no scan.
I0123 10:58:04.732043 139630718992384 transformer_layer.py:389] tlayer: self-attention.
I0123 10:58:04.732076 139630718992384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.732142 139630718992384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.732717 139630718992384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.732796 139630718992384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.733169 139630718992384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.733925 139630718992384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.736458 139630718992384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.737097 139630718992384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.737177 139630718992384 transformer_layer.py:468] tlayer: End windows.
I0123 10:58:04.737214 139630718992384 transformer_layer.py:472] tlayer: final FFN.
I0123 10:58:04.737275 139630718992384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.737406 139630718992384 transformer_base.py:410] tbase: post-attention MLP.
I0123 10:58:04.737518 139630718992384 nn_components.py:325] mlp: activation = None
I0123 10:58:04.737560 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 10:58:04.739467 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.739565 139630718992384 nn_components.py:261] mlp: residual
I0123 10:58:04.742032 139630718992384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.742115 139630718992384 transformer_base.py:443] tbase: final FFN
I0123 10:58:04.742227 139630718992384 nn_components.py:320] mlp: hidden 4096, relu
I0123 10:58:04.744539 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 10:58:04.746448 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.746548 139630718992384 nn_components.py:261] mlp: residual
I0123 10:58:04.746843 139630718992384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.746937 139630718992384 decoder_stack.py:344] dstack: Final layernorm.
I0123 10:58:04.749880 139630718992384 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 10:58:04.805644 139630718992384 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.805738 139630718992384 decoder_stack.py:333] dstack: autoregressive generator.
I0123 10:58:04.805795 139630718992384 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 10:58:04.805903 139630718992384 transformer_layer.py:154] tlayer: recurrent = False
I0123 10:58:04.805943 139630718992384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 10:58:04.805976 139630718992384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 10:58:04.806042 139630718992384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.808734 139630718992384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 10:58:04.814191 139630718992384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.814469 139630718992384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 10:58:04.817114 139630718992384 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 10:58:04.829565 139630718992384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 10:58:04.829623 139630718992384 attention.py:418] Single window, no scan.
I0123 10:58:04.829667 139630718992384 transformer_layer.py:389] tlayer: self-attention.
I0123 10:58:04.829702 139630718992384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.829765 139630718992384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.830344 139630718992384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.830424 139630718992384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.830799 139630718992384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.831496 139630718992384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.834038 139630718992384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.834674 139630718992384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.834754 139630718992384 transformer_layer.py:468] tlayer: End windows.
I0123 10:58:04.834792 139630718992384 transformer_layer.py:472] tlayer: final FFN.
I0123 10:58:04.834854 139630718992384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.834991 139630718992384 transformer_base.py:410] tbase: post-attention MLP.
I0123 10:58:04.835124 139630718992384 nn_components.py:325] mlp: activation = None
I0123 10:58:04.835166 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 10:58:04.837032 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.837128 139630718992384 nn_components.py:261] mlp: residual
I0123 10:58:04.839576 139630718992384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.839659 139630718992384 transformer_base.py:443] tbase: final FFN
I0123 10:58:04.839771 139630718992384 nn_components.py:320] mlp: hidden 4096, relu
I0123 10:58:04.842030 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 10:58:04.843907 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.844005 139630718992384 nn_components.py:261] mlp: residual
I0123 10:58:04.844306 139630718992384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.844391 139630718992384 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 10:58:04.844503 139630718992384 transformer_layer.py:154] tlayer: recurrent = False
I0123 10:58:04.844544 139630718992384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 10:58:04.844577 139630718992384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 10:58:04.844646 139630718992384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.846926 139630718992384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 10:58:04.852374 139630718992384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.852646 139630718992384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 10:58:04.855344 139630718992384 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 10:58:04.867722 139630718992384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 10:58:04.867791 139630718992384 attention.py:418] Single window, no scan.
I0123 10:58:04.867830 139630718992384 transformer_layer.py:389] tlayer: self-attention.
I0123 10:58:04.867862 139630718992384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.867927 139630718992384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.868501 139630718992384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.868582 139630718992384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.868955 139630718992384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.869660 139630718992384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.872289 139630718992384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.872926 139630718992384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.873007 139630718992384 transformer_layer.py:468] tlayer: End windows.
I0123 10:58:04.873044 139630718992384 transformer_layer.py:472] tlayer: final FFN.
I0123 10:58:04.873105 139630718992384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.873237 139630718992384 transformer_base.py:410] tbase: post-attention MLP.
I0123 10:58:04.873354 139630718992384 nn_components.py:325] mlp: activation = None
I0123 10:58:04.873407 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 10:58:04.875320 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.875418 139630718992384 nn_components.py:261] mlp: residual
I0123 10:58:04.877913 139630718992384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.878012 139630718992384 transformer_base.py:443] tbase: final FFN
I0123 10:58:04.878135 139630718992384 nn_components.py:320] mlp: hidden 4096, relu
I0123 10:58:04.880428 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 10:58:04.882339 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.882438 139630718992384 nn_components.py:261] mlp: residual
I0123 10:58:04.882744 139630718992384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.882832 139630718992384 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 10:58:04.882946 139630718992384 transformer_layer.py:154] tlayer: recurrent = False
I0123 10:58:04.882986 139630718992384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 10:58:04.883019 139630718992384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 10:58:04.883086 139630718992384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.885363 139630718992384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 10:58:04.890897 139630718992384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.891178 139630718992384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 10:58:04.893879 139630718992384 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 10:58:04.906290 139630718992384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 10:58:04.906349 139630718992384 attention.py:418] Single window, no scan.
I0123 10:58:04.906387 139630718992384 transformer_layer.py:389] tlayer: self-attention.
I0123 10:58:04.906420 139630718992384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.906484 139630718992384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.907049 139630718992384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.907130 139630718992384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.907505 139630718992384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.908200 139630718992384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.910810 139630718992384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.911453 139630718992384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.911535 139630718992384 transformer_layer.py:468] tlayer: End windows.
I0123 10:58:04.911575 139630718992384 transformer_layer.py:472] tlayer: final FFN.
I0123 10:58:04.911637 139630718992384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.911775 139630718992384 transformer_base.py:410] tbase: post-attention MLP.
I0123 10:58:04.911902 139630718992384 nn_components.py:325] mlp: activation = None
I0123 10:58:04.911944 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 10:58:04.913868 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.913984 139630718992384 nn_components.py:261] mlp: residual
I0123 10:58:04.916455 139630718992384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.916537 139630718992384 transformer_base.py:443] tbase: final FFN
I0123 10:58:04.916651 139630718992384 nn_components.py:320] mlp: hidden 4096, relu
I0123 10:58:04.919378 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 10:58:04.921251 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.921351 139630718992384 nn_components.py:261] mlp: residual
I0123 10:58:04.921664 139630718992384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.921754 139630718992384 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 10:58:04.921868 139630718992384 transformer_layer.py:154] tlayer: recurrent = False
I0123 10:58:04.921910 139630718992384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 10:58:04.921943 139630718992384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 10:58:04.922011 139630718992384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.924287 139630718992384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 10:58:04.929722 139630718992384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.929995 139630718992384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 10:58:04.932686 139630718992384 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 10:58:04.945152 139630718992384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 10:58:04.945212 139630718992384 attention.py:418] Single window, no scan.
I0123 10:58:04.945251 139630718992384 transformer_layer.py:389] tlayer: self-attention.
I0123 10:58:04.945292 139630718992384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.945358 139630718992384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.945953 139630718992384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.946031 139630718992384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.946408 139630718992384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.947113 139630718992384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.949685 139630718992384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.950329 139630718992384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.950407 139630718992384 transformer_layer.py:468] tlayer: End windows.
I0123 10:58:04.950442 139630718992384 transformer_layer.py:472] tlayer: final FFN.
I0123 10:58:04.950505 139630718992384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.950636 139630718992384 transformer_base.py:410] tbase: post-attention MLP.
I0123 10:58:04.950754 139630718992384 nn_components.py:325] mlp: activation = None
I0123 10:58:04.950809 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 10:58:04.952721 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.952819 139630718992384 nn_components.py:261] mlp: residual
I0123 10:58:04.955265 139630718992384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.955347 139630718992384 transformer_base.py:443] tbase: final FFN
I0123 10:58:04.955457 139630718992384 nn_components.py:320] mlp: hidden 4096, relu
I0123 10:58:04.957799 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 10:58:04.959699 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.959798 139630718992384 nn_components.py:261] mlp: residual
I0123 10:58:04.960087 139630718992384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.960170 139630718992384 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 10:58:04.960281 139630718992384 transformer_layer.py:154] tlayer: recurrent = False
I0123 10:58:04.960320 139630718992384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 10:58:04.960351 139630718992384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 10:58:04.960418 139630718992384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.962669 139630718992384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 10:58:04.968068 139630718992384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.968332 139630718992384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 10:58:04.971031 139630718992384 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 10:58:04.983676 139630718992384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 10:58:04.983739 139630718992384 attention.py:418] Single window, no scan.
I0123 10:58:04.983776 139630718992384 transformer_layer.py:389] tlayer: self-attention.
I0123 10:58:04.983807 139630718992384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.983870 139630718992384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.984434 139630718992384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.984520 139630718992384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.984883 139630718992384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.985573 139630718992384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.988141 139630718992384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.988777 139630718992384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.988857 139630718992384 transformer_layer.py:468] tlayer: End windows.
I0123 10:58:04.988893 139630718992384 transformer_layer.py:472] tlayer: final FFN.
I0123 10:58:04.988954 139630718992384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.989085 139630718992384 transformer_base.py:410] tbase: post-attention MLP.
I0123 10:58:04.989202 139630718992384 nn_components.py:325] mlp: activation = None
I0123 10:58:04.989244 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 10:58:04.991137 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.991240 139630718992384 nn_components.py:261] mlp: residual
I0123 10:58:04.993672 139630718992384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.993753 139630718992384 transformer_base.py:443] tbase: final FFN
I0123 10:58:04.993863 139630718992384 nn_components.py:320] mlp: hidden 4096, relu
I0123 10:58:04.996163 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 10:58:04.998048 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.998148 139630718992384 nn_components.py:261] mlp: residual
I0123 10:58:04.998438 139630718992384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:04.998521 139630718992384 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 10:58:04.998630 139630718992384 transformer_layer.py:154] tlayer: recurrent = False
I0123 10:58:04.998670 139630718992384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 10:58:04.998701 139630718992384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 10:58:04.998766 139630718992384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.001010 139630718992384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 10:58:05.006436 139630718992384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.006696 139630718992384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 10:58:05.009373 139630718992384 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 10:58:05.021895 139630718992384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 10:58:05.021952 139630718992384 attention.py:418] Single window, no scan.
I0123 10:58:05.021988 139630718992384 transformer_layer.py:389] tlayer: self-attention.
I0123 10:58:05.022020 139630718992384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.022082 139630718992384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.022661 139630718992384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.022738 139630718992384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.023104 139630718992384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.023798 139630718992384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.026372 139630718992384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.026998 139630718992384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.027076 139630718992384 transformer_layer.py:468] tlayer: End windows.
I0123 10:58:05.027112 139630718992384 transformer_layer.py:472] tlayer: final FFN.
I0123 10:58:05.027171 139630718992384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.027302 139630718992384 transformer_base.py:410] tbase: post-attention MLP.
I0123 10:58:05.027414 139630718992384 nn_components.py:325] mlp: activation = None
I0123 10:58:05.027454 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 10:58:05.029331 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.029433 139630718992384 nn_components.py:261] mlp: residual
I0123 10:58:05.031866 139630718992384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.031946 139630718992384 transformer_base.py:443] tbase: final FFN
I0123 10:58:05.032057 139630718992384 nn_components.py:320] mlp: hidden 4096, relu
I0123 10:58:05.034768 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 10:58:05.036665 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.036764 139630718992384 nn_components.py:261] mlp: residual
I0123 10:58:05.037056 139630718992384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.037141 139630718992384 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 10:58:05.037252 139630718992384 transformer_layer.py:154] tlayer: recurrent = False
I0123 10:58:05.037292 139630718992384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 10:58:05.037323 139630718992384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 10:58:05.037387 139630718992384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.039645 139630718992384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 10:58:05.045104 139630718992384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.045368 139630718992384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 10:58:05.048078 139630718992384 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 10:58:05.060594 139630718992384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 10:58:05.060654 139630718992384 attention.py:418] Single window, no scan.
I0123 10:58:05.060690 139630718992384 transformer_layer.py:389] tlayer: self-attention.
I0123 10:58:05.060721 139630718992384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.060788 139630718992384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.061358 139630718992384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.061436 139630718992384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.061808 139630718992384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.062501 139630718992384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.065060 139630718992384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.065696 139630718992384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.065776 139630718992384 transformer_layer.py:468] tlayer: End windows.
I0123 10:58:05.065811 139630718992384 transformer_layer.py:472] tlayer: final FFN.
I0123 10:58:05.065872 139630718992384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.066001 139630718992384 transformer_base.py:410] tbase: post-attention MLP.
I0123 10:58:05.066112 139630718992384 nn_components.py:325] mlp: activation = None
I0123 10:58:05.066152 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 10:58:05.068042 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.068137 139630718992384 nn_components.py:261] mlp: residual
I0123 10:58:05.070571 139630718992384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.070653 139630718992384 transformer_base.py:443] tbase: final FFN
I0123 10:58:05.070763 139630718992384 nn_components.py:320] mlp: hidden 4096, relu
I0123 10:58:05.073060 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 10:58:05.074931 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.075029 139630718992384 nn_components.py:261] mlp: residual
I0123 10:58:05.075318 139630718992384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.075400 139630718992384 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 10:58:05.075509 139630718992384 transformer_layer.py:154] tlayer: recurrent = False
I0123 10:58:05.075548 139630718992384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 10:58:05.075581 139630718992384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 10:58:05.075647 139630718992384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.077889 139630718992384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 10:58:05.083374 139630718992384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.083635 139630718992384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 10:58:05.086354 139630718992384 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 10:58:05.098912 139630718992384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 10:58:05.098971 139630718992384 attention.py:418] Single window, no scan.
I0123 10:58:05.099007 139630718992384 transformer_layer.py:389] tlayer: self-attention.
I0123 10:58:05.099039 139630718992384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.099100 139630718992384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.099669 139630718992384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.099748 139630718992384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.100113 139630718992384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.100803 139630718992384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.103359 139630718992384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.103992 139630718992384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.104071 139630718992384 transformer_layer.py:468] tlayer: End windows.
I0123 10:58:05.104106 139630718992384 transformer_layer.py:472] tlayer: final FFN.
I0123 10:58:05.104170 139630718992384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.104302 139630718992384 transformer_base.py:410] tbase: post-attention MLP.
I0123 10:58:05.104413 139630718992384 nn_components.py:325] mlp: activation = None
I0123 10:58:05.104453 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 10:58:05.106348 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.106445 139630718992384 nn_components.py:261] mlp: residual
I0123 10:58:05.108864 139630718992384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.108951 139630718992384 transformer_base.py:443] tbase: final FFN
I0123 10:58:05.109065 139630718992384 nn_components.py:320] mlp: hidden 4096, relu
I0123 10:58:05.111382 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 10:58:05.113260 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.113358 139630718992384 nn_components.py:261] mlp: residual
I0123 10:58:05.113656 139630718992384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.113742 139630718992384 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 10:58:05.113852 139630718992384 transformer_layer.py:154] tlayer: recurrent = False
I0123 10:58:05.113892 139630718992384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 10:58:05.113925 139630718992384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 10:58:05.113991 139630718992384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.116239 139630718992384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 10:58:05.121672 139630718992384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.121935 139630718992384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 10:58:05.124700 139630718992384 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 10:58:05.137491 139630718992384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 10:58:05.137562 139630718992384 attention.py:418] Single window, no scan.
I0123 10:58:05.137599 139630718992384 transformer_layer.py:389] tlayer: self-attention.
I0123 10:58:05.137631 139630718992384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.137705 139630718992384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.138291 139630718992384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.138369 139630718992384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.138730 139630718992384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.139434 139630718992384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.142034 139630718992384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.142673 139630718992384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.142753 139630718992384 transformer_layer.py:468] tlayer: End windows.
I0123 10:58:05.142788 139630718992384 transformer_layer.py:472] tlayer: final FFN.
I0123 10:58:05.142850 139630718992384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.142980 139630718992384 transformer_base.py:410] tbase: post-attention MLP.
I0123 10:58:05.143093 139630718992384 nn_components.py:325] mlp: activation = None
I0123 10:58:05.143132 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 10:58:05.145035 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.145132 139630718992384 nn_components.py:261] mlp: residual
I0123 10:58:05.147575 139630718992384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.147664 139630718992384 transformer_base.py:443] tbase: final FFN
I0123 10:58:05.147778 139630718992384 nn_components.py:320] mlp: hidden 4096, relu
I0123 10:58:05.150528 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 10:58:05.152409 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.152506 139630718992384 nn_components.py:261] mlp: residual
I0123 10:58:05.152798 139630718992384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.152881 139630718992384 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 10:58:05.152992 139630718992384 transformer_layer.py:154] tlayer: recurrent = False
I0123 10:58:05.153031 139630718992384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 10:58:05.153062 139630718992384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 10:58:05.153127 139630718992384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.155408 139630718992384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 10:58:05.160842 139630718992384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.161105 139630718992384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 10:58:05.163841 139630718992384 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 10:58:05.176372 139630718992384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 10:58:05.176431 139630718992384 attention.py:418] Single window, no scan.
I0123 10:58:05.176468 139630718992384 transformer_layer.py:389] tlayer: self-attention.
I0123 10:58:05.176500 139630718992384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.176564 139630718992384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.177146 139630718992384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.177223 139630718992384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.177591 139630718992384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.178295 139630718992384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.180847 139630718992384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.181471 139630718992384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.181550 139630718992384 transformer_layer.py:468] tlayer: End windows.
I0123 10:58:05.181586 139630718992384 transformer_layer.py:472] tlayer: final FFN.
I0123 10:58:05.181653 139630718992384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.181790 139630718992384 transformer_base.py:410] tbase: post-attention MLP.
I0123 10:58:05.181900 139630718992384 nn_components.py:325] mlp: activation = None
I0123 10:58:05.181940 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 10:58:05.184454 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.184553 139630718992384 nn_components.py:261] mlp: residual
I0123 10:58:05.186968 139630718992384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.187050 139630718992384 transformer_base.py:443] tbase: final FFN
I0123 10:58:05.187171 139630718992384 nn_components.py:320] mlp: hidden 4096, relu
I0123 10:58:05.189432 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 10:58:05.191293 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.191391 139630718992384 nn_components.py:261] mlp: residual
I0123 10:58:05.191681 139630718992384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.191763 139630718992384 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 10:58:05.191872 139630718992384 transformer_layer.py:154] tlayer: recurrent = False
I0123 10:58:05.191911 139630718992384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 10:58:05.191943 139630718992384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 10:58:05.192007 139630718992384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.194269 139630718992384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 10:58:05.199704 139630718992384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.199968 139630718992384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 10:58:05.202646 139630718992384 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 10:58:05.215046 139630718992384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 10:58:05.215103 139630718992384 attention.py:418] Single window, no scan.
I0123 10:58:05.215139 139630718992384 transformer_layer.py:389] tlayer: self-attention.
I0123 10:58:05.215170 139630718992384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.215232 139630718992384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.215806 139630718992384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.215883 139630718992384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.216239 139630718992384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.216933 139630718992384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.219497 139630718992384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.220117 139630718992384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.220196 139630718992384 transformer_layer.py:468] tlayer: End windows.
I0123 10:58:05.220231 139630718992384 transformer_layer.py:472] tlayer: final FFN.
I0123 10:58:05.220291 139630718992384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.220426 139630718992384 transformer_base.py:410] tbase: post-attention MLP.
I0123 10:58:05.220537 139630718992384 nn_components.py:325] mlp: activation = None
I0123 10:58:05.220577 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 10:58:05.222489 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.222584 139630718992384 nn_components.py:261] mlp: residual
I0123 10:58:05.225011 139630718992384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.225091 139630718992384 transformer_base.py:443] tbase: final FFN
I0123 10:58:05.225202 139630718992384 nn_components.py:320] mlp: hidden 4096, relu
I0123 10:58:05.227679 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 10:58:05.229535 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.229631 139630718992384 nn_components.py:261] mlp: residual
I0123 10:58:05.229930 139630718992384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.230013 139630718992384 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 10:58:05.230122 139630718992384 transformer_layer.py:154] tlayer: recurrent = False
I0123 10:58:05.230161 139630718992384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 10:58:05.230192 139630718992384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 10:58:05.230257 139630718992384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.232487 139630718992384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 10:58:05.237948 139630718992384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.238212 139630718992384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 10:58:05.240899 139630718992384 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 10:58:05.253393 139630718992384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 10:58:05.253449 139630718992384 attention.py:418] Single window, no scan.
I0123 10:58:05.253485 139630718992384 transformer_layer.py:389] tlayer: self-attention.
I0123 10:58:05.253517 139630718992384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.253580 139630718992384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.254153 139630718992384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.254232 139630718992384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.254589 139630718992384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.255294 139630718992384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.257849 139630718992384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.258478 139630718992384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.258557 139630718992384 transformer_layer.py:468] tlayer: End windows.
I0123 10:58:05.258592 139630718992384 transformer_layer.py:472] tlayer: final FFN.
I0123 10:58:05.258652 139630718992384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.258782 139630718992384 transformer_base.py:410] tbase: post-attention MLP.
I0123 10:58:05.258898 139630718992384 nn_components.py:325] mlp: activation = None
I0123 10:58:05.258938 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 10:58:05.260814 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.260910 139630718992384 nn_components.py:261] mlp: residual
I0123 10:58:05.263327 139630718992384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.263409 139630718992384 transformer_base.py:443] tbase: final FFN
I0123 10:58:05.263519 139630718992384 nn_components.py:320] mlp: hidden 4096, relu
I0123 10:58:05.266171 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 10:58:05.268051 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.268150 139630718992384 nn_components.py:261] mlp: residual
I0123 10:58:05.268440 139630718992384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:05.268529 139630718992384 decoder_stack.py:344] dstack: Final layernorm.
I0123 10:58:05.271373 139630718992384 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 10:58:09.682160 139630718992384 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
I0123 10:58:10.240984 139630718992384 training_loop.py:409] No working directory specified.
I0123 10:58:10.241114 139630718992384 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0123 10:58:10.241952 139630718992384 checkpoints.py:1062] Restoring legacy Flax checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0123 10:58:13.584411 139630718992384 training_loop.py:447] Only restoring trainable parameters.
I0123 10:58:13.585021 139630718992384 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0123 10:58:13.585112 139630718992384 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0123 10:58:13.585164 139630718992384 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 10:58:13.585210 139630718992384 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 10:58:13.585253 139630718992384 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:58:13.585295 139630718992384 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 10:58:13.585337 139630718992384 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:58:13.585377 139630718992384 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:58:13.585417 139630718992384 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 10:58:13.585456 139630718992384 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 10:58:13.585496 139630718992384 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:58:13.585535 139630718992384 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 10:58:13.585572 139630718992384 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 10:58:13.585609 139630718992384 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 10:58:13.585653 139630718992384 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:58:13.585694 139630718992384 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 10:58:13.585732 139630718992384 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:58:13.585770 139630718992384 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:58:13.585806 139630718992384 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 10:58:13.585843 139630718992384 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 10:58:13.585895 139630718992384 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:58:13.585935 139630718992384 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 10:58:13.585973 139630718992384 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 10:58:13.586010 139630718992384 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 10:58:13.586047 139630718992384 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:58:13.586085 139630718992384 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 10:58:13.586122 139630718992384 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:58:13.586158 139630718992384 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:58:13.586197 139630718992384 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 10:58:13.586236 139630718992384 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 10:58:13.586273 139630718992384 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:58:13.586311 139630718992384 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 10:58:13.586349 139630718992384 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 10:58:13.586386 139630718992384 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 10:58:13.586424 139630718992384 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:58:13.586461 139630718992384 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 10:58:13.586498 139630718992384 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:58:13.586535 139630718992384 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:58:13.586571 139630718992384 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 10:58:13.586607 139630718992384 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 10:58:13.586643 139630718992384 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:58:13.586680 139630718992384 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 10:58:13.586717 139630718992384 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 10:58:13.586753 139630718992384 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 10:58:13.586789 139630718992384 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:58:13.586825 139630718992384 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 10:58:13.586867 139630718992384 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:58:13.586905 139630718992384 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:58:13.586941 139630718992384 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 10:58:13.586978 139630718992384 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 10:58:13.587014 139630718992384 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:58:13.587052 139630718992384 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 10:58:13.587088 139630718992384 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 10:58:13.587125 139630718992384 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 10:58:13.587161 139630718992384 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:58:13.587197 139630718992384 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 10:58:13.587233 139630718992384 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:58:13.587269 139630718992384 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:58:13.587307 139630718992384 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 10:58:13.587344 139630718992384 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 10:58:13.587381 139630718992384 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:58:13.587418 139630718992384 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 10:58:13.587455 139630718992384 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 10:58:13.587491 139630718992384 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 10:58:13.587529 139630718992384 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:58:13.587565 139630718992384 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 10:58:13.587601 139630718992384 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:58:13.587638 139630718992384 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:58:13.587674 139630718992384 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 10:58:13.587712 139630718992384 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 10:58:13.587749 139630718992384 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:58:13.587785 139630718992384 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 10:58:13.587823 139630718992384 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 10:58:13.587866 139630718992384 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 10:58:13.587905 139630718992384 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:58:13.587942 139630718992384 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 10:58:13.587979 139630718992384 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:58:13.588015 139630718992384 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:58:13.588052 139630718992384 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 10:58:13.588089 139630718992384 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 10:58:13.588126 139630718992384 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:58:13.588162 139630718992384 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 10:58:13.588199 139630718992384 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 10:58:13.588235 139630718992384 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 10:58:13.588273 139630718992384 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:58:13.588309 139630718992384 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 10:58:13.588345 139630718992384 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:58:13.588382 139630718992384 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:58:13.588418 139630718992384 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 10:58:13.588455 139630718992384 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 10:58:13.588492 139630718992384 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:58:13.588529 139630718992384 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 10:58:13.588566 139630718992384 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 10:58:13.588603 139630718992384 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 10:58:13.588640 139630718992384 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:58:13.588677 139630718992384 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 10:58:13.588715 139630718992384 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:58:13.588752 139630718992384 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:58:13.588791 139630718992384 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 10:58:13.588827 139630718992384 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 10:58:13.588870 139630718992384 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:58:13.588909 139630718992384 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 10:58:13.588946 139630718992384 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 10:58:13.588983 139630718992384 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 10:58:13.589019 139630718992384 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:58:13.589056 139630718992384 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 10:58:13.589093 139630718992384 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:58:13.589130 139630718992384 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:58:13.589167 139630718992384 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 10:58:13.589204 139630718992384 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 10:58:13.589241 139630718992384 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:58:13.589277 139630718992384 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 10:58:13.589313 139630718992384 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 10:58:13.589350 139630718992384 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 10:58:13.589387 139630718992384 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:58:13.589423 139630718992384 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 10:58:13.589459 139630718992384 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:58:13.589496 139630718992384 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:58:13.589532 139630718992384 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 10:58:13.589569 139630718992384 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 10:58:13.589604 139630718992384 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 10:58:13.589649 139630718992384 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 10:58:13.589680 139630718992384 training_loop.py:725] Total parameters: 152072288
I0123 10:58:13.589903 139630718992384 training_loop.py:739] Total state size: 0
I0123 10:58:13.615525 139630718992384 training_loop.py:492] Training loop: creating task for mode beam_search
I0123 10:58:13.615749 139630718992384 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0123 10:58:13.616175 139630718992384 training_loop.py:652] Compiling mode beam_search with jit.
I0123 10:58:13.616512 139630718992384 training_loop.py:89] registering functions: dict_keys([])
I0123 10:58:13.633052 139630718992384 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g b f, on_line g c e; h = lc_tangent h a g, on_line h f g; i = on_line i a h, on_line i c g; j = on_line j a g, on_line j b i; k = on_circle k d b; l = circle l k g i; m = circle m k j h; n = foot n k l m; o = mirror o k n; p = foot p k c g; q = mirror q k p; r = on_line r b a, on_line r c q ? coll o r k
I0123 10:58:18.871461 139630718992384 ddar.py:60] Depth 1/1000 time = 5.173426866531372
I0123 10:58:28.497138 139630718992384 ddar.py:60] Depth 2/1000 time = 9.625425815582275
I0123 10:58:39.722474 139630718992384 ddar.py:60] Depth 3/1000 time = 11.225005626678467
I0123 10:58:50.555644 139630718992384 ddar.py:60] Depth 4/1000 time = 10.832894086837769
I0123 10:59:01.201308 139630718992384 ddar.py:60] Depth 5/1000 time = 10.645354509353638
I0123 10:59:11.978511 139630718992384 ddar.py:60] Depth 6/1000 time = 10.776902914047241
I0123 10:59:22.854173 139630718992384 ddar.py:60] Depth 7/1000 time = 10.875091314315796
I0123 10:59:33.755254 139630718992384 ddar.py:60] Depth 8/1000 time = 10.8956298828125
I0123 10:59:45.247215 139630718992384 ddar.py:60] Depth 9/1000 time = 11.44703459739685
I0123 10:59:56.741374 139630718992384 ddar.py:60] Depth 10/1000 time = 11.493853330612183
I0123 11:00:09.748914 139630718992384 ddar.py:60] Depth 11/1000 time = 12.987308263778687
I0123 11:00:23.179030 139630718992384 ddar.py:60] Depth 12/1000 time = 13.429784059524536
I0123 11:00:41.452188 139630718992384 ddar.py:60] Depth 13/1000 time = 18.272912740707397
I0123 11:00:59.967610 139630718992384 ddar.py:60] Depth 14/1000 time = 18.515138387680054
I0123 11:01:22.793816 139630718992384 ddar.py:60] Depth 15/1000 time = 22.82590389251709
I0123 11:01:45.938856 139630718992384 ddar.py:60] Depth 16/1000 time = 23.144651651382446
I0123 11:02:08.776934 139630718992384 ddar.py:60] Depth 17/1000 time = 22.813666105270386
I0123 11:02:33.086566 139630718992384 ddar.py:60] Depth 18/1000 time = 24.309213876724243
I0123 11:02:57.067202 139630718992384 ddar.py:60] Depth 19/1000 time = 23.98005437850952
I0123 11:03:21.473724 139630718992384 ddar.py:60] Depth 20/1000 time = 24.381184339523315
I0123 11:03:46.844044 139630718992384 ddar.py:60] Depth 21/1000 time = 25.284892082214355
I0123 11:04:13.469962 139630718992384 ddar.py:60] Depth 22/1000 time = 26.62533712387085
I0123 11:04:40.482337 139630718992384 ddar.py:60] Depth 23/1000 time = 27.0119149684906
I0123 11:05:07.267944 139630718992384 ddar.py:60] Depth 24/1000 time = 26.73945426940918
I0123 11:05:07.268964 139630718992384 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:05:07.269115 139630718992384 alphageometry.py:540] Depth 0. There are 1 nodes to expand:
I0123 11:05:07.269158 139630718992384 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a b e 02 ^ c a c e c e c b 03 ; f : C a c f 04 ^ b a b f b f b c 05 ; g : C b f g 06 C c e g 07 ; h : C f g h 08 T a g a h 09 ; i : C a h i 10 C c g i 11 ; j : C a g j 12 C b i j 13 ; k : D b d d k 14 ; l : D g l i l 15 D g l k l 16 ; m : D h m j m 17 D j m k m 18 ; n : C l m n 19 T k n l m 20 ; o : C k n o 21 D k n n o 22 ; p : C c g p 23 T c g k p 24 ; q : C k p q 25 D k p p q 26 ; r : C a b r 27 C c q r 28 ? C o r k {F1} x00
I0123 11:05:07.269194 139630718992384 alphageometry.py:549] Decoding from {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a b e 02 ^ c a c e c e c b 03 ; f : C a c f 04 ^ b a b f b f b c 05 ; g : C b f g 06 C c e g 07 ; h : C f g h 08 T a g a h 09 ; i : C a h i 10 C c g i 11 ; j : C a g j 12 C b i j 13 ; k : D b d d k 14 ; l : D g l i l 15 D g l k l 16 ; m : D h m j m 17 D j m k m 18 ; n : C l m n 19 T k n l m 20 ; o : C k n o 21 D k n n o 22 ; p : C c g p 23 T c g k p 24 ; q : C k p q 25 D k p p q 26 ; r : C a b r 27 C c q r 28 ? C o r k {F1} x00
I0123 11:05:07.414055 139630718992384 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.414249 139630718992384 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 11:05:07.414360 139630718992384 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 11:05:07.414443 139630718992384 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 11:05:07.414519 139630718992384 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 11:05:07.414595 139630718992384 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 11:05:07.414670 139630718992384 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 11:05:07.414744 139630718992384 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 11:05:07.414816 139630718992384 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 11:05:07.414888 139630718992384 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 11:05:07.414959 139630718992384 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 11:05:07.415029 139630718992384 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 11:05:07.415099 139630718992384 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 11:05:07.415169 139630718992384 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 11:05:07.415210 139630718992384 attention.py:418] Single window, no scan.
I0123 11:05:07.415257 139630718992384 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 11:05:07.415374 139630718992384 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:05:07.415417 139630718992384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:05:07.415450 139630718992384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:05:07.417474 139630718992384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.420067 139630718992384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:05:07.425923 139630718992384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.426203 139630718992384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:05:07.429068 139630718992384 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:05:07.433005 139630718992384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:05:07.433066 139630718992384 attention.py:418] Single window, no scan.
I0123 11:05:07.433107 139630718992384 transformer_layer.py:389] tlayer: self-attention.
I0123 11:05:07.433141 139630718992384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.433207 139630718992384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.434301 139630718992384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.434386 139630718992384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.434775 139630718992384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.435590 139630718992384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.438214 139630718992384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.438854 139630718992384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.438939 139630718992384 transformer_layer.py:468] tlayer: End windows.
I0123 11:05:07.438986 139630718992384 transformer_layer.py:472] tlayer: final FFN.
I0123 11:05:07.439052 139630718992384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.439189 139630718992384 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:05:07.439531 139630718992384 nn_components.py:325] mlp: activation = None
I0123 11:05:07.439580 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 11:05:07.441636 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.441746 139630718992384 nn_components.py:261] mlp: residual
I0123 11:05:07.444298 139630718992384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.444380 139630718992384 transformer_base.py:443] tbase: final FFN
I0123 11:05:07.444802 139630718992384 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:05:07.447188 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 11:05:07.449173 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.449272 139630718992384 nn_components.py:261] mlp: residual
I0123 11:05:07.449566 139630718992384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.449657 139630718992384 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 11:05:07.449769 139630718992384 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:05:07.449810 139630718992384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:05:07.449842 139630718992384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:05:07.451915 139630718992384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.454516 139630718992384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:05:07.460162 139630718992384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.460434 139630718992384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:05:07.463187 139630718992384 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:05:07.466849 139630718992384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:05:07.466908 139630718992384 attention.py:418] Single window, no scan.
I0123 11:05:07.466946 139630718992384 transformer_layer.py:389] tlayer: self-attention.
I0123 11:05:07.466978 139630718992384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.467042 139630718992384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.467612 139630718992384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.467690 139630718992384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.468052 139630718992384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.468824 139630718992384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.471322 139630718992384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.472001 139630718992384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.472082 139630718992384 transformer_layer.py:468] tlayer: End windows.
I0123 11:05:07.472119 139630718992384 transformer_layer.py:472] tlayer: final FFN.
I0123 11:05:07.472179 139630718992384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.472320 139630718992384 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:05:07.472640 139630718992384 nn_components.py:325] mlp: activation = None
I0123 11:05:07.472686 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 11:05:07.474623 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.474720 139630718992384 nn_components.py:261] mlp: residual
I0123 11:05:07.477185 139630718992384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.477267 139630718992384 transformer_base.py:443] tbase: final FFN
I0123 11:05:07.477697 139630718992384 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:05:07.480052 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 11:05:07.481972 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.482071 139630718992384 nn_components.py:261] mlp: residual
I0123 11:05:07.482365 139630718992384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.482448 139630718992384 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 11:05:07.482558 139630718992384 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:05:07.482597 139630718992384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:05:07.482628 139630718992384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:05:07.484394 139630718992384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.486716 139630718992384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:05:07.492382 139630718992384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.492644 139630718992384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:05:07.495239 139630718992384 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:05:07.498902 139630718992384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:05:07.498961 139630718992384 attention.py:418] Single window, no scan.
I0123 11:05:07.498999 139630718992384 transformer_layer.py:389] tlayer: self-attention.
I0123 11:05:07.499031 139630718992384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.499096 139630718992384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.499718 139630718992384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.499798 139630718992384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.500163 139630718992384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.500935 139630718992384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.503412 139630718992384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.504037 139630718992384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.504116 139630718992384 transformer_layer.py:468] tlayer: End windows.
I0123 11:05:07.504152 139630718992384 transformer_layer.py:472] tlayer: final FFN.
I0123 11:05:07.504212 139630718992384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.504352 139630718992384 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:05:07.504667 139630718992384 nn_components.py:325] mlp: activation = None
I0123 11:05:07.504713 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 11:05:07.506689 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.506791 139630718992384 nn_components.py:261] mlp: residual
I0123 11:05:07.509308 139630718992384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.509389 139630718992384 transformer_base.py:443] tbase: final FFN
I0123 11:05:07.509820 139630718992384 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:05:07.512105 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 11:05:07.514045 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.514145 139630718992384 nn_components.py:261] mlp: residual
I0123 11:05:07.514441 139630718992384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.514526 139630718992384 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 11:05:07.514636 139630718992384 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:05:07.514676 139630718992384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:05:07.514708 139630718992384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:05:07.516602 139630718992384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.518976 139630718992384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:05:07.524630 139630718992384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.524889 139630718992384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:05:07.527512 139630718992384 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:05:07.531255 139630718992384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:05:07.531315 139630718992384 attention.py:418] Single window, no scan.
I0123 11:05:07.531353 139630718992384 transformer_layer.py:389] tlayer: self-attention.
I0123 11:05:07.531386 139630718992384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.531451 139630718992384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.532027 139630718992384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.532104 139630718992384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.532463 139630718992384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.533234 139630718992384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.535757 139630718992384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.536385 139630718992384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.536468 139630718992384 transformer_layer.py:468] tlayer: End windows.
I0123 11:05:07.536504 139630718992384 transformer_layer.py:472] tlayer: final FFN.
I0123 11:05:07.536564 139630718992384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.536695 139630718992384 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:05:07.537072 139630718992384 nn_components.py:325] mlp: activation = None
I0123 11:05:07.537119 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 11:05:07.539090 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.539192 139630718992384 nn_components.py:261] mlp: residual
I0123 11:05:07.541707 139630718992384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.541789 139630718992384 transformer_base.py:443] tbase: final FFN
I0123 11:05:07.542211 139630718992384 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:05:07.544530 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 11:05:07.546878 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.546977 139630718992384 nn_components.py:261] mlp: residual
I0123 11:05:07.547275 139630718992384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.547359 139630718992384 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 11:05:07.547470 139630718992384 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:05:07.547509 139630718992384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:05:07.547541 139630718992384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:05:07.549339 139630718992384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.551685 139630718992384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:05:07.557780 139630718992384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.558042 139630718992384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:05:07.560622 139630718992384 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:05:07.564313 139630718992384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:05:07.564371 139630718992384 attention.py:418] Single window, no scan.
I0123 11:05:07.564407 139630718992384 transformer_layer.py:389] tlayer: self-attention.
I0123 11:05:07.564439 139630718992384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.564501 139630718992384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.565119 139630718992384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.565199 139630718992384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.565564 139630718992384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.566334 139630718992384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.568816 139630718992384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.569433 139630718992384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.569513 139630718992384 transformer_layer.py:468] tlayer: End windows.
I0123 11:05:07.569549 139630718992384 transformer_layer.py:472] tlayer: final FFN.
I0123 11:05:07.569609 139630718992384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.569745 139630718992384 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:05:07.570058 139630718992384 nn_components.py:325] mlp: activation = None
I0123 11:05:07.570110 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 11:05:07.572085 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.572182 139630718992384 nn_components.py:261] mlp: residual
I0123 11:05:07.574637 139630718992384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.574719 139630718992384 transformer_base.py:443] tbase: final FFN
I0123 11:05:07.575138 139630718992384 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:05:07.577475 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 11:05:07.579386 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.579485 139630718992384 nn_components.py:261] mlp: residual
I0123 11:05:07.579778 139630718992384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.579862 139630718992384 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 11:05:07.579973 139630718992384 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:05:07.580013 139630718992384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:05:07.580045 139630718992384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:05:07.581922 139630718992384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.584245 139630718992384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:05:07.589843 139630718992384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.590107 139630718992384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:05:07.592690 139630718992384 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:05:07.596383 139630718992384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:05:07.596441 139630718992384 attention.py:418] Single window, no scan.
I0123 11:05:07.596478 139630718992384 transformer_layer.py:389] tlayer: self-attention.
I0123 11:05:07.596509 139630718992384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.596572 139630718992384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.597133 139630718992384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.597212 139630718992384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.597571 139630718992384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.598347 139630718992384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.600837 139630718992384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.601458 139630718992384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.601538 139630718992384 transformer_layer.py:468] tlayer: End windows.
I0123 11:05:07.601575 139630718992384 transformer_layer.py:472] tlayer: final FFN.
I0123 11:05:07.601635 139630718992384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.601843 139630718992384 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:05:07.602218 139630718992384 nn_components.py:325] mlp: activation = None
I0123 11:05:07.602265 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 11:05:07.604207 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.604305 139630718992384 nn_components.py:261] mlp: residual
I0123 11:05:07.606789 139630718992384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.606873 139630718992384 transformer_base.py:443] tbase: final FFN
I0123 11:05:07.607303 139630718992384 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:05:07.609586 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 11:05:07.611596 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.611696 139630718992384 nn_components.py:261] mlp: residual
I0123 11:05:07.611994 139630718992384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.612078 139630718992384 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 11:05:07.612189 139630718992384 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:05:07.612229 139630718992384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:05:07.612262 139630718992384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:05:07.614081 139630718992384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.616408 139630718992384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:05:07.622075 139630718992384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.622340 139630718992384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:05:07.624906 139630718992384 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:05:07.628528 139630718992384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:05:07.628587 139630718992384 attention.py:418] Single window, no scan.
I0123 11:05:07.628623 139630718992384 transformer_layer.py:389] tlayer: self-attention.
I0123 11:05:07.628655 139630718992384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.628717 139630718992384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.629325 139630718992384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.629405 139630718992384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.629772 139630718992384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.630532 139630718992384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.632979 139630718992384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.633593 139630718992384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.633680 139630718992384 transformer_layer.py:468] tlayer: End windows.
I0123 11:05:07.633716 139630718992384 transformer_layer.py:472] tlayer: final FFN.
I0123 11:05:07.633775 139630718992384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.633907 139630718992384 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:05:07.634223 139630718992384 nn_components.py:325] mlp: activation = None
I0123 11:05:07.634268 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 11:05:07.636223 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.636329 139630718992384 nn_components.py:261] mlp: residual
I0123 11:05:07.638796 139630718992384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.638879 139630718992384 transformer_base.py:443] tbase: final FFN
I0123 11:05:07.639295 139630718992384 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:05:07.641533 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 11:05:07.643491 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.643593 139630718992384 nn_components.py:261] mlp: residual
I0123 11:05:07.643895 139630718992384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.643981 139630718992384 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 11:05:07.644095 139630718992384 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:05:07.644136 139630718992384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:05:07.644170 139630718992384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:05:07.646052 139630718992384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.648445 139630718992384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:05:07.654260 139630718992384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.654529 139630718992384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:05:07.657368 139630718992384 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:05:07.661630 139630718992384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:05:07.661695 139630718992384 attention.py:418] Single window, no scan.
I0123 11:05:07.661733 139630718992384 transformer_layer.py:389] tlayer: self-attention.
I0123 11:05:07.661764 139630718992384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.661827 139630718992384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.662391 139630718992384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.662473 139630718992384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.662849 139630718992384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.663643 139630718992384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.666128 139630718992384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.666774 139630718992384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.666859 139630718992384 transformer_layer.py:468] tlayer: End windows.
I0123 11:05:07.666898 139630718992384 transformer_layer.py:472] tlayer: final FFN.
I0123 11:05:07.666961 139630718992384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.667097 139630718992384 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:05:07.667485 139630718992384 nn_components.py:325] mlp: activation = None
I0123 11:05:07.667534 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 11:05:07.669475 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.669572 139630718992384 nn_components.py:261] mlp: residual
I0123 11:05:07.672124 139630718992384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.672207 139630718992384 transformer_base.py:443] tbase: final FFN
I0123 11:05:07.672626 139630718992384 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:05:07.674923 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 11:05:07.676951 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.677050 139630718992384 nn_components.py:261] mlp: residual
I0123 11:05:07.677345 139630718992384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.677431 139630718992384 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 11:05:07.677541 139630718992384 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:05:07.677581 139630718992384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:05:07.677613 139630718992384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:05:07.679443 139630718992384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.681770 139630718992384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:05:07.687490 139630718992384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.687753 139630718992384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:05:07.690341 139630718992384 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:05:07.693989 139630718992384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:05:07.694047 139630718992384 attention.py:418] Single window, no scan.
I0123 11:05:07.694084 139630718992384 transformer_layer.py:389] tlayer: self-attention.
I0123 11:05:07.694116 139630718992384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.694180 139630718992384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.694799 139630718992384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.694879 139630718992384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.695241 139630718992384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.696013 139630718992384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.698488 139630718992384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.699115 139630718992384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.699195 139630718992384 transformer_layer.py:468] tlayer: End windows.
I0123 11:05:07.699231 139630718992384 transformer_layer.py:472] tlayer: final FFN.
I0123 11:05:07.699291 139630718992384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.699423 139630718992384 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:05:07.699741 139630718992384 nn_components.py:325] mlp: activation = None
I0123 11:05:07.699786 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 11:05:07.701698 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.701795 139630718992384 nn_components.py:261] mlp: residual
I0123 11:05:07.704365 139630718992384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.704455 139630718992384 transformer_base.py:443] tbase: final FFN
I0123 11:05:07.704872 139630718992384 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:05:07.707174 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 11:05:07.709123 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.709221 139630718992384 nn_components.py:261] mlp: residual
I0123 11:05:07.709517 139630718992384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.709602 139630718992384 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 11:05:07.709717 139630718992384 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:05:07.709758 139630718992384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:05:07.709790 139630718992384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:05:07.711570 139630718992384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.713975 139630718992384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:05:07.719559 139630718992384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.719824 139630718992384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:05:07.722413 139630718992384 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:05:07.726041 139630718992384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:05:07.726099 139630718992384 attention.py:418] Single window, no scan.
I0123 11:05:07.726135 139630718992384 transformer_layer.py:389] tlayer: self-attention.
I0123 11:05:07.726169 139630718992384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.726235 139630718992384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.726871 139630718992384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.726954 139630718992384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.727328 139630718992384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.728114 139630718992384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.730617 139630718992384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.731263 139630718992384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.731346 139630718992384 transformer_layer.py:468] tlayer: End windows.
I0123 11:05:07.731383 139630718992384 transformer_layer.py:472] tlayer: final FFN.
I0123 11:05:07.731446 139630718992384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.731581 139630718992384 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:05:07.731909 139630718992384 nn_components.py:325] mlp: activation = None
I0123 11:05:07.731955 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 11:05:07.733875 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.733973 139630718992384 nn_components.py:261] mlp: residual
I0123 11:05:07.736490 139630718992384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.736579 139630718992384 transformer_base.py:443] tbase: final FFN
I0123 11:05:07.737005 139630718992384 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:05:07.739290 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 11:05:07.741203 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.741302 139630718992384 nn_components.py:261] mlp: residual
I0123 11:05:07.741596 139630718992384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.741688 139630718992384 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 11:05:07.741798 139630718992384 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:05:07.741837 139630718992384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:05:07.741870 139630718992384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:05:07.743658 139630718992384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.746058 139630718992384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:05:07.751668 139630718992384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.751930 139630718992384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:05:07.754506 139630718992384 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:05:07.758257 139630718992384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:05:07.758315 139630718992384 attention.py:418] Single window, no scan.
I0123 11:05:07.758352 139630718992384 transformer_layer.py:389] tlayer: self-attention.
I0123 11:05:07.758384 139630718992384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.758449 139630718992384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.759068 139630718992384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.759148 139630718992384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.759509 139630718992384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.760271 139630718992384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.762785 139630718992384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.763437 139630718992384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.763520 139630718992384 transformer_layer.py:468] tlayer: End windows.
I0123 11:05:07.763558 139630718992384 transformer_layer.py:472] tlayer: final FFN.
I0123 11:05:07.763619 139630718992384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.763756 139630718992384 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:05:07.764083 139630718992384 nn_components.py:325] mlp: activation = None
I0123 11:05:07.764130 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 11:05:07.766063 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.766161 139630718992384 nn_components.py:261] mlp: residual
I0123 11:05:07.768705 139630718992384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.768787 139630718992384 transformer_base.py:443] tbase: final FFN
I0123 11:05:07.769218 139630718992384 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:05:07.771482 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 11:05:07.773411 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.773510 139630718992384 nn_components.py:261] mlp: residual
I0123 11:05:07.773817 139630718992384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.773901 139630718992384 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 11:05:07.774011 139630718992384 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:05:07.774051 139630718992384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:05:07.774082 139630718992384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:05:07.775872 139630718992384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.778629 139630718992384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:05:07.784210 139630718992384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.784471 139630718992384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:05:07.787047 139630718992384 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:05:07.790658 139630718992384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:05:07.790716 139630718992384 attention.py:418] Single window, no scan.
I0123 11:05:07.790752 139630718992384 transformer_layer.py:389] tlayer: self-attention.
I0123 11:05:07.790785 139630718992384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.790848 139630718992384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.791462 139630718992384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.791541 139630718992384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.791902 139630718992384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.792665 139630718992384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.795122 139630718992384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.795752 139630718992384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.795832 139630718992384 transformer_layer.py:468] tlayer: End windows.
I0123 11:05:07.795868 139630718992384 transformer_layer.py:472] tlayer: final FFN.
I0123 11:05:07.795928 139630718992384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.796058 139630718992384 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:05:07.796374 139630718992384 nn_components.py:325] mlp: activation = None
I0123 11:05:07.796419 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 11:05:07.798336 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.798433 139630718992384 nn_components.py:261] mlp: residual
I0123 11:05:07.800942 139630718992384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.801025 139630718992384 transformer_base.py:443] tbase: final FFN
I0123 11:05:07.801448 139630718992384 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:05:07.803698 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 11:05:07.805601 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.805707 139630718992384 nn_components.py:261] mlp: residual
I0123 11:05:07.806001 139630718992384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.806254 139630718992384 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 11:05:07.806325 139630718992384 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 11:05:07.806384 139630718992384 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 11:05:07.806440 139630718992384 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 11:05:07.806494 139630718992384 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 11:05:07.806548 139630718992384 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 11:05:07.806602 139630718992384 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 11:05:07.806656 139630718992384 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 11:05:07.806709 139630718992384 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 11:05:07.806763 139630718992384 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 11:05:07.806817 139630718992384 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 11:05:07.806871 139630718992384 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 11:05:07.806908 139630718992384 decoder_stack.py:344] dstack: Final layernorm.
I0123 11:05:07.809807 139630718992384 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:05:07.854568 139630718992384 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:07.854657 139630718992384 decoder_stack.py:333] dstack: autoregressive generator.
I0123 11:05:07.854713 139630718992384 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 11:05:07.854821 139630718992384 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:05:07.854861 139630718992384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:05:07.854894 139630718992384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:05:07.854959 139630718992384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:07.857358 139630718992384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:05:07.862943 139630718992384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:07.863220 139630718992384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:05:07.865863 139630718992384 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:05:07.879101 139630718992384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:05:07.879160 139630718992384 attention.py:418] Single window, no scan.
I0123 11:05:07.879196 139630718992384 transformer_layer.py:389] tlayer: self-attention.
I0123 11:05:07.879228 139630718992384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:07.879291 139630718992384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:07.879862 139630718992384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:07.879943 139630718992384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:07.880330 139630718992384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:07.881038 139630718992384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:07.883703 139630718992384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:07.884358 139630718992384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:07.884443 139630718992384 transformer_layer.py:468] tlayer: End windows.
I0123 11:05:07.884480 139630718992384 transformer_layer.py:472] tlayer: final FFN.
I0123 11:05:07.884542 139630718992384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:07.884680 139630718992384 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:05:07.884796 139630718992384 nn_components.py:325] mlp: activation = None
I0123 11:05:07.884838 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 11:05:07.886791 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:07.886894 139630718992384 nn_components.py:261] mlp: residual
I0123 11:05:07.889382 139630718992384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:07.889468 139630718992384 transformer_base.py:443] tbase: final FFN
I0123 11:05:07.889583 139630718992384 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:05:07.891928 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 11:05:07.893815 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:07.893916 139630718992384 nn_components.py:261] mlp: residual
I0123 11:05:07.894217 139630718992384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:07.894303 139630718992384 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 11:05:07.894417 139630718992384 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:05:07.894458 139630718992384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:05:07.894490 139630718992384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:05:07.894557 139630718992384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:07.896879 139630718992384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:05:07.902454 139630718992384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:07.902729 139630718992384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:05:07.905461 139630718992384 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:05:07.918156 139630718992384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:05:07.918215 139630718992384 attention.py:418] Single window, no scan.
I0123 11:05:07.918254 139630718992384 transformer_layer.py:389] tlayer: self-attention.
I0123 11:05:07.918288 139630718992384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:07.918357 139630718992384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:07.918945 139630718992384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:07.919028 139630718992384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:07.919413 139630718992384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:07.920177 139630718992384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:07.922694 139630718992384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:07.923341 139630718992384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:07.923424 139630718992384 transformer_layer.py:468] tlayer: End windows.
I0123 11:05:07.923461 139630718992384 transformer_layer.py:472] tlayer: final FFN.
I0123 11:05:07.923525 139630718992384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:07.923662 139630718992384 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:05:07.923779 139630718992384 nn_components.py:325] mlp: activation = None
I0123 11:05:07.923821 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 11:05:07.925696 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:07.925794 139630718992384 nn_components.py:261] mlp: residual
I0123 11:05:07.928277 139630718992384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:07.928359 139630718992384 transformer_base.py:443] tbase: final FFN
I0123 11:05:07.928470 139630718992384 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:05:07.930764 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 11:05:07.932702 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:07.932804 139630718992384 nn_components.py:261] mlp: residual
I0123 11:05:07.933110 139630718992384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:07.933198 139630718992384 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 11:05:07.933311 139630718992384 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:05:07.933354 139630718992384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:05:07.933387 139630718992384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:05:07.933456 139630718992384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:07.935821 139630718992384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:05:07.941389 139630718992384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:07.941670 139630718992384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:05:07.944451 139630718992384 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:05:07.957048 139630718992384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:05:07.957107 139630718992384 attention.py:418] Single window, no scan.
I0123 11:05:07.957144 139630718992384 transformer_layer.py:389] tlayer: self-attention.
I0123 11:05:07.957176 139630718992384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:07.957240 139630718992384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:07.957816 139630718992384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:07.957899 139630718992384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:07.958276 139630718992384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:07.959486 139630718992384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:07.962029 139630718992384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:07.962680 139630718992384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:07.962764 139630718992384 transformer_layer.py:468] tlayer: End windows.
I0123 11:05:07.962801 139630718992384 transformer_layer.py:472] tlayer: final FFN.
I0123 11:05:07.962865 139630718992384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:07.963003 139630718992384 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:05:07.963118 139630718992384 nn_components.py:325] mlp: activation = None
I0123 11:05:07.963160 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 11:05:07.965040 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:07.965137 139630718992384 nn_components.py:261] mlp: residual
I0123 11:05:07.967634 139630718992384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:07.967720 139630718992384 transformer_base.py:443] tbase: final FFN
I0123 11:05:07.967833 139630718992384 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:05:07.970111 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 11:05:07.971967 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:07.972066 139630718992384 nn_components.py:261] mlp: residual
I0123 11:05:07.972553 139630718992384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:07.972637 139630718992384 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 11:05:07.972749 139630718992384 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:05:07.972790 139630718992384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:05:07.972822 139630718992384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:05:07.972888 139630718992384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:07.975132 139630718992384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:05:07.980541 139630718992384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:07.980807 139630718992384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:05:07.983562 139630718992384 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:05:07.996234 139630718992384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:05:07.996293 139630718992384 attention.py:418] Single window, no scan.
I0123 11:05:07.996330 139630718992384 transformer_layer.py:389] tlayer: self-attention.
I0123 11:05:07.996361 139630718992384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:07.996425 139630718992384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:07.996988 139630718992384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:07.997068 139630718992384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:07.997429 139630718992384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:07.998196 139630718992384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.000656 139630718992384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.001279 139630718992384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.001360 139630718992384 transformer_layer.py:468] tlayer: End windows.
I0123 11:05:08.001397 139630718992384 transformer_layer.py:472] tlayer: final FFN.
I0123 11:05:08.001459 139630718992384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.001591 139630718992384 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:05:08.001708 139630718992384 nn_components.py:325] mlp: activation = None
I0123 11:05:08.001749 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 11:05:08.003619 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.003717 139630718992384 nn_components.py:261] mlp: residual
I0123 11:05:08.006161 139630718992384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.006245 139630718992384 transformer_base.py:443] tbase: final FFN
I0123 11:05:08.006357 139630718992384 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:05:08.008669 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 11:05:08.010578 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.010682 139630718992384 nn_components.py:261] mlp: residual
I0123 11:05:08.010992 139630718992384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.011080 139630718992384 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 11:05:08.011198 139630718992384 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:05:08.011240 139630718992384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:05:08.011274 139630718992384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:05:08.011341 139630718992384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.013619 139630718992384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:05:08.019123 139630718992384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.019401 139630718992384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:05:08.022146 139630718992384 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:05:08.034546 139630718992384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:05:08.034605 139630718992384 attention.py:418] Single window, no scan.
I0123 11:05:08.034642 139630718992384 transformer_layer.py:389] tlayer: self-attention.
I0123 11:05:08.034673 139630718992384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.034735 139630718992384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.035295 139630718992384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.035375 139630718992384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.035740 139630718992384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.036497 139630718992384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.039032 139630718992384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.039672 139630718992384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.039753 139630718992384 transformer_layer.py:468] tlayer: End windows.
I0123 11:05:08.039790 139630718992384 transformer_layer.py:472] tlayer: final FFN.
I0123 11:05:08.039850 139630718992384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.039985 139630718992384 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:05:08.040097 139630718992384 nn_components.py:325] mlp: activation = None
I0123 11:05:08.040136 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 11:05:08.042034 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.042134 139630718992384 nn_components.py:261] mlp: residual
I0123 11:05:08.044570 139630718992384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.044654 139630718992384 transformer_base.py:443] tbase: final FFN
I0123 11:05:08.044765 139630718992384 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:05:08.047085 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 11:05:08.048974 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.049074 139630718992384 nn_components.py:261] mlp: residual
I0123 11:05:08.049375 139630718992384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.049461 139630718992384 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 11:05:08.049574 139630718992384 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:05:08.049615 139630718992384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:05:08.049654 139630718992384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:05:08.049722 139630718992384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.052007 139630718992384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:05:08.057492 139630718992384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.057765 139630718992384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:05:08.060488 139630718992384 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:05:08.073050 139630718992384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:05:08.073110 139630718992384 attention.py:418] Single window, no scan.
I0123 11:05:08.073147 139630718992384 transformer_layer.py:389] tlayer: self-attention.
I0123 11:05:08.073180 139630718992384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.073245 139630718992384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.073812 139630718992384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.073893 139630718992384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.074257 139630718992384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.075407 139630718992384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.077944 139630718992384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.078575 139630718992384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.078656 139630718992384 transformer_layer.py:468] tlayer: End windows.
I0123 11:05:08.078692 139630718992384 transformer_layer.py:472] tlayer: final FFN.
I0123 11:05:08.078752 139630718992384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.078884 139630718992384 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:05:08.078994 139630718992384 nn_components.py:325] mlp: activation = None
I0123 11:05:08.079033 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 11:05:08.080898 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.080997 139630718992384 nn_components.py:261] mlp: residual
I0123 11:05:08.083441 139630718992384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.083525 139630718992384 transformer_base.py:443] tbase: final FFN
I0123 11:05:08.083637 139630718992384 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:05:08.085901 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 11:05:08.087762 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.087859 139630718992384 nn_components.py:261] mlp: residual
I0123 11:05:08.088153 139630718992384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.088238 139630718992384 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 11:05:08.088498 139630718992384 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:05:08.088538 139630718992384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:05:08.088570 139630718992384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:05:08.088634 139630718992384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.090916 139630718992384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:05:08.096405 139630718992384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.096670 139630718992384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:05:08.099397 139630718992384 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:05:08.111820 139630718992384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:05:08.111878 139630718992384 attention.py:418] Single window, no scan.
I0123 11:05:08.111915 139630718992384 transformer_layer.py:389] tlayer: self-attention.
I0123 11:05:08.111948 139630718992384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.112012 139630718992384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.112575 139630718992384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.112655 139630718992384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.113017 139630718992384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.113771 139630718992384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.116255 139630718992384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.116885 139630718992384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.116966 139630718992384 transformer_layer.py:468] tlayer: End windows.
I0123 11:05:08.117002 139630718992384 transformer_layer.py:472] tlayer: final FFN.
I0123 11:05:08.117062 139630718992384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.117194 139630718992384 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:05:08.117304 139630718992384 nn_components.py:325] mlp: activation = None
I0123 11:05:08.117345 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 11:05:08.119239 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.119337 139630718992384 nn_components.py:261] mlp: residual
I0123 11:05:08.121771 139630718992384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.121856 139630718992384 transformer_base.py:443] tbase: final FFN
I0123 11:05:08.121967 139630718992384 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:05:08.124250 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 11:05:08.126140 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.126241 139630718992384 nn_components.py:261] mlp: residual
I0123 11:05:08.126541 139630718992384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.126626 139630718992384 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 11:05:08.126738 139630718992384 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:05:08.126778 139630718992384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:05:08.126810 139630718992384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:05:08.126874 139630718992384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.129112 139630718992384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:05:08.134568 139630718992384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.134836 139630718992384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:05:08.137532 139630718992384 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:05:08.150173 139630718992384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:05:08.150233 139630718992384 attention.py:418] Single window, no scan.
I0123 11:05:08.150271 139630718992384 transformer_layer.py:389] tlayer: self-attention.
I0123 11:05:08.150305 139630718992384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.150371 139630718992384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.150956 139630718992384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.151038 139630718992384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.151412 139630718992384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.152181 139630718992384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.154677 139630718992384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.155315 139630718992384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.155397 139630718992384 transformer_layer.py:468] tlayer: End windows.
I0123 11:05:08.155433 139630718992384 transformer_layer.py:472] tlayer: final FFN.
I0123 11:05:08.155494 139630718992384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.155628 139630718992384 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:05:08.155738 139630718992384 nn_components.py:325] mlp: activation = None
I0123 11:05:08.155777 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 11:05:08.157647 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.157745 139630718992384 nn_components.py:261] mlp: residual
I0123 11:05:08.160185 139630718992384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.160268 139630718992384 transformer_base.py:443] tbase: final FFN
I0123 11:05:08.160378 139630718992384 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:05:08.162672 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 11:05:08.164559 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.164658 139630718992384 nn_components.py:261] mlp: residual
I0123 11:05:08.164955 139630718992384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.165041 139630718992384 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 11:05:08.165152 139630718992384 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:05:08.165192 139630718992384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:05:08.165224 139630718992384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:05:08.165288 139630718992384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.167542 139630718992384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:05:08.172954 139630718992384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.173221 139630718992384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:05:08.175921 139630718992384 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:05:08.188302 139630718992384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:05:08.188362 139630718992384 attention.py:418] Single window, no scan.
I0123 11:05:08.188399 139630718992384 transformer_layer.py:389] tlayer: self-attention.
I0123 11:05:08.188432 139630718992384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.188495 139630718992384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.189057 139630718992384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.189137 139630718992384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.189502 139630718992384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.190205 139630718992384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.193163 139630718992384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.193791 139630718992384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.193883 139630718992384 transformer_layer.py:468] tlayer: End windows.
I0123 11:05:08.193921 139630718992384 transformer_layer.py:472] tlayer: final FFN.
I0123 11:05:08.193982 139630718992384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.194115 139630718992384 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:05:08.194225 139630718992384 nn_components.py:325] mlp: activation = None
I0123 11:05:08.194265 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 11:05:08.196116 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.196214 139630718992384 nn_components.py:261] mlp: residual
I0123 11:05:08.198652 139630718992384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.198736 139630718992384 transformer_base.py:443] tbase: final FFN
I0123 11:05:08.198847 139630718992384 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:05:08.201130 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 11:05:08.203042 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.203141 139630718992384 nn_components.py:261] mlp: residual
I0123 11:05:08.203438 139630718992384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.203524 139630718992384 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 11:05:08.203634 139630718992384 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:05:08.203673 139630718992384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:05:08.203705 139630718992384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:05:08.203770 139630718992384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.206053 139630718992384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:05:08.211471 139630718992384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.211736 139630718992384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:05:08.214425 139630718992384 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:05:08.226846 139630718992384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:05:08.226906 139630718992384 attention.py:418] Single window, no scan.
I0123 11:05:08.226942 139630718992384 transformer_layer.py:389] tlayer: self-attention.
I0123 11:05:08.226974 139630718992384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.227037 139630718992384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.227599 139630718992384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.227680 139630718992384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.228045 139630718992384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.228736 139630718992384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.231285 139630718992384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.231906 139630718992384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.231995 139630718992384 transformer_layer.py:468] tlayer: End windows.
I0123 11:05:08.232032 139630718992384 transformer_layer.py:472] tlayer: final FFN.
I0123 11:05:08.232093 139630718992384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.232224 139630718992384 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:05:08.232335 139630718992384 nn_components.py:325] mlp: activation = None
I0123 11:05:08.232374 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 11:05:08.234254 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.234356 139630718992384 nn_components.py:261] mlp: residual
I0123 11:05:08.236841 139630718992384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.236923 139630718992384 transformer_base.py:443] tbase: final FFN
I0123 11:05:08.237035 139630718992384 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:05:08.239328 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 11:05:08.241223 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.241323 139630718992384 nn_components.py:261] mlp: residual
I0123 11:05:08.241621 139630718992384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.241715 139630718992384 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 11:05:08.241829 139630718992384 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:05:08.241870 139630718992384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:05:08.241902 139630718992384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:05:08.241967 139630718992384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.244226 139630718992384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:05:08.249708 139630718992384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.249973 139630718992384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:05:08.252676 139630718992384 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:05:08.265091 139630718992384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:05:08.265149 139630718992384 attention.py:418] Single window, no scan.
I0123 11:05:08.265186 139630718992384 transformer_layer.py:389] tlayer: self-attention.
I0123 11:05:08.265218 139630718992384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.265282 139630718992384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.265850 139630718992384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.265930 139630718992384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.266298 139630718992384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.266996 139630718992384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.269538 139630718992384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.270176 139630718992384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.270261 139630718992384 transformer_layer.py:468] tlayer: End windows.
I0123 11:05:08.270308 139630718992384 transformer_layer.py:472] tlayer: final FFN.
I0123 11:05:08.270370 139630718992384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.270507 139630718992384 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:05:08.270619 139630718992384 nn_components.py:325] mlp: activation = None
I0123 11:05:08.270659 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 11:05:08.272530 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.272626 139630718992384 nn_components.py:261] mlp: residual
I0123 11:05:08.275064 139630718992384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.275147 139630718992384 transformer_base.py:443] tbase: final FFN
I0123 11:05:08.275258 139630718992384 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:05:08.277544 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 11:05:08.279406 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.279505 139630718992384 nn_components.py:261] mlp: residual
I0123 11:05:08.279801 139630718992384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.279886 139630718992384 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 11:05:08.279996 139630718992384 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:05:08.280037 139630718992384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:05:08.280069 139630718992384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:05:08.280133 139630718992384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.282387 139630718992384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:05:08.287806 139630718992384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.288074 139630718992384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:05:08.290767 139630718992384 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:05:08.303055 139630718992384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:05:08.303113 139630718992384 attention.py:418] Single window, no scan.
I0123 11:05:08.303149 139630718992384 transformer_layer.py:389] tlayer: self-attention.
I0123 11:05:08.303179 139630718992384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.303241 139630718992384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.303794 139630718992384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.303874 139630718992384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.304236 139630718992384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.304927 139630718992384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.307860 139630718992384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.308486 139630718992384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.308567 139630718992384 transformer_layer.py:468] tlayer: End windows.
I0123 11:05:08.308603 139630718992384 transformer_layer.py:472] tlayer: final FFN.
I0123 11:05:08.308671 139630718992384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.308806 139630718992384 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:05:08.308920 139630718992384 nn_components.py:325] mlp: activation = None
I0123 11:05:08.308959 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 11:05:08.310835 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.310933 139630718992384 nn_components.py:261] mlp: residual
I0123 11:05:08.313345 139630718992384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.313427 139630718992384 transformer_base.py:443] tbase: final FFN
I0123 11:05:08.313537 139630718992384 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:05:08.315810 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 11:05:08.317676 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.317775 139630718992384 nn_components.py:261] mlp: residual
I0123 11:05:08.318072 139630718992384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.318161 139630718992384 decoder_stack.py:344] dstack: Final layernorm.
I0123 11:05:08.321014 139630718992384 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:05:08.563756 139630718992384 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.563943 139630718992384 decoder_stack.py:333] dstack: autoregressive generator.
I0123 11:05:08.564009 139630718992384 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 11:05:08.564122 139630718992384 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:05:08.564164 139630718992384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:05:08.564197 139630718992384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:05:08.564269 139630718992384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.566973 139630718992384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:05:08.572689 139630718992384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.572959 139630718992384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:05:08.575907 139630718992384 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:05:08.588478 139630718992384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:05:08.588540 139630718992384 attention.py:418] Single window, no scan.
I0123 11:05:08.588578 139630718992384 transformer_layer.py:389] tlayer: self-attention.
I0123 11:05:08.588612 139630718992384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.588677 139630718992384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.589273 139630718992384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.589354 139630718992384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.589909 139630718992384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.590605 139630718992384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.593120 139630718992384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.593815 139630718992384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.593898 139630718992384 transformer_layer.py:468] tlayer: End windows.
I0123 11:05:08.593935 139630718992384 transformer_layer.py:472] tlayer: final FFN.
I0123 11:05:08.593997 139630718992384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.594128 139630718992384 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:05:08.594242 139630718992384 nn_components.py:325] mlp: activation = None
I0123 11:05:08.594282 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 11:05:08.596146 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.596244 139630718992384 nn_components.py:261] mlp: residual
I0123 11:05:08.598700 139630718992384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.598784 139630718992384 transformer_base.py:443] tbase: final FFN
I0123 11:05:08.598897 139630718992384 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:05:08.601135 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 11:05:08.603087 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.603188 139630718992384 nn_components.py:261] mlp: residual
I0123 11:05:08.603485 139630718992384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.603570 139630718992384 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 11:05:08.603680 139630718992384 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:05:08.603720 139630718992384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:05:08.603752 139630718992384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:05:08.603817 139630718992384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.606072 139630718992384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:05:08.611532 139630718992384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.611798 139630718992384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:05:08.614475 139630718992384 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:05:08.627122 139630718992384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:05:08.627183 139630718992384 attention.py:418] Single window, no scan.
I0123 11:05:08.627222 139630718992384 transformer_layer.py:389] tlayer: self-attention.
I0123 11:05:08.627256 139630718992384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.627321 139630718992384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.627908 139630718992384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.627987 139630718992384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.628348 139630718992384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.629034 139630718992384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.631599 139630718992384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.632246 139630718992384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.632327 139630718992384 transformer_layer.py:468] tlayer: End windows.
I0123 11:05:08.632364 139630718992384 transformer_layer.py:472] tlayer: final FFN.
I0123 11:05:08.632425 139630718992384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.632555 139630718992384 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:05:08.632666 139630718992384 nn_components.py:325] mlp: activation = None
I0123 11:05:08.632706 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 11:05:08.634649 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.634753 139630718992384 nn_components.py:261] mlp: residual
I0123 11:05:08.637196 139630718992384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.637278 139630718992384 transformer_base.py:443] tbase: final FFN
I0123 11:05:08.637389 139630718992384 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:05:08.639673 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 11:05:08.641614 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.641726 139630718992384 nn_components.py:261] mlp: residual
I0123 11:05:08.642027 139630718992384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.642113 139630718992384 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 11:05:08.642224 139630718992384 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:05:08.642265 139630718992384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:05:08.642298 139630718992384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:05:08.642363 139630718992384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.644613 139630718992384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:05:08.650022 139630718992384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.650288 139630718992384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:05:08.652933 139630718992384 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:05:08.665781 139630718992384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:05:08.665841 139630718992384 attention.py:418] Single window, no scan.
I0123 11:05:08.665880 139630718992384 transformer_layer.py:389] tlayer: self-attention.
I0123 11:05:08.665912 139630718992384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.665977 139630718992384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.666539 139630718992384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.666618 139630718992384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.666982 139630718992384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.667674 139630718992384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.670173 139630718992384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.670807 139630718992384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.670888 139630718992384 transformer_layer.py:468] tlayer: End windows.
I0123 11:05:08.670924 139630718992384 transformer_layer.py:472] tlayer: final FFN.
I0123 11:05:08.670984 139630718992384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.671113 139630718992384 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:05:08.671222 139630718992384 nn_components.py:325] mlp: activation = None
I0123 11:05:08.671262 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 11:05:08.673174 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.673271 139630718992384 nn_components.py:261] mlp: residual
I0123 11:05:08.675694 139630718992384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.675778 139630718992384 transformer_base.py:443] tbase: final FFN
I0123 11:05:08.675889 139630718992384 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:05:08.678101 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 11:05:08.680034 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.680134 139630718992384 nn_components.py:261] mlp: residual
I0123 11:05:08.680432 139630718992384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.680521 139630718992384 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 11:05:08.680632 139630718992384 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:05:08.680672 139630718992384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:05:08.680704 139630718992384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:05:08.680769 139630718992384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.683022 139630718992384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:05:08.688671 139630718992384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.688935 139630718992384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:05:08.691626 139630718992384 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:05:08.703889 139630718992384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:05:08.703948 139630718992384 attention.py:418] Single window, no scan.
I0123 11:05:08.703985 139630718992384 transformer_layer.py:389] tlayer: self-attention.
I0123 11:05:08.704017 139630718992384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.704080 139630718992384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.704642 139630718992384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.704722 139630718992384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.705087 139630718992384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.705780 139630718992384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.708245 139630718992384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.708870 139630718992384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.708959 139630718992384 transformer_layer.py:468] tlayer: End windows.
I0123 11:05:08.708996 139630718992384 transformer_layer.py:472] tlayer: final FFN.
I0123 11:05:08.709056 139630718992384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.709186 139630718992384 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:05:08.709296 139630718992384 nn_components.py:325] mlp: activation = None
I0123 11:05:08.709336 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 11:05:08.711232 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.711329 139630718992384 nn_components.py:261] mlp: residual
I0123 11:05:08.713754 139630718992384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.713837 139630718992384 transformer_base.py:443] tbase: final FFN
I0123 11:05:08.713949 139630718992384 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:05:08.716159 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 11:05:08.718094 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.718193 139630718992384 nn_components.py:261] mlp: residual
I0123 11:05:08.718489 139630718992384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.718573 139630718992384 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 11:05:08.718683 139630718992384 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:05:08.718724 139630718992384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:05:08.718755 139630718992384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:05:08.718819 139630718992384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.721043 139630718992384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:05:08.726435 139630718992384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.726700 139630718992384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:05:08.729420 139630718992384 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:05:08.741831 139630718992384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:05:08.741891 139630718992384 attention.py:418] Single window, no scan.
I0123 11:05:08.741928 139630718992384 transformer_layer.py:389] tlayer: self-attention.
I0123 11:05:08.741959 139630718992384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.742022 139630718992384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.742582 139630718992384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.742661 139630718992384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.743021 139630718992384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.743707 139630718992384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.746154 139630718992384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.746770 139630718992384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.746857 139630718992384 transformer_layer.py:468] tlayer: End windows.
I0123 11:05:08.746894 139630718992384 transformer_layer.py:472] tlayer: final FFN.
I0123 11:05:08.746955 139630718992384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.747086 139630718992384 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:05:08.747196 139630718992384 nn_components.py:325] mlp: activation = None
I0123 11:05:08.747235 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 11:05:08.749123 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.749220 139630718992384 nn_components.py:261] mlp: residual
I0123 11:05:08.751683 139630718992384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.751769 139630718992384 transformer_base.py:443] tbase: final FFN
I0123 11:05:08.751884 139630718992384 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:05:08.754094 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 11:05:08.756374 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.756473 139630718992384 nn_components.py:261] mlp: residual
I0123 11:05:08.756775 139630718992384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.756862 139630718992384 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 11:05:08.756973 139630718992384 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:05:08.757013 139630718992384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:05:08.757045 139630718992384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:05:08.757111 139630718992384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.759433 139630718992384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:05:08.764873 139630718992384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.765137 139630718992384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:05:08.767811 139630718992384 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:05:08.780279 139630718992384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:05:08.780338 139630718992384 attention.py:418] Single window, no scan.
I0123 11:05:08.780375 139630718992384 transformer_layer.py:389] tlayer: self-attention.
I0123 11:05:08.780408 139630718992384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.780471 139630718992384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.781029 139630718992384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.781109 139630718992384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.781467 139630718992384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.782159 139630718992384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.784644 139630718992384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.785265 139630718992384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.785345 139630718992384 transformer_layer.py:468] tlayer: End windows.
I0123 11:05:08.785388 139630718992384 transformer_layer.py:472] tlayer: final FFN.
I0123 11:05:08.785450 139630718992384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.785581 139630718992384 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:05:08.785695 139630718992384 nn_components.py:325] mlp: activation = None
I0123 11:05:08.785736 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 11:05:08.787642 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.787740 139630718992384 nn_components.py:261] mlp: residual
I0123 11:05:08.790302 139630718992384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.790387 139630718992384 transformer_base.py:443] tbase: final FFN
I0123 11:05:08.790498 139630718992384 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:05:08.792690 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 11:05:08.794637 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.794739 139630718992384 nn_components.py:261] mlp: residual
I0123 11:05:08.795037 139630718992384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.795123 139630718992384 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 11:05:08.795234 139630718992384 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:05:08.795275 139630718992384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:05:08.795307 139630718992384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:05:08.795373 139630718992384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.797606 139630718992384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:05:08.803022 139630718992384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.803292 139630718992384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:05:08.805947 139630718992384 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:05:08.818477 139630718992384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:05:08.818536 139630718992384 attention.py:418] Single window, no scan.
I0123 11:05:08.818573 139630718992384 transformer_layer.py:389] tlayer: self-attention.
I0123 11:05:08.818606 139630718992384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.818669 139630718992384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.819227 139630718992384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.819305 139630718992384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.819663 139630718992384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.820350 139630718992384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.822824 139630718992384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.823449 139630718992384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.823529 139630718992384 transformer_layer.py:468] tlayer: End windows.
I0123 11:05:08.823566 139630718992384 transformer_layer.py:472] tlayer: final FFN.
I0123 11:05:08.823636 139630718992384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.823769 139630718992384 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:05:08.823879 139630718992384 nn_components.py:325] mlp: activation = None
I0123 11:05:08.823919 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 11:05:08.825838 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.825936 139630718992384 nn_components.py:261] mlp: residual
I0123 11:05:08.828354 139630718992384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.828437 139630718992384 transformer_base.py:443] tbase: final FFN
I0123 11:05:08.828548 139630718992384 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:05:08.830754 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 11:05:08.832627 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.832726 139630718992384 nn_components.py:261] mlp: residual
I0123 11:05:08.833023 139630718992384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.833109 139630718992384 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 11:05:08.833220 139630718992384 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:05:08.833261 139630718992384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:05:08.833294 139630718992384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:05:08.833359 139630718992384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.835656 139630718992384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:05:08.841074 139630718992384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.841337 139630718992384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:05:08.843916 139630718992384 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:05:08.856585 139630718992384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:05:08.856644 139630718992384 attention.py:418] Single window, no scan.
I0123 11:05:08.856682 139630718992384 transformer_layer.py:389] tlayer: self-attention.
I0123 11:05:08.856714 139630718992384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.856779 139630718992384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.857345 139630718992384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.857425 139630718992384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.857803 139630718992384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.858498 139630718992384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.861102 139630718992384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.861750 139630718992384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.861839 139630718992384 transformer_layer.py:468] tlayer: End windows.
I0123 11:05:08.861877 139630718992384 transformer_layer.py:472] tlayer: final FFN.
I0123 11:05:08.861940 139630718992384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.862085 139630718992384 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:05:08.862200 139630718992384 nn_components.py:325] mlp: activation = None
I0123 11:05:08.862241 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 11:05:08.864501 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.864597 139630718992384 nn_components.py:261] mlp: residual
I0123 11:05:08.867039 139630718992384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.867124 139630718992384 transformer_base.py:443] tbase: final FFN
I0123 11:05:08.867238 139630718992384 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:05:08.869508 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 11:05:08.871431 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.871534 139630718992384 nn_components.py:261] mlp: residual
I0123 11:05:08.871843 139630718992384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.871931 139630718992384 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 11:05:08.872042 139630718992384 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:05:08.872083 139630718992384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:05:08.872115 139630718992384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:05:08.872180 139630718992384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.874428 139630718992384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:05:08.879946 139630718992384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.880209 139630718992384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:05:08.882903 139630718992384 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:05:08.895429 139630718992384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:05:08.895490 139630718992384 attention.py:418] Single window, no scan.
I0123 11:05:08.895528 139630718992384 transformer_layer.py:389] tlayer: self-attention.
I0123 11:05:08.895562 139630718992384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.895627 139630718992384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.896249 139630718992384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.896327 139630718992384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.896689 139630718992384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.897370 139630718992384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.899887 139630718992384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.900506 139630718992384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.900585 139630718992384 transformer_layer.py:468] tlayer: End windows.
I0123 11:05:08.900621 139630718992384 transformer_layer.py:472] tlayer: final FFN.
I0123 11:05:08.900681 139630718992384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.900819 139630718992384 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:05:08.900930 139630718992384 nn_components.py:325] mlp: activation = None
I0123 11:05:08.900970 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 11:05:08.902837 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.902939 139630718992384 nn_components.py:261] mlp: residual
I0123 11:05:08.905440 139630718992384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.905520 139630718992384 transformer_base.py:443] tbase: final FFN
I0123 11:05:08.905631 139630718992384 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:05:08.907892 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 11:05:08.909800 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.909902 139630718992384 nn_components.py:261] mlp: residual
I0123 11:05:08.910209 139630718992384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.910296 139630718992384 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 11:05:08.910410 139630718992384 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:05:08.910451 139630718992384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:05:08.910485 139630718992384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:05:08.910551 139630718992384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.912827 139630718992384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:05:08.918354 139630718992384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.918629 139630718992384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:05:08.921258 139630718992384 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:05:08.933811 139630718992384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:05:08.933871 139630718992384 attention.py:418] Single window, no scan.
I0123 11:05:08.933908 139630718992384 transformer_layer.py:389] tlayer: self-attention.
I0123 11:05:08.933941 139630718992384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.934002 139630718992384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.934572 139630718992384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.934653 139630718992384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.935027 139630718992384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.935743 139630718992384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.938212 139630718992384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.938913 139630718992384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.938996 139630718992384 transformer_layer.py:468] tlayer: End windows.
I0123 11:05:08.939033 139630718992384 transformer_layer.py:472] tlayer: final FFN.
I0123 11:05:08.939095 139630718992384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.939229 139630718992384 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:05:08.939353 139630718992384 nn_components.py:325] mlp: activation = None
I0123 11:05:08.939395 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 11:05:08.941258 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.941354 139630718992384 nn_components.py:261] mlp: residual
I0123 11:05:08.943839 139630718992384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.943923 139630718992384 transformer_base.py:443] tbase: final FFN
I0123 11:05:08.944033 139630718992384 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:05:08.946237 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 11:05:08.948224 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.948321 139630718992384 nn_components.py:261] mlp: residual
I0123 11:05:08.948618 139630718992384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.948702 139630718992384 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 11:05:08.948812 139630718992384 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:05:08.948852 139630718992384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:05:08.948884 139630718992384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:05:08.948949 139630718992384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.951231 139630718992384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:05:08.956708 139630718992384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.956973 139630718992384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:05:08.959642 139630718992384 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:05:08.972798 139630718992384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:05:08.972857 139630718992384 attention.py:418] Single window, no scan.
I0123 11:05:08.972894 139630718992384 transformer_layer.py:389] tlayer: self-attention.
I0123 11:05:08.972925 139630718992384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.972986 139630718992384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.973541 139630718992384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.973617 139630718992384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.973982 139630718992384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.974661 139630718992384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.977178 139630718992384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.977801 139630718992384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.977881 139630718992384 transformer_layer.py:468] tlayer: End windows.
I0123 11:05:08.977917 139630718992384 transformer_layer.py:472] tlayer: final FFN.
I0123 11:05:08.977976 139630718992384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.978105 139630718992384 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:05:08.978227 139630718992384 nn_components.py:325] mlp: activation = None
I0123 11:05:08.978269 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 11:05:08.980189 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.980287 139630718992384 nn_components.py:261] mlp: residual
I0123 11:05:08.982729 139630718992384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.982811 139630718992384 transformer_base.py:443] tbase: final FFN
I0123 11:05:08.982922 139630718992384 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:05:08.985211 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 11:05:08.987148 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.987251 139630718992384 nn_components.py:261] mlp: residual
I0123 11:05:08.987560 139630718992384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.987646 139630718992384 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 11:05:08.987761 139630718992384 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:05:08.987804 139630718992384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:05:08.987837 139630718992384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:05:08.987903 139630718992384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.990150 139630718992384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:05:08.995548 139630718992384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:08.995813 139630718992384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:05:08.998483 139630718992384 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:05:09.010893 139630718992384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:05:09.010953 139630718992384 attention.py:418] Single window, no scan.
I0123 11:05:09.010991 139630718992384 transformer_layer.py:389] tlayer: self-attention.
I0123 11:05:09.011023 139630718992384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:09.011084 139630718992384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:09.011693 139630718992384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:09.011771 139630718992384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:09.012133 139630718992384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:09.012816 139630718992384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:09.015252 139630718992384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:09.015877 139630718992384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:09.015956 139630718992384 transformer_layer.py:468] tlayer: End windows.
I0123 11:05:09.015993 139630718992384 transformer_layer.py:472] tlayer: final FFN.
I0123 11:05:09.016053 139630718992384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:09.016184 139630718992384 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:05:09.016298 139630718992384 nn_components.py:325] mlp: activation = None
I0123 11:05:09.016346 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 11:05:09.018275 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:09.018371 139630718992384 nn_components.py:261] mlp: residual
I0123 11:05:09.020815 139630718992384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:09.020896 139630718992384 transformer_base.py:443] tbase: final FFN
I0123 11:05:09.021006 139630718992384 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:05:09.023200 139630718992384 nn_components.py:329] mlp: final activation = None
I0123 11:05:09.025050 139630718992384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:09.025147 139630718992384 nn_components.py:261] mlp: residual
I0123 11:05:09.025440 139630718992384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:09.025527 139630718992384 decoder_stack.py:344] dstack: Final layernorm.
I0123 11:05:09.028355 139630718992384 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:05:26.044602 139630718992384 alphageometry.py:566] LM output (score=-2.142421): "s : C e q s 29 D e s q s 30 ;"
I0123 11:05:26.044763 139630718992384 alphageometry.py:567] Translation: "s = on_line s e q, on_bline s q e"

I0123 11:05:26.044809 139630718992384 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g b f, on_line g c e; h = lc_tangent h a g, on_line h f g; i = on_line i a h, on_line i c g; j = on_line j a g, on_line j b i; k = on_circle k d b; l = circle l k g i; m = circle m k j h; n = foot n k l m; o = mirror o k n; p = foot p k c g; q = mirror q k p; r = on_line r b a, on_line r c q; s = on_line s e q, on_bline s q e ? coll o r k"
I0123 11:05:26.045071 139630718992384 graph.py:498] 
I0123 11:05:26.045136 139630718992384 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g b f, on_line g c e; h = lc_tangent h a g, on_line h f g; i = on_line i a h, on_line i c g; j = on_line j a g, on_line j b i; k = on_circle k d b; l = circle l k g i; m = circle m k j h; n = foot n k l m; o = mirror o k n; p = foot p k c g; q = mirror q k p; r = on_line r b a, on_line r c q; s = on_line s e q, on_bline s q e ? coll o r k
I0123 11:05:32.261270 139630718992384 ddar.py:60] Depth 1/1000 time = 6.1410839557647705
I0123 11:05:43.932706 139630718992384 ddar.py:60] Depth 2/1000 time = 11.671252250671387
I0123 11:05:56.844094 139630718992384 ddar.py:60] Depth 3/1000 time = 12.911170721054077
I0123 11:06:09.192716 139630718992384 ddar.py:60] Depth 4/1000 time = 12.348413944244385
I0123 11:06:21.755164 139630718992384 ddar.py:60] Depth 5/1000 time = 12.562169551849365
I0123 11:06:34.204076 139630718992384 ddar.py:60] Depth 6/1000 time = 12.448650598526001
I0123 11:06:46.413487 139630718992384 ddar.py:60] Depth 7/1000 time = 12.208716869354248
I0123 11:06:58.982745 139630718992384 ddar.py:60] Depth 8/1000 time = 12.562883615493774
I0123 11:07:12.364637 139630718992384 ddar.py:60] Depth 9/1000 time = 13.32970380783081
I0123 11:07:25.363121 139630718992384 ddar.py:60] Depth 10/1000 time = 12.998100280761719
I0123 11:07:40.043006 139630718992384 ddar.py:60] Depth 11/1000 time = 14.660094022750854
I0123 11:07:55.471891 139630718992384 ddar.py:60] Depth 12/1000 time = 15.428482055664062
I0123 11:08:15.639856 139630718992384 ddar.py:60] Depth 13/1000 time = 20.167579412460327
I0123 11:08:36.589954 139630718992384 ddar.py:60] Depth 14/1000 time = 20.9496808052063
I0123 11:09:01.084070 139630718992384 ddar.py:60] Depth 15/1000 time = 24.49370813369751
I0123 11:09:25.028448 139630718992384 ddar.py:60] Depth 16/1000 time = 23.943820238113403
I0123 11:09:49.530345 139630718992384 ddar.py:60] Depth 17/1000 time = 24.475688934326172
I0123 11:10:15.175335 139630718992384 ddar.py:60] Depth 18/1000 time = 25.644360542297363
I0123 11:10:41.538086 139630718992384 ddar.py:60] Depth 19/1000 time = 26.36222553253174
I0123 11:11:07.625891 139630718992384 ddar.py:60] Depth 20/1000 time = 26.063129663467407
I0123 11:11:34.880045 139630718992384 ddar.py:60] Depth 21/1000 time = 27.167685747146606
I0123 11:12:03.226162 139630718992384 ddar.py:60] Depth 22/1000 time = 28.345614433288574
I0123 11:12:31.626780 139630718992384 ddar.py:60] Depth 23/1000 time = 28.400165796279907
I0123 11:12:59.769100 139630718992384 ddar.py:60] Depth 24/1000 time = 28.096577405929565
I0123 11:12:59.769856 139630718992384 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:12:59.770013 139630718992384 alphageometry.py:566] LM output (score=-2.171599): "s : T b h o s 29 ;"
I0123 11:12:59.770051 139630718992384 alphageometry.py:567] Translation: "s = on_tline s o b h"

I0123 11:12:59.770106 139630718992384 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g b f, on_line g c e; h = lc_tangent h a g, on_line h f g; i = on_line i a h, on_line i c g; j = on_line j a g, on_line j b i; k = on_circle k d b; l = circle l k g i; m = circle m k j h; n = foot n k l m; o = mirror o k n; p = foot p k c g; q = mirror q k p; r = on_line r b a, on_line r c q; s = on_tline s o b h ? coll o r k"
I0123 11:12:59.770345 139630718992384 graph.py:498] 
I0123 11:12:59.770409 139630718992384 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g b f, on_line g c e; h = lc_tangent h a g, on_line h f g; i = on_line i a h, on_line i c g; j = on_line j a g, on_line j b i; k = on_circle k d b; l = circle l k g i; m = circle m k j h; n = foot n k l m; o = mirror o k n; p = foot p k c g; q = mirror q k p; r = on_line r b a, on_line r c q; s = on_tline s o b h ? coll o r k
I0123 11:13:05.219646 139630718992384 ddar.py:60] Depth 1/1000 time = 5.3752686977386475
I0123 11:13:16.576037 139630718992384 ddar.py:60] Depth 2/1000 time = 11.356152534484863
I0123 11:13:28.429074 139630718992384 ddar.py:60] Depth 3/1000 time = 11.852708339691162
I0123 11:13:40.392076 139630718992384 ddar.py:60] Depth 4/1000 time = 11.962764024734497
I0123 11:13:52.699491 139630718992384 ddar.py:60] Depth 5/1000 time = 12.30713939666748
I0123 11:14:04.842516 139630718992384 ddar.py:60] Depth 6/1000 time = 12.142749071121216
I0123 11:14:17.246934 139630718992384 ddar.py:60] Depth 7/1000 time = 12.403831005096436
I0123 11:14:29.482390 139630718992384 ddar.py:60] Depth 8/1000 time = 12.227470874786377
I0123 11:14:42.361389 139630718992384 ddar.py:60] Depth 9/1000 time = 12.82801103591919
I0123 11:14:55.471683 139630718992384 ddar.py:60] Depth 10/1000 time = 13.109910488128662
I0123 11:15:10.231066 139630718992384 ddar.py:60] Depth 11/1000 time = 14.73878526687622
I0123 11:15:25.443768 139630718992384 ddar.py:60] Depth 12/1000 time = 15.21232295036316
I0123 11:15:45.571198 139630718992384 ddar.py:60] Depth 13/1000 time = 20.12701392173767
I0123 11:16:06.260287 139630718992384 ddar.py:60] Depth 14/1000 time = 20.68865466117859
I0123 11:16:30.548580 139630718992384 ddar.py:60] Depth 15/1000 time = 24.28784155845642
I0123 11:16:55.123518 139630718992384 ddar.py:60] Depth 16/1000 time = 24.57415795326233
I0123 11:17:19.428173 139630718992384 ddar.py:60] Depth 17/1000 time = 24.281224012374878
I0123 11:17:45.460179 139630718992384 ddar.py:60] Depth 18/1000 time = 26.031501054763794
I0123 11:18:11.842218 139630718992384 ddar.py:60] Depth 19/1000 time = 26.381474256515503
I0123 11:18:38.962413 139630718992384 ddar.py:60] Depth 20/1000 time = 27.016979217529297
I0123 11:19:07.359556 139630718992384 ddar.py:60] Depth 21/1000 time = 28.396619081497192
I0123 11:19:36.212285 139630718992384 ddar.py:60] Depth 22/1000 time = 28.85191774368286
I0123 11:20:04.332704 139630718992384 ddar.py:60] Depth 23/1000 time = 28.072810173034668
I0123 11:20:04.333539 139630718992384 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:20:04.333682 139630718992384 alphageometry.py:566] LM output (score=-2.462539): "s : D d k d s 29 ;"
I0123 11:20:04.333724 139630718992384 alphageometry.py:567] Translation: "s = on_circle s d k"

I0123 11:20:04.333781 139630718992384 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g b f, on_line g c e; h = lc_tangent h a g, on_line h f g; i = on_line i a h, on_line i c g; j = on_line j a g, on_line j b i; k = on_circle k d b; l = circle l k g i; m = circle m k j h; n = foot n k l m; o = mirror o k n; p = foot p k c g; q = mirror q k p; r = on_line r b a, on_line r c q; s = on_circle s d k ? coll o r k"
I0123 11:20:04.334024 139630718992384 graph.py:498] 
I0123 11:20:04.334089 139630718992384 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g b f, on_line g c e; h = lc_tangent h a g, on_line h f g; i = on_line i a h, on_line i c g; j = on_line j a g, on_line j b i; k = on_circle k d b; l = circle l k g i; m = circle m k j h; n = foot n k l m; o = mirror o k n; p = foot p k c g; q = mirror q k p; r = on_line r b a, on_line r c q; s = on_circle s d k ? coll o r k
I0123 11:20:10.906412 139630718992384 ddar.py:60] Depth 1/1000 time = 6.498951435089111
I0123 11:20:23.518583 139630718992384 ddar.py:60] Depth 2/1000 time = 12.611916780471802
I0123 11:20:38.668788 139630718992384 ddar.py:60] Depth 3/1000 time = 15.14987564086914
I0123 11:20:52.643612 139630718992384 ddar.py:60] Depth 4/1000 time = 13.974575281143188
I0123 11:21:06.751679 139630718992384 ddar.py:60] Depth 5/1000 time = 14.10769009590149
I0123 11:21:20.726083 139630718992384 ddar.py:60] Depth 6/1000 time = 13.974042415618896
I0123 11:21:34.926826 139630718992384 ddar.py:60] Depth 7/1000 time = 14.200151920318604
I0123 11:21:49.219750 139630718992384 ddar.py:60] Depth 8/1000 time = 14.28666067123413
I0123 11:22:04.227064 139630718992384 ddar.py:60] Depth 9/1000 time = 14.951655387878418
I0123 11:22:19.301316 139630718992384 ddar.py:60] Depth 10/1000 time = 15.073945760726929
I0123 11:22:35.745918 139630718992384 ddar.py:60] Depth 11/1000 time = 16.422258615493774
I0123 11:22:52.701908 139630718992384 ddar.py:60] Depth 12/1000 time = 16.955508947372437
I0123 11:23:15.772395 139630718992384 ddar.py:60] Depth 13/1000 time = 23.07004427909851
I0123 11:23:39.040676 139630718992384 ddar.py:60] Depth 14/1000 time = 23.267825603485107
I0123 11:24:06.274216 139630718992384 ddar.py:60] Depth 15/1000 time = 27.233095169067383
I0123 11:24:34.192621 139630718992384 ddar.py:60] Depth 16/1000 time = 27.917948722839355
I0123 11:25:01.577368 139630718992384 ddar.py:60] Depth 17/1000 time = 27.358741521835327
I0123 11:25:30.787103 139630718992384 ddar.py:60] Depth 18/1000 time = 29.209160327911377
I0123 11:25:59.470003 139630718992384 ddar.py:60] Depth 19/1000 time = 28.68214988708496
I0123 11:26:29.104608 139630718992384 ddar.py:60] Depth 20/1000 time = 29.606006622314453
I0123 11:26:59.419599 139630718992384 ddar.py:60] Depth 21/1000 time = 30.223012447357178
I0123 11:27:30.754372 139630718992384 ddar.py:60] Depth 22/1000 time = 31.33431077003479
I0123 11:28:01.585797 139630718992384 ddar.py:60] Depth 23/1000 time = 30.830572843551636
I0123 11:28:34.152439 139630718992384 ddar.py:60] Depth 24/1000 time = 32.51597762107849
I0123 11:28:34.153293 139630718992384 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:28:34.153429 139630718992384 alphageometry.py:566] LM output (score=-2.478998): "s : T f h h s 29 ;"
I0123 11:28:34.153474 139630718992384 alphageometry.py:567] Translation: "s = on_tline s h f h"

I0123 11:28:34.153523 139630718992384 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g b f, on_line g c e; h = lc_tangent h a g, on_line h f g; i = on_line i a h, on_line i c g; j = on_line j a g, on_line j b i; k = on_circle k d b; l = circle l k g i; m = circle m k j h; n = foot n k l m; o = mirror o k n; p = foot p k c g; q = mirror q k p; r = on_line r b a, on_line r c q; s = on_tline s h f h ? coll o r k"
I0123 11:28:34.153795 139630718992384 graph.py:498] 
I0123 11:28:34.153869 139630718992384 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g b f, on_line g c e; h = lc_tangent h a g, on_line h f g; i = on_line i a h, on_line i c g; j = on_line j a g, on_line j b i; k = on_circle k d b; l = circle l k g i; m = circle m k j h; n = foot n k l m; o = mirror o k n; p = foot p k c g; q = mirror q k p; r = on_line r b a, on_line r c q; s = on_tline s h f h ? coll o r k
I0123 11:28:40.106640 139630718992384 ddar.py:60] Depth 1/1000 time = 5.883130073547363
I0123 11:28:51.573652 139630718992384 ddar.py:60] Depth 2/1000 time = 11.466826438903809
I0123 11:29:04.242870 139630718992384 ddar.py:60] Depth 3/1000 time = 12.668988704681396
I0123 11:29:16.961511 139630718992384 ddar.py:60] Depth 4/1000 time = 12.718364477157593
I0123 11:29:29.134149 139630718992384 ddar.py:60] Depth 5/1000 time = 12.172247648239136
I0123 11:29:42.084468 139630718992384 ddar.py:60] Depth 6/1000 time = 12.950078010559082
I0123 11:29:54.492361 139630718992384 ddar.py:60] Depth 7/1000 time = 12.407361268997192
I0123 11:30:06.771519 139630718992384 ddar.py:60] Depth 8/1000 time = 12.271362543106079
I0123 11:30:20.081810 139630718992384 ddar.py:60] Depth 9/1000 time = 13.258711814880371
I0123 11:30:33.103504 139630718992384 ddar.py:60] Depth 10/1000 time = 13.02111291885376
I0123 11:30:47.900365 139630718992384 ddar.py:60] Depth 11/1000 time = 14.777390241622925
I0123 11:31:04.047136 139630718992384 ddar.py:60] Depth 12/1000 time = 16.14639449119568
I0123 11:31:24.536771 139630718992384 ddar.py:60] Depth 13/1000 time = 20.48926281929016
I0123 11:31:45.780806 139630718992384 ddar.py:60] Depth 14/1000 time = 21.243622064590454
I0123 11:32:10.884431 139630718992384 ddar.py:60] Depth 15/1000 time = 25.1031596660614
I0123 11:32:36.128027 139630718992384 ddar.py:60] Depth 16/1000 time = 25.243135690689087
I0123 11:33:01.683714 139630718992384 ddar.py:60] Depth 17/1000 time = 25.531516313552856
I0123 11:33:28.956213 139630718992384 ddar.py:60] Depth 18/1000 time = 27.271814346313477
I0123 11:33:55.815077 139630718992384 ddar.py:60] Depth 19/1000 time = 26.85831093788147
I0123 11:34:24.397537 139630718992384 ddar.py:60] Depth 20/1000 time = 28.47681736946106
I0123 11:34:52.821110 139630718992384 ddar.py:60] Depth 21/1000 time = 28.42304515838623
I0123 11:35:22.510951 139630718992384 ddar.py:60] Depth 22/1000 time = 29.689321279525757
I0123 11:35:52.317327 139630718992384 ddar.py:60] Depth 23/1000 time = 29.75765347480774
I0123 11:35:52.318047 139630718992384 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:35:52.318173 139630718992384 alphageometry.py:566] LM output (score=-2.491834): "s : T d l l s 29 ;"
I0123 11:35:52.318211 139630718992384 alphageometry.py:567] Translation: "s = on_tline s l d l"

I0123 11:35:52.318265 139630718992384 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g b f, on_line g c e; h = lc_tangent h a g, on_line h f g; i = on_line i a h, on_line i c g; j = on_line j a g, on_line j b i; k = on_circle k d b; l = circle l k g i; m = circle m k j h; n = foot n k l m; o = mirror o k n; p = foot p k c g; q = mirror q k p; r = on_line r b a, on_line r c q; s = on_tline s l d l ? coll o r k"
I0123 11:35:52.318503 139630718992384 graph.py:498] 
I0123 11:35:52.318567 139630718992384 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g b f, on_line g c e; h = lc_tangent h a g, on_line h f g; i = on_line i a h, on_line i c g; j = on_line j a g, on_line j b i; k = on_circle k d b; l = circle l k g i; m = circle m k j h; n = foot n k l m; o = mirror o k n; p = foot p k c g; q = mirror q k p; r = on_line r b a, on_line r c q; s = on_tline s l d l ? coll o r k
I0123 11:35:57.543475 139630718992384 ddar.py:60] Depth 1/1000 time = 5.154864311218262
I0123 11:36:09.141560 139630718992384 ddar.py:60] Depth 2/1000 time = 11.597862720489502
I0123 11:36:21.609400 139630718992384 ddar.py:60] Depth 3/1000 time = 12.467566967010498
I0123 11:36:33.940406 139630718992384 ddar.py:60] Depth 4/1000 time = 12.330684900283813
I0123 11:36:46.353492 139630718992384 ddar.py:60] Depth 5/1000 time = 12.412771224975586
I0123 11:36:58.894906 139630718992384 ddar.py:60] Depth 6/1000 time = 12.541042566299438
I0123 11:37:11.028972 139630718992384 ddar.py:60] Depth 7/1000 time = 12.13350224494934
I0123 11:37:23.718517 139630718992384 ddar.py:60] Depth 8/1000 time = 12.68250560760498
I0123 11:37:37.121293 139630718992384 ddar.py:60] Depth 9/1000 time = 13.35439395904541
I0123 11:37:50.369377 139630718992384 ddar.py:60] Depth 10/1000 time = 13.24776291847229
I0123 11:38:05.731661 139630718992384 ddar.py:60] Depth 11/1000 time = 15.340497255325317
I0123 11:38:21.469679 139630718992384 ddar.py:60] Depth 12/1000 time = 15.737704038619995
I0123 11:38:41.611517 139630718992384 ddar.py:60] Depth 13/1000 time = 20.141576290130615
I0123 11:39:03.335837 139630718992384 ddar.py:60] Depth 14/1000 time = 21.723997354507446
I0123 11:39:28.279796 139630718992384 ddar.py:60] Depth 15/1000 time = 24.943570137023926
I0123 11:39:54.159136 139630718992384 ddar.py:60] Depth 16/1000 time = 25.878944873809814
I0123 11:40:19.848406 139630718992384 ddar.py:60] Depth 17/1000 time = 25.664761781692505
I0123 11:40:46.100797 139630718992384 ddar.py:60] Depth 18/1000 time = 26.251840114593506
I0123 11:41:13.546619 139630718992384 ddar.py:60] Depth 19/1000 time = 27.445059776306152
I0123 11:41:41.226540 139630718992384 ddar.py:60] Depth 20/1000 time = 27.65052056312561
I0123 11:42:09.223644 139630718992384 ddar.py:60] Depth 21/1000 time = 27.90947723388672
I0123 11:42:38.307394 139630718992384 ddar.py:60] Depth 22/1000 time = 29.083208322525024
I0123 11:43:08.484424 139630718992384 ddar.py:60] Depth 23/1000 time = 30.176154375076294
I0123 11:43:37.802353 139630718992384 ddar.py:60] Depth 24/1000 time = 29.272923946380615
I0123 11:43:37.802965 139630718992384 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:43:37.803102 139630718992384 alphageometry.py:566] LM output (score=-2.531011): "s : T d s f h 29 ;"
I0123 11:43:37.803138 139630718992384 alphageometry.py:567] Translation: "s = on_tline s d f h"

I0123 11:43:37.803190 139630718992384 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g b f, on_line g c e; h = lc_tangent h a g, on_line h f g; i = on_line i a h, on_line i c g; j = on_line j a g, on_line j b i; k = on_circle k d b; l = circle l k g i; m = circle m k j h; n = foot n k l m; o = mirror o k n; p = foot p k c g; q = mirror q k p; r = on_line r b a, on_line r c q; s = on_tline s d f h ? coll o r k"
I0123 11:43:37.803425 139630718992384 graph.py:498] 
I0123 11:43:37.803488 139630718992384 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g b f, on_line g c e; h = lc_tangent h a g, on_line h f g; i = on_line i a h, on_line i c g; j = on_line j a g, on_line j b i; k = on_circle k d b; l = circle l k g i; m = circle m k j h; n = foot n k l m; o = mirror o k n; p = foot p k c g; q = mirror q k p; r = on_line r b a, on_line r c q; s = on_tline s d f h ? coll o r k
I0123 11:43:43.463405 139630718992384 ddar.py:60] Depth 1/1000 time = 5.5903544425964355
I0123 11:43:55.247163 139630718992384 ddar.py:60] Depth 2/1000 time = 11.78355598449707
I0123 11:44:07.924384 139630718992384 ddar.py:60] Depth 3/1000 time = 12.676931142807007
I0123 11:44:20.686270 139630718992384 ddar.py:60] Depth 4/1000 time = 12.761547088623047
I0123 11:44:32.722200 139630718992384 ddar.py:60] Depth 5/1000 time = 12.03560733795166
I0123 11:44:45.167557 139630718992384 ddar.py:60] Depth 6/1000 time = 12.444978475570679
I0123 11:44:57.770073 139630718992384 ddar.py:60] Depth 7/1000 time = 12.601903676986694
I0123 11:45:09.885461 139630718992384 ddar.py:60] Depth 8/1000 time = 12.10763931274414
I0123 11:45:23.405911 139630718992384 ddar.py:60] Depth 9/1000 time = 13.468333959579468
I0123 11:45:36.610189 139630718992384 ddar.py:60] Depth 10/1000 time = 13.203954219818115
I0123 11:45:51.761970 139630718992384 ddar.py:60] Depth 11/1000 time = 15.131007432937622
I0123 11:46:07.671090 139630718992384 ddar.py:60] Depth 12/1000 time = 15.90878677368164
I0123 11:46:28.804214 139630718992384 ddar.py:60] Depth 13/1000 time = 21.13283395767212
I0123 11:46:49.742882 139630718992384 ddar.py:60] Depth 14/1000 time = 20.93837547302246
I0123 11:47:15.189978 139630718992384 ddar.py:60] Depth 15/1000 time = 25.446800708770752
I0123 11:47:41.113871 139630718992384 ddar.py:60] Depth 16/1000 time = 25.923343420028687
I0123 11:48:07.049208 139630718992384 ddar.py:60] Depth 17/1000 time = 25.91085457801819
I0123 11:48:33.936540 139630718992384 ddar.py:60] Depth 18/1000 time = 26.886768341064453
I0123 11:49:00.255482 139630718992384 ddar.py:60] Depth 19/1000 time = 26.318170309066772
I0123 11:49:29.015953 139630718992384 ddar.py:60] Depth 20/1000 time = 28.6561598777771
I0123 11:49:57.905238 139630718992384 ddar.py:60] Depth 21/1000 time = 28.88865828514099
I0123 11:50:27.074697 139630718992384 ddar.py:60] Depth 22/1000 time = 29.168660879135132
I0123 11:50:56.695816 139630718992384 ddar.py:60] Depth 23/1000 time = 29.57217264175415
I0123 11:50:56.696531 139630718992384 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:50:56.696666 139630718992384 alphageometry.py:566] LM output (score=-2.618222): "s : P d s f h 29 ;"
I0123 11:50:56.696704 139630718992384 alphageometry.py:567] Translation: "s = on_pline s d f h"

I0123 11:50:56.696758 139630718992384 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g b f, on_line g c e; h = lc_tangent h a g, on_line h f g; i = on_line i a h, on_line i c g; j = on_line j a g, on_line j b i; k = on_circle k d b; l = circle l k g i; m = circle m k j h; n = foot n k l m; o = mirror o k n; p = foot p k c g; q = mirror q k p; r = on_line r b a, on_line r c q; s = on_pline s d f h ? coll o r k"
I0123 11:50:56.696998 139630718992384 graph.py:498] 
I0123 11:50:56.697062 139630718992384 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g b f, on_line g c e; h = lc_tangent h a g, on_line h f g; i = on_line i a h, on_line i c g; j = on_line j a g, on_line j b i; k = on_circle k d b; l = circle l k g i; m = circle m k j h; n = foot n k l m; o = mirror o k n; p = foot p k c g; q = mirror q k p; r = on_line r b a, on_line r c q; s = on_pline s d f h ? coll o r k
I0123 11:51:02.721316 139630718992384 ddar.py:60] Depth 1/1000 time = 5.950304985046387
I0123 11:51:13.910758 139630718992384 ddar.py:60] Depth 2/1000 time = 11.189236164093018
I0123 11:51:25.639826 139630718992384 ddar.py:60] Depth 3/1000 time = 11.728824138641357
I0123 11:51:37.548678 139630718992384 ddar.py:60] Depth 4/1000 time = 11.908636808395386
I0123 11:51:49.523625 139630718992384 ddar.py:60] Depth 5/1000 time = 11.974612474441528
I0123 11:52:01.513238 139630718992384 ddar.py:60] Depth 6/1000 time = 11.989236831665039
I0123 11:52:13.953502 139630718992384 ddar.py:60] Depth 7/1000 time = 12.439607381820679
I0123 11:52:25.766787 139630718992384 ddar.py:60] Depth 8/1000 time = 11.807276725769043
I0123 11:52:38.143500 139630718992384 ddar.py:60] Depth 9/1000 time = 12.329304933547974
I0123 11:52:50.856034 139630718992384 ddar.py:60] Depth 10/1000 time = 12.712172269821167
I0123 11:53:05.319530 139630718992384 ddar.py:60] Depth 11/1000 time = 14.443855047225952
I0123 11:53:20.744508 139630718992384 ddar.py:60] Depth 12/1000 time = 15.424670457839966
I0123 11:53:40.899940 139630718992384 ddar.py:60] Depth 13/1000 time = 20.15510630607605
I0123 11:54:01.913383 139630718992384 ddar.py:60] Depth 14/1000 time = 21.01278805732727
I0123 11:54:26.484837 139630718992384 ddar.py:60] Depth 15/1000 time = 24.570980548858643
I0123 11:54:51.132457 139630718992384 ddar.py:60] Depth 16/1000 time = 24.647172927856445
I0123 11:55:16.770070 139630718992384 ddar.py:60] Depth 17/1000 time = 25.613345861434937
I0123 11:55:42.759743 139630718992384 ddar.py:60] Depth 18/1000 time = 25.989057064056396
I0123 11:56:08.983197 139630718992384 ddar.py:60] Depth 19/1000 time = 26.222544193267822
I0123 11:56:36.209593 139630718992384 ddar.py:60] Depth 20/1000 time = 27.201417922973633
I0123 11:57:03.631010 139630718992384 ddar.py:60] Depth 21/1000 time = 27.34047818183899
I0123 11:57:31.599147 139630718992384 ddar.py:60] Depth 22/1000 time = 27.967527389526367
I0123 11:58:00.516080 139630718992384 ddar.py:60] Depth 23/1000 time = 28.91633176803589
I0123 11:58:29.741118 139630718992384 ddar.py:60] Depth 24/1000 time = 29.178407192230225
I0123 11:58:29.742019 139630718992384 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:58:29.742198 139630718992384 alphageometry.py:566] LM output (score=-2.653028): "s : C d l s 29 D d s l s 30 ;"
I0123 11:58:29.742244 139630718992384 alphageometry.py:567] Translation: "s = on_line s d l, on_bline s l d"

I0123 11:58:29.742303 139630718992384 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g b f, on_line g c e; h = lc_tangent h a g, on_line h f g; i = on_line i a h, on_line i c g; j = on_line j a g, on_line j b i; k = on_circle k d b; l = circle l k g i; m = circle m k j h; n = foot n k l m; o = mirror o k n; p = foot p k c g; q = mirror q k p; r = on_line r b a, on_line r c q; s = on_line s d l, on_bline s l d ? coll o r k"
I0123 11:58:29.742565 139630718992384 graph.py:498] 
I0123 11:58:29.742635 139630718992384 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g b f, on_line g c e; h = lc_tangent h a g, on_line h f g; i = on_line i a h, on_line i c g; j = on_line j a g, on_line j b i; k = on_circle k d b; l = circle l k g i; m = circle m k j h; n = foot n k l m; o = mirror o k n; p = foot p k c g; q = mirror q k p; r = on_line r b a, on_line r c q; s = on_line s d l, on_bline s l d ? coll o r k
I0123 11:58:36.371427 139630718992384 ddar.py:60] Depth 1/1000 time = 6.5448572635650635
I0123 11:58:48.334820 139630718992384 ddar.py:60] Depth 2/1000 time = 11.963133096694946
I0123 11:59:00.763113 139630718992384 ddar.py:60] Depth 3/1000 time = 12.427958965301514
I0123 11:59:13.495756 139630718992384 ddar.py:60] Depth 4/1000 time = 12.732385635375977
I0123 11:59:25.759745 139630718992384 ddar.py:60] Depth 5/1000 time = 12.263649463653564
I0123 11:59:38.794725 139630718992384 ddar.py:60] Depth 6/1000 time = 13.034580945968628
I0123 11:59:51.512830 139630718992384 ddar.py:60] Depth 7/1000 time = 12.717370748519897
I0123 12:00:03.833732 139630718992384 ddar.py:60] Depth 8/1000 time = 12.314467430114746
I0123 12:00:17.317158 139630718992384 ddar.py:60] Depth 9/1000 time = 13.43416452407837
I0123 12:00:30.707195 139630718992384 ddar.py:60] Depth 10/1000 time = 13.389726161956787
I0123 12:00:45.523936 139630718992384 ddar.py:60] Depth 11/1000 time = 14.797945737838745
I0123 12:01:01.373617 139630718992384 ddar.py:60] Depth 12/1000 time = 15.84936809539795
I0123 12:01:21.714797 139630718992384 ddar.py:60] Depth 13/1000 time = 20.34090495109558
I0123 12:01:42.968289 139630718992384 ddar.py:60] Depth 14/1000 time = 21.25314211845398
I0123 12:02:08.384204 139630718992384 ddar.py:60] Depth 15/1000 time = 25.41560173034668
I0123 12:02:33.238032 139630718992384 ddar.py:60] Depth 16/1000 time = 24.853484630584717
I0123 12:02:58.717618 139630718992384 ddar.py:60] Depth 17/1000 time = 25.454259395599365
I0123 12:03:25.411826 139630718992384 ddar.py:60] Depth 18/1000 time = 26.69363808631897
I0123 12:03:51.718342 139630718992384 ddar.py:60] Depth 19/1000 time = 26.305985689163208
I0123 12:04:18.354354 139630718992384 ddar.py:60] Depth 20/1000 time = 26.610673427581787
I0123 12:04:46.690285 139630718992384 ddar.py:60] Depth 21/1000 time = 28.25600242614746
I0123 12:05:15.786869 139630718992384 ddar.py:60] Depth 22/1000 time = 29.096065044403076
I0123 12:05:44.841739 139630718992384 ddar.py:60] Depth 23/1000 time = 29.054193258285522
I0123 12:06:13.741997 139630718992384 ddar.py:60] Depth 24/1000 time = 28.854569911956787
I0123 12:06:13.742641 139630718992384 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:06:13.742814 139630718992384 alphageometry.py:566] LM output (score=-2.662737): "s : D d o d s 29 D l o l s 30 ;"
I0123 12:06:13.742853 139630718992384 alphageometry.py:567] Translation: "s = on_circle s d o, on_circle s l o"

I0123 12:06:13.742908 139630718992384 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g b f, on_line g c e; h = lc_tangent h a g, on_line h f g; i = on_line i a h, on_line i c g; j = on_line j a g, on_line j b i; k = on_circle k d b; l = circle l k g i; m = circle m k j h; n = foot n k l m; o = mirror o k n; p = foot p k c g; q = mirror q k p; r = on_line r b a, on_line r c q; s = on_circle s d o, on_circle s l o ? coll o r k"
I0123 12:06:13.743156 139630718992384 graph.py:498] 
I0123 12:06:13.743221 139630718992384 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g b f, on_line g c e; h = lc_tangent h a g, on_line h f g; i = on_line i a h, on_line i c g; j = on_line j a g, on_line j b i; k = on_circle k d b; l = circle l k g i; m = circle m k j h; n = foot n k l m; o = mirror o k n; p = foot p k c g; q = mirror q k p; r = on_line r b a, on_line r c q; s = on_circle s d o, on_circle s l o ? coll o r k
I0123 12:06:19.330760 139630718992384 ddar.py:60] Depth 1/1000 time = 5.492900371551514
I0123 12:06:33.047518 139630718992384 ddar.py:60] Depth 2/1000 time = 13.716545820236206
I0123 12:06:48.678103 139630718992384 ddar.py:60] Depth 3/1000 time = 15.630303859710693
I0123 12:07:04.295236 139630718992384 ddar.py:60] Depth 4/1000 time = 15.616704940795898
I0123 12:07:20.410201 139630718992384 ddar.py:60] Depth 5/1000 time = 16.114592790603638
I0123 12:07:36.636437 139630718992384 ddar.py:60] Depth 6/1000 time = 16.225984811782837
I0123 12:07:53.180866 139630718992384 ddar.py:60] Depth 7/1000 time = 16.544116497039795
I0123 12:08:08.823112 139630718992384 ddar.py:60] Depth 8/1000 time = 15.64149808883667
I0123 12:08:25.301576 139630718992384 ddar.py:60] Depth 9/1000 time = 16.469987869262695
I0123 12:08:42.597024 139630718992384 ddar.py:60] Depth 10/1000 time = 17.234758138656616
I0123 12:08:59.904770 139630718992384 ddar.py:60] Depth 11/1000 time = 17.307411432266235
I0123 12:09:19.016517 139630718992384 ddar.py:60] Depth 12/1000 time = 19.06678295135498
I0123 12:09:38.356945 139630718992384 ddar.py:60] Depth 13/1000 time = 19.340078830718994
I0123 12:10:03.673306 139630718992384 ddar.py:60] Depth 14/1000 time = 25.31605625152588
I0123 12:10:30.106969 139630718992384 ddar.py:60] Depth 15/1000 time = 26.433314323425293
I0123 12:10:59.906246 139630718992384 ddar.py:60] Depth 16/1000 time = 29.798831701278687
I0123 12:11:31.285212 139630718992384 ddar.py:60] Depth 17/1000 time = 31.37846803665161
I0123 12:12:02.342163 139630718992384 ddar.py:60] Depth 18/1000 time = 31.031840801239014
I0123 12:12:34.928670 139630718992384 ddar.py:60] Depth 19/1000 time = 32.58599901199341
I0123 12:13:07.491461 139630718992384 ddar.py:60] Depth 20/1000 time = 32.56236004829407
I0123 12:13:41.010168 139630718992384 ddar.py:60] Depth 21/1000 time = 33.48499083518982
I0123 12:14:15.922245 139630718992384 ddar.py:60] Depth 22/1000 time = 34.818359375
I0123 12:14:52.148637 139630718992384 ddar.py:60] Depth 23/1000 time = 36.22581648826599
I0123 12:15:28.125083 139630718992384 ddar.py:60] Depth 24/1000 time = 35.97560000419617
I0123 12:16:05.134517 139630718992384 ddar.py:60] Depth 25/1000 time = 36.953413009643555
I0123 12:16:05.135441 139630718992384 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:16:05.135770 139630718992384 alphageometry.py:566] LM output (score=-2.686419): "s : T d s i p 29 ;"
I0123 12:16:05.135821 139630718992384 alphageometry.py:567] Translation: "s = on_tline s d i p"

I0123 12:16:05.135881 139630718992384 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g b f, on_line g c e; h = lc_tangent h a g, on_line h f g; i = on_line i a h, on_line i c g; j = on_line j a g, on_line j b i; k = on_circle k d b; l = circle l k g i; m = circle m k j h; n = foot n k l m; o = mirror o k n; p = foot p k c g; q = mirror q k p; r = on_line r b a, on_line r c q; s = on_tline s d i p ? coll o r k"
I0123 12:16:05.136133 139630718992384 graph.py:498] 
I0123 12:16:05.136204 139630718992384 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g b f, on_line g c e; h = lc_tangent h a g, on_line h f g; i = on_line i a h, on_line i c g; j = on_line j a g, on_line j b i; k = on_circle k d b; l = circle l k g i; m = circle m k j h; n = foot n k l m; o = mirror o k n; p = foot p k c g; q = mirror q k p; r = on_line r b a, on_line r c q; s = on_tline s d i p ? coll o r k
I0123 12:16:10.592821 139630718992384 ddar.py:60] Depth 1/1000 time = 5.380852937698364
I0123 12:16:21.993419 139630718992384 ddar.py:60] Depth 2/1000 time = 11.400355815887451
I0123 12:16:34.991408 139630718992384 ddar.py:60] Depth 3/1000 time = 12.997642755508423
I0123 12:16:47.244514 139630718992384 ddar.py:60] Depth 4/1000 time = 12.252838134765625
I0123 12:16:59.478679 139630718992384 ddar.py:60] Depth 5/1000 time = 12.23375391960144
I0123 12:17:11.868989 139630718992384 ddar.py:60] Depth 6/1000 time = 12.39004898071289
I0123 12:17:24.126635 139630718992384 ddar.py:60] Depth 7/1000 time = 12.257007598876953
I0123 12:17:36.917505 139630718992384 ddar.py:60] Depth 8/1000 time = 12.785313367843628
I0123 12:17:49.556994 139630718992384 ddar.py:60] Depth 9/1000 time = 12.587902545928955
I0123 12:18:02.541171 139630718992384 ddar.py:60] Depth 10/1000 time = 12.983764171600342
I0123 12:18:17.865113 139630718992384 ddar.py:60] Depth 11/1000 time = 15.30340051651001
I0123 12:18:32.690120 139630718992384 ddar.py:60] Depth 12/1000 time = 14.824653148651123
I0123 12:18:53.360762 139630718992384 ddar.py:60] Depth 13/1000 time = 20.6702618598938
I0123 12:19:14.533332 139630718992384 ddar.py:60] Depth 14/1000 time = 21.17203450202942
I0123 12:19:39.536424 139630718992384 ddar.py:60] Depth 15/1000 time = 25.002613067626953
I0123 12:20:05.378175 139630718992384 ddar.py:60] Depth 16/1000 time = 25.841236352920532
I0123 12:20:31.206336 139630718992384 ddar.py:60] Depth 17/1000 time = 25.804842710494995
I0123 12:20:57.417257 139630718992384 ddar.py:60] Depth 18/1000 time = 26.210282564163208
I0123 12:21:24.607187 139630718992384 ddar.py:60] Depth 19/1000 time = 27.189212799072266
I0123 12:21:52.326490 139630718992384 ddar.py:60] Depth 20/1000 time = 27.694676399230957
I0123 12:22:19.527828 139630718992384 ddar.py:60] Depth 21/1000 time = 27.118746280670166
I0123 12:22:48.697722 139630718992384 ddar.py:60] Depth 22/1000 time = 29.16930603981018
I0123 12:23:16.928035 139630718992384 ddar.py:60] Depth 23/1000 time = 28.229785442352295
I0123 12:23:46.233510 139630718992384 ddar.py:60] Depth 24/1000 time = 29.262142181396484
I0123 12:23:46.234128 139630718992384 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:23:46.234274 139630718992384 alphageometry.py:566] LM output (score=-2.699662): "s : D d h d s 29 D d h h s 30 ;"
I0123 12:23:46.234314 139630718992384 alphageometry.py:567] Translation: "s = on_circle s d h, on_circle s h d"

I0123 12:23:46.234369 139630718992384 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g b f, on_line g c e; h = lc_tangent h a g, on_line h f g; i = on_line i a h, on_line i c g; j = on_line j a g, on_line j b i; k = on_circle k d b; l = circle l k g i; m = circle m k j h; n = foot n k l m; o = mirror o k n; p = foot p k c g; q = mirror q k p; r = on_line r b a, on_line r c q; s = on_circle s d h, on_circle s h d ? coll o r k"
I0123 12:23:46.234609 139630718992384 graph.py:498] 
I0123 12:23:46.234674 139630718992384 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g b f, on_line g c e; h = lc_tangent h a g, on_line h f g; i = on_line i a h, on_line i c g; j = on_line j a g, on_line j b i; k = on_circle k d b; l = circle l k g i; m = circle m k j h; n = foot n k l m; o = mirror o k n; p = foot p k c g; q = mirror q k p; r = on_line r b a, on_line r c q; s = on_circle s d h, on_circle s h d ? coll o r k
I0123 12:23:52.792837 139630718992384 ddar.py:60] Depth 1/1000 time = 6.463790416717529
I0123 12:24:04.982582 139630718992384 ddar.py:60] Depth 2/1000 time = 12.189536094665527
I0123 12:24:17.834327 139630718992384 ddar.py:60] Depth 3/1000 time = 12.851502180099487
I0123 12:24:30.855766 139630718992384 ddar.py:60] Depth 4/1000 time = 13.02119779586792
I0123 12:24:43.975779 139630718992384 ddar.py:60] Depth 5/1000 time = 13.119688749313354
I0123 12:24:57.107388 139630718992384 ddar.py:60] Depth 6/1000 time = 13.131356239318848
I0123 12:25:09.837925 139630718992384 ddar.py:60] Depth 7/1000 time = 12.72996711730957
I0123 12:25:23.087181 139630718992384 ddar.py:60] Depth 8/1000 time = 13.248337507247925
I0123 12:25:36.214388 139630718992384 ddar.py:60] Depth 9/1000 time = 13.120176076889038
I0123 12:25:50.125910 139630718992384 ddar.py:60] Depth 10/1000 time = 13.86296558380127
I0123 12:26:03.452079 139630718992384 ddar.py:60] Depth 11/1000 time = 13.325810194015503
I0123 12:26:19.446035 139630718992384 ddar.py:60] Depth 12/1000 time = 15.972892999649048
I0123 12:26:35.718159 139630718992384 ddar.py:60] Depth 13/1000 time = 16.27181100845337
I0123 12:26:57.232927 139630718992384 ddar.py:60] Depth 14/1000 time = 21.51450276374817
I0123 12:27:19.231903 139630718992384 ddar.py:60] Depth 15/1000 time = 21.998644828796387
I0123 12:27:45.324035 139630718992384 ddar.py:60] Depth 16/1000 time = 26.091858863830566
I0123 12:28:11.119998 139630718992384 ddar.py:60] Depth 17/1000 time = 25.795520544052124
I0123 12:28:38.219048 139630718992384 ddar.py:60] Depth 18/1000 time = 27.072102546691895
I0123 12:29:05.446878 139630718992384 ddar.py:60] Depth 19/1000 time = 27.227476119995117
I0123 12:29:32.915081 139630718992384 ddar.py:60] Depth 20/1000 time = 27.467763900756836
I0123 12:29:32.940405 139630718992384 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:29:32.940460 139630718992384 alphageometry.py:585] Timeout.
