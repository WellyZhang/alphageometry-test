I0123 12:32:15.529923 140479117180928 inference_utils.py:69] Parsing gin configuration.
I0123 12:32:15.530024 140479117180928 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0123 12:32:15.530222 140479117180928 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0123 12:32:15.530257 140479117180928 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0123 12:32:15.530289 140479117180928 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0123 12:32:15.530320 140479117180928 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0123 12:32:15.530350 140479117180928 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0123 12:32:15.530377 140479117180928 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0123 12:32:15.530405 140479117180928 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0123 12:32:15.530431 140479117180928 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=32
I0123 12:32:15.530457 140479117180928 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0123 12:32:15.530483 140479117180928 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0123 12:32:15.530529 140479117180928 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0123 12:32:15.530663 140479117180928 resource_reader.py:55] Path not found: base_htrans.gin
I0123 12:32:15.530870 140479117180928 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0123 12:32:15.530969 140479117180928 resource_reader.py:55] Path not found: trainer_configuration.gin
I0123 12:32:15.537367 140479117180928 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0123 12:32:15.537489 140479117180928 resource_reader.py:55] Path not found: size/medium_150M.gin
I0123 12:32:15.537832 140479117180928 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0123 12:32:15.537941 140479117180928 resource_reader.py:55] Path not found: options/positions_t5.gin
I0123 12:32:15.538227 140479117180928 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0123 12:32:15.538328 140479117180928 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0123 12:32:15.538746 140479117180928 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0123 12:32:15.538848 140479117180928 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0123 12:32:15.542623 140479117180928 training_loop.py:334] ==== Training loop: initializing model ====
I0123 12:32:15.641977 140479117180928 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0123 12:32:15.642891 140479117180928 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0123 12:32:15.649816 140479117180928 training_loop.py:335] Process 0 of 1
I0123 12:32:15.649871 140479117180928 training_loop.py:336] Local device count = 1
I0123 12:32:15.649911 140479117180928 training_loop.py:337] Number of replicas = 1
I0123 12:32:15.649942 140479117180928 training_loop.py:339] Using random number seed 42
I0123 12:32:16.145461 140479117180928 training_loop.py:359] Initializing the model.
I0123 12:32:16.577090 140479117180928 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.577367 140479117180928 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 12:32:16.577479 140479117180928 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:32:16.577563 140479117180928 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:32:16.577660 140479117180928 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:32:16.577753 140479117180928 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:32:16.577832 140479117180928 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:32:16.577907 140479117180928 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:32:16.577979 140479117180928 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:32:16.578053 140479117180928 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:32:16.578126 140479117180928 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:32:16.578199 140479117180928 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:32:16.578271 140479117180928 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:32:16.578344 140479117180928 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:32:16.578386 140479117180928 attention.py:418] Single window, no scan.
I0123 12:32:16.578434 140479117180928 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 12:32:16.578555 140479117180928 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:16.578596 140479117180928 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:16.578629 140479117180928 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:16.580773 140479117180928 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.586530 140479117180928 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:16.598661 140479117180928 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.598946 140479117180928 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:16.603518 140479117180928 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:32:16.615206 140479117180928 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:16.615320 140479117180928 attention.py:418] Single window, no scan.
I0123 12:32:16.615361 140479117180928 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:16.615395 140479117180928 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.615469 140479117180928 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.616724 140479117180928 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.616807 140479117180928 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.617540 140479117180928 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.620145 140479117180928 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.626108 140479117180928 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.627892 140479117180928 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.627975 140479117180928 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:16.628011 140479117180928 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:16.628073 140479117180928 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.628205 140479117180928 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:16.628576 140479117180928 nn_components.py:325] mlp: activation = None
I0123 12:32:16.628625 140479117180928 nn_components.py:329] mlp: final activation = None
I0123 12:32:16.630815 140479117180928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.630922 140479117180928 nn_components.py:261] mlp: residual
I0123 12:32:16.633944 140479117180928 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.634033 140479117180928 transformer_base.py:443] tbase: final FFN
I0123 12:32:16.634549 140479117180928 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:16.645380 140479117180928 nn_components.py:329] mlp: final activation = None
I0123 12:32:16.654604 140479117180928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.654707 140479117180928 nn_components.py:261] mlp: residual
I0123 12:32:16.655019 140479117180928 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.655103 140479117180928 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 12:32:16.655218 140479117180928 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:16.655259 140479117180928 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:16.655291 140479117180928 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:16.657311 140479117180928 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.659883 140479117180928 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:16.665771 140479117180928 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.666049 140479117180928 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:16.668783 140479117180928 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:32:16.672814 140479117180928 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:16.672876 140479117180928 attention.py:418] Single window, no scan.
I0123 12:32:16.672914 140479117180928 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:16.672945 140479117180928 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.673010 140479117180928 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.673609 140479117180928 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.673697 140479117180928 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.674088 140479117180928 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.674894 140479117180928 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.677490 140479117180928 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.678168 140479117180928 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.678249 140479117180928 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:16.678287 140479117180928 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:16.678350 140479117180928 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.678483 140479117180928 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:16.678826 140479117180928 nn_components.py:325] mlp: activation = None
I0123 12:32:16.678872 140479117180928 nn_components.py:329] mlp: final activation = None
I0123 12:32:16.680997 140479117180928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.681097 140479117180928 nn_components.py:261] mlp: residual
I0123 12:32:16.683748 140479117180928 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.683836 140479117180928 transformer_base.py:443] tbase: final FFN
I0123 12:32:16.684280 140479117180928 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:16.686694 140479117180928 nn_components.py:329] mlp: final activation = None
I0123 12:32:16.688651 140479117180928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.688750 140479117180928 nn_components.py:261] mlp: residual
I0123 12:32:16.689049 140479117180928 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.689128 140479117180928 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 12:32:16.689238 140479117180928 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:16.689277 140479117180928 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:16.689308 140479117180928 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:16.691314 140479117180928 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.693762 140479117180928 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:16.700014 140479117180928 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.700278 140479117180928 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:16.703080 140479117180928 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:32:16.707088 140479117180928 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:16.707147 140479117180928 attention.py:418] Single window, no scan.
I0123 12:32:16.707189 140479117180928 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:16.707221 140479117180928 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.707286 140479117180928 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.707867 140479117180928 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.707942 140479117180928 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.708304 140479117180928 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.709067 140479117180928 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.711652 140479117180928 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.712349 140479117180928 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.712430 140479117180928 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:16.712467 140479117180928 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:16.712529 140479117180928 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.712665 140479117180928 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:16.712997 140479117180928 nn_components.py:325] mlp: activation = None
I0123 12:32:16.713041 140479117180928 nn_components.py:329] mlp: final activation = None
I0123 12:32:16.715244 140479117180928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.715351 140479117180928 nn_components.py:261] mlp: residual
I0123 12:32:16.718055 140479117180928 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.718145 140479117180928 transformer_base.py:443] tbase: final FFN
I0123 12:32:16.718677 140479117180928 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:16.721128 140479117180928 nn_components.py:329] mlp: final activation = None
I0123 12:32:16.723214 140479117180928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.723322 140479117180928 nn_components.py:261] mlp: residual
I0123 12:32:16.723644 140479117180928 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.723733 140479117180928 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 12:32:16.723856 140479117180928 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:16.723901 140479117180928 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:16.723935 140479117180928 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:16.725977 140479117180928 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.728510 140479117180928 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:16.734380 140479117180928 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.734670 140479117180928 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:16.737443 140479117180928 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:32:16.741380 140479117180928 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:16.741436 140479117180928 attention.py:418] Single window, no scan.
I0123 12:32:16.741473 140479117180928 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:16.741505 140479117180928 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.741567 140479117180928 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.742145 140479117180928 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.742228 140479117180928 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.742614 140479117180928 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.743424 140479117180928 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.746042 140479117180928 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.746698 140479117180928 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.746784 140479117180928 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:16.746825 140479117180928 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:16.746891 140479117180928 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.747030 140479117180928 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:16.747367 140479117180928 nn_components.py:325] mlp: activation = None
I0123 12:32:16.747414 140479117180928 nn_components.py:329] mlp: final activation = None
I0123 12:32:16.749425 140479117180928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.749520 140479117180928 nn_components.py:261] mlp: residual
I0123 12:32:16.752198 140479117180928 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.752288 140479117180928 transformer_base.py:443] tbase: final FFN
I0123 12:32:16.752720 140479117180928 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:16.755101 140479117180928 nn_components.py:329] mlp: final activation = None
I0123 12:32:16.757063 140479117180928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.757158 140479117180928 nn_components.py:261] mlp: residual
I0123 12:32:16.757449 140479117180928 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.757529 140479117180928 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 12:32:16.757645 140479117180928 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:16.757686 140479117180928 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:16.757720 140479117180928 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:16.759812 140479117180928 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.762223 140479117180928 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:16.768316 140479117180928 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.768582 140479117180928 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:16.771398 140479117180928 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:32:16.775285 140479117180928 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:16.775342 140479117180928 attention.py:418] Single window, no scan.
I0123 12:32:16.775379 140479117180928 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:16.775410 140479117180928 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.775473 140479117180928 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.776077 140479117180928 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.776152 140479117180928 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.776508 140479117180928 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.777272 140479117180928 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.780252 140479117180928 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.780912 140479117180928 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.780992 140479117180928 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:16.781027 140479117180928 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:16.781089 140479117180928 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.781223 140479117180928 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:16.781544 140479117180928 nn_components.py:325] mlp: activation = None
I0123 12:32:16.781587 140479117180928 nn_components.py:329] mlp: final activation = None
I0123 12:32:16.783583 140479117180928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.783683 140479117180928 nn_components.py:261] mlp: residual
I0123 12:32:16.786289 140479117180928 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.786377 140479117180928 transformer_base.py:443] tbase: final FFN
I0123 12:32:16.786828 140479117180928 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:16.789227 140479117180928 nn_components.py:329] mlp: final activation = None
I0123 12:32:16.791255 140479117180928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.791355 140479117180928 nn_components.py:261] mlp: residual
I0123 12:32:16.791662 140479117180928 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.791748 140479117180928 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 12:32:16.791883 140479117180928 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:16.791930 140479117180928 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:16.791962 140479117180928 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:16.793833 140479117180928 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.796319 140479117180928 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:16.802037 140479117180928 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.802312 140479117180928 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:16.805043 140479117180928 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:32:16.808892 140479117180928 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:16.808951 140479117180928 attention.py:418] Single window, no scan.
I0123 12:32:16.808988 140479117180928 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:16.809020 140479117180928 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.809091 140479117180928 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.809745 140479117180928 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.809827 140479117180928 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.810208 140479117180928 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.811020 140479117180928 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.813529 140479117180928 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.814174 140479117180928 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.814256 140479117180928 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:16.814295 140479117180928 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:16.814361 140479117180928 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.814492 140479117180928 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:16.814827 140479117180928 nn_components.py:325] mlp: activation = None
I0123 12:32:16.814871 140479117180928 nn_components.py:329] mlp: final activation = None
I0123 12:32:16.816821 140479117180928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.816914 140479117180928 nn_components.py:261] mlp: residual
I0123 12:32:16.819525 140479117180928 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.819612 140479117180928 transformer_base.py:443] tbase: final FFN
I0123 12:32:16.820057 140479117180928 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:16.822426 140479117180928 nn_components.py:329] mlp: final activation = None
I0123 12:32:16.824393 140479117180928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.824492 140479117180928 nn_components.py:261] mlp: residual
I0123 12:32:16.824789 140479117180928 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.824871 140479117180928 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 12:32:16.824985 140479117180928 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:16.825026 140479117180928 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:16.825058 140479117180928 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:16.826967 140479117180928 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.829440 140479117180928 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:16.835162 140479117180928 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.835425 140479117180928 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:16.838129 140479117180928 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:32:16.841949 140479117180928 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:16.842005 140479117180928 attention.py:418] Single window, no scan.
I0123 12:32:16.842041 140479117180928 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:16.842071 140479117180928 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.842132 140479117180928 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.842701 140479117180928 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.842778 140479117180928 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.843143 140479117180928 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.843925 140479117180928 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.846540 140479117180928 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.847166 140479117180928 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.847243 140479117180928 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:16.847279 140479117180928 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:16.847338 140479117180928 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.847464 140479117180928 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:16.847792 140479117180928 nn_components.py:325] mlp: activation = None
I0123 12:32:16.847836 140479117180928 nn_components.py:329] mlp: final activation = None
I0123 12:32:16.849950 140479117180928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.850048 140479117180928 nn_components.py:261] mlp: residual
I0123 12:32:16.852562 140479117180928 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.852642 140479117180928 transformer_base.py:443] tbase: final FFN
I0123 12:32:16.853074 140479117180928 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:16.855746 140479117180928 nn_components.py:329] mlp: final activation = None
I0123 12:32:16.857674 140479117180928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.857777 140479117180928 nn_components.py:261] mlp: residual
I0123 12:32:16.858073 140479117180928 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:16.858152 140479117180928 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 12:32:16.858262 140479117180928 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:16.858300 140479117180928 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:16.858331 140479117180928 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:17.004744 140479117180928 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.008262 140479117180928 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:17.014658 140479117180928 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.014994 140479117180928 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:17.017948 140479117180928 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:32:17.022257 140479117180928 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:17.022321 140479117180928 attention.py:418] Single window, no scan.
I0123 12:32:17.022367 140479117180928 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:17.022404 140479117180928 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.022475 140479117180928 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.023145 140479117180928 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.023229 140479117180928 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.023623 140479117180928 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.024478 140479117180928 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.027314 140479117180928 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.028011 140479117180928 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.028095 140479117180928 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:17.028135 140479117180928 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:17.028202 140479117180928 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.028340 140479117180928 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:17.028705 140479117180928 nn_components.py:325] mlp: activation = None
I0123 12:32:17.028753 140479117180928 nn_components.py:329] mlp: final activation = None
I0123 12:32:17.030824 140479117180928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.030932 140479117180928 nn_components.py:261] mlp: residual
I0123 12:32:17.033745 140479117180928 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.033833 140479117180928 transformer_base.py:443] tbase: final FFN
I0123 12:32:17.034306 140479117180928 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:17.036822 140479117180928 nn_components.py:329] mlp: final activation = None
I0123 12:32:17.038910 140479117180928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.039027 140479117180928 nn_components.py:261] mlp: residual
I0123 12:32:17.039349 140479117180928 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.039439 140479117180928 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 12:32:17.039562 140479117180928 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:17.039605 140479117180928 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:17.039639 140479117180928 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:17.041759 140479117180928 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.044312 140479117180928 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:17.050514 140479117180928 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.050802 140479117180928 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:17.053670 140479117180928 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:32:17.057600 140479117180928 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:17.057666 140479117180928 attention.py:418] Single window, no scan.
I0123 12:32:17.057706 140479117180928 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:17.057737 140479117180928 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.057802 140479117180928 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.058388 140479117180928 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.058467 140479117180928 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.058836 140479117180928 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.059775 140479117180928 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.062582 140479117180928 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.063231 140479117180928 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.063310 140479117180928 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:17.063346 140479117180928 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:17.063406 140479117180928 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.063539 140479117180928 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:17.063870 140479117180928 nn_components.py:325] mlp: activation = None
I0123 12:32:17.063916 140479117180928 nn_components.py:329] mlp: final activation = None
I0123 12:32:17.065901 140479117180928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.066005 140479117180928 nn_components.py:261] mlp: residual
I0123 12:32:17.068667 140479117180928 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.068755 140479117180928 transformer_base.py:443] tbase: final FFN
I0123 12:32:17.069196 140479117180928 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:17.071576 140479117180928 nn_components.py:329] mlp: final activation = None
I0123 12:32:17.073602 140479117180928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.073715 140479117180928 nn_components.py:261] mlp: residual
I0123 12:32:17.074020 140479117180928 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.074111 140479117180928 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 12:32:17.074229 140479117180928 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:17.074271 140479117180928 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:17.074303 140479117180928 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:17.076230 140479117180928 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.078778 140479117180928 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:17.084633 140479117180928 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.084910 140479117180928 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:17.088096 140479117180928 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:32:17.091971 140479117180928 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:17.092030 140479117180928 attention.py:418] Single window, no scan.
I0123 12:32:17.092066 140479117180928 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:17.092098 140479117180928 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.092165 140479117180928 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.092794 140479117180928 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.092874 140479117180928 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.093249 140479117180928 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.094056 140479117180928 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.096598 140479117180928 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.097245 140479117180928 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.097326 140479117180928 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:17.097362 140479117180928 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:17.097423 140479117180928 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.097558 140479117180928 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:17.097905 140479117180928 nn_components.py:325] mlp: activation = None
I0123 12:32:17.097952 140479117180928 nn_components.py:329] mlp: final activation = None
I0123 12:32:17.099954 140479117180928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.100052 140479117180928 nn_components.py:261] mlp: residual
I0123 12:32:17.102705 140479117180928 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.102792 140479117180928 transformer_base.py:443] tbase: final FFN
I0123 12:32:17.103227 140479117180928 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:17.105614 140479117180928 nn_components.py:329] mlp: final activation = None
I0123 12:32:17.107607 140479117180928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.107707 140479117180928 nn_components.py:261] mlp: residual
I0123 12:32:17.108014 140479117180928 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.108104 140479117180928 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 12:32:17.108221 140479117180928 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:17.108263 140479117180928 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:17.108295 140479117180928 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:17.110213 140479117180928 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.112751 140479117180928 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:17.118571 140479117180928 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.118843 140479117180928 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:17.121604 140479117180928 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:32:17.125556 140479117180928 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:17.125613 140479117180928 attention.py:418] Single window, no scan.
I0123 12:32:17.125660 140479117180928 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:17.125694 140479117180928 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.125760 140479117180928 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.126345 140479117180928 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.126424 140479117180928 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.126800 140479117180928 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.127607 140479117180928 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.130176 140479117180928 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.130840 140479117180928 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.130919 140479117180928 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:17.130955 140479117180928 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:17.131018 140479117180928 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.131154 140479117180928 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:17.131485 140479117180928 nn_components.py:325] mlp: activation = None
I0123 12:32:17.131529 140479117180928 nn_components.py:329] mlp: final activation = None
I0123 12:32:17.133570 140479117180928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.133674 140479117180928 nn_components.py:261] mlp: residual
I0123 12:32:17.136510 140479117180928 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.136593 140479117180928 transformer_base.py:443] tbase: final FFN
I0123 12:32:17.137028 140479117180928 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:17.139459 140479117180928 nn_components.py:329] mlp: final activation = None
I0123 12:32:17.141443 140479117180928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.141541 140479117180928 nn_components.py:261] mlp: residual
I0123 12:32:17.141851 140479117180928 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.141938 140479117180928 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 12:32:17.142061 140479117180928 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:17.142102 140479117180928 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:17.142134 140479117180928 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:17.144107 140479117180928 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.146563 140479117180928 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:17.152365 140479117180928 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.152644 140479117180928 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:17.155415 140479117180928 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:32:17.159415 140479117180928 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:17.159473 140479117180928 attention.py:418] Single window, no scan.
I0123 12:32:17.159510 140479117180928 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:17.159541 140479117180928 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.159605 140479117180928 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.160374 140479117180928 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.160453 140479117180928 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.160828 140479117180928 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.161653 140479117180928 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.164425 140479117180928 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.165477 140479117180928 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.165563 140479117180928 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:17.165601 140479117180928 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:17.165674 140479117180928 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.165820 140479117180928 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:17.166172 140479117180928 nn_components.py:325] mlp: activation = None
I0123 12:32:17.166218 140479117180928 nn_components.py:329] mlp: final activation = None
I0123 12:32:17.168240 140479117180928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.168338 140479117180928 nn_components.py:261] mlp: residual
I0123 12:32:17.171013 140479117180928 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.171096 140479117180928 transformer_base.py:443] tbase: final FFN
I0123 12:32:17.171604 140479117180928 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:17.174021 140479117180928 nn_components.py:329] mlp: final activation = None
I0123 12:32:17.176029 140479117180928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.176134 140479117180928 nn_components.py:261] mlp: residual
I0123 12:32:17.176441 140479117180928 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.176754 140479117180928 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:32:17.176827 140479117180928 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:32:17.176901 140479117180928 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:32:17.176961 140479117180928 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:32:17.177018 140479117180928 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:32:17.177074 140479117180928 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:32:17.177129 140479117180928 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:32:17.177184 140479117180928 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:32:17.177239 140479117180928 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:32:17.177295 140479117180928 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:32:17.177350 140479117180928 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:32:17.177405 140479117180928 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:32:17.177446 140479117180928 decoder_stack.py:344] dstack: Final layernorm.
I0123 12:32:17.181205 140479117180928 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:17.231516 140479117180928 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.231610 140479117180928 decoder_stack.py:333] dstack: autoregressive generator.
I0123 12:32:17.231668 140479117180928 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 12:32:17.231779 140479117180928 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:17.231820 140479117180928 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:17.231853 140479117180928 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:17.231924 140479117180928 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.234515 140479117180928 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:17.240322 140479117180928 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.240599 140479117180928 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:17.243414 140479117180928 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:17.261146 140479117180928 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:17.261207 140479117180928 attention.py:418] Single window, no scan.
I0123 12:32:17.261247 140479117180928 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:17.261281 140479117180928 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.261346 140479117180928 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.262592 140479117180928 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.262677 140479117180928 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.263419 140479117180928 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.265518 140479117180928 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.275180 140479117180928 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.276729 140479117180928 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.276828 140479117180928 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:17.276866 140479117180928 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:17.276945 140479117180928 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.277095 140479117180928 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:17.277225 140479117180928 nn_components.py:325] mlp: activation = None
I0123 12:32:17.277267 140479117180928 nn_components.py:329] mlp: final activation = None
I0123 12:32:17.279395 140479117180928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.279496 140479117180928 nn_components.py:261] mlp: residual
I0123 12:32:17.282154 140479117180928 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.282240 140479117180928 transformer_base.py:443] tbase: final FFN
I0123 12:32:17.282357 140479117180928 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:17.284789 140479117180928 nn_components.py:329] mlp: final activation = None
I0123 12:32:17.286859 140479117180928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.286963 140479117180928 nn_components.py:261] mlp: residual
I0123 12:32:17.287274 140479117180928 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.287363 140479117180928 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 12:32:17.287483 140479117180928 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:17.287528 140479117180928 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:17.287561 140479117180928 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:17.287633 140479117180928 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.290033 140479117180928 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:17.295746 140479117180928 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.296010 140479117180928 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:17.298887 140479117180928 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:17.312580 140479117180928 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:17.312638 140479117180928 attention.py:418] Single window, no scan.
I0123 12:32:17.312674 140479117180928 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:17.312705 140479117180928 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.312769 140479117180928 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.313357 140479117180928 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.313433 140479117180928 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.313813 140479117180928 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.314531 140479117180928 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.317120 140479117180928 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.317758 140479117180928 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.317839 140479117180928 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:17.317882 140479117180928 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:17.317943 140479117180928 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.318076 140479117180928 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:17.318189 140479117180928 nn_components.py:325] mlp: activation = None
I0123 12:32:17.318228 140479117180928 nn_components.py:329] mlp: final activation = None
I0123 12:32:17.320196 140479117180928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.320293 140479117180928 nn_components.py:261] mlp: residual
I0123 12:32:17.322744 140479117180928 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.322826 140479117180928 transformer_base.py:443] tbase: final FFN
I0123 12:32:17.322938 140479117180928 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:17.325248 140479117180928 nn_components.py:329] mlp: final activation = None
I0123 12:32:17.327209 140479117180928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.327307 140479117180928 nn_components.py:261] mlp: residual
I0123 12:32:17.327602 140479117180928 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.327685 140479117180928 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 12:32:17.327799 140479117180928 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:17.327838 140479117180928 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:17.327869 140479117180928 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:17.327932 140479117180928 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.330279 140479117180928 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:17.335795 140479117180928 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.336069 140479117180928 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:17.338844 140479117180928 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:17.351888 140479117180928 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:17.351944 140479117180928 attention.py:418] Single window, no scan.
I0123 12:32:17.351980 140479117180928 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:17.352011 140479117180928 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.352073 140479117180928 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.352644 140479117180928 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.352722 140479117180928 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.353090 140479117180928 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.353949 140479117180928 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.356620 140479117180928 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.357250 140479117180928 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.357327 140479117180928 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:17.357362 140479117180928 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:17.357426 140479117180928 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.357555 140479117180928 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:17.357672 140479117180928 nn_components.py:325] mlp: activation = None
I0123 12:32:17.357713 140479117180928 nn_components.py:329] mlp: final activation = None
I0123 12:32:17.359697 140479117180928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.359797 140479117180928 nn_components.py:261] mlp: residual
I0123 12:32:17.362262 140479117180928 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.362344 140479117180928 transformer_base.py:443] tbase: final FFN
I0123 12:32:17.362454 140479117180928 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:17.364755 140479117180928 nn_components.py:329] mlp: final activation = None
I0123 12:32:17.366718 140479117180928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.366816 140479117180928 nn_components.py:261] mlp: residual
I0123 12:32:17.367105 140479117180928 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.367187 140479117180928 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 12:32:17.367299 140479117180928 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:17.367340 140479117180928 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:17.367371 140479117180928 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:17.367437 140479117180928 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.369713 140479117180928 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:17.375197 140479117180928 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.375458 140479117180928 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:17.378155 140479117180928 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:17.390980 140479117180928 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:17.391036 140479117180928 attention.py:418] Single window, no scan.
I0123 12:32:17.391072 140479117180928 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:17.391103 140479117180928 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.391364 140479117180928 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.391925 140479117180928 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.392005 140479117180928 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.392424 140479117180928 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.393118 140479117180928 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.395602 140479117180928 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.396223 140479117180928 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.396301 140479117180928 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:17.396335 140479117180928 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:17.396395 140479117180928 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.396531 140479117180928 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:17.396644 140479117180928 nn_components.py:325] mlp: activation = None
I0123 12:32:17.396683 140479117180928 nn_components.py:329] mlp: final activation = None
I0123 12:32:17.398655 140479117180928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.398751 140479117180928 nn_components.py:261] mlp: residual
I0123 12:32:17.401166 140479117180928 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.401246 140479117180928 transformer_base.py:443] tbase: final FFN
I0123 12:32:17.401356 140479117180928 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:17.403606 140479117180928 nn_components.py:329] mlp: final activation = None
I0123 12:32:17.405514 140479117180928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.405611 140479117180928 nn_components.py:261] mlp: residual
I0123 12:32:17.405907 140479117180928 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.405990 140479117180928 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 12:32:17.406100 140479117180928 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:17.406139 140479117180928 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:17.406170 140479117180928 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:17.406234 140479117180928 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.408844 140479117180928 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:17.414386 140479117180928 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.414671 140479117180928 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:17.417387 140479117180928 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:17.430754 140479117180928 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:17.430812 140479117180928 attention.py:418] Single window, no scan.
I0123 12:32:17.430849 140479117180928 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:17.430881 140479117180928 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.430947 140479117180928 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.431540 140479117180928 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.431621 140479117180928 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.431992 140479117180928 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.432687 140479117180928 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.435311 140479117180928 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.435963 140479117180928 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.436044 140479117180928 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:17.436078 140479117180928 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:17.436138 140479117180928 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.436274 140479117180928 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:17.436385 140479117180928 nn_components.py:325] mlp: activation = None
I0123 12:32:17.436424 140479117180928 nn_components.py:329] mlp: final activation = None
I0123 12:32:17.438333 140479117180928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.438433 140479117180928 nn_components.py:261] mlp: residual
I0123 12:32:17.440967 140479117180928 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.441050 140479117180928 transformer_base.py:443] tbase: final FFN
I0123 12:32:17.441163 140479117180928 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:17.443586 140479117180928 nn_components.py:329] mlp: final activation = None
I0123 12:32:17.445575 140479117180928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.445682 140479117180928 nn_components.py:261] mlp: residual
I0123 12:32:17.445984 140479117180928 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.446069 140479117180928 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 12:32:17.446183 140479117180928 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:17.446224 140479117180928 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:17.446257 140479117180928 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:17.446323 140479117180928 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.448700 140479117180928 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:17.454442 140479117180928 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.454715 140479117180928 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:17.457532 140479117180928 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:17.470916 140479117180928 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:17.470978 140479117180928 attention.py:418] Single window, no scan.
I0123 12:32:17.471015 140479117180928 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:17.471046 140479117180928 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.471110 140479117180928 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.471707 140479117180928 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.471786 140479117180928 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.472143 140479117180928 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.472847 140479117180928 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.475320 140479117180928 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.475976 140479117180928 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.476056 140479117180928 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:17.476092 140479117180928 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:17.476152 140479117180928 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.476289 140479117180928 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:17.476412 140479117180928 nn_components.py:325] mlp: activation = None
I0123 12:32:17.476453 140479117180928 nn_components.py:329] mlp: final activation = None
I0123 12:32:17.478434 140479117180928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.478531 140479117180928 nn_components.py:261] mlp: residual
I0123 12:32:17.480979 140479117180928 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.481058 140479117180928 transformer_base.py:443] tbase: final FFN
I0123 12:32:17.481169 140479117180928 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:17.483458 140479117180928 nn_components.py:329] mlp: final activation = None
I0123 12:32:17.485311 140479117180928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.485407 140479117180928 nn_components.py:261] mlp: residual
I0123 12:32:17.485699 140479117180928 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.485781 140479117180928 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 12:32:17.485890 140479117180928 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:17.485929 140479117180928 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:17.485960 140479117180928 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:17.486023 140479117180928 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.488306 140479117180928 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:17.494078 140479117180928 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.494340 140479117180928 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:17.496946 140479117180928 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:17.509884 140479117180928 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:17.509941 140479117180928 attention.py:418] Single window, no scan.
I0123 12:32:17.509977 140479117180928 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:17.510008 140479117180928 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.510069 140479117180928 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.510633 140479117180928 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.510709 140479117180928 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.511063 140479117180928 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.511749 140479117180928 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.514227 140479117180928 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.515214 140479117180928 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.515293 140479117180928 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:17.515327 140479117180928 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:17.515386 140479117180928 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.515516 140479117180928 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:17.515628 140479117180928 nn_components.py:325] mlp: activation = None
I0123 12:32:17.515672 140479117180928 nn_components.py:329] mlp: final activation = None
I0123 12:32:17.517626 140479117180928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.517736 140479117180928 nn_components.py:261] mlp: residual
I0123 12:32:17.520120 140479117180928 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.520201 140479117180928 transformer_base.py:443] tbase: final FFN
I0123 12:32:17.520310 140479117180928 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:17.522546 140479117180928 nn_components.py:329] mlp: final activation = None
I0123 12:32:17.524479 140479117180928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.524574 140479117180928 nn_components.py:261] mlp: residual
I0123 12:32:17.524859 140479117180928 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.524940 140479117180928 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 12:32:17.525050 140479117180928 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:17.525090 140479117180928 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:17.525121 140479117180928 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:17.525185 140479117180928 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.527437 140479117180928 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:17.533070 140479117180928 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.533350 140479117180928 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:17.536023 140479117180928 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:17.548704 140479117180928 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:17.548759 140479117180928 attention.py:418] Single window, no scan.
I0123 12:32:17.548795 140479117180928 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:17.548825 140479117180928 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.548887 140479117180928 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.549498 140479117180928 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.549574 140479117180928 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.549949 140479117180928 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.550633 140479117180928 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.553111 140479117180928 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.553739 140479117180928 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.553819 140479117180928 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:17.553854 140479117180928 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:17.553914 140479117180928 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.554045 140479117180928 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:17.554156 140479117180928 nn_components.py:325] mlp: activation = None
I0123 12:32:17.554201 140479117180928 nn_components.py:329] mlp: final activation = None
I0123 12:32:17.556100 140479117180928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.556194 140479117180928 nn_components.py:261] mlp: residual
I0123 12:32:17.558677 140479117180928 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.558758 140479117180928 transformer_base.py:443] tbase: final FFN
I0123 12:32:17.558867 140479117180928 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:17.561114 140479117180928 nn_components.py:329] mlp: final activation = None
I0123 12:32:17.562994 140479117180928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.563092 140479117180928 nn_components.py:261] mlp: residual
I0123 12:32:17.563384 140479117180928 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.563466 140479117180928 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 12:32:17.563577 140479117180928 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:17.563616 140479117180928 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:17.563645 140479117180928 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:17.563710 140479117180928 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.565967 140479117180928 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:17.571739 140479117180928 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.572003 140479117180928 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:17.574632 140479117180928 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:17.587437 140479117180928 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:17.587493 140479117180928 attention.py:418] Single window, no scan.
I0123 12:32:17.587529 140479117180928 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:17.587559 140479117180928 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.587622 140479117180928 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.588197 140479117180928 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.588274 140479117180928 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.588635 140479117180928 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.589339 140479117180928 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.591829 140479117180928 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.592494 140479117180928 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.592570 140479117180928 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:17.592606 140479117180928 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:17.592666 140479117180928 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.592800 140479117180928 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:17.592911 140479117180928 nn_components.py:325] mlp: activation = None
I0123 12:32:17.592949 140479117180928 nn_components.py:329] mlp: final activation = None
I0123 12:32:17.594853 140479117180928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.594949 140479117180928 nn_components.py:261] mlp: residual
I0123 12:32:17.597342 140479117180928 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.597421 140479117180928 transformer_base.py:443] tbase: final FFN
I0123 12:32:17.597528 140479117180928 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:17.599763 140479117180928 nn_components.py:329] mlp: final activation = None
I0123 12:32:17.601697 140479117180928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.601797 140479117180928 nn_components.py:261] mlp: residual
I0123 12:32:17.602095 140479117180928 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.602179 140479117180928 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 12:32:17.602293 140479117180928 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:17.602335 140479117180928 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:17.602367 140479117180928 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:17.602434 140479117180928 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.604698 140479117180928 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:17.610192 140479117180928 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.610456 140479117180928 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:17.613110 140479117180928 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:17.626184 140479117180928 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:17.626240 140479117180928 attention.py:418] Single window, no scan.
I0123 12:32:17.626276 140479117180928 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:17.626306 140479117180928 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.626366 140479117180928 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.626974 140479117180928 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.627051 140479117180928 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.627413 140479117180928 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.628105 140479117180928 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.630606 140479117180928 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.631224 140479117180928 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.631300 140479117180928 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:17.631335 140479117180928 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:17.631392 140479117180928 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.631523 140479117180928 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:17.631632 140479117180928 nn_components.py:325] mlp: activation = None
I0123 12:32:17.631671 140479117180928 nn_components.py:329] mlp: final activation = None
I0123 12:32:17.633543 140479117180928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.633652 140479117180928 nn_components.py:261] mlp: residual
I0123 12:32:17.636103 140479117180928 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.636183 140479117180928 transformer_base.py:443] tbase: final FFN
I0123 12:32:17.636292 140479117180928 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:17.638530 140479117180928 nn_components.py:329] mlp: final activation = None
I0123 12:32:17.640403 140479117180928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.640500 140479117180928 nn_components.py:261] mlp: residual
I0123 12:32:17.640785 140479117180928 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.640866 140479117180928 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 12:32:17.640975 140479117180928 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:17.641013 140479117180928 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:17.641043 140479117180928 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:17.641105 140479117180928 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.643410 140479117180928 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:17.648934 140479117180928 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.649194 140479117180928 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:17.651916 140479117180928 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:17.664690 140479117180928 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:17.664747 140479117180928 attention.py:418] Single window, no scan.
I0123 12:32:17.664783 140479117180928 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:17.664815 140479117180928 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.664877 140479117180928 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.665438 140479117180928 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.665514 140479117180928 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.665877 140479117180928 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.666572 140479117180928 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.669105 140479117180928 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.669785 140479117180928 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.669864 140479117180928 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:17.669899 140479117180928 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:17.669957 140479117180928 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.670084 140479117180928 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:17.670192 140479117180928 nn_components.py:325] mlp: activation = None
I0123 12:32:17.670231 140479117180928 nn_components.py:329] mlp: final activation = None
I0123 12:32:17.672113 140479117180928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.672214 140479117180928 nn_components.py:261] mlp: residual
I0123 12:32:17.674645 140479117180928 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.674727 140479117180928 transformer_base.py:443] tbase: final FFN
I0123 12:32:17.674836 140479117180928 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:17.677059 140479117180928 nn_components.py:329] mlp: final activation = None
I0123 12:32:17.678998 140479117180928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.679096 140479117180928 nn_components.py:261] mlp: residual
I0123 12:32:17.679383 140479117180928 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.679465 140479117180928 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 12:32:17.679574 140479117180928 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:17.679614 140479117180928 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:17.679644 140479117180928 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:17.679708 140479117180928 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.681960 140479117180928 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:17.687418 140479117180928 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.687677 140479117180928 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:17.690371 140479117180928 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:17.703041 140479117180928 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:17.703097 140479117180928 attention.py:418] Single window, no scan.
I0123 12:32:17.703134 140479117180928 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:17.703165 140479117180928 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.703227 140479117180928 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.703787 140479117180928 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.703862 140479117180928 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.704217 140479117180928 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.704957 140479117180928 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.707451 140479117180928 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.708071 140479117180928 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.708149 140479117180928 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:17.708184 140479117180928 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:17.708242 140479117180928 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.708375 140479117180928 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:17.708486 140479117180928 nn_components.py:325] mlp: activation = None
I0123 12:32:17.708524 140479117180928 nn_components.py:329] mlp: final activation = None
I0123 12:32:17.710424 140479117180928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.710519 140479117180928 nn_components.py:261] mlp: residual
I0123 12:32:17.712935 140479117180928 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.713016 140479117180928 transformer_base.py:443] tbase: final FFN
I0123 12:32:17.713124 140479117180928 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:17.715411 140479117180928 nn_components.py:329] mlp: final activation = None
I0123 12:32:17.717291 140479117180928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.717387 140479117180928 nn_components.py:261] mlp: residual
I0123 12:32:17.717682 140479117180928 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.717775 140479117180928 decoder_stack.py:344] dstack: Final layernorm.
I0123 12:32:17.720635 140479117180928 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:17.776506 140479117180928 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.776592 140479117180928 decoder_stack.py:333] dstack: autoregressive generator.
I0123 12:32:17.776647 140479117180928 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 12:32:17.776756 140479117180928 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:17.776798 140479117180928 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:17.776829 140479117180928 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:17.776896 140479117180928 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.779609 140479117180928 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:17.785017 140479117180928 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.785291 140479117180928 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:17.787875 140479117180928 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:17.800331 140479117180928 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:17.800386 140479117180928 attention.py:418] Single window, no scan.
I0123 12:32:17.800422 140479117180928 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:17.800453 140479117180928 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.800513 140479117180928 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.801069 140479117180928 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.801146 140479117180928 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.801504 140479117180928 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.802193 140479117180928 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.804692 140479117180928 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.805305 140479117180928 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.805383 140479117180928 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:17.805418 140479117180928 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:17.805477 140479117180928 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.805604 140479117180928 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:17.805727 140479117180928 nn_components.py:325] mlp: activation = None
I0123 12:32:17.805768 140479117180928 nn_components.py:329] mlp: final activation = None
I0123 12:32:17.807615 140479117180928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.807710 140479117180928 nn_components.py:261] mlp: residual
I0123 12:32:17.810111 140479117180928 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.810196 140479117180928 transformer_base.py:443] tbase: final FFN
I0123 12:32:17.810307 140479117180928 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:17.812654 140479117180928 nn_components.py:329] mlp: final activation = None
I0123 12:32:17.814635 140479117180928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.814736 140479117180928 nn_components.py:261] mlp: residual
I0123 12:32:17.815026 140479117180928 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.815107 140479117180928 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 12:32:17.815216 140479117180928 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:17.815255 140479117180928 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:17.815285 140479117180928 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:17.815348 140479117180928 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.817592 140479117180928 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:17.823133 140479117180928 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.823405 140479117180928 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:17.826086 140479117180928 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:17.838418 140479117180928 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:17.838476 140479117180928 attention.py:418] Single window, no scan.
I0123 12:32:17.838513 140479117180928 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:17.838545 140479117180928 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.838610 140479117180928 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.839171 140479117180928 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.839247 140479117180928 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.839601 140479117180928 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.840281 140479117180928 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.842787 140479117180928 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.843408 140479117180928 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.843486 140479117180928 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:17.843521 140479117180928 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:17.843580 140479117180928 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.843708 140479117180928 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:17.843819 140479117180928 nn_components.py:325] mlp: activation = None
I0123 12:32:17.843865 140479117180928 nn_components.py:329] mlp: final activation = None
I0123 12:32:17.845724 140479117180928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.845820 140479117180928 nn_components.py:261] mlp: residual
I0123 12:32:17.848209 140479117180928 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.848290 140479117180928 transformer_base.py:443] tbase: final FFN
I0123 12:32:17.848398 140479117180928 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:17.850660 140479117180928 nn_components.py:329] mlp: final activation = None
I0123 12:32:17.852504 140479117180928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.852602 140479117180928 nn_components.py:261] mlp: residual
I0123 12:32:17.852889 140479117180928 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.852971 140479117180928 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 12:32:17.853080 140479117180928 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:17.853118 140479117180928 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:17.853149 140479117180928 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:17.853212 140479117180928 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.855450 140479117180928 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:17.860848 140479117180928 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.861114 140479117180928 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:17.863902 140479117180928 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:17.876556 140479117180928 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:17.876612 140479117180928 attention.py:418] Single window, no scan.
I0123 12:32:17.876649 140479117180928 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:17.876680 140479117180928 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.876742 140479117180928 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.877297 140479117180928 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.877374 140479117180928 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.877736 140479117180928 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.878433 140479117180928 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.880983 140479117180928 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.881597 140479117180928 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.881684 140479117180928 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:17.881720 140479117180928 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:17.881779 140479117180928 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.881906 140479117180928 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:17.882015 140479117180928 nn_components.py:325] mlp: activation = None
I0123 12:32:17.882053 140479117180928 nn_components.py:329] mlp: final activation = None
I0123 12:32:17.883965 140479117180928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.884064 140479117180928 nn_components.py:261] mlp: residual
I0123 12:32:17.886524 140479117180928 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.886608 140479117180928 transformer_base.py:443] tbase: final FFN
I0123 12:32:17.886722 140479117180928 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:17.889530 140479117180928 nn_components.py:329] mlp: final activation = None
I0123 12:32:17.891464 140479117180928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.891567 140479117180928 nn_components.py:261] mlp: residual
I0123 12:32:17.891868 140479117180928 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.891953 140479117180928 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 12:32:17.892067 140479117180928 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:17.892108 140479117180928 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:17.892140 140479117180928 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:17.892207 140479117180928 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.894528 140479117180928 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:17.900021 140479117180928 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.900285 140479117180928 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:17.903004 140479117180928 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:17.915744 140479117180928 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:17.915803 140479117180928 attention.py:418] Single window, no scan.
I0123 12:32:17.915841 140479117180928 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:17.915888 140479117180928 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.915953 140479117180928 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.916526 140479117180928 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.916602 140479117180928 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.916961 140479117180928 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.917652 140479117180928 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.920271 140479117180928 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.920915 140479117180928 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.920994 140479117180928 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:17.921029 140479117180928 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:17.921090 140479117180928 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.921228 140479117180928 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:17.921343 140479117180928 nn_components.py:325] mlp: activation = None
I0123 12:32:17.921385 140479117180928 nn_components.py:329] mlp: final activation = None
I0123 12:32:17.923317 140479117180928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.923416 140479117180928 nn_components.py:261] mlp: residual
I0123 12:32:17.926016 140479117180928 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.926095 140479117180928 transformer_base.py:443] tbase: final FFN
I0123 12:32:17.926202 140479117180928 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:17.928834 140479117180928 nn_components.py:329] mlp: final activation = None
I0123 12:32:17.930712 140479117180928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.930812 140479117180928 nn_components.py:261] mlp: residual
I0123 12:32:17.931112 140479117180928 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.931194 140479117180928 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 12:32:17.931306 140479117180928 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:17.931345 140479117180928 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:17.931376 140479117180928 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:17.931441 140479117180928 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.933709 140479117180928 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:17.939283 140479117180928 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.939553 140479117180928 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:17.942283 140479117180928 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:17.955141 140479117180928 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:17.955195 140479117180928 attention.py:418] Single window, no scan.
I0123 12:32:17.955231 140479117180928 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:17.955262 140479117180928 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.955325 140479117180928 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.955904 140479117180928 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.955979 140479117180928 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.956335 140479117180928 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.957037 140479117180928 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.959677 140479117180928 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.960307 140479117180928 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.960382 140479117180928 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:17.960415 140479117180928 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:17.960473 140479117180928 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.960600 140479117180928 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:17.960713 140479117180928 nn_components.py:325] mlp: activation = None
I0123 12:32:17.960750 140479117180928 nn_components.py:329] mlp: final activation = None
I0123 12:32:17.962649 140479117180928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.962754 140479117180928 nn_components.py:261] mlp: residual
I0123 12:32:17.965190 140479117180928 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.965269 140479117180928 transformer_base.py:443] tbase: final FFN
I0123 12:32:17.965376 140479117180928 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:17.967700 140479117180928 nn_components.py:329] mlp: final activation = None
I0123 12:32:17.969565 140479117180928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.969669 140479117180928 nn_components.py:261] mlp: residual
I0123 12:32:17.969956 140479117180928 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.970036 140479117180928 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 12:32:17.970143 140479117180928 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:17.970181 140479117180928 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:17.970210 140479117180928 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:17.970271 140479117180928 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.972561 140479117180928 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:17.978045 140479117180928 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.978304 140479117180928 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:17.981045 140479117180928 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:17.993596 140479117180928 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:17.993656 140479117180928 attention.py:418] Single window, no scan.
I0123 12:32:17.993692 140479117180928 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:17.993722 140479117180928 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.993783 140479117180928 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.994341 140479117180928 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.994415 140479117180928 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.994763 140479117180928 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.995448 140479117180928 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.998001 140479117180928 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.998624 140479117180928 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.998700 140479117180928 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:17.998733 140479117180928 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:17.998789 140479117180928 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:17.998916 140479117180928 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:17.999024 140479117180928 nn_components.py:325] mlp: activation = None
I0123 12:32:17.999061 140479117180928 nn_components.py:329] mlp: final activation = None
I0123 12:32:18.000950 140479117180928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.001055 140479117180928 nn_components.py:261] mlp: residual
I0123 12:32:18.003469 140479117180928 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.003549 140479117180928 transformer_base.py:443] tbase: final FFN
I0123 12:32:18.003655 140479117180928 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:18.006376 140479117180928 nn_components.py:329] mlp: final activation = None
I0123 12:32:18.008255 140479117180928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.008350 140479117180928 nn_components.py:261] mlp: residual
I0123 12:32:18.008635 140479117180928 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.008716 140479117180928 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 12:32:18.008824 140479117180928 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:18.008862 140479117180928 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:18.008893 140479117180928 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:18.008954 140479117180928 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.011186 140479117180928 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:18.016636 140479117180928 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.016892 140479117180928 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:18.019585 140479117180928 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:18.032242 140479117180928 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:18.032296 140479117180928 attention.py:418] Single window, no scan.
I0123 12:32:18.032331 140479117180928 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:18.032360 140479117180928 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.032421 140479117180928 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.032980 140479117180928 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.033055 140479117180928 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.033411 140479117180928 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.034100 140479117180928 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.036622 140479117180928 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.037247 140479117180928 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.037323 140479117180928 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:18.037357 140479117180928 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:18.037415 140479117180928 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.037540 140479117180928 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:18.037655 140479117180928 nn_components.py:325] mlp: activation = None
I0123 12:32:18.037694 140479117180928 nn_components.py:329] mlp: final activation = None
I0123 12:32:18.039541 140479117180928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.039633 140479117180928 nn_components.py:261] mlp: residual
I0123 12:32:18.042018 140479117180928 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.042097 140479117180928 transformer_base.py:443] tbase: final FFN
I0123 12:32:18.042203 140479117180928 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:18.044478 140479117180928 nn_components.py:329] mlp: final activation = None
I0123 12:32:18.046326 140479117180928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.046421 140479117180928 nn_components.py:261] mlp: residual
I0123 12:32:18.046704 140479117180928 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.046783 140479117180928 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 12:32:18.046889 140479117180928 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:18.046927 140479117180928 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:18.046956 140479117180928 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:18.047017 140479117180928 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.049207 140479117180928 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:18.054664 140479117180928 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.054919 140479117180928 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:18.057599 140479117180928 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:18.070088 140479117180928 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:18.070141 140479117180928 attention.py:418] Single window, no scan.
I0123 12:32:18.070175 140479117180928 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:18.070204 140479117180928 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.070265 140479117180928 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.070822 140479117180928 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.070898 140479117180928 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.071257 140479117180928 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.071937 140479117180928 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.074469 140479117180928 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.075092 140479117180928 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.075166 140479117180928 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:18.075199 140479117180928 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:18.075256 140479117180928 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.075379 140479117180928 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:18.075486 140479117180928 nn_components.py:325] mlp: activation = None
I0123 12:32:18.075523 140479117180928 nn_components.py:329] mlp: final activation = None
I0123 12:32:18.077460 140479117180928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.077556 140479117180928 nn_components.py:261] mlp: residual
I0123 12:32:18.079956 140479117180928 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.080041 140479117180928 transformer_base.py:443] tbase: final FFN
I0123 12:32:18.080155 140479117180928 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:18.082416 140479117180928 nn_components.py:329] mlp: final activation = None
I0123 12:32:18.084252 140479117180928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.084348 140479117180928 nn_components.py:261] mlp: residual
I0123 12:32:18.084632 140479117180928 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.084711 140479117180928 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 12:32:18.084817 140479117180928 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:18.084854 140479117180928 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:18.084883 140479117180928 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:18.084944 140479117180928 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.087155 140479117180928 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:18.092538 140479117180928 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.092798 140479117180928 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:18.095506 140479117180928 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:18.108084 140479117180928 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:18.108138 140479117180928 attention.py:418] Single window, no scan.
I0123 12:32:18.108172 140479117180928 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:18.108201 140479117180928 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.108262 140479117180928 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.108819 140479117180928 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.108893 140479117180928 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.109250 140479117180928 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.109941 140479117180928 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.112496 140479117180928 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.113108 140479117180928 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.113184 140479117180928 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:18.113218 140479117180928 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:18.113276 140479117180928 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.113406 140479117180928 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:18.113514 140479117180928 nn_components.py:325] mlp: activation = None
I0123 12:32:18.113552 140479117180928 nn_components.py:329] mlp: final activation = None
I0123 12:32:18.115409 140479117180928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.115503 140479117180928 nn_components.py:261] mlp: residual
I0123 12:32:18.117904 140479117180928 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.117990 140479117180928 transformer_base.py:443] tbase: final FFN
I0123 12:32:18.118099 140479117180928 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:18.120760 140479117180928 nn_components.py:329] mlp: final activation = None
I0123 12:32:18.122627 140479117180928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.122723 140479117180928 nn_components.py:261] mlp: residual
I0123 12:32:18.123007 140479117180928 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.123088 140479117180928 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 12:32:18.123194 140479117180928 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:18.123232 140479117180928 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:18.123261 140479117180928 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:18.123323 140479117180928 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.125549 140479117180928 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:18.130936 140479117180928 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.131196 140479117180928 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:18.133853 140479117180928 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:18.146351 140479117180928 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:18.146405 140479117180928 attention.py:418] Single window, no scan.
I0123 12:32:18.146438 140479117180928 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:18.146468 140479117180928 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.146527 140479117180928 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.147084 140479117180928 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.147158 140479117180928 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.147505 140479117180928 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.148192 140479117180928 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.150735 140479117180928 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.151369 140479117180928 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.151445 140479117180928 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:18.151478 140479117180928 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:18.151535 140479117180928 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.151662 140479117180928 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:18.151769 140479117180928 nn_components.py:325] mlp: activation = None
I0123 12:32:18.151807 140479117180928 nn_components.py:329] mlp: final activation = None
I0123 12:32:18.154165 140479117180928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.154263 140479117180928 nn_components.py:261] mlp: residual
I0123 12:32:18.156616 140479117180928 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.156695 140479117180928 transformer_base.py:443] tbase: final FFN
I0123 12:32:18.156809 140479117180928 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:18.159059 140479117180928 nn_components.py:329] mlp: final activation = None
I0123 12:32:18.160897 140479117180928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.160991 140479117180928 nn_components.py:261] mlp: residual
I0123 12:32:18.161278 140479117180928 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.161357 140479117180928 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 12:32:18.161463 140479117180928 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:18.161501 140479117180928 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:18.161531 140479117180928 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:18.161593 140479117180928 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.163844 140479117180928 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:18.169210 140479117180928 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.169468 140479117180928 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:18.172139 140479117180928 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:18.185004 140479117180928 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:18.185058 140479117180928 attention.py:418] Single window, no scan.
I0123 12:32:18.185092 140479117180928 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:18.185121 140479117180928 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.185181 140479117180928 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.185743 140479117180928 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.185819 140479117180928 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.186177 140479117180928 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.186870 140479117180928 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.189405 140479117180928 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.190034 140479117180928 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.190110 140479117180928 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:18.190145 140479117180928 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:18.190202 140479117180928 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.190325 140479117180928 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:18.190432 140479117180928 nn_components.py:325] mlp: activation = None
I0123 12:32:18.190468 140479117180928 nn_components.py:329] mlp: final activation = None
I0123 12:32:18.192317 140479117180928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.192410 140479117180928 nn_components.py:261] mlp: residual
I0123 12:32:18.194767 140479117180928 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.194846 140479117180928 transformer_base.py:443] tbase: final FFN
I0123 12:32:18.194952 140479117180928 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:18.197206 140479117180928 nn_components.py:329] mlp: final activation = None
I0123 12:32:18.199043 140479117180928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.199136 140479117180928 nn_components.py:261] mlp: residual
I0123 12:32:18.199417 140479117180928 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.199496 140479117180928 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 12:32:18.199603 140479117180928 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:18.199640 140479117180928 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:18.199670 140479117180928 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:18.199729 140479117180928 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.201948 140479117180928 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:18.207333 140479117180928 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.207590 140479117180928 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:18.210246 140479117180928 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:18.222675 140479117180928 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:18.222728 140479117180928 attention.py:418] Single window, no scan.
I0123 12:32:18.222762 140479117180928 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:18.222791 140479117180928 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.222853 140479117180928 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.223400 140479117180928 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.223484 140479117180928 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.223846 140479117180928 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.224527 140479117180928 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.227064 140479117180928 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.227681 140479117180928 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.227759 140479117180928 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:18.227792 140479117180928 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:18.227848 140479117180928 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.227975 140479117180928 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:18.228083 140479117180928 nn_components.py:325] mlp: activation = None
I0123 12:32:18.228121 140479117180928 nn_components.py:329] mlp: final activation = None
I0123 12:32:18.229990 140479117180928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.230083 140479117180928 nn_components.py:261] mlp: residual
I0123 12:32:18.232439 140479117180928 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.232517 140479117180928 transformer_base.py:443] tbase: final FFN
I0123 12:32:18.232622 140479117180928 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:18.235278 140479117180928 nn_components.py:329] mlp: final activation = None
I0123 12:32:18.237121 140479117180928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.237215 140479117180928 nn_components.py:261] mlp: residual
I0123 12:32:18.237499 140479117180928 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:18.237583 140479117180928 decoder_stack.py:344] dstack: Final layernorm.
I0123 12:32:18.240392 140479117180928 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:22.665562 140479117180928 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
I0123 12:32:23.171302 140479117180928 training_loop.py:409] No working directory specified.
I0123 12:32:23.171412 140479117180928 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0123 12:32:23.172141 140479117180928 checkpoints.py:1062] Restoring legacy Flax checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0123 12:32:26.311215 140479117180928 training_loop.py:447] Only restoring trainable parameters.
I0123 12:32:26.311916 140479117180928 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0123 12:32:26.311975 140479117180928 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0123 12:32:26.312021 140479117180928 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:32:26.312061 140479117180928 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:32:26.312101 140479117180928 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:26.312138 140479117180928 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:26.312176 140479117180928 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:26.312213 140479117180928 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:26.312250 140479117180928 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:32:26.312286 140479117180928 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:32:26.312321 140479117180928 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:26.312357 140479117180928 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:26.312392 140479117180928 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:32:26.312428 140479117180928 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:32:26.312463 140479117180928 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:26.312498 140479117180928 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:26.312535 140479117180928 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:26.312570 140479117180928 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:26.312605 140479117180928 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:32:26.312641 140479117180928 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:32:26.312690 140479117180928 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:26.312728 140479117180928 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:26.312765 140479117180928 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:32:26.312801 140479117180928 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:32:26.312836 140479117180928 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:26.312871 140479117180928 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:26.312906 140479117180928 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:26.312941 140479117180928 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:26.312976 140479117180928 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:32:26.313011 140479117180928 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:32:26.313045 140479117180928 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:26.313080 140479117180928 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:26.313115 140479117180928 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:32:26.313149 140479117180928 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:32:26.313185 140479117180928 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:26.313220 140479117180928 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:26.313255 140479117180928 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:26.313290 140479117180928 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:26.313325 140479117180928 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:32:26.313361 140479117180928 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:32:26.313397 140479117180928 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:26.313432 140479117180928 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:26.313468 140479117180928 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:32:26.313503 140479117180928 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:32:26.313539 140479117180928 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:26.313573 140479117180928 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:26.313613 140479117180928 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:26.313664 140479117180928 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:26.313704 140479117180928 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:32:26.313741 140479117180928 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:32:26.313777 140479117180928 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:26.313812 140479117180928 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:26.313846 140479117180928 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:32:26.313881 140479117180928 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:32:26.313916 140479117180928 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:26.313951 140479117180928 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:26.313986 140479117180928 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:26.314021 140479117180928 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:26.314056 140479117180928 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:32:26.314090 140479117180928 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:32:26.314124 140479117180928 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:26.314159 140479117180928 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:26.314194 140479117180928 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:32:26.314229 140479117180928 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:32:26.314263 140479117180928 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:26.314299 140479117180928 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:26.314334 140479117180928 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:26.314370 140479117180928 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:26.314405 140479117180928 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:32:26.314440 140479117180928 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:32:26.314476 140479117180928 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:26.314511 140479117180928 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:26.314546 140479117180928 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:32:26.314589 140479117180928 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:32:26.314626 140479117180928 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:26.314662 140479117180928 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:26.314698 140479117180928 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:26.314734 140479117180928 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:26.314770 140479117180928 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:32:26.314805 140479117180928 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:32:26.314841 140479117180928 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:26.314875 140479117180928 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:26.314911 140479117180928 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:32:26.314946 140479117180928 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:32:26.314980 140479117180928 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:26.315015 140479117180928 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:26.315049 140479117180928 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:26.315085 140479117180928 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:26.315120 140479117180928 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:32:26.315154 140479117180928 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:32:26.315188 140479117180928 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:26.315223 140479117180928 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:26.315258 140479117180928 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:32:26.315294 140479117180928 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:32:26.315329 140479117180928 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:26.315363 140479117180928 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:26.315398 140479117180928 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:26.315433 140479117180928 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:26.315468 140479117180928 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:32:26.315503 140479117180928 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:32:26.315542 140479117180928 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:26.315579 140479117180928 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:26.315615 140479117180928 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:32:26.315650 140479117180928 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:32:26.315685 140479117180928 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:26.315720 140479117180928 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:26.315756 140479117180928 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:26.315790 140479117180928 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:26.315825 140479117180928 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:32:26.315859 140479117180928 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:32:26.315893 140479117180928 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:26.315927 140479117180928 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:26.315962 140479117180928 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:32:26.315996 140479117180928 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:32:26.316030 140479117180928 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:26.316065 140479117180928 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:26.316100 140479117180928 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:26.316135 140479117180928 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:26.316171 140479117180928 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:32:26.316207 140479117180928 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:32:26.316242 140479117180928 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:26.316276 140479117180928 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:26.316303 140479117180928 training_loop.py:725] Total parameters: 152072288
I0123 12:32:26.316513 140479117180928 training_loop.py:739] Total state size: 0
I0123 12:32:26.336932 140479117180928 training_loop.py:492] Training loop: creating task for mode beam_search
I0123 12:32:26.337195 140479117180928 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0123 12:32:26.337548 140479117180928 training_loop.py:652] Compiling mode beam_search with jit.
I0123 12:32:26.337878 140479117180928 training_loop.py:89] registering functions: dict_keys([])
I0123 12:32:26.353979 140479117180928 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = mirror e b a; f = on_circle f d b; g = circle g f a e; h = lc_tangent h b d, on_line h a d; i = on_circle i g f, on_line i h b; j = on_circle j g f, on_line j h b; k = on_circle k d f, on_line k j f ? perp k e e g
I0123 12:32:27.991055 140479117180928 ddar.py:60] Depth 1/1000 time = 1.5420684814453125
I0123 12:32:31.167812 140479117180928 ddar.py:60] Depth 2/1000 time = 3.1765878200531006
I0123 12:32:34.527866 140479117180928 ddar.py:60] Depth 3/1000 time = 3.359874963760376
I0123 12:32:38.431186 140479117180928 ddar.py:60] Depth 4/1000 time = 3.9029927253723145
I0123 12:32:38.438178 140479117180928 alphageometry.py:191] 
==========================
 * From theorem premises:
A B D E F G H J K : Points
DA = DB [00]
E,B,A are collinear [01]
AB = AE [02]
DF = DB [03]
GA = GE [04]
GF = GA [05]
BH  BD [06]
GJ = GF [07]
J,H,B are collinear [08]
DK = DF [09]
J,K,F are collinear [10]

 * Auxiliary Constructions:
I : Points
GI = GF [11]
H,I,B are collinear [12]

 * Proof steps:
001. GA = GE [04] & GF = GA [05] & GJ = GF [07] & GI = GF [11]   G is the circumcenter of \Delta EJI [13]
002. GF = GA [05] & GJ = GF [07] & GI = GF [11] & GA = GE [04]   J,E,F,A are concyclic [14]
003. GF = GA [05] & GJ = GF [07] & GI = GF [11] & J,E,F,A are concyclic [14] & GA = GE [04]   I,J,E,A are concyclic [15]
004. I,J,E,A are concyclic [15]   IJA = IEA [16]
005. DA = DB [00] & DF = DB [03] & DK = DF [09]   K,B,F,A are concyclic [17]
006. DA = DB [00] & DF = DB [03] & DK = DF [09]   D is the circumcenter of \Delta BAK [18]
007. K,B,F,A are concyclic [17]   KBA = KFA [19]
008. J,E,F,A are concyclic [14]   JEA = JFA [20]
009. E,B,A are collinear [01] & KBA = KFA [19] & J,K,F are collinear [10] & JEA = JFA [20]   JEB = KBA [21]
010. D is the circumcenter of \Delta BAK [18] & BH  BD [06]   HBA = BKA [22]
011. J,H,B are collinear [08] & E,B,A are collinear [01] & HBA = BKA [22]   JBE = BKA [23]
012. JEB = KBA [21] & JBE = BKA [23] (Similar Triangles)  EJ:BA = EB:BK [24]
013. EJ:BA = EB:BK [24] & AB = AE [02]   JE:EA = EB:KB [25]
014. E,B,A are collinear [01] & KBA = KFA [19] & J,K,F are collinear [10] & JEA = JFA [20]   JEA = KBE [26]
015. JE:EA = EB:KB [25] & JEA = KBE [26] (Similar Triangles)  EJA = KEB [27]
016. J,H,B are collinear [08] & H,I,B are collinear [12] & IJA = IEA [16] & E,B,A are collinear [01] & EJA = KEB [27]   KEJ = EIJ [28]
017. G is the circumcenter of \Delta EJI [13] & KEJ = EIJ [28]   EG  EK
==========================

