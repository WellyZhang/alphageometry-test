I0123 20:40:09.717590 140446420529152 inference_utils.py:69] Parsing gin configuration.
I0123 20:40:09.717693 140446420529152 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0123 20:40:09.717881 140446420529152 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0123 20:40:09.717913 140446420529152 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0123 20:40:09.717941 140446420529152 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0123 20:40:09.717967 140446420529152 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0123 20:40:09.717996 140446420529152 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0123 20:40:09.718024 140446420529152 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0123 20:40:09.718050 140446420529152 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0123 20:40:09.718077 140446420529152 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=32
I0123 20:40:09.718102 140446420529152 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0123 20:40:09.718128 140446420529152 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0123 20:40:09.718173 140446420529152 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0123 20:40:09.718289 140446420529152 resource_reader.py:55] Path not found: base_htrans.gin
I0123 20:40:09.718465 140446420529152 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0123 20:40:09.718563 140446420529152 resource_reader.py:55] Path not found: trainer_configuration.gin
I0123 20:40:09.724796 140446420529152 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0123 20:40:09.724914 140446420529152 resource_reader.py:55] Path not found: size/medium_150M.gin
I0123 20:40:09.725229 140446420529152 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0123 20:40:09.725332 140446420529152 resource_reader.py:55] Path not found: options/positions_t5.gin
I0123 20:40:09.725603 140446420529152 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0123 20:40:09.725708 140446420529152 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0123 20:40:09.726111 140446420529152 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0123 20:40:09.726208 140446420529152 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0123 20:40:09.729854 140446420529152 training_loop.py:334] ==== Training loop: initializing model ====
I0123 20:40:09.843373 140446420529152 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0123 20:40:09.844074 140446420529152 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0123 20:40:09.850752 140446420529152 training_loop.py:335] Process 0 of 1
I0123 20:40:09.850805 140446420529152 training_loop.py:336] Local device count = 1
I0123 20:40:09.850843 140446420529152 training_loop.py:337] Number of replicas = 1
I0123 20:40:09.850873 140446420529152 training_loop.py:339] Using random number seed 42
I0123 20:40:10.334976 140446420529152 training_loop.py:359] Initializing the model.
I0123 20:40:10.734239 140446420529152 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.734503 140446420529152 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 20:40:10.734607 140446420529152 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 20:40:10.734684 140446420529152 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 20:40:10.734760 140446420529152 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 20:40:10.734842 140446420529152 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 20:40:10.734915 140446420529152 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 20:40:10.734986 140446420529152 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 20:40:10.735054 140446420529152 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 20:40:10.735121 140446420529152 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 20:40:10.735188 140446420529152 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 20:40:10.735255 140446420529152 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 20:40:10.735321 140446420529152 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 20:40:10.735389 140446420529152 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 20:40:10.735428 140446420529152 attention.py:418] Single window, no scan.
I0123 20:40:10.735472 140446420529152 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 20:40:10.735584 140446420529152 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:40:10.735621 140446420529152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:40:10.735651 140446420529152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:40:10.737612 140446420529152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.742877 140446420529152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:40:10.753443 140446420529152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.753725 140446420529152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:40:10.758069 140446420529152 transformer_layer.py:213] tlayer: windowed attention.
I0123 20:40:10.768619 140446420529152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:40:10.768675 140446420529152 attention.py:418] Single window, no scan.
I0123 20:40:10.768713 140446420529152 transformer_layer.py:389] tlayer: self-attention.
I0123 20:40:10.768745 140446420529152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.768808 140446420529152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.769991 140446420529152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.770068 140446420529152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.770775 140446420529152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.773232 140446420529152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.778974 140446420529152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.780680 140446420529152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.780759 140446420529152 transformer_layer.py:468] tlayer: End windows.
I0123 20:40:10.780793 140446420529152 transformer_layer.py:472] tlayer: final FFN.
I0123 20:40:10.780858 140446420529152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.780988 140446420529152 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:40:10.781312 140446420529152 nn_components.py:325] mlp: activation = None
I0123 20:40:10.781358 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:40:10.783263 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.783365 140446420529152 nn_components.py:261] mlp: residual
I0123 20:40:10.786240 140446420529152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.786318 140446420529152 transformer_base.py:443] tbase: final FFN
I0123 20:40:10.786803 140446420529152 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:40:10.796893 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:40:10.805628 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.805736 140446420529152 nn_components.py:261] mlp: residual
I0123 20:40:10.806035 140446420529152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.806115 140446420529152 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 20:40:10.806225 140446420529152 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:40:10.806262 140446420529152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:40:10.806293 140446420529152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:40:10.808092 140446420529152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.810559 140446420529152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:40:10.816071 140446420529152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.816325 140446420529152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:40:10.819003 140446420529152 transformer_layer.py:213] tlayer: windowed attention.
I0123 20:40:10.822763 140446420529152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:40:10.822818 140446420529152 attention.py:418] Single window, no scan.
I0123 20:40:10.822853 140446420529152 transformer_layer.py:389] tlayer: self-attention.
I0123 20:40:10.822882 140446420529152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.822943 140446420529152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.823501 140446420529152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.823575 140446420529152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.823938 140446420529152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.824701 140446420529152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.827183 140446420529152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.827799 140446420529152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.827873 140446420529152 transformer_layer.py:468] tlayer: End windows.
I0123 20:40:10.827907 140446420529152 transformer_layer.py:472] tlayer: final FFN.
I0123 20:40:10.827965 140446420529152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.828091 140446420529152 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:40:10.828404 140446420529152 nn_components.py:325] mlp: activation = None
I0123 20:40:10.828446 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:40:10.830374 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.830467 140446420529152 nn_components.py:261] mlp: residual
I0123 20:40:10.832944 140446420529152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.833025 140446420529152 transformer_base.py:443] tbase: final FFN
I0123 20:40:10.833442 140446420529152 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:40:10.835735 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:40:10.837599 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.837700 140446420529152 nn_components.py:261] mlp: residual
I0123 20:40:10.837992 140446420529152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.838071 140446420529152 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 20:40:10.838179 140446420529152 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:40:10.838216 140446420529152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:40:10.838246 140446420529152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:40:10.840125 140446420529152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.842459 140446420529152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:40:10.848302 140446420529152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.848562 140446420529152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:40:10.851197 140446420529152 transformer_layer.py:213] tlayer: windowed attention.
I0123 20:40:10.855030 140446420529152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:40:10.855084 140446420529152 attention.py:418] Single window, no scan.
I0123 20:40:10.855119 140446420529152 transformer_layer.py:389] tlayer: self-attention.
I0123 20:40:10.855150 140446420529152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.855211 140446420529152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.855772 140446420529152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.855846 140446420529152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.856204 140446420529152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.856970 140446420529152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.859481 140446420529152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.860147 140446420529152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.860223 140446420529152 transformer_layer.py:468] tlayer: End windows.
I0123 20:40:10.860256 140446420529152 transformer_layer.py:472] tlayer: final FFN.
I0123 20:40:10.860314 140446420529152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.860441 140446420529152 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:40:10.860760 140446420529152 nn_components.py:325] mlp: activation = None
I0123 20:40:10.860804 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:40:10.862719 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.862811 140446420529152 nn_components.py:261] mlp: residual
I0123 20:40:10.865314 140446420529152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.865395 140446420529152 transformer_base.py:443] tbase: final FFN
I0123 20:40:10.865887 140446420529152 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:40:10.868158 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:40:10.870082 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.870178 140446420529152 nn_components.py:261] mlp: residual
I0123 20:40:10.870472 140446420529152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.870550 140446420529152 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 20:40:10.870658 140446420529152 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:40:10.870696 140446420529152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:40:10.870727 140446420529152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:40:10.872605 140446420529152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.875028 140446420529152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:40:10.880646 140446420529152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.880903 140446420529152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:40:10.883536 140446420529152 transformer_layer.py:213] tlayer: windowed attention.
I0123 20:40:10.887331 140446420529152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:40:10.887385 140446420529152 attention.py:418] Single window, no scan.
I0123 20:40:10.887420 140446420529152 transformer_layer.py:389] tlayer: self-attention.
I0123 20:40:10.887449 140446420529152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.887509 140446420529152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.888061 140446420529152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.888134 140446420529152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.888497 140446420529152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.889266 140446420529152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.891797 140446420529152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.892418 140446420529152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.892493 140446420529152 transformer_layer.py:468] tlayer: End windows.
I0123 20:40:10.892527 140446420529152 transformer_layer.py:472] tlayer: final FFN.
I0123 20:40:10.892589 140446420529152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.892713 140446420529152 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:40:10.893034 140446420529152 nn_components.py:325] mlp: activation = None
I0123 20:40:10.893076 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:40:10.894974 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.895066 140446420529152 nn_components.py:261] mlp: residual
I0123 20:40:10.897619 140446420529152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.897712 140446420529152 transformer_base.py:443] tbase: final FFN
I0123 20:40:10.898137 140446420529152 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:40:10.900386 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:40:10.902279 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.902373 140446420529152 nn_components.py:261] mlp: residual
I0123 20:40:10.902665 140446420529152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.902745 140446420529152 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 20:40:10.902854 140446420529152 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:40:10.902891 140446420529152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:40:10.902921 140446420529152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:40:10.904801 140446420529152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.907164 140446420529152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:40:10.912723 140446420529152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.912985 140446420529152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:40:10.915700 140446420529152 transformer_layer.py:213] tlayer: windowed attention.
I0123 20:40:10.920270 140446420529152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:40:10.920382 140446420529152 attention.py:418] Single window, no scan.
I0123 20:40:10.920418 140446420529152 transformer_layer.py:389] tlayer: self-attention.
I0123 20:40:10.920448 140446420529152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.920517 140446420529152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.921116 140446420529152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.921196 140446420529152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.921563 140446420529152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.922341 140446420529152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.925220 140446420529152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.925850 140446420529152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.925927 140446420529152 transformer_layer.py:468] tlayer: End windows.
I0123 20:40:10.925961 140446420529152 transformer_layer.py:472] tlayer: final FFN.
I0123 20:40:10.926018 140446420529152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.926148 140446420529152 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:40:10.926489 140446420529152 nn_components.py:325] mlp: activation = None
I0123 20:40:10.926532 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:40:10.928438 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.928530 140446420529152 nn_components.py:261] mlp: residual
I0123 20:40:10.931133 140446420529152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.931211 140446420529152 transformer_base.py:443] tbase: final FFN
I0123 20:40:10.931642 140446420529152 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:40:10.933926 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:40:10.935895 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.935989 140446420529152 nn_components.py:261] mlp: residual
I0123 20:40:10.936285 140446420529152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.936366 140446420529152 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 20:40:10.936476 140446420529152 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:40:10.936515 140446420529152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:40:10.936547 140446420529152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:40:10.938406 140446420529152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.940811 140446420529152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:40:10.946409 140446420529152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.946662 140446420529152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:40:10.949340 140446420529152 transformer_layer.py:213] tlayer: windowed attention.
I0123 20:40:10.953104 140446420529152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:40:10.953158 140446420529152 attention.py:418] Single window, no scan.
I0123 20:40:10.953192 140446420529152 transformer_layer.py:389] tlayer: self-attention.
I0123 20:40:10.953221 140446420529152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.953280 140446420529152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.953887 140446420529152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.953963 140446420529152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.954323 140446420529152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.955110 140446420529152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.957611 140446420529152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.958247 140446420529152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.958323 140446420529152 transformer_layer.py:468] tlayer: End windows.
I0123 20:40:10.958358 140446420529152 transformer_layer.py:472] tlayer: final FFN.
I0123 20:40:10.958416 140446420529152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.958539 140446420529152 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:40:10.958860 140446420529152 nn_components.py:325] mlp: activation = None
I0123 20:40:10.958902 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:40:10.960808 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.960901 140446420529152 nn_components.py:261] mlp: residual
I0123 20:40:10.963490 140446420529152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.963569 140446420529152 transformer_base.py:443] tbase: final FFN
I0123 20:40:10.963997 140446420529152 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:40:10.966326 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:40:10.968256 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.968349 140446420529152 nn_components.py:261] mlp: residual
I0123 20:40:10.968648 140446420529152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.968729 140446420529152 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 20:40:10.968839 140446420529152 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:40:10.968878 140446420529152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:40:10.968908 140446420529152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:40:10.970738 140446420529152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.973199 140446420529152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:40:10.978817 140446420529152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.979080 140446420529152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:40:10.981746 140446420529152 transformer_layer.py:213] tlayer: windowed attention.
I0123 20:40:10.985509 140446420529152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:40:10.985562 140446420529152 attention.py:418] Single window, no scan.
I0123 20:40:10.985597 140446420529152 transformer_layer.py:389] tlayer: self-attention.
I0123 20:40:10.985627 140446420529152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.985695 140446420529152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.986259 140446420529152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.986335 140446420529152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.986691 140446420529152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.987462 140446420529152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.989971 140446420529152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.990587 140446420529152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.990664 140446420529152 transformer_layer.py:468] tlayer: End windows.
I0123 20:40:10.990699 140446420529152 transformer_layer.py:472] tlayer: final FFN.
I0123 20:40:10.990755 140446420529152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.990878 140446420529152 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:40:10.991197 140446420529152 nn_components.py:325] mlp: activation = None
I0123 20:40:10.991240 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:40:10.993197 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.993290 140446420529152 nn_components.py:261] mlp: residual
I0123 20:40:10.995839 140446420529152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:10.995917 140446420529152 transformer_base.py:443] tbase: final FFN
I0123 20:40:10.996345 140446420529152 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:40:10.998985 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:40:11.000881 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.000980 140446420529152 nn_components.py:261] mlp: residual
I0123 20:40:11.001281 140446420529152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.001361 140446420529152 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 20:40:11.001472 140446420529152 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:40:11.001510 140446420529152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:40:11.001541 140446420529152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:40:11.138736 140446420529152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.141785 140446420529152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:40:11.147656 140446420529152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.147950 140446420529152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:40:11.150704 140446420529152 transformer_layer.py:213] tlayer: windowed attention.
I0123 20:40:11.154681 140446420529152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:40:11.154739 140446420529152 attention.py:418] Single window, no scan.
I0123 20:40:11.154776 140446420529152 transformer_layer.py:389] tlayer: self-attention.
I0123 20:40:11.154809 140446420529152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.154876 140446420529152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.155499 140446420529152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.155578 140446420529152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.155954 140446420529152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.156764 140446420529152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.159375 140446420529152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.160022 140446420529152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.160101 140446420529152 transformer_layer.py:468] tlayer: End windows.
I0123 20:40:11.160140 140446420529152 transformer_layer.py:472] tlayer: final FFN.
I0123 20:40:11.160203 140446420529152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.160334 140446420529152 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:40:11.160678 140446420529152 nn_components.py:325] mlp: activation = None
I0123 20:40:11.160724 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:40:11.162667 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.162760 140446420529152 nn_components.py:261] mlp: residual
I0123 20:40:11.165428 140446420529152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.165508 140446420529152 transformer_base.py:443] tbase: final FFN
I0123 20:40:11.165960 140446420529152 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:40:11.168312 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:40:11.170242 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.170349 140446420529152 nn_components.py:261] mlp: residual
I0123 20:40:11.170652 140446420529152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.170737 140446420529152 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 20:40:11.170848 140446420529152 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:40:11.170887 140446420529152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:40:11.170918 140446420529152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:40:11.172855 140446420529152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.175283 140446420529152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:40:11.180888 140446420529152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.181148 140446420529152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:40:11.183847 140446420529152 transformer_layer.py:213] tlayer: windowed attention.
I0123 20:40:11.187622 140446420529152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:40:11.187676 140446420529152 attention.py:418] Single window, no scan.
I0123 20:40:11.187712 140446420529152 transformer_layer.py:389] tlayer: self-attention.
I0123 20:40:11.187742 140446420529152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.187805 140446420529152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.188358 140446420529152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.188432 140446420529152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.188786 140446420529152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.189549 140446420529152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.192093 140446420529152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.192704 140446420529152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.192780 140446420529152 transformer_layer.py:468] tlayer: End windows.
I0123 20:40:11.192814 140446420529152 transformer_layer.py:472] tlayer: final FFN.
I0123 20:40:11.192870 140446420529152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.192994 140446420529152 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:40:11.193312 140446420529152 nn_components.py:325] mlp: activation = None
I0123 20:40:11.193355 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:40:11.195241 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.195334 140446420529152 nn_components.py:261] mlp: residual
I0123 20:40:11.197876 140446420529152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.197953 140446420529152 transformer_base.py:443] tbase: final FFN
I0123 20:40:11.198379 140446420529152 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:40:11.200655 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:40:11.202618 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.202715 140446420529152 nn_components.py:261] mlp: residual
I0123 20:40:11.203006 140446420529152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.203091 140446420529152 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 20:40:11.203203 140446420529152 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:40:11.203242 140446420529152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:40:11.203273 140446420529152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:40:11.205075 140446420529152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.207489 140446420529152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:40:11.213146 140446420529152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.213404 140446420529152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:40:11.216415 140446420529152 transformer_layer.py:213] tlayer: windowed attention.
I0123 20:40:11.220169 140446420529152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:40:11.220222 140446420529152 attention.py:418] Single window, no scan.
I0123 20:40:11.220257 140446420529152 transformer_layer.py:389] tlayer: self-attention.
I0123 20:40:11.220287 140446420529152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.220347 140446420529152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.220942 140446420529152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.221018 140446420529152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.221373 140446420529152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.222157 140446420529152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.224624 140446420529152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.225228 140446420529152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.225303 140446420529152 transformer_layer.py:468] tlayer: End windows.
I0123 20:40:11.225338 140446420529152 transformer_layer.py:472] tlayer: final FFN.
I0123 20:40:11.225395 140446420529152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.225524 140446420529152 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:40:11.225851 140446420529152 nn_components.py:325] mlp: activation = None
I0123 20:40:11.225895 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:40:11.227800 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.227895 140446420529152 nn_components.py:261] mlp: residual
I0123 20:40:11.230451 140446420529152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.230528 140446420529152 transformer_base.py:443] tbase: final FFN
I0123 20:40:11.230956 140446420529152 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:40:11.233268 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:40:11.235168 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.235261 140446420529152 nn_components.py:261] mlp: residual
I0123 20:40:11.235554 140446420529152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.235639 140446420529152 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 20:40:11.235752 140446420529152 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:40:11.235791 140446420529152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:40:11.235821 140446420529152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:40:11.237648 140446420529152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.240082 140446420529152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:40:11.245629 140446420529152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.245893 140446420529152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:40:11.248504 140446420529152 transformer_layer.py:213] tlayer: windowed attention.
I0123 20:40:11.252278 140446420529152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:40:11.252331 140446420529152 attention.py:418] Single window, no scan.
I0123 20:40:11.252366 140446420529152 transformer_layer.py:389] tlayer: self-attention.
I0123 20:40:11.252395 140446420529152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.252456 140446420529152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.253013 140446420529152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.253087 140446420529152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.253442 140446420529152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.254221 140446420529152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.256700 140446420529152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.257316 140446420529152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.257391 140446420529152 transformer_layer.py:468] tlayer: End windows.
I0123 20:40:11.257426 140446420529152 transformer_layer.py:472] tlayer: final FFN.
I0123 20:40:11.257486 140446420529152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.257611 140446420529152 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:40:11.257939 140446420529152 nn_components.py:325] mlp: activation = None
I0123 20:40:11.257986 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:40:11.259923 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.260015 140446420529152 nn_components.py:261] mlp: residual
I0123 20:40:11.262803 140446420529152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.262881 140446420529152 transformer_base.py:443] tbase: final FFN
I0123 20:40:11.263304 140446420529152 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:40:11.265607 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:40:11.267487 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.267585 140446420529152 nn_components.py:261] mlp: residual
I0123 20:40:11.267876 140446420529152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.267954 140446420529152 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 20:40:11.268071 140446420529152 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:40:11.268110 140446420529152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:40:11.268140 140446420529152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:40:11.270045 140446420529152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.272416 140446420529152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:40:11.277975 140446420529152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.278225 140446420529152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:40:11.280847 140446420529152 transformer_layer.py:213] tlayer: windowed attention.
I0123 20:40:11.284624 140446420529152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:40:11.284679 140446420529152 attention.py:418] Single window, no scan.
I0123 20:40:11.284714 140446420529152 transformer_layer.py:389] tlayer: self-attention.
I0123 20:40:11.284744 140446420529152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.284807 140446420529152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.285362 140446420529152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.285437 140446420529152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.285800 140446420529152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.286568 140446420529152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.289038 140446420529152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.290012 140446420529152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.290090 140446420529152 transformer_layer.py:468] tlayer: End windows.
I0123 20:40:11.290124 140446420529152 transformer_layer.py:472] tlayer: final FFN.
I0123 20:40:11.290182 140446420529152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.290311 140446420529152 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:40:11.290629 140446420529152 nn_components.py:325] mlp: activation = None
I0123 20:40:11.290671 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:40:11.292541 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.292632 140446420529152 nn_components.py:261] mlp: residual
I0123 20:40:11.295130 140446420529152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.295208 140446420529152 transformer_base.py:443] tbase: final FFN
I0123 20:40:11.295679 140446420529152 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:40:11.297909 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:40:11.299801 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.299894 140446420529152 nn_components.py:261] mlp: residual
I0123 20:40:11.300197 140446420529152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.300472 140446420529152 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 20:40:11.300541 140446420529152 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 20:40:11.300606 140446420529152 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 20:40:11.300664 140446420529152 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 20:40:11.300718 140446420529152 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 20:40:11.300771 140446420529152 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 20:40:11.300823 140446420529152 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 20:40:11.300875 140446420529152 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 20:40:11.300925 140446420529152 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 20:40:11.300976 140446420529152 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 20:40:11.301026 140446420529152 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 20:40:11.301078 140446420529152 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 20:40:11.301115 140446420529152 decoder_stack.py:344] dstack: Final layernorm.
I0123 20:40:11.304614 140446420529152 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:40:11.351573 140446420529152 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.351656 140446420529152 decoder_stack.py:333] dstack: autoregressive generator.
I0123 20:40:11.351711 140446420529152 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 20:40:11.351815 140446420529152 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:40:11.351853 140446420529152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:40:11.351882 140446420529152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:40:11.351945 140446420529152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.354357 140446420529152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:40:11.359773 140446420529152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.360031 140446420529152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:40:11.362663 140446420529152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:40:11.378929 140446420529152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:40:11.378984 140446420529152 attention.py:418] Single window, no scan.
I0123 20:40:11.379020 140446420529152 transformer_layer.py:389] tlayer: self-attention.
I0123 20:40:11.379051 140446420529152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.379112 140446420529152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.380231 140446420529152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.380308 140446420529152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.381010 140446420529152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.382990 140446420529152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.387672 140446420529152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.388955 140446420529152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.389039 140446420529152 transformer_layer.py:468] tlayer: End windows.
I0123 20:40:11.389075 140446420529152 transformer_layer.py:472] tlayer: final FFN.
I0123 20:40:11.389135 140446420529152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.389266 140446420529152 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:40:11.389377 140446420529152 nn_components.py:325] mlp: activation = None
I0123 20:40:11.389415 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:40:11.391302 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.391395 140446420529152 nn_components.py:261] mlp: residual
I0123 20:40:11.393812 140446420529152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.393890 140446420529152 transformer_base.py:443] tbase: final FFN
I0123 20:40:11.393998 140446420529152 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:40:11.396193 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:40:11.398131 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.398226 140446420529152 nn_components.py:261] mlp: residual
I0123 20:40:11.398514 140446420529152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.398594 140446420529152 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 20:40:11.398701 140446420529152 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:40:11.398738 140446420529152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:40:11.398769 140446420529152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:40:11.398832 140446420529152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.401089 140446420529152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:40:11.406497 140446420529152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.406749 140446420529152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:40:11.409409 140446420529152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:40:11.422283 140446420529152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:40:11.422338 140446420529152 attention.py:418] Single window, no scan.
I0123 20:40:11.422373 140446420529152 transformer_layer.py:389] tlayer: self-attention.
I0123 20:40:11.422404 140446420529152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.422465 140446420529152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.423012 140446420529152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.423086 140446420529152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.423440 140446420529152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.424126 140446420529152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.426609 140446420529152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.427217 140446420529152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.427293 140446420529152 transformer_layer.py:468] tlayer: End windows.
I0123 20:40:11.427332 140446420529152 transformer_layer.py:472] tlayer: final FFN.
I0123 20:40:11.427394 140446420529152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.427520 140446420529152 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:40:11.427631 140446420529152 nn_components.py:325] mlp: activation = None
I0123 20:40:11.427670 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:40:11.429583 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.429681 140446420529152 nn_components.py:261] mlp: residual
I0123 20:40:11.432100 140446420529152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.432176 140446420529152 transformer_base.py:443] tbase: final FFN
I0123 20:40:11.432285 140446420529152 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:40:11.434497 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:40:11.436404 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.436500 140446420529152 nn_components.py:261] mlp: residual
I0123 20:40:11.436792 140446420529152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.436872 140446420529152 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 20:40:11.436980 140446420529152 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:40:11.437019 140446420529152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:40:11.437049 140446420529152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:40:11.437112 140446420529152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.439377 140446420529152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:40:11.444791 140446420529152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.445046 140446420529152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:40:11.447720 140446420529152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:40:11.460325 140446420529152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:40:11.460381 140446420529152 attention.py:418] Single window, no scan.
I0123 20:40:11.460416 140446420529152 transformer_layer.py:389] tlayer: self-attention.
I0123 20:40:11.460446 140446420529152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.460507 140446420529152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.461062 140446420529152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.461138 140446420529152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.461501 140446420529152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.462198 140446420529152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.464675 140446420529152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.465291 140446420529152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.465368 140446420529152 transformer_layer.py:468] tlayer: End windows.
I0123 20:40:11.465402 140446420529152 transformer_layer.py:472] tlayer: final FFN.
I0123 20:40:11.465467 140446420529152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.465594 140446420529152 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:40:11.465708 140446420529152 nn_components.py:325] mlp: activation = None
I0123 20:40:11.465747 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:40:11.467652 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.467743 140446420529152 nn_components.py:261] mlp: residual
I0123 20:40:11.470173 140446420529152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.470251 140446420529152 transformer_base.py:443] tbase: final FFN
I0123 20:40:11.470358 140446420529152 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:40:11.472543 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:40:11.474443 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.474538 140446420529152 nn_components.py:261] mlp: residual
I0123 20:40:11.474826 140446420529152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.474905 140446420529152 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 20:40:11.475013 140446420529152 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:40:11.475050 140446420529152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:40:11.475081 140446420529152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:40:11.475144 140446420529152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.477373 140446420529152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:40:11.482778 140446420529152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.483034 140446420529152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:40:11.485689 140446420529152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:40:11.498248 140446420529152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:40:11.498304 140446420529152 attention.py:418] Single window, no scan.
I0123 20:40:11.498339 140446420529152 transformer_layer.py:389] tlayer: self-attention.
I0123 20:40:11.498369 140446420529152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.498431 140446420529152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.498986 140446420529152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.499061 140446420529152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.499415 140446420529152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.500101 140446420529152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.502551 140446420529152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.503165 140446420529152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.503239 140446420529152 transformer_layer.py:468] tlayer: End windows.
I0123 20:40:11.503273 140446420529152 transformer_layer.py:472] tlayer: final FFN.
I0123 20:40:11.503331 140446420529152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.503476 140446420529152 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:40:11.503585 140446420529152 nn_components.py:325] mlp: activation = None
I0123 20:40:11.503623 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:40:11.505548 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.505644 140446420529152 nn_components.py:261] mlp: residual
I0123 20:40:11.508049 140446420529152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.508130 140446420529152 transformer_base.py:443] tbase: final FFN
I0123 20:40:11.508238 140446420529152 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:40:11.510427 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:40:11.512281 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.512375 140446420529152 nn_components.py:261] mlp: residual
I0123 20:40:11.512657 140446420529152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.512737 140446420529152 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 20:40:11.512846 140446420529152 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:40:11.512884 140446420529152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:40:11.512914 140446420529152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:40:11.512976 140446420529152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.515564 140446420529152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:40:11.520958 140446420529152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.521222 140446420529152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:40:11.523857 140446420529152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:40:11.536412 140446420529152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:40:11.536468 140446420529152 attention.py:418] Single window, no scan.
I0123 20:40:11.536503 140446420529152 transformer_layer.py:389] tlayer: self-attention.
I0123 20:40:11.536532 140446420529152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.536597 140446420529152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.537147 140446420529152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.537221 140446420529152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.537572 140446420529152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.538275 140446420529152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.540802 140446420529152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.541426 140446420529152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.541503 140446420529152 transformer_layer.py:468] tlayer: End windows.
I0123 20:40:11.541538 140446420529152 transformer_layer.py:472] tlayer: final FFN.
I0123 20:40:11.541596 140446420529152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.541738 140446420529152 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:40:11.541848 140446420529152 nn_components.py:325] mlp: activation = None
I0123 20:40:11.541886 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:40:11.543759 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.543853 140446420529152 nn_components.py:261] mlp: residual
I0123 20:40:11.546277 140446420529152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.546355 140446420529152 transformer_base.py:443] tbase: final FFN
I0123 20:40:11.546461 140446420529152 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:40:11.548706 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:40:11.550560 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.550654 140446420529152 nn_components.py:261] mlp: residual
I0123 20:40:11.550941 140446420529152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.551021 140446420529152 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 20:40:11.551128 140446420529152 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:40:11.551166 140446420529152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:40:11.551196 140446420529152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:40:11.551258 140446420529152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.553492 140446420529152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:40:11.558917 140446420529152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.559170 140446420529152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:40:11.561854 140446420529152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:40:11.574400 140446420529152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:40:11.574453 140446420529152 attention.py:418] Single window, no scan.
I0123 20:40:11.574488 140446420529152 transformer_layer.py:389] tlayer: self-attention.
I0123 20:40:11.574518 140446420529152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.574581 140446420529152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.575127 140446420529152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.575204 140446420529152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.575558 140446420529152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.576265 140446420529152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.586449 140446420529152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.587180 140446420529152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.587264 140446420529152 transformer_layer.py:468] tlayer: End windows.
I0123 20:40:11.587298 140446420529152 transformer_layer.py:472] tlayer: final FFN.
I0123 20:40:11.587370 140446420529152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.587511 140446420529152 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:40:11.587643 140446420529152 nn_components.py:325] mlp: activation = None
I0123 20:40:11.587683 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:40:11.589739 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.589833 140446420529152 nn_components.py:261] mlp: residual
I0123 20:40:11.592304 140446420529152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.592383 140446420529152 transformer_base.py:443] tbase: final FFN
I0123 20:40:11.592493 140446420529152 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:40:11.594748 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:40:11.596613 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.596708 140446420529152 nn_components.py:261] mlp: residual
I0123 20:40:11.596993 140446420529152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.597076 140446420529152 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 20:40:11.597187 140446420529152 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:40:11.597229 140446420529152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:40:11.597259 140446420529152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:40:11.597322 140446420529152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.599581 140446420529152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:40:11.605116 140446420529152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.605375 140446420529152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:40:11.608029 140446420529152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:40:11.620830 140446420529152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:40:11.620886 140446420529152 attention.py:418] Single window, no scan.
I0123 20:40:11.620921 140446420529152 transformer_layer.py:389] tlayer: self-attention.
I0123 20:40:11.620951 140446420529152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.621014 140446420529152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.621570 140446420529152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.621652 140446420529152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.622020 140446420529152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.622714 140446420529152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.625176 140446420529152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.626161 140446420529152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.626239 140446420529152 transformer_layer.py:468] tlayer: End windows.
I0123 20:40:11.626274 140446420529152 transformer_layer.py:472] tlayer: final FFN.
I0123 20:40:11.626332 140446420529152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.626468 140446420529152 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:40:11.626578 140446420529152 nn_components.py:325] mlp: activation = None
I0123 20:40:11.626621 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:40:11.628507 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.628599 140446420529152 nn_components.py:261] mlp: residual
I0123 20:40:11.631032 140446420529152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.631110 140446420529152 transformer_base.py:443] tbase: final FFN
I0123 20:40:11.631220 140446420529152 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:40:11.633409 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:40:11.635343 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.635437 140446420529152 nn_components.py:261] mlp: residual
I0123 20:40:11.635727 140446420529152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.635806 140446420529152 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 20:40:11.635913 140446420529152 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:40:11.635951 140446420529152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:40:11.635981 140446420529152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:40:11.636044 140446420529152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.638291 140446420529152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:40:11.643672 140446420529152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.643950 140446420529152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:40:11.646664 140446420529152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:40:11.659269 140446420529152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:40:11.659324 140446420529152 attention.py:418] Single window, no scan.
I0123 20:40:11.659359 140446420529152 transformer_layer.py:389] tlayer: self-attention.
I0123 20:40:11.659389 140446420529152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.659448 140446420529152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.660051 140446420529152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.660125 140446420529152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.660477 140446420529152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.661164 140446420529152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.663658 140446420529152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.664285 140446420529152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.664361 140446420529152 transformer_layer.py:468] tlayer: End windows.
I0123 20:40:11.664394 140446420529152 transformer_layer.py:472] tlayer: final FFN.
I0123 20:40:11.664452 140446420529152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.664580 140446420529152 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:40:11.664690 140446420529152 nn_components.py:325] mlp: activation = None
I0123 20:40:11.664732 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:40:11.666619 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.666713 140446420529152 nn_components.py:261] mlp: residual
I0123 20:40:11.669185 140446420529152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.669262 140446420529152 transformer_base.py:443] tbase: final FFN
I0123 20:40:11.669369 140446420529152 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:40:11.671585 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:40:11.673437 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.673530 140446420529152 nn_components.py:261] mlp: residual
I0123 20:40:11.673822 140446420529152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.673902 140446420529152 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 20:40:11.674010 140446420529152 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:40:11.674047 140446420529152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:40:11.674077 140446420529152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:40:11.674140 140446420529152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.676394 140446420529152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:40:11.681875 140446420529152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.682129 140446420529152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:40:11.684744 140446420529152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:40:11.697250 140446420529152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:40:11.697305 140446420529152 attention.py:418] Single window, no scan.
I0123 20:40:11.697340 140446420529152 transformer_layer.py:389] tlayer: self-attention.
I0123 20:40:11.697369 140446420529152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.697430 140446420529152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.697994 140446420529152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.698070 140446420529152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.698428 140446420529152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.699118 140446420529152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.701581 140446420529152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.702255 140446420529152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.702333 140446420529152 transformer_layer.py:468] tlayer: End windows.
I0123 20:40:11.702367 140446420529152 transformer_layer.py:472] tlayer: final FFN.
I0123 20:40:11.702426 140446420529152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.702556 140446420529152 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:40:11.702666 140446420529152 nn_components.py:325] mlp: activation = None
I0123 20:40:11.702704 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:40:11.704559 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.704651 140446420529152 nn_components.py:261] mlp: residual
I0123 20:40:11.707078 140446420529152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.707157 140446420529152 transformer_base.py:443] tbase: final FFN
I0123 20:40:11.707265 140446420529152 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:40:11.709490 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:40:11.711412 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.711506 140446420529152 nn_components.py:261] mlp: residual
I0123 20:40:11.711796 140446420529152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.711875 140446420529152 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 20:40:11.711983 140446420529152 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:40:11.712022 140446420529152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:40:11.712052 140446420529152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:40:11.712114 140446420529152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.714355 140446420529152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:40:11.719704 140446420529152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.719957 140446420529152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:40:11.722618 140446420529152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:40:11.735459 140446420529152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:40:11.735513 140446420529152 attention.py:418] Single window, no scan.
I0123 20:40:11.735548 140446420529152 transformer_layer.py:389] tlayer: self-attention.
I0123 20:40:11.735578 140446420529152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.735638 140446420529152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.736240 140446420529152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.736315 140446420529152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.736673 140446420529152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.737359 140446420529152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.739817 140446420529152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.740434 140446420529152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.740510 140446420529152 transformer_layer.py:468] tlayer: End windows.
I0123 20:40:11.740544 140446420529152 transformer_layer.py:472] tlayer: final FFN.
I0123 20:40:11.740601 140446420529152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.740731 140446420529152 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:40:11.740839 140446420529152 nn_components.py:325] mlp: activation = None
I0123 20:40:11.740877 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:40:11.742753 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.742852 140446420529152 nn_components.py:261] mlp: residual
I0123 20:40:11.745309 140446420529152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.745386 140446420529152 transformer_base.py:443] tbase: final FFN
I0123 20:40:11.745494 140446420529152 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:40:11.747697 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:40:11.749561 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.749660 140446420529152 nn_components.py:261] mlp: residual
I0123 20:40:11.749949 140446420529152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.750028 140446420529152 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 20:40:11.750137 140446420529152 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:40:11.750176 140446420529152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:40:11.750207 140446420529152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:40:11.750269 140446420529152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.752490 140446420529152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:40:11.757989 140446420529152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.758245 140446420529152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:40:11.760894 140446420529152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:40:11.773436 140446420529152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:40:11.773490 140446420529152 attention.py:418] Single window, no scan.
I0123 20:40:11.773529 140446420529152 transformer_layer.py:389] tlayer: self-attention.
I0123 20:40:11.773560 140446420529152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.773621 140446420529152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.774173 140446420529152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.774250 140446420529152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.774599 140446420529152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.775291 140446420529152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.777796 140446420529152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.778445 140446420529152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.778521 140446420529152 transformer_layer.py:468] tlayer: End windows.
I0123 20:40:11.778554 140446420529152 transformer_layer.py:472] tlayer: final FFN.
I0123 20:40:11.778611 140446420529152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.778742 140446420529152 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:40:11.778855 140446420529152 nn_components.py:325] mlp: activation = None
I0123 20:40:11.778894 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:40:11.780759 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.780856 140446420529152 nn_components.py:261] mlp: residual
I0123 20:40:11.783297 140446420529152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.783375 140446420529152 transformer_base.py:443] tbase: final FFN
I0123 20:40:11.783481 140446420529152 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:40:11.785660 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:40:11.787568 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.787661 140446420529152 nn_components.py:261] mlp: residual
I0123 20:40:11.787950 140446420529152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.788029 140446420529152 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 20:40:11.788138 140446420529152 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:40:11.788176 140446420529152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:40:11.788206 140446420529152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:40:11.788268 140446420529152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.790506 140446420529152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:40:11.795930 140446420529152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.796184 140446420529152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:40:11.798908 140446420529152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:40:11.811396 140446420529152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:40:11.811450 140446420529152 attention.py:418] Single window, no scan.
I0123 20:40:11.811485 140446420529152 transformer_layer.py:389] tlayer: self-attention.
I0123 20:40:11.811515 140446420529152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.811575 140446420529152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.812139 140446420529152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.812213 140446420529152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.812573 140446420529152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.813312 140446420529152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.815786 140446420529152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.816407 140446420529152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.816482 140446420529152 transformer_layer.py:468] tlayer: End windows.
I0123 20:40:11.816515 140446420529152 transformer_layer.py:472] tlayer: final FFN.
I0123 20:40:11.816572 140446420529152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.816702 140446420529152 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:40:11.816811 140446420529152 nn_components.py:325] mlp: activation = None
I0123 20:40:11.816848 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:40:11.818713 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.818805 140446420529152 nn_components.py:261] mlp: residual
I0123 20:40:11.821216 140446420529152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.821293 140446420529152 transformer_base.py:443] tbase: final FFN
I0123 20:40:11.821400 140446420529152 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:40:11.823652 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:40:11.825501 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.825595 140446420529152 nn_components.py:261] mlp: residual
I0123 20:40:11.825888 140446420529152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.825975 140446420529152 decoder_stack.py:344] dstack: Final layernorm.
I0123 20:40:11.828846 140446420529152 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:40:11.883985 140446420529152 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:11.884067 140446420529152 decoder_stack.py:333] dstack: autoregressive generator.
I0123 20:40:11.884120 140446420529152 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 20:40:11.884226 140446420529152 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:40:11.884263 140446420529152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:40:11.884291 140446420529152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:40:11.884352 140446420529152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:11.886990 140446420529152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:40:11.892256 140446420529152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:11.892510 140446420529152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:40:11.895101 140446420529152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:40:11.907304 140446420529152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:40:11.907359 140446420529152 attention.py:418] Single window, no scan.
I0123 20:40:11.907394 140446420529152 transformer_layer.py:389] tlayer: self-attention.
I0123 20:40:11.907423 140446420529152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:11.907484 140446420529152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:11.908037 140446420529152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:11.908111 140446420529152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:11.908459 140446420529152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:11.909121 140446420529152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:11.911612 140446420529152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:11.912220 140446420529152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:11.912295 140446420529152 transformer_layer.py:468] tlayer: End windows.
I0123 20:40:11.912329 140446420529152 transformer_layer.py:472] tlayer: final FFN.
I0123 20:40:11.912386 140446420529152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:11.912511 140446420529152 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:40:11.912628 140446420529152 nn_components.py:325] mlp: activation = None
I0123 20:40:11.912668 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:40:11.914499 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:11.914592 140446420529152 nn_components.py:261] mlp: residual
I0123 20:40:11.916964 140446420529152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:11.917042 140446420529152 transformer_base.py:443] tbase: final FFN
I0123 20:40:11.917150 140446420529152 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:40:11.919377 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:40:11.921175 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:11.921268 140446420529152 nn_components.py:261] mlp: residual
I0123 20:40:11.921550 140446420529152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:11.921629 140446420529152 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 20:40:11.921748 140446420529152 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:40:11.921785 140446420529152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:40:11.921815 140446420529152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:40:11.921877 140446420529152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:11.924093 140446420529152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:40:11.929365 140446420529152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:11.929617 140446420529152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:40:11.932265 140446420529152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:40:11.944319 140446420529152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:40:11.944373 140446420529152 attention.py:418] Single window, no scan.
I0123 20:40:11.944408 140446420529152 transformer_layer.py:389] tlayer: self-attention.
I0123 20:40:11.944438 140446420529152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:11.944499 140446420529152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:11.945039 140446420529152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:11.945113 140446420529152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:11.945463 140446420529152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:11.946146 140446420529152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:11.948641 140446420529152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:11.949241 140446420529152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:11.949317 140446420529152 transformer_layer.py:468] tlayer: End windows.
I0123 20:40:11.949351 140446420529152 transformer_layer.py:472] tlayer: final FFN.
I0123 20:40:11.949408 140446420529152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:11.949531 140446420529152 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:40:11.949638 140446420529152 nn_components.py:325] mlp: activation = None
I0123 20:40:11.949687 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:40:11.951504 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:11.951595 140446420529152 nn_components.py:261] mlp: residual
I0123 20:40:11.953980 140446420529152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:11.954058 140446420529152 transformer_base.py:443] tbase: final FFN
I0123 20:40:11.954167 140446420529152 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:40:11.956390 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:40:11.958222 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:11.958317 140446420529152 nn_components.py:261] mlp: residual
I0123 20:40:11.958599 140446420529152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:11.958677 140446420529152 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 20:40:11.958782 140446420529152 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:40:11.958819 140446420529152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:40:11.958849 140446420529152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:40:11.958910 140446420529152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:11.961122 140446420529152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:40:11.966402 140446420529152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:11.966652 140446420529152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:40:11.969267 140446420529152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:40:11.981302 140446420529152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:40:11.981357 140446420529152 attention.py:418] Single window, no scan.
I0123 20:40:11.981391 140446420529152 transformer_layer.py:389] tlayer: self-attention.
I0123 20:40:11.981421 140446420529152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:11.981482 140446420529152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:11.982038 140446420529152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:11.982113 140446420529152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:11.982467 140446420529152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:11.983136 140446420529152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:11.985591 140446420529152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:11.986203 140446420529152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:11.986278 140446420529152 transformer_layer.py:468] tlayer: End windows.
I0123 20:40:11.986312 140446420529152 transformer_layer.py:472] tlayer: final FFN.
I0123 20:40:11.986371 140446420529152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:11.986495 140446420529152 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:40:11.986602 140446420529152 nn_components.py:325] mlp: activation = None
I0123 20:40:11.986640 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:40:11.988455 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:11.988546 140446420529152 nn_components.py:261] mlp: residual
I0123 20:40:11.990906 140446420529152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:11.990983 140446420529152 transformer_base.py:443] tbase: final FFN
I0123 20:40:11.991090 140446420529152 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:40:11.993741 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:40:11.995560 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:11.995654 140446420529152 nn_components.py:261] mlp: residual
I0123 20:40:11.995940 140446420529152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:11.996019 140446420529152 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 20:40:11.996126 140446420529152 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:40:11.996164 140446420529152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:40:11.996193 140446420529152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:40:11.996254 140446420529152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:11.998457 140446420529152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:40:12.003727 140446420529152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.003982 140446420529152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:40:12.006627 140446420529152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:40:12.018754 140446420529152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:40:12.018809 140446420529152 attention.py:418] Single window, no scan.
I0123 20:40:12.018846 140446420529152 transformer_layer.py:389] tlayer: self-attention.
I0123 20:40:12.018887 140446420529152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.018951 140446420529152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.019495 140446420529152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.019567 140446420529152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.019921 140446420529152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.020595 140446420529152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.023103 140446420529152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.023707 140446420529152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.023780 140446420529152 transformer_layer.py:468] tlayer: End windows.
I0123 20:40:12.023813 140446420529152 transformer_layer.py:472] tlayer: final FFN.
I0123 20:40:12.023874 140446420529152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.024000 140446420529152 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:40:12.024106 140446420529152 nn_components.py:325] mlp: activation = None
I0123 20:40:12.024144 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:40:12.025992 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.026082 140446420529152 nn_components.py:261] mlp: residual
I0123 20:40:12.028463 140446420529152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.028538 140446420529152 transformer_base.py:443] tbase: final FFN
I0123 20:40:12.028643 140446420529152 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:40:12.030895 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:40:12.032711 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.032801 140446420529152 nn_components.py:261] mlp: residual
I0123 20:40:12.033081 140446420529152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.033158 140446420529152 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 20:40:12.033262 140446420529152 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:40:12.033298 140446420529152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:40:12.033326 140446420529152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:40:12.033387 140446420529152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.035598 140446420529152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:40:12.040913 140446420529152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.041166 140446420529152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:40:12.043829 140446420529152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:40:12.056141 140446420529152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:40:12.056195 140446420529152 attention.py:418] Single window, no scan.
I0123 20:40:12.056228 140446420529152 transformer_layer.py:389] tlayer: self-attention.
I0123 20:40:12.056257 140446420529152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.056316 140446420529152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.056864 140446420529152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.056937 140446420529152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.057285 140446420529152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.057966 140446420529152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.060449 140446420529152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.061063 140446420529152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.061137 140446420529152 transformer_layer.py:468] tlayer: End windows.
I0123 20:40:12.061169 140446420529152 transformer_layer.py:472] tlayer: final FFN.
I0123 20:40:12.061225 140446420529152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.061351 140446420529152 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:40:12.061457 140446420529152 nn_components.py:325] mlp: activation = None
I0123 20:40:12.061493 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:40:12.063358 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.063452 140446420529152 nn_components.py:261] mlp: residual
I0123 20:40:12.065845 140446420529152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.065921 140446420529152 transformer_base.py:443] tbase: final FFN
I0123 20:40:12.066027 140446420529152 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:40:12.068256 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:40:12.070088 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.070181 140446420529152 nn_components.py:261] mlp: residual
I0123 20:40:12.070465 140446420529152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.070542 140446420529152 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 20:40:12.070647 140446420529152 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:40:12.070683 140446420529152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:40:12.070711 140446420529152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:40:12.070771 140446420529152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.072981 140446420529152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:40:12.078307 140446420529152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.078557 140446420529152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:40:12.081227 140446420529152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:40:12.093541 140446420529152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:40:12.093594 140446420529152 attention.py:418] Single window, no scan.
I0123 20:40:12.093627 140446420529152 transformer_layer.py:389] tlayer: self-attention.
I0123 20:40:12.093663 140446420529152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.093724 140446420529152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.094271 140446420529152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.094344 140446420529152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.094700 140446420529152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.095376 140446420529152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.097919 140446420529152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.098521 140446420529152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.098595 140446420529152 transformer_layer.py:468] tlayer: End windows.
I0123 20:40:12.098626 140446420529152 transformer_layer.py:472] tlayer: final FFN.
I0123 20:40:12.098681 140446420529152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.098802 140446420529152 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:40:12.098908 140446420529152 nn_components.py:325] mlp: activation = None
I0123 20:40:12.098944 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:40:12.100769 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.100863 140446420529152 nn_components.py:261] mlp: residual
I0123 20:40:12.103263 140446420529152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.103340 140446420529152 transformer_base.py:443] tbase: final FFN
I0123 20:40:12.103445 140446420529152 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:40:12.106082 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:40:12.107917 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.108009 140446420529152 nn_components.py:261] mlp: residual
I0123 20:40:12.108293 140446420529152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.108371 140446420529152 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 20:40:12.108476 140446420529152 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:40:12.108512 140446420529152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:40:12.108541 140446420529152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:40:12.108601 140446420529152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.110806 140446420529152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:40:12.116148 140446420529152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.116400 140446420529152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:40:12.119080 140446420529152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:40:12.131365 140446420529152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:40:12.131417 140446420529152 attention.py:418] Single window, no scan.
I0123 20:40:12.131449 140446420529152 transformer_layer.py:389] tlayer: self-attention.
I0123 20:40:12.131478 140446420529152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.131539 140446420529152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.132092 140446420529152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.132165 140446420529152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.132519 140446420529152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.133193 140446420529152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.135707 140446420529152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.136329 140446420529152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.136402 140446420529152 transformer_layer.py:468] tlayer: End windows.
I0123 20:40:12.136434 140446420529152 transformer_layer.py:472] tlayer: final FFN.
I0123 20:40:12.136489 140446420529152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.136612 140446420529152 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:40:12.136717 140446420529152 nn_components.py:325] mlp: activation = None
I0123 20:40:12.136753 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:40:12.138601 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.138692 140446420529152 nn_components.py:261] mlp: residual
I0123 20:40:12.141074 140446420529152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.141148 140446420529152 transformer_base.py:443] tbase: final FFN
I0123 20:40:12.141252 140446420529152 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:40:12.143497 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:40:12.145305 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.145397 140446420529152 nn_components.py:261] mlp: residual
I0123 20:40:12.145688 140446420529152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.145767 140446420529152 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 20:40:12.145872 140446420529152 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:40:12.145908 140446420529152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:40:12.145936 140446420529152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:40:12.145997 140446420529152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.148204 140446420529152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:40:12.153539 140446420529152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.153799 140446420529152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:40:12.156460 140446420529152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:40:12.168753 140446420529152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:40:12.168806 140446420529152 attention.py:418] Single window, no scan.
I0123 20:40:12.168839 140446420529152 transformer_layer.py:389] tlayer: self-attention.
I0123 20:40:12.168867 140446420529152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.168925 140446420529152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.169474 140446420529152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.169546 140446420529152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.169909 140446420529152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.170586 140446420529152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.173084 140446420529152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.173697 140446420529152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.173770 140446420529152 transformer_layer.py:468] tlayer: End windows.
I0123 20:40:12.173803 140446420529152 transformer_layer.py:472] tlayer: final FFN.
I0123 20:40:12.173858 140446420529152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.173980 140446420529152 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:40:12.174085 140446420529152 nn_components.py:325] mlp: activation = None
I0123 20:40:12.174122 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:40:12.175958 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.176046 140446420529152 nn_components.py:261] mlp: residual
I0123 20:40:12.178413 140446420529152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.178497 140446420529152 transformer_base.py:443] tbase: final FFN
I0123 20:40:12.178604 140446420529152 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:40:12.180853 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:40:12.182678 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.182771 140446420529152 nn_components.py:261] mlp: residual
I0123 20:40:12.183055 140446420529152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.183135 140446420529152 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 20:40:12.183240 140446420529152 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:40:12.183277 140446420529152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:40:12.183305 140446420529152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:40:12.183366 140446420529152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.185567 140446420529152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:40:12.190905 140446420529152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.191155 140446420529152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:40:12.193806 140446420529152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:40:12.206107 140446420529152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:40:12.206159 140446420529152 attention.py:418] Single window, no scan.
I0123 20:40:12.206191 140446420529152 transformer_layer.py:389] tlayer: self-attention.
I0123 20:40:12.206219 140446420529152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.206278 140446420529152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.206819 140446420529152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.206891 140446420529152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.207245 140446420529152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.207920 140446420529152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.210426 140446420529152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.211026 140446420529152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.211099 140446420529152 transformer_layer.py:468] tlayer: End windows.
I0123 20:40:12.211131 140446420529152 transformer_layer.py:472] tlayer: final FFN.
I0123 20:40:12.211186 140446420529152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.211307 140446420529152 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:40:12.211416 140446420529152 nn_components.py:325] mlp: activation = None
I0123 20:40:12.211452 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:40:12.213304 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.213394 140446420529152 nn_components.py:261] mlp: residual
I0123 20:40:12.215756 140446420529152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.215838 140446420529152 transformer_base.py:443] tbase: final FFN
I0123 20:40:12.215948 140446420529152 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:40:12.218579 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:40:12.220403 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.220496 140446420529152 nn_components.py:261] mlp: residual
I0123 20:40:12.220782 140446420529152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.220860 140446420529152 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 20:40:12.220965 140446420529152 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:40:12.221001 140446420529152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:40:12.221029 140446420529152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:40:12.221089 140446420529152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.223307 140446420529152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:40:12.228648 140446420529152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.228899 140446420529152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:40:12.231541 140446420529152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:40:12.243794 140446420529152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:40:12.243847 140446420529152 attention.py:418] Single window, no scan.
I0123 20:40:12.243880 140446420529152 transformer_layer.py:389] tlayer: self-attention.
I0123 20:40:12.243909 140446420529152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.243969 140446420529152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.244525 140446420529152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.244598 140446420529152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.244946 140446420529152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.245616 140446420529152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.248117 140446420529152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.248726 140446420529152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.248801 140446420529152 transformer_layer.py:468] tlayer: End windows.
I0123 20:40:12.248835 140446420529152 transformer_layer.py:472] tlayer: final FFN.
I0123 20:40:12.248890 140446420529152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.249011 140446420529152 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:40:12.249116 140446420529152 nn_components.py:325] mlp: activation = None
I0123 20:40:12.249152 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:40:12.251529 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.251621 140446420529152 nn_components.py:261] mlp: residual
I0123 20:40:12.253980 140446420529152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.254056 140446420529152 transformer_base.py:443] tbase: final FFN
I0123 20:40:12.254166 140446420529152 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:40:12.256371 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:40:12.258172 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.258264 140446420529152 nn_components.py:261] mlp: residual
I0123 20:40:12.258542 140446420529152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.258619 140446420529152 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 20:40:12.258723 140446420529152 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:40:12.258759 140446420529152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:40:12.258788 140446420529152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:40:12.258848 140446420529152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.261044 140446420529152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:40:12.266370 140446420529152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.266621 140446420529152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:40:12.269269 140446420529152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:40:12.281520 140446420529152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:40:12.281573 140446420529152 attention.py:418] Single window, no scan.
I0123 20:40:12.281606 140446420529152 transformer_layer.py:389] tlayer: self-attention.
I0123 20:40:12.281635 140446420529152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.281703 140446420529152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.282256 140446420529152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.282330 140446420529152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.282684 140446420529152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.283362 140446420529152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.285881 140446420529152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.286482 140446420529152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.286555 140446420529152 transformer_layer.py:468] tlayer: End windows.
I0123 20:40:12.286587 140446420529152 transformer_layer.py:472] tlayer: final FFN.
I0123 20:40:12.286643 140446420529152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.286764 140446420529152 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:40:12.286868 140446420529152 nn_components.py:325] mlp: activation = None
I0123 20:40:12.286904 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:40:12.288756 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.288844 140446420529152 nn_components.py:261] mlp: residual
I0123 20:40:12.291192 140446420529152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.291267 140446420529152 transformer_base.py:443] tbase: final FFN
I0123 20:40:12.291371 140446420529152 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:40:12.293612 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:40:12.295424 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.295516 140446420529152 nn_components.py:261] mlp: residual
I0123 20:40:12.295800 140446420529152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.295877 140446420529152 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 20:40:12.295982 140446420529152 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:40:12.296019 140446420529152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:40:12.296047 140446420529152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:40:12.296106 140446420529152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.298313 140446420529152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:40:12.303645 140446420529152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.303894 140446420529152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:40:12.306556 140446420529152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:40:12.318870 140446420529152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:40:12.318922 140446420529152 attention.py:418] Single window, no scan.
I0123 20:40:12.318956 140446420529152 transformer_layer.py:389] tlayer: self-attention.
I0123 20:40:12.318985 140446420529152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.319045 140446420529152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.319591 140446420529152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.319668 140446420529152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.320023 140446420529152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.320707 140446420529152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.323240 140446420529152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.323842 140446420529152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.323915 140446420529152 transformer_layer.py:468] tlayer: End windows.
I0123 20:40:12.323947 140446420529152 transformer_layer.py:472] tlayer: final FFN.
I0123 20:40:12.324002 140446420529152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.324128 140446420529152 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:40:12.324238 140446420529152 nn_components.py:325] mlp: activation = None
I0123 20:40:12.324275 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:40:12.326137 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.326227 140446420529152 nn_components.py:261] mlp: residual
I0123 20:40:12.328593 140446420529152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.328668 140446420529152 transformer_base.py:443] tbase: final FFN
I0123 20:40:12.328775 140446420529152 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:40:12.331417 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:40:12.333266 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.333359 140446420529152 nn_components.py:261] mlp: residual
I0123 20:40:12.333647 140446420529152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:12.333732 140446420529152 decoder_stack.py:344] dstack: Final layernorm.
I0123 20:40:12.336528 140446420529152 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:40:16.754467 140446420529152 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
I0123 20:40:17.279388 140446420529152 training_loop.py:409] No working directory specified.
I0123 20:40:17.279509 140446420529152 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0123 20:40:17.280262 140446420529152 checkpoints.py:1062] Restoring legacy Flax checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0123 20:40:20.761789 140446420529152 training_loop.py:447] Only restoring trainable parameters.
I0123 20:40:20.762405 140446420529152 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0123 20:40:20.762482 140446420529152 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0123 20:40:20.762534 140446420529152 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 20:40:20.762578 140446420529152 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 20:40:20.762619 140446420529152 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:40:20.762660 140446420529152 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 20:40:20.762700 140446420529152 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:40:20.762741 140446420529152 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:40:20.762780 140446420529152 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 20:40:20.762818 140446420529152 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 20:40:20.762856 140446420529152 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:40:20.762895 140446420529152 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 20:40:20.762932 140446420529152 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 20:40:20.762969 140446420529152 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 20:40:20.763006 140446420529152 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:40:20.763043 140446420529152 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 20:40:20.763079 140446420529152 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:40:20.763115 140446420529152 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:40:20.763151 140446420529152 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 20:40:20.763187 140446420529152 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 20:40:20.763238 140446420529152 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:40:20.763276 140446420529152 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 20:40:20.763312 140446420529152 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 20:40:20.763349 140446420529152 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 20:40:20.763385 140446420529152 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:40:20.763420 140446420529152 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 20:40:20.763457 140446420529152 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:40:20.763492 140446420529152 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:40:20.763528 140446420529152 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 20:40:20.763563 140446420529152 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 20:40:20.763597 140446420529152 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:40:20.763633 140446420529152 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 20:40:20.763669 140446420529152 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 20:40:20.763704 140446420529152 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 20:40:20.763739 140446420529152 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:40:20.763773 140446420529152 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 20:40:20.763809 140446420529152 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:40:20.763844 140446420529152 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:40:20.763879 140446420529152 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 20:40:20.763914 140446420529152 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 20:40:20.763948 140446420529152 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:40:20.763983 140446420529152 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 20:40:20.764018 140446420529152 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 20:40:20.764053 140446420529152 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 20:40:20.764088 140446420529152 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:40:20.764123 140446420529152 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 20:40:20.764162 140446420529152 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:40:20.764198 140446420529152 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:40:20.764233 140446420529152 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 20:40:20.764268 140446420529152 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 20:40:20.764302 140446420529152 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:40:20.764337 140446420529152 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 20:40:20.764372 140446420529152 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 20:40:20.764408 140446420529152 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 20:40:20.764443 140446420529152 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:40:20.764478 140446420529152 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 20:40:20.764513 140446420529152 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:40:20.764548 140446420529152 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:40:20.764583 140446420529152 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 20:40:20.764619 140446420529152 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 20:40:20.764654 140446420529152 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:40:20.764690 140446420529152 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 20:40:20.764725 140446420529152 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 20:40:20.764760 140446420529152 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 20:40:20.764796 140446420529152 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:40:20.764830 140446420529152 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 20:40:20.764865 140446420529152 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:40:20.764900 140446420529152 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:40:20.764935 140446420529152 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 20:40:20.764969 140446420529152 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 20:40:20.765003 140446420529152 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:40:20.765039 140446420529152 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 20:40:20.765074 140446420529152 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 20:40:20.765115 140446420529152 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 20:40:20.765152 140446420529152 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:40:20.765187 140446420529152 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 20:40:20.765222 140446420529152 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:40:20.765257 140446420529152 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:40:20.765291 140446420529152 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 20:40:20.765327 140446420529152 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 20:40:20.765362 140446420529152 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:40:20.765398 140446420529152 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 20:40:20.765433 140446420529152 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 20:40:20.765469 140446420529152 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 20:40:20.765504 140446420529152 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:40:20.765539 140446420529152 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 20:40:20.765574 140446420529152 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:40:20.765609 140446420529152 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:40:20.765652 140446420529152 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 20:40:20.765691 140446420529152 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 20:40:20.765727 140446420529152 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:40:20.765763 140446420529152 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 20:40:20.765799 140446420529152 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 20:40:20.765834 140446420529152 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 20:40:20.765870 140446420529152 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:40:20.765904 140446420529152 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 20:40:20.765940 140446420529152 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:40:20.765975 140446420529152 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:40:20.766010 140446420529152 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 20:40:20.766044 140446420529152 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 20:40:20.766084 140446420529152 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:40:20.766120 140446420529152 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 20:40:20.766155 140446420529152 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 20:40:20.766191 140446420529152 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 20:40:20.766227 140446420529152 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:40:20.766262 140446420529152 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 20:40:20.766297 140446420529152 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:40:20.766332 140446420529152 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:40:20.766367 140446420529152 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 20:40:20.766401 140446420529152 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 20:40:20.766436 140446420529152 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:40:20.766471 140446420529152 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 20:40:20.766507 140446420529152 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 20:40:20.766542 140446420529152 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 20:40:20.766577 140446420529152 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:40:20.766612 140446420529152 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 20:40:20.766647 140446420529152 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:40:20.766681 140446420529152 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:40:20.766716 140446420529152 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 20:40:20.766751 140446420529152 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 20:40:20.766786 140446420529152 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:40:20.766821 140446420529152 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 20:40:20.766850 140446420529152 training_loop.py:725] Total parameters: 152072288
I0123 20:40:20.767070 140446420529152 training_loop.py:739] Total state size: 0
I0123 20:40:20.792195 140446420529152 training_loop.py:492] Training loop: creating task for mode beam_search
I0123 20:40:20.792419 140446420529152 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0123 20:40:20.792800 140446420529152 training_loop.py:652] Compiling mode beam_search with jit.
I0123 20:40:20.793132 140446420529152 training_loop.py:89] registering functions: dict_keys([])
I0123 20:40:20.809803 140446420529152 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e ? perp l n n e
I0123 20:40:23.376471 140446420529152 ddar.py:60] Depth 1/1000 time = 2.5366077423095703
I0123 20:40:26.486112 140446420529152 ddar.py:60] Depth 2/1000 time = 3.1094470024108887
I0123 20:40:29.984229 140446420529152 ddar.py:60] Depth 3/1000 time = 3.4979357719421387
I0123 20:40:33.711788 140446420529152 ddar.py:60] Depth 4/1000 time = 3.727339744567871
I0123 20:40:37.453978 140446420529152 ddar.py:60] Depth 5/1000 time = 3.7419064044952393
I0123 20:40:41.910516 140446420529152 ddar.py:60] Depth 6/1000 time = 4.444700717926025
I0123 20:40:46.738368 140446420529152 ddar.py:60] Depth 7/1000 time = 4.827655792236328
I0123 20:40:51.884055 140446420529152 ddar.py:60] Depth 8/1000 time = 5.145427703857422
I0123 20:40:59.942069 140446420529152 ddar.py:60] Depth 9/1000 time = 8.057668924331665
I0123 20:41:08.281212 140446420529152 ddar.py:60] Depth 10/1000 time = 8.338780403137207
I0123 20:41:16.505182 140446420529152 ddar.py:60] Depth 11/1000 time = 8.206039905548096
I0123 20:41:24.735747 140446420529152 ddar.py:60] Depth 12/1000 time = 8.208034992218018
I0123 20:41:24.757630 140446420529152 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 20:41:24.757737 140446420529152 alphageometry.py:540] Depth 0. There are 1 nodes to expand:
I0123 20:41:24.757772 140446420529152 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a c e 02 ^ b a b e b e b c 03 ; f : C a c f 04 T a c b f 05 ; g : C b c g 06 ^ a b a g a g a c 07 ; h : C b c h 08 T a h b c 09 ; i : C a g i 10 C b e i 11 ; j : C g i j 12 T b i b j 13 ; k : C e i k 14 T a i a k 15 ; l : C f j l 16 C h k l 17 ; m : D i m j m 18 D j m k m 19 ; n : C d m n 20 C e g n 21 ? T l n n e {F1} x00
I0123 20:41:24.757801 140446420529152 alphageometry.py:549] Decoding from {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a c e 02 ^ b a b e b e b c 03 ; f : C a c f 04 T a c b f 05 ; g : C b c g 06 ^ a b a g a g a c 07 ; h : C b c h 08 T a h b c 09 ; i : C a g i 10 C b e i 11 ; j : C g i j 12 T b i b j 13 ; k : C e i k 14 T a i a k 15 ; l : C f j l 16 C h k l 17 ; m : D i m j m 18 D j m k m 19 ; n : C d m n 20 C e g n 21 ? T l n n e {F1} x00
I0123 20:41:24.905653 140446420529152 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:24.905838 140446420529152 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 20:41:24.905941 140446420529152 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 20:41:24.906018 140446420529152 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 20:41:24.906090 140446420529152 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 20:41:24.906160 140446420529152 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 20:41:24.906229 140446420529152 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 20:41:24.906298 140446420529152 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 20:41:24.906367 140446420529152 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 20:41:24.906436 140446420529152 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 20:41:24.906504 140446420529152 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 20:41:24.906572 140446420529152 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 20:41:24.906641 140446420529152 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 20:41:24.906718 140446420529152 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 20:41:24.906760 140446420529152 attention.py:418] Single window, no scan.
I0123 20:41:24.906805 140446420529152 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 20:41:24.906916 140446420529152 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:41:24.906954 140446420529152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:41:24.906984 140446420529152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:41:24.908865 140446420529152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:24.911356 140446420529152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:41:24.917074 140446420529152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:24.917342 140446420529152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:41:24.920025 140446420529152 transformer_layer.py:213] tlayer: windowed attention.
I0123 20:41:24.923879 140446420529152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:41:24.923932 140446420529152 attention.py:418] Single window, no scan.
I0123 20:41:24.923967 140446420529152 transformer_layer.py:389] tlayer: self-attention.
I0123 20:41:24.924000 140446420529152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:24.924062 140446420529152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:24.924718 140446420529152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:24.924793 140446420529152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:24.925155 140446420529152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:24.925933 140446420529152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:24.928479 140446420529152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:24.929099 140446420529152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:24.929174 140446420529152 transformer_layer.py:468] tlayer: End windows.
I0123 20:41:24.929208 140446420529152 transformer_layer.py:472] tlayer: final FFN.
I0123 20:41:24.929264 140446420529152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:24.929388 140446420529152 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:41:24.929712 140446420529152 nn_components.py:325] mlp: activation = None
I0123 20:41:24.929754 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:41:24.931724 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:24.931815 140446420529152 nn_components.py:261] mlp: residual
I0123 20:41:24.934274 140446420529152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:24.934355 140446420529152 transformer_base.py:443] tbase: final FFN
I0123 20:41:24.934786 140446420529152 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:41:24.937087 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:41:24.939047 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:24.939139 140446420529152 nn_components.py:261] mlp: residual
I0123 20:41:24.939433 140446420529152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:24.939511 140446420529152 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 20:41:24.939616 140446420529152 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:41:24.939653 140446420529152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:41:24.939682 140446420529152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:41:24.941490 140446420529152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:24.943820 140446420529152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:41:24.949386 140446420529152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:24.949646 140446420529152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:41:24.952249 140446420529152 transformer_layer.py:213] tlayer: windowed attention.
I0123 20:41:24.955858 140446420529152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:41:24.955911 140446420529152 attention.py:418] Single window, no scan.
I0123 20:41:24.955944 140446420529152 transformer_layer.py:389] tlayer: self-attention.
I0123 20:41:24.955972 140446420529152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:24.956033 140446420529152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:24.956584 140446420529152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:24.956658 140446420529152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:24.957013 140446420529152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:24.957770 140446420529152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:24.960204 140446420529152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:24.960863 140446420529152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:24.960938 140446420529152 transformer_layer.py:468] tlayer: End windows.
I0123 20:41:24.960972 140446420529152 transformer_layer.py:472] tlayer: final FFN.
I0123 20:41:24.961034 140446420529152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:24.961162 140446420529152 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:41:24.961469 140446420529152 nn_components.py:325] mlp: activation = None
I0123 20:41:24.961509 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:41:24.963401 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:24.963492 140446420529152 nn_components.py:261] mlp: residual
I0123 20:41:24.965936 140446420529152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:24.966014 140446420529152 transformer_base.py:443] tbase: final FFN
I0123 20:41:24.966428 140446420529152 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:41:24.968716 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:41:24.970609 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:24.970701 140446420529152 nn_components.py:261] mlp: residual
I0123 20:41:24.970986 140446420529152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:24.971071 140446420529152 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 20:41:24.971179 140446420529152 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:41:24.971215 140446420529152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:41:24.971245 140446420529152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:41:24.972991 140446420529152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:24.975267 140446420529152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:41:24.980865 140446420529152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:24.981113 140446420529152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:41:24.983667 140446420529152 transformer_layer.py:213] tlayer: windowed attention.
I0123 20:41:24.987289 140446420529152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:41:24.987342 140446420529152 attention.py:418] Single window, no scan.
I0123 20:41:24.987376 140446420529152 transformer_layer.py:389] tlayer: self-attention.
I0123 20:41:24.987405 140446420529152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:24.987467 140446420529152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:24.988394 140446420529152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:24.988470 140446420529152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:24.988826 140446420529152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:24.989581 140446420529152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:24.992028 140446420529152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:24.992637 140446420529152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:24.992712 140446420529152 transformer_layer.py:468] tlayer: End windows.
I0123 20:41:24.992745 140446420529152 transformer_layer.py:472] tlayer: final FFN.
I0123 20:41:24.992801 140446420529152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:24.992925 140446420529152 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:41:24.993232 140446420529152 nn_components.py:325] mlp: activation = None
I0123 20:41:24.993272 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:41:24.995211 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:24.995302 140446420529152 nn_components.py:261] mlp: residual
I0123 20:41:24.997756 140446420529152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:24.997833 140446420529152 transformer_base.py:443] tbase: final FFN
I0123 20:41:24.998247 140446420529152 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:41:25.000473 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:41:25.002371 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.002464 140446420529152 nn_components.py:261] mlp: residual
I0123 20:41:25.002748 140446420529152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.002826 140446420529152 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 20:41:25.002937 140446420529152 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:41:25.002974 140446420529152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:41:25.003003 140446420529152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:41:25.004823 140446420529152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.007115 140446420529152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:41:25.012611 140446420529152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.012863 140446420529152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:41:25.015420 140446420529152 transformer_layer.py:213] tlayer: windowed attention.
I0123 20:41:25.019099 140446420529152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:41:25.019152 140446420529152 attention.py:418] Single window, no scan.
I0123 20:41:25.019186 140446420529152 transformer_layer.py:389] tlayer: self-attention.
I0123 20:41:25.019217 140446420529152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.019278 140446420529152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.019825 140446420529152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.019900 140446420529152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.020260 140446420529152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.021025 140446420529152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.023494 140446420529152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.024111 140446420529152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.024187 140446420529152 transformer_layer.py:468] tlayer: End windows.
I0123 20:41:25.024221 140446420529152 transformer_layer.py:472] tlayer: final FFN.
I0123 20:41:25.024279 140446420529152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.024404 140446420529152 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:41:25.024762 140446420529152 nn_components.py:325] mlp: activation = None
I0123 20:41:25.024804 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:41:25.026719 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.026810 140446420529152 nn_components.py:261] mlp: residual
I0123 20:41:25.029256 140446420529152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.029332 140446420529152 transformer_base.py:443] tbase: final FFN
I0123 20:41:25.029752 140446420529152 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:41:25.031994 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:41:25.033984 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.034078 140446420529152 nn_components.py:261] mlp: residual
I0123 20:41:25.034368 140446420529152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.034446 140446420529152 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 20:41:25.034552 140446420529152 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:41:25.034595 140446420529152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:41:25.034627 140446420529152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:41:25.036393 140446420529152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.038717 140446420529152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:41:25.044362 140446420529152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.044615 140446420529152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:41:25.047194 140446420529152 transformer_layer.py:213] tlayer: windowed attention.
I0123 20:41:25.050833 140446420529152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:41:25.050886 140446420529152 attention.py:418] Single window, no scan.
I0123 20:41:25.050921 140446420529152 transformer_layer.py:389] tlayer: self-attention.
I0123 20:41:25.050952 140446420529152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.051012 140446420529152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.051618 140446420529152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.051692 140446420529152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.052043 140446420529152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.052803 140446420529152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.055290 140446420529152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.055903 140446420529152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.055978 140446420529152 transformer_layer.py:468] tlayer: End windows.
I0123 20:41:25.056014 140446420529152 transformer_layer.py:472] tlayer: final FFN.
I0123 20:41:25.056071 140446420529152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.056196 140446420529152 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:41:25.056509 140446420529152 nn_components.py:325] mlp: activation = None
I0123 20:41:25.056550 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:41:25.058550 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.058642 140446420529152 nn_components.py:261] mlp: residual
I0123 20:41:25.061120 140446420529152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.061197 140446420529152 transformer_base.py:443] tbase: final FFN
I0123 20:41:25.061619 140446420529152 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:41:25.063886 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:41:25.065826 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.065920 140446420529152 nn_components.py:261] mlp: residual
I0123 20:41:25.066213 140446420529152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.066293 140446420529152 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 20:41:25.066400 140446420529152 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:41:25.066437 140446420529152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:41:25.066473 140446420529152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:41:25.068312 140446420529152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.070646 140446420529152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:41:25.076251 140446420529152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.076507 140446420529152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:41:25.079077 140446420529152 transformer_layer.py:213] tlayer: windowed attention.
I0123 20:41:25.082756 140446420529152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:41:25.082810 140446420529152 attention.py:418] Single window, no scan.
I0123 20:41:25.082844 140446420529152 transformer_layer.py:389] tlayer: self-attention.
I0123 20:41:25.082875 140446420529152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.082937 140446420529152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.083487 140446420529152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.083561 140446420529152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.083919 140446420529152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.084681 140446420529152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.087125 140446420529152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.087731 140446420529152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.087806 140446420529152 transformer_layer.py:468] tlayer: End windows.
I0123 20:41:25.087838 140446420529152 transformer_layer.py:472] tlayer: final FFN.
I0123 20:41:25.087896 140446420529152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.088050 140446420529152 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:41:25.088414 140446420529152 nn_components.py:325] mlp: activation = None
I0123 20:41:25.088457 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:41:25.090365 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.090457 140446420529152 nn_components.py:261] mlp: residual
I0123 20:41:25.092924 140446420529152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.093002 140446420529152 transformer_base.py:443] tbase: final FFN
I0123 20:41:25.093419 140446420529152 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:41:25.095679 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:41:25.098034 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.098128 140446420529152 nn_components.py:261] mlp: residual
I0123 20:41:25.098425 140446420529152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.098504 140446420529152 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 20:41:25.098611 140446420529152 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:41:25.098649 140446420529152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:41:25.098679 140446420529152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:41:25.100431 140446420529152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.102735 140446420529152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:41:25.108355 140446420529152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.108608 140446420529152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:41:25.111183 140446420529152 transformer_layer.py:213] tlayer: windowed attention.
I0123 20:41:25.114828 140446420529152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:41:25.114882 140446420529152 attention.py:418] Single window, no scan.
I0123 20:41:25.114915 140446420529152 transformer_layer.py:389] tlayer: self-attention.
I0123 20:41:25.114945 140446420529152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.115007 140446420529152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.115608 140446420529152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.115683 140446420529152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.116040 140446420529152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.116802 140446420529152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.119272 140446420529152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.119888 140446420529152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.119965 140446420529152 transformer_layer.py:468] tlayer: End windows.
I0123 20:41:25.119999 140446420529152 transformer_layer.py:472] tlayer: final FFN.
I0123 20:41:25.120056 140446420529152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.120181 140446420529152 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:41:25.120494 140446420529152 nn_components.py:325] mlp: activation = None
I0123 20:41:25.120536 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:41:25.122521 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.122613 140446420529152 nn_components.py:261] mlp: residual
I0123 20:41:25.125072 140446420529152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.125149 140446420529152 transformer_base.py:443] tbase: final FFN
I0123 20:41:25.125561 140446420529152 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:41:25.127810 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:41:25.129729 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.129821 140446420529152 nn_components.py:261] mlp: residual
I0123 20:41:25.130112 140446420529152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.130190 140446420529152 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 20:41:25.130295 140446420529152 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:41:25.130333 140446420529152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:41:25.130364 140446420529152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:41:25.132190 140446420529152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.134517 140446420529152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:41:25.140114 140446420529152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.140369 140446420529152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:41:25.142918 140446420529152 transformer_layer.py:213] tlayer: windowed attention.
I0123 20:41:25.146603 140446420529152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:41:25.146657 140446420529152 attention.py:418] Single window, no scan.
I0123 20:41:25.146691 140446420529152 transformer_layer.py:389] tlayer: self-attention.
I0123 20:41:25.146721 140446420529152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.146781 140446420529152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.147335 140446420529152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.147409 140446420529152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.147764 140446420529152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.148526 140446420529152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.151023 140446420529152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.151677 140446420529152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.151754 140446420529152 transformer_layer.py:468] tlayer: End windows.
I0123 20:41:25.151788 140446420529152 transformer_layer.py:472] tlayer: final FFN.
I0123 20:41:25.151846 140446420529152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.151974 140446420529152 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:41:25.152339 140446420529152 nn_components.py:325] mlp: activation = None
I0123 20:41:25.152381 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:41:25.154307 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.154399 140446420529152 nn_components.py:261] mlp: residual
I0123 20:41:25.156843 140446420529152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.156920 140446420529152 transformer_base.py:443] tbase: final FFN
I0123 20:41:25.157341 140446420529152 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:41:25.159614 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:41:25.161611 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.161714 140446420529152 nn_components.py:261] mlp: residual
I0123 20:41:25.162008 140446420529152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.162087 140446420529152 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 20:41:25.162194 140446420529152 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:41:25.162232 140446420529152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:41:25.162263 140446420529152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:41:25.164019 140446420529152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.166362 140446420529152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:41:25.172022 140446420529152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.172278 140446420529152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:41:25.174835 140446420529152 transformer_layer.py:213] tlayer: windowed attention.
I0123 20:41:25.178490 140446420529152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:41:25.178544 140446420529152 attention.py:418] Single window, no scan.
I0123 20:41:25.178578 140446420529152 transformer_layer.py:389] tlayer: self-attention.
I0123 20:41:25.178609 140446420529152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.178671 140446420529152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.179280 140446420529152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.179355 140446420529152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.179713 140446420529152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.180473 140446420529152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.182947 140446420529152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.183559 140446420529152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.183634 140446420529152 transformer_layer.py:468] tlayer: End windows.
I0123 20:41:25.183668 140446420529152 transformer_layer.py:472] tlayer: final FFN.
I0123 20:41:25.183727 140446420529152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.183854 140446420529152 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:41:25.184165 140446420529152 nn_components.py:325] mlp: activation = None
I0123 20:41:25.184207 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:41:25.186137 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.186230 140446420529152 nn_components.py:261] mlp: residual
I0123 20:41:25.188762 140446420529152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.188838 140446420529152 transformer_base.py:443] tbase: final FFN
I0123 20:41:25.189259 140446420529152 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:41:25.191532 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:41:25.193446 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.193539 140446420529152 nn_components.py:261] mlp: residual
I0123 20:41:25.193835 140446420529152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.193916 140446420529152 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 20:41:25.194024 140446420529152 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:41:25.194062 140446420529152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:41:25.194092 140446420529152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:41:25.195873 140446420529152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.198293 140446420529152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:41:25.203899 140446420529152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.204163 140446420529152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:41:25.206755 140446420529152 transformer_layer.py:213] tlayer: windowed attention.
I0123 20:41:25.210372 140446420529152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:41:25.210426 140446420529152 attention.py:418] Single window, no scan.
I0123 20:41:25.210460 140446420529152 transformer_layer.py:389] tlayer: self-attention.
I0123 20:41:25.210491 140446420529152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.210934 140446420529152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.211499 140446420529152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.211574 140446420529152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.211940 140446420529152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.212712 140446420529152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.215219 140446420529152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.215838 140446420529152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.215914 140446420529152 transformer_layer.py:468] tlayer: End windows.
I0123 20:41:25.215949 140446420529152 transformer_layer.py:472] tlayer: final FFN.
I0123 20:41:25.216007 140446420529152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.216133 140446420529152 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:41:25.216450 140446420529152 nn_components.py:325] mlp: activation = None
I0123 20:41:25.216492 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:41:25.218416 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.218510 140446420529152 nn_components.py:261] mlp: residual
I0123 20:41:25.221037 140446420529152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.221115 140446420529152 transformer_base.py:443] tbase: final FFN
I0123 20:41:25.221530 140446420529152 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:41:25.223790 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:41:25.225705 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.225799 140446420529152 nn_components.py:261] mlp: residual
I0123 20:41:25.226089 140446420529152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.226169 140446420529152 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 20:41:25.226276 140446420529152 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:41:25.226314 140446420529152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:41:25.226344 140446420529152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:41:25.228118 140446420529152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.230521 140446420529152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:41:25.236110 140446420529152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.236368 140446420529152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:41:25.238954 140446420529152 transformer_layer.py:213] tlayer: windowed attention.
I0123 20:41:25.242555 140446420529152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:41:25.242608 140446420529152 attention.py:418] Single window, no scan.
I0123 20:41:25.242643 140446420529152 transformer_layer.py:389] tlayer: self-attention.
I0123 20:41:25.242674 140446420529152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.242789 140446420529152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.243347 140446420529152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.243420 140446420529152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.243773 140446420529152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.244532 140446420529152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.246999 140446420529152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.247616 140446420529152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.247690 140446420529152 transformer_layer.py:468] tlayer: End windows.
I0123 20:41:25.247724 140446420529152 transformer_layer.py:472] tlayer: final FFN.
I0123 20:41:25.247782 140446420529152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.247908 140446420529152 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:41:25.248222 140446420529152 nn_components.py:325] mlp: activation = None
I0123 20:41:25.248262 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:41:25.250192 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.250286 140446420529152 nn_components.py:261] mlp: residual
I0123 20:41:25.252814 140446420529152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.252892 140446420529152 transformer_base.py:443] tbase: final FFN
I0123 20:41:25.253309 140446420529152 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:41:25.255570 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:41:25.257476 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.257570 140446420529152 nn_components.py:261] mlp: residual
I0123 20:41:25.257869 140446420529152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.257950 140446420529152 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 20:41:25.258058 140446420529152 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:41:25.258096 140446420529152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:41:25.258126 140446420529152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:41:25.259889 140446420529152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.262319 140446420529152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:41:25.267912 140446420529152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.268164 140446420529152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:41:25.270783 140446420529152 transformer_layer.py:213] tlayer: windowed attention.
I0123 20:41:25.274424 140446420529152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:41:25.274478 140446420529152 attention.py:418] Single window, no scan.
I0123 20:41:25.274512 140446420529152 transformer_layer.py:389] tlayer: self-attention.
I0123 20:41:25.274543 140446420529152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.274657 140446420529152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.275209 140446420529152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.275284 140446420529152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.275644 140446420529152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.276414 140446420529152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.278895 140446420529152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.279509 140446420529152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.279585 140446420529152 transformer_layer.py:468] tlayer: End windows.
I0123 20:41:25.279620 140446420529152 transformer_layer.py:472] tlayer: final FFN.
I0123 20:41:25.279678 140446420529152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.279805 140446420529152 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:41:25.280121 140446420529152 nn_components.py:325] mlp: activation = None
I0123 20:41:25.280163 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:41:25.282067 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.282158 140446420529152 nn_components.py:261] mlp: residual
I0123 20:41:25.284667 140446420529152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.284743 140446420529152 transformer_base.py:443] tbase: final FFN
I0123 20:41:25.285154 140446420529152 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:41:25.287407 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:41:25.289294 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.289386 140446420529152 nn_components.py:261] mlp: residual
I0123 20:41:25.289683 140446420529152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.289930 140446420529152 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 20:41:25.289997 140446420529152 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 20:41:25.290054 140446420529152 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 20:41:25.290110 140446420529152 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 20:41:25.290164 140446420529152 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 20:41:25.290217 140446420529152 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 20:41:25.290269 140446420529152 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 20:41:25.290321 140446420529152 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 20:41:25.290379 140446420529152 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 20:41:25.290432 140446420529152 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 20:41:25.290484 140446420529152 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 20:41:25.290535 140446420529152 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 20:41:25.290570 140446420529152 decoder_stack.py:344] dstack: Final layernorm.
I0123 20:41:25.293490 140446420529152 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:41:25.338365 140446420529152 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.338449 140446420529152 decoder_stack.py:333] dstack: autoregressive generator.
I0123 20:41:25.338501 140446420529152 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 20:41:25.338603 140446420529152 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:41:25.338639 140446420529152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:41:25.338667 140446420529152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:41:25.338726 140446420529152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.341068 140446420529152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:41:25.346356 140446420529152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.346610 140446420529152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:41:25.349148 140446420529152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:41:25.361694 140446420529152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:41:25.361747 140446420529152 attention.py:418] Single window, no scan.
I0123 20:41:25.361781 140446420529152 transformer_layer.py:389] tlayer: self-attention.
I0123 20:41:25.361810 140446420529152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.361869 140446420529152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.362421 140446420529152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.362494 140446420529152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.362854 140446420529152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.363532 140446420529152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.366058 140446420529152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.366675 140446420529152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.366750 140446420529152 transformer_layer.py:468] tlayer: End windows.
I0123 20:41:25.366784 140446420529152 transformer_layer.py:472] tlayer: final FFN.
I0123 20:41:25.366843 140446420529152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.366971 140446420529152 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:41:25.367079 140446420529152 nn_components.py:325] mlp: activation = None
I0123 20:41:25.367116 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:41:25.368971 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.369070 140446420529152 nn_components.py:261] mlp: residual
I0123 20:41:25.371494 140446420529152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.371572 140446420529152 transformer_base.py:443] tbase: final FFN
I0123 20:41:25.371679 140446420529152 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:41:25.373933 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:41:25.375776 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.375870 140446420529152 nn_components.py:261] mlp: residual
I0123 20:41:25.376163 140446420529152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.376241 140446420529152 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 20:41:25.376347 140446420529152 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:41:25.376384 140446420529152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:41:25.376414 140446420529152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:41:25.376476 140446420529152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.378718 140446420529152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:41:25.384107 140446420529152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.384364 140446420529152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:41:25.387030 140446420529152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:41:25.399858 140446420529152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:41:25.399914 140446420529152 attention.py:418] Single window, no scan.
I0123 20:41:25.399948 140446420529152 transformer_layer.py:389] tlayer: self-attention.
I0123 20:41:25.399978 140446420529152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.400039 140446420529152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.400587 140446420529152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.400662 140446420529152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.401018 140446420529152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.401759 140446420529152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.404221 140446420529152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.404830 140446420529152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.404906 140446420529152 transformer_layer.py:468] tlayer: End windows.
I0123 20:41:25.404940 140446420529152 transformer_layer.py:472] tlayer: final FFN.
I0123 20:41:25.404998 140446420529152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.405126 140446420529152 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:41:25.405236 140446420529152 nn_components.py:325] mlp: activation = None
I0123 20:41:25.405274 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:41:25.407138 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.407229 140446420529152 nn_components.py:261] mlp: residual
I0123 20:41:25.409636 140446420529152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.409721 140446420529152 transformer_base.py:443] tbase: final FFN
I0123 20:41:25.409829 140446420529152 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:41:25.412085 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:41:25.413955 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.414048 140446420529152 nn_components.py:261] mlp: residual
I0123 20:41:25.414341 140446420529152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.414420 140446420529152 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 20:41:25.414528 140446420529152 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:41:25.414566 140446420529152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:41:25.414597 140446420529152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:41:25.414659 140446420529152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.416905 140446420529152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:41:25.422354 140446420529152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.422615 140446420529152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:41:25.425306 140446420529152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:41:25.437718 140446420529152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:41:25.437772 140446420529152 attention.py:418] Single window, no scan.
I0123 20:41:25.437807 140446420529152 transformer_layer.py:389] tlayer: self-attention.
I0123 20:41:25.437837 140446420529152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.437898 140446420529152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.438456 140446420529152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.438531 140446420529152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.438892 140446420529152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.439630 140446420529152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.442102 140446420529152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.442713 140446420529152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.442789 140446420529152 transformer_layer.py:468] tlayer: End windows.
I0123 20:41:25.442823 140446420529152 transformer_layer.py:472] tlayer: final FFN.
I0123 20:41:25.442882 140446420529152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.443009 140446420529152 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:41:25.443115 140446420529152 nn_components.py:325] mlp: activation = None
I0123 20:41:25.443153 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:41:25.445007 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.445097 140446420529152 nn_components.py:261] mlp: residual
I0123 20:41:25.447512 140446420529152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.447596 140446420529152 transformer_base.py:443] tbase: final FFN
I0123 20:41:25.447705 140446420529152 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:41:25.449970 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:41:25.451811 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.451904 140446420529152 nn_components.py:261] mlp: residual
I0123 20:41:25.452196 140446420529152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.452275 140446420529152 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 20:41:25.452383 140446420529152 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:41:25.452422 140446420529152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:41:25.452452 140446420529152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:41:25.452513 140446420529152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.454770 140446420529152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:41:25.460169 140446420529152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.460427 140446420529152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:41:25.463104 140446420529152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:41:25.475347 140446420529152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:41:25.475400 140446420529152 attention.py:418] Single window, no scan.
I0123 20:41:25.475435 140446420529152 transformer_layer.py:389] tlayer: self-attention.
I0123 20:41:25.475465 140446420529152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.475525 140446420529152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.476074 140446420529152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.476150 140446420529152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.476507 140446420529152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.477242 140446420529152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.479698 140446420529152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.480302 140446420529152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.480377 140446420529152 transformer_layer.py:468] tlayer: End windows.
I0123 20:41:25.480410 140446420529152 transformer_layer.py:472] tlayer: final FFN.
I0123 20:41:25.480468 140446420529152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.480594 140446420529152 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:41:25.480700 140446420529152 nn_components.py:325] mlp: activation = None
I0123 20:41:25.480738 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:41:25.482597 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.482688 140446420529152 nn_components.py:261] mlp: residual
I0123 20:41:25.485091 140446420529152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.485172 140446420529152 transformer_base.py:443] tbase: final FFN
I0123 20:41:25.485283 140446420529152 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:41:25.487541 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:41:25.489389 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.489481 140446420529152 nn_components.py:261] mlp: residual
I0123 20:41:25.489782 140446420529152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.489862 140446420529152 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 20:41:25.489970 140446420529152 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:41:25.490007 140446420529152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:41:25.490038 140446420529152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:41:25.490101 140446420529152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.492347 140446420529152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:41:25.497726 140446420529152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.497983 140446420529152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:41:25.500646 140446420529152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:41:25.513378 140446420529152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:41:25.513432 140446420529152 attention.py:418] Single window, no scan.
I0123 20:41:25.513466 140446420529152 transformer_layer.py:389] tlayer: self-attention.
I0123 20:41:25.513496 140446420529152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.513558 140446420529152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.514114 140446420529152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.514189 140446420529152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.514546 140446420529152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.515272 140446420529152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.517708 140446420529152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.518313 140446420529152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.518388 140446420529152 transformer_layer.py:468] tlayer: End windows.
I0123 20:41:25.518420 140446420529152 transformer_layer.py:472] tlayer: final FFN.
I0123 20:41:25.518477 140446420529152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.518604 140446420529152 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:41:25.518713 140446420529152 nn_components.py:325] mlp: activation = None
I0123 20:41:25.518751 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:41:25.520606 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.520698 140446420529152 nn_components.py:261] mlp: residual
I0123 20:41:25.523118 140446420529152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.523202 140446420529152 transformer_base.py:443] tbase: final FFN
I0123 20:41:25.523313 140446420529152 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:41:25.525561 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:41:25.527414 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.527508 140446420529152 nn_components.py:261] mlp: residual
I0123 20:41:25.527799 140446420529152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.527879 140446420529152 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 20:41:25.527987 140446420529152 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:41:25.528025 140446420529152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:41:25.528055 140446420529152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:41:25.528118 140446420529152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.530370 140446420529152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:41:25.535722 140446420529152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.535983 140446420529152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:41:25.538662 140446420529152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:41:25.551013 140446420529152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:41:25.551067 140446420529152 attention.py:418] Single window, no scan.
I0123 20:41:25.551101 140446420529152 transformer_layer.py:389] tlayer: self-attention.
I0123 20:41:25.551132 140446420529152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.551192 140446420529152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.551739 140446420529152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.551814 140446420529152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.552173 140446420529152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.552910 140446420529152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.555383 140446420529152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.555990 140446420529152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.556065 140446420529152 transformer_layer.py:468] tlayer: End windows.
I0123 20:41:25.556099 140446420529152 transformer_layer.py:472] tlayer: final FFN.
I0123 20:41:25.556158 140446420529152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.556286 140446420529152 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:41:25.556392 140446420529152 nn_components.py:325] mlp: activation = None
I0123 20:41:25.556429 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:41:25.558273 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.558365 140446420529152 nn_components.py:261] mlp: residual
I0123 20:41:25.560767 140446420529152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.560844 140446420529152 transformer_base.py:443] tbase: final FFN
I0123 20:41:25.560958 140446420529152 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:41:25.563221 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:41:25.565069 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.565161 140446420529152 nn_components.py:261] mlp: residual
I0123 20:41:25.565454 140446420529152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.565533 140446420529152 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 20:41:25.565645 140446420529152 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:41:25.565684 140446420529152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:41:25.565714 140446420529152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:41:25.565777 140446420529152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.568011 140446420529152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:41:25.573419 140446420529152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.573680 140446420529152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:41:25.576343 140446420529152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:41:25.588685 140446420529152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:41:25.588740 140446420529152 attention.py:418] Single window, no scan.
I0123 20:41:25.588774 140446420529152 transformer_layer.py:389] tlayer: self-attention.
I0123 20:41:25.588804 140446420529152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.588866 140446420529152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.589414 140446420529152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.589487 140446420529152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.589852 140446420529152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.590589 140446420529152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.593042 140446420529152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.593663 140446420529152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.593739 140446420529152 transformer_layer.py:468] tlayer: End windows.
I0123 20:41:25.593772 140446420529152 transformer_layer.py:472] tlayer: final FFN.
I0123 20:41:25.593830 140446420529152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.593960 140446420529152 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:41:25.594067 140446420529152 nn_components.py:325] mlp: activation = None
I0123 20:41:25.594105 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:41:25.595942 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.596033 140446420529152 nn_components.py:261] mlp: residual
I0123 20:41:25.598445 140446420529152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.598523 140446420529152 transformer_base.py:443] tbase: final FFN
I0123 20:41:25.598631 140446420529152 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:41:25.600892 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:41:25.602749 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.602843 140446420529152 nn_components.py:261] mlp: residual
I0123 20:41:25.603137 140446420529152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.603216 140446420529152 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 20:41:25.603324 140446420529152 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:41:25.603362 140446420529152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:41:25.603392 140446420529152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:41:25.603455 140446420529152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.605717 140446420529152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:41:25.611115 140446420529152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.611377 140446420529152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:41:25.614038 140446420529152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:41:25.626718 140446420529152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:41:25.626772 140446420529152 attention.py:418] Single window, no scan.
I0123 20:41:25.626806 140446420529152 transformer_layer.py:389] tlayer: self-attention.
I0123 20:41:25.626837 140446420529152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.626899 140446420529152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.627451 140446420529152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.627524 140446420529152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.627879 140446420529152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.628611 140446420529152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.631076 140446420529152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.631684 140446420529152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.631758 140446420529152 transformer_layer.py:468] tlayer: End windows.
I0123 20:41:25.631792 140446420529152 transformer_layer.py:472] tlayer: final FFN.
I0123 20:41:25.631850 140446420529152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.631975 140446420529152 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:41:25.632081 140446420529152 nn_components.py:325] mlp: activation = None
I0123 20:41:25.632118 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:41:25.633961 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.634052 140446420529152 nn_components.py:261] mlp: residual
I0123 20:41:25.636448 140446420529152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.636525 140446420529152 transformer_base.py:443] tbase: final FFN
I0123 20:41:25.636632 140446420529152 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:41:25.638895 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:41:25.640759 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.640852 140446420529152 nn_components.py:261] mlp: residual
I0123 20:41:25.641148 140446420529152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.641227 140446420529152 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 20:41:25.641332 140446420529152 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:41:25.641369 140446420529152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:41:25.641399 140446420529152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:41:25.641463 140446420529152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.643696 140446420529152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:41:25.649083 140446420529152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.649345 140446420529152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:41:25.652019 140446420529152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:41:25.664415 140446420529152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:41:25.664469 140446420529152 attention.py:418] Single window, no scan.
I0123 20:41:25.664503 140446420529152 transformer_layer.py:389] tlayer: self-attention.
I0123 20:41:25.664533 140446420529152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.664594 140446420529152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.665150 140446420529152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.665226 140446420529152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.665579 140446420529152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.666270 140446420529152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.668775 140446420529152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.669384 140446420529152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.669459 140446420529152 transformer_layer.py:468] tlayer: End windows.
I0123 20:41:25.669492 140446420529152 transformer_layer.py:472] tlayer: final FFN.
I0123 20:41:25.669550 140446420529152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.669683 140446420529152 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:41:25.669794 140446420529152 nn_components.py:325] mlp: activation = None
I0123 20:41:25.669832 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:41:25.671682 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.671774 140446420529152 nn_components.py:261] mlp: residual
I0123 20:41:25.674193 140446420529152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.674270 140446420529152 transformer_base.py:443] tbase: final FFN
I0123 20:41:25.674380 140446420529152 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:41:25.676648 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:41:25.678505 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.678607 140446420529152 nn_components.py:261] mlp: residual
I0123 20:41:25.678900 140446420529152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.678979 140446420529152 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 20:41:25.679086 140446420529152 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:41:25.679124 140446420529152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:41:25.679155 140446420529152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:41:25.679218 140446420529152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.681447 140446420529152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:41:25.686821 140446420529152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.687078 140446420529152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:41:25.689742 140446420529152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:41:25.702065 140446420529152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:41:25.702118 140446420529152 attention.py:418] Single window, no scan.
I0123 20:41:25.702153 140446420529152 transformer_layer.py:389] tlayer: self-attention.
I0123 20:41:25.702182 140446420529152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.702244 140446420529152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.702795 140446420529152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.702869 140446420529152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.703229 140446420529152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.703913 140446420529152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.706436 140446420529152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.707045 140446420529152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.707120 140446420529152 transformer_layer.py:468] tlayer: End windows.
I0123 20:41:25.707154 140446420529152 transformer_layer.py:472] tlayer: final FFN.
I0123 20:41:25.707212 140446420529152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.707339 140446420529152 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:41:25.707446 140446420529152 nn_components.py:325] mlp: activation = None
I0123 20:41:25.707484 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:41:25.709324 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.709416 140446420529152 nn_components.py:261] mlp: residual
I0123 20:41:25.711828 140446420529152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.711906 140446420529152 transformer_base.py:443] tbase: final FFN
I0123 20:41:25.712014 140446420529152 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:41:25.714286 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:41:25.716147 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.716247 140446420529152 nn_components.py:261] mlp: residual
I0123 20:41:25.716545 140446420529152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.716626 140446420529152 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 20:41:25.716734 140446420529152 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:41:25.716772 140446420529152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:41:25.716802 140446420529152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:41:25.716864 140446420529152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.719121 140446420529152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:41:25.724522 140446420529152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.724776 140446420529152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:41:25.727451 140446420529152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:41:25.740189 140446420529152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:41:25.740242 140446420529152 attention.py:418] Single window, no scan.
I0123 20:41:25.740277 140446420529152 transformer_layer.py:389] tlayer: self-attention.
I0123 20:41:25.740307 140446420529152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.740368 140446420529152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.740927 140446420529152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.741002 140446420529152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.741360 140446420529152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.742054 140446420529152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.744573 140446420529152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.745182 140446420529152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.745257 140446420529152 transformer_layer.py:468] tlayer: End windows.
I0123 20:41:25.745291 140446420529152 transformer_layer.py:472] tlayer: final FFN.
I0123 20:41:25.745348 140446420529152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.745474 140446420529152 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:41:25.745581 140446420529152 nn_components.py:325] mlp: activation = None
I0123 20:41:25.745618 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:41:25.747471 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.747563 140446420529152 nn_components.py:261] mlp: residual
I0123 20:41:25.749971 140446420529152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.750049 140446420529152 transformer_base.py:443] tbase: final FFN
I0123 20:41:25.750158 140446420529152 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:41:25.752409 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:41:25.754261 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.754356 140446420529152 nn_components.py:261] mlp: residual
I0123 20:41:25.754654 140446420529152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.754734 140446420529152 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 20:41:25.754841 140446420529152 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:41:25.754878 140446420529152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:41:25.754908 140446420529152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:41:25.754971 140446420529152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.757212 140446420529152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:41:25.762613 140446420529152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.762868 140446420529152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:41:25.765528 140446420529152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:41:25.777868 140446420529152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:41:25.777921 140446420529152 attention.py:418] Single window, no scan.
I0123 20:41:25.777955 140446420529152 transformer_layer.py:389] tlayer: self-attention.
I0123 20:41:25.777985 140446420529152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.778047 140446420529152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.778601 140446420529152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.778675 140446420529152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.779033 140446420529152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.779713 140446420529152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.782210 140446420529152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.782816 140446420529152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.782892 140446420529152 transformer_layer.py:468] tlayer: End windows.
I0123 20:41:25.782927 140446420529152 transformer_layer.py:472] tlayer: final FFN.
I0123 20:41:25.782985 140446420529152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.783112 140446420529152 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:41:25.783219 140446420529152 nn_components.py:325] mlp: activation = None
I0123 20:41:25.783256 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:41:25.785096 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.785186 140446420529152 nn_components.py:261] mlp: residual
I0123 20:41:25.787593 140446420529152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.787670 140446420529152 transformer_base.py:443] tbase: final FFN
I0123 20:41:25.787776 140446420529152 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:41:25.790017 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:41:25.791848 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.791941 140446420529152 nn_components.py:261] mlp: residual
I0123 20:41:25.792233 140446420529152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.792321 140446420529152 decoder_stack.py:344] dstack: Final layernorm.
I0123 20:41:25.795168 140446420529152 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:41:25.844381 140446420529152 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:25.844464 140446420529152 decoder_stack.py:333] dstack: autoregressive generator.
I0123 20:41:25.844516 140446420529152 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 20:41:25.844617 140446420529152 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:41:25.844653 140446420529152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:41:25.844683 140446420529152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:41:25.844745 140446420529152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:25.847034 140446420529152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:41:25.852486 140446420529152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:25.852744 140446420529152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:41:25.855314 140446420529152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:41:25.867915 140446420529152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:41:25.867968 140446420529152 attention.py:418] Single window, no scan.
I0123 20:41:25.868002 140446420529152 transformer_layer.py:389] tlayer: self-attention.
I0123 20:41:25.868031 140446420529152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:25.868092 140446420529152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:25.868647 140446420529152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:25.868721 140446420529152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:25.869076 140446420529152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:25.869765 140446420529152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:25.872214 140446420529152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:25.872819 140446420529152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:25.872894 140446420529152 transformer_layer.py:468] tlayer: End windows.
I0123 20:41:25.872928 140446420529152 transformer_layer.py:472] tlayer: final FFN.
I0123 20:41:25.872987 140446420529152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:25.873116 140446420529152 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:41:25.873223 140446420529152 nn_components.py:325] mlp: activation = None
I0123 20:41:25.873260 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:41:25.875190 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:25.875282 140446420529152 nn_components.py:261] mlp: residual
I0123 20:41:25.877672 140446420529152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:25.877749 140446420529152 transformer_base.py:443] tbase: final FFN
I0123 20:41:25.877865 140446420529152 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:41:25.880048 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:41:25.881914 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:25.882008 140446420529152 nn_components.py:261] mlp: residual
I0123 20:41:25.882302 140446420529152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:25.882382 140446420529152 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 20:41:25.882489 140446420529152 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:41:25.882526 140446420529152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:41:25.882557 140446420529152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:41:25.882622 140446420529152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:25.884859 140446420529152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:41:25.890311 140446420529152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:25.890567 140446420529152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:41:25.893144 140446420529152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:41:25.905385 140446420529152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:41:25.905437 140446420529152 attention.py:418] Single window, no scan.
I0123 20:41:25.905471 140446420529152 transformer_layer.py:389] tlayer: self-attention.
I0123 20:41:25.905499 140446420529152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:25.905558 140446420529152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:25.906107 140446420529152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:25.906180 140446420529152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:25.906528 140446420529152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:25.907196 140446420529152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:25.909611 140446420529152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:25.910223 140446420529152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:25.910297 140446420529152 transformer_layer.py:468] tlayer: End windows.
I0123 20:41:25.910331 140446420529152 transformer_layer.py:472] tlayer: final FFN.
I0123 20:41:25.910388 140446420529152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:25.910512 140446420529152 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:41:25.910617 140446420529152 nn_components.py:325] mlp: activation = None
I0123 20:41:25.910653 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:41:25.912975 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:25.913068 140446420529152 nn_components.py:261] mlp: residual
I0123 20:41:25.915437 140446420529152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:25.915513 140446420529152 transformer_base.py:443] tbase: final FFN
I0123 20:41:25.915621 140446420529152 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:41:25.917785 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:41:25.919585 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:25.919677 140446420529152 nn_components.py:261] mlp: residual
I0123 20:41:25.919961 140446420529152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:25.920040 140446420529152 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 20:41:25.920143 140446420529152 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:41:25.920180 140446420529152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:41:25.920208 140446420529152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:41:25.920269 140446420529152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:25.922461 140446420529152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:41:25.927785 140446420529152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:25.928036 140446420529152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:41:25.930562 140446420529152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:41:25.942660 140446420529152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:41:25.942713 140446420529152 attention.py:418] Single window, no scan.
I0123 20:41:25.942745 140446420529152 transformer_layer.py:389] tlayer: self-attention.
I0123 20:41:25.942774 140446420529152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:25.942836 140446420529152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:25.943382 140446420529152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:25.943457 140446420529152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:25.943810 140446420529152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:25.944478 140446420529152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:25.946880 140446420529152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:25.947481 140446420529152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:25.947554 140446420529152 transformer_layer.py:468] tlayer: End windows.
I0123 20:41:25.947587 140446420529152 transformer_layer.py:472] tlayer: final FFN.
I0123 20:41:25.947643 140446420529152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:25.947767 140446420529152 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:41:25.947873 140446420529152 nn_components.py:325] mlp: activation = None
I0123 20:41:25.947910 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:41:25.949797 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:25.949888 140446420529152 nn_components.py:261] mlp: residual
I0123 20:41:25.952253 140446420529152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:25.952329 140446420529152 transformer_base.py:443] tbase: final FFN
I0123 20:41:25.952435 140446420529152 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:41:25.954566 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:41:25.956382 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:25.956473 140446420529152 nn_components.py:261] mlp: residual
I0123 20:41:25.956759 140446420529152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:25.956836 140446420529152 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 20:41:25.956942 140446420529152 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:41:25.956978 140446420529152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:41:25.957007 140446420529152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:41:25.957066 140446420529152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:25.959270 140446420529152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:41:25.964681 140446420529152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:25.964932 140446420529152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:41:25.967466 140446420529152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:41:25.979516 140446420529152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:41:25.979569 140446420529152 attention.py:418] Single window, no scan.
I0123 20:41:25.979602 140446420529152 transformer_layer.py:389] tlayer: self-attention.
I0123 20:41:25.979631 140446420529152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:25.979692 140446420529152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:25.980236 140446420529152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:25.980309 140446420529152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:25.980659 140446420529152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:25.981318 140446420529152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:25.983716 140446420529152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:25.984317 140446420529152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:25.984391 140446420529152 transformer_layer.py:468] tlayer: End windows.
I0123 20:41:25.984424 140446420529152 transformer_layer.py:472] tlayer: final FFN.
I0123 20:41:25.984481 140446420529152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:25.984606 140446420529152 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:41:25.984713 140446420529152 nn_components.py:325] mlp: activation = None
I0123 20:41:25.984750 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:41:25.986644 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:25.986736 140446420529152 nn_components.py:261] mlp: residual
I0123 20:41:25.989103 140446420529152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:25.989179 140446420529152 transformer_base.py:443] tbase: final FFN
I0123 20:41:25.989286 140446420529152 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:41:25.991440 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:41:25.993255 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:25.993355 140446420529152 nn_components.py:261] mlp: residual
I0123 20:41:25.993650 140446420529152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:25.993730 140446420529152 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 20:41:25.993843 140446420529152 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:41:25.993880 140446420529152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:41:25.993908 140446420529152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:41:25.993969 140446420529152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:25.996161 140446420529152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:41:26.001511 140446420529152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.001770 140446420529152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:41:26.004303 140446420529152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:41:26.016354 140446420529152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:41:26.016407 140446420529152 attention.py:418] Single window, no scan.
I0123 20:41:26.016440 140446420529152 transformer_layer.py:389] tlayer: self-attention.
I0123 20:41:26.016470 140446420529152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.016530 140446420529152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.017071 140446420529152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.017145 140446420529152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.017493 140446420529152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.018163 140446420529152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.020546 140446420529152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.021146 140446420529152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.021220 140446420529152 transformer_layer.py:468] tlayer: End windows.
I0123 20:41:26.021252 140446420529152 transformer_layer.py:472] tlayer: final FFN.
I0123 20:41:26.021307 140446420529152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.021430 140446420529152 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:41:26.021535 140446420529152 nn_components.py:325] mlp: activation = None
I0123 20:41:26.021571 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:41:26.023884 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.023977 140446420529152 nn_components.py:261] mlp: residual
I0123 20:41:26.026329 140446420529152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.026407 140446420529152 transformer_base.py:443] tbase: final FFN
I0123 20:41:26.026513 140446420529152 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:41:26.028632 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:41:26.030434 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.030532 140446420529152 nn_components.py:261] mlp: residual
I0123 20:41:26.030821 140446420529152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.030901 140446420529152 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 20:41:26.031005 140446420529152 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:41:26.031042 140446420529152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:41:26.031071 140446420529152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:41:26.031132 140446420529152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.033335 140446420529152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:41:26.038654 140446420529152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.038905 140446420529152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:41:26.041438 140446420529152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:41:26.053505 140446420529152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:41:26.053559 140446420529152 attention.py:418] Single window, no scan.
I0123 20:41:26.053592 140446420529152 transformer_layer.py:389] tlayer: self-attention.
I0123 20:41:26.053620 140446420529152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.053687 140446420529152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.054234 140446420529152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.054308 140446420529152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.054660 140446420529152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.055331 140446420529152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.057767 140446420529152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.058373 140446420529152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.058447 140446420529152 transformer_layer.py:468] tlayer: End windows.
I0123 20:41:26.058480 140446420529152 transformer_layer.py:472] tlayer: final FFN.
I0123 20:41:26.058537 140446420529152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.058661 140446420529152 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:41:26.058766 140446420529152 nn_components.py:325] mlp: activation = None
I0123 20:41:26.058804 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:41:26.060686 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.060776 140446420529152 nn_components.py:261] mlp: residual
I0123 20:41:26.063152 140446420529152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.063229 140446420529152 transformer_base.py:443] tbase: final FFN
I0123 20:41:26.063335 140446420529152 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:41:26.065468 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:41:26.067270 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.067363 140446420529152 nn_components.py:261] mlp: residual
I0123 20:41:26.067661 140446420529152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.067740 140446420529152 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 20:41:26.067846 140446420529152 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:41:26.067882 140446420529152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:41:26.067912 140446420529152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:41:26.067972 140446420529152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.070173 140446420529152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:41:26.075524 140446420529152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.075778 140446420529152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:41:26.078300 140446420529152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:41:26.090536 140446420529152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:41:26.090590 140446420529152 attention.py:418] Single window, no scan.
I0123 20:41:26.090623 140446420529152 transformer_layer.py:389] tlayer: self-attention.
I0123 20:41:26.090652 140446420529152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.090713 140446420529152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.091260 140446420529152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.091334 140446420529152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.091688 140446420529152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.092351 140446420529152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.094752 140446420529152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.095354 140446420529152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.095429 140446420529152 transformer_layer.py:468] tlayer: End windows.
I0123 20:41:26.095462 140446420529152 transformer_layer.py:472] tlayer: final FFN.
I0123 20:41:26.095519 140446420529152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.095643 140446420529152 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:41:26.095748 140446420529152 nn_components.py:325] mlp: activation = None
I0123 20:41:26.095784 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:41:26.097668 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.097761 140446420529152 nn_components.py:261] mlp: residual
I0123 20:41:26.100109 140446420529152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.100184 140446420529152 transformer_base.py:443] tbase: final FFN
I0123 20:41:26.100290 140446420529152 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:41:26.102432 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:41:26.104223 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.104315 140446420529152 nn_components.py:261] mlp: residual
I0123 20:41:26.104600 140446420529152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.104684 140446420529152 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 20:41:26.104790 140446420529152 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:41:26.104826 140446420529152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:41:26.104856 140446420529152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:41:26.104917 140446420529152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.107121 140446420529152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:41:26.112456 140446420529152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.112705 140446420529152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:41:26.115265 140446420529152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:41:26.127500 140446420529152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:41:26.127554 140446420529152 attention.py:418] Single window, no scan.
I0123 20:41:26.127588 140446420529152 transformer_layer.py:389] tlayer: self-attention.
I0123 20:41:26.127618 140446420529152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.127679 140446420529152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.128233 140446420529152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.128308 140446420529152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.128662 140446420529152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.129338 140446420529152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.131796 140446420529152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.132404 140446420529152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.132477 140446420529152 transformer_layer.py:468] tlayer: End windows.
I0123 20:41:26.132511 140446420529152 transformer_layer.py:472] tlayer: final FFN.
I0123 20:41:26.132568 140446420529152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.132691 140446420529152 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:41:26.132797 140446420529152 nn_components.py:325] mlp: activation = None
I0123 20:41:26.132834 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:41:26.135129 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.135220 140446420529152 nn_components.py:261] mlp: residual
I0123 20:41:26.137612 140446420529152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.137694 140446420529152 transformer_base.py:443] tbase: final FFN
I0123 20:41:26.137804 140446420529152 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:41:26.139977 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:41:26.141808 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.141900 140446420529152 nn_components.py:261] mlp: residual
I0123 20:41:26.142188 140446420529152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.142273 140446420529152 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 20:41:26.142380 140446420529152 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:41:26.142417 140446420529152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:41:26.142448 140446420529152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:41:26.142511 140446420529152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.144743 140446420529152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:41:26.150165 140446420529152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.150422 140446420529152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:41:26.152993 140446420529152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:41:26.165328 140446420529152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:41:26.165381 140446420529152 attention.py:418] Single window, no scan.
I0123 20:41:26.165416 140446420529152 transformer_layer.py:389] tlayer: self-attention.
I0123 20:41:26.165445 140446420529152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.165504 140446420529152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.166057 140446420529152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.166133 140446420529152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.166490 140446420529152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.167162 140446420529152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.169582 140446420529152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.170200 140446420529152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.170274 140446420529152 transformer_layer.py:468] tlayer: End windows.
I0123 20:41:26.170308 140446420529152 transformer_layer.py:472] tlayer: final FFN.
I0123 20:41:26.170366 140446420529152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.170490 140446420529152 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:41:26.170598 140446420529152 nn_components.py:325] mlp: activation = None
I0123 20:41:26.170635 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:41:26.172536 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.172627 140446420529152 nn_components.py:261] mlp: residual
I0123 20:41:26.175026 140446420529152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.175103 140446420529152 transformer_base.py:443] tbase: final FFN
I0123 20:41:26.175213 140446420529152 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:41:26.177381 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:41:26.179222 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.179314 140446420529152 nn_components.py:261] mlp: residual
I0123 20:41:26.179607 140446420529152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.179685 140446420529152 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 20:41:26.179800 140446420529152 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:41:26.179838 140446420529152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:41:26.179868 140446420529152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:41:26.179930 140446420529152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.182161 140446420529152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:41:26.187728 140446420529152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.187986 140446420529152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:41:26.190579 140446420529152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:41:26.202910 140446420529152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:41:26.202963 140446420529152 attention.py:418] Single window, no scan.
I0123 20:41:26.202997 140446420529152 transformer_layer.py:389] tlayer: self-attention.
I0123 20:41:26.203027 140446420529152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.203088 140446420529152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.203637 140446420529152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.203711 140446420529152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.204064 140446420529152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.204736 140446420529152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.207168 140446420529152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.207769 140446420529152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.207843 140446420529152 transformer_layer.py:468] tlayer: End windows.
I0123 20:41:26.207877 140446420529152 transformer_layer.py:472] tlayer: final FFN.
I0123 20:41:26.207933 140446420529152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.208059 140446420529152 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:41:26.208167 140446420529152 nn_components.py:325] mlp: activation = None
I0123 20:41:26.208204 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:41:26.210123 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.210213 140446420529152 nn_components.py:261] mlp: residual
I0123 20:41:26.212620 140446420529152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.212697 140446420529152 transformer_base.py:443] tbase: final FFN
I0123 20:41:26.212806 140446420529152 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:41:26.214993 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:41:26.216834 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.216926 140446420529152 nn_components.py:261] mlp: residual
I0123 20:41:26.217220 140446420529152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.217299 140446420529152 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 20:41:26.217407 140446420529152 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:41:26.217451 140446420529152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:41:26.217483 140446420529152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:41:26.217547 140446420529152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.219787 140446420529152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:41:26.225232 140446420529152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.225486 140446420529152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:41:26.228054 140446420529152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:41:26.240322 140446420529152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:41:26.240376 140446420529152 attention.py:418] Single window, no scan.
I0123 20:41:26.240411 140446420529152 transformer_layer.py:389] tlayer: self-attention.
I0123 20:41:26.240443 140446420529152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.240503 140446420529152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.241048 140446420529152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.241121 140446420529152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.241473 140446420529152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.242149 140446420529152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.244580 140446420529152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.245183 140446420529152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.245256 140446420529152 transformer_layer.py:468] tlayer: End windows.
I0123 20:41:26.245290 140446420529152 transformer_layer.py:472] tlayer: final FFN.
I0123 20:41:26.245347 140446420529152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.245473 140446420529152 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:41:26.245580 140446420529152 nn_components.py:325] mlp: activation = None
I0123 20:41:26.245617 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:41:26.247893 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.247987 140446420529152 nn_components.py:261] mlp: residual
I0123 20:41:26.250380 140446420529152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.250457 140446420529152 transformer_base.py:443] tbase: final FFN
I0123 20:41:26.250566 140446420529152 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:41:26.252730 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:41:26.254563 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.254656 140446420529152 nn_components.py:261] mlp: residual
I0123 20:41:26.254948 140446420529152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.255025 140446420529152 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 20:41:26.255131 140446420529152 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:41:26.255174 140446420529152 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:41:26.255206 140446420529152 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:41:26.255270 140446420529152 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.257482 140446420529152 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:41:26.262894 140446420529152 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.263149 140446420529152 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:41:26.265702 140446420529152 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:41:26.277956 140446420529152 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:41:26.278011 140446420529152 attention.py:418] Single window, no scan.
I0123 20:41:26.278047 140446420529152 transformer_layer.py:389] tlayer: self-attention.
I0123 20:41:26.278077 140446420529152 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.278138 140446420529152 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.278689 140446420529152 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.278764 140446420529152 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.279118 140446420529152 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.279789 140446420529152 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.282241 140446420529152 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.282850 140446420529152 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.282925 140446420529152 transformer_layer.py:468] tlayer: End windows.
I0123 20:41:26.282960 140446420529152 transformer_layer.py:472] tlayer: final FFN.
I0123 20:41:26.283017 140446420529152 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.283143 140446420529152 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:41:26.283252 140446420529152 nn_components.py:325] mlp: activation = None
I0123 20:41:26.283290 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:41:26.285189 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.285279 140446420529152 nn_components.py:261] mlp: residual
I0123 20:41:26.287654 140446420529152 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.287730 140446420529152 transformer_base.py:443] tbase: final FFN
I0123 20:41:26.287835 140446420529152 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:41:26.290008 140446420529152 nn_components.py:329] mlp: final activation = None
I0123 20:41:26.291834 140446420529152 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.291926 140446420529152 nn_components.py:261] mlp: residual
I0123 20:41:26.292215 140446420529152 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:26.292297 140446420529152 decoder_stack.py:344] dstack: Final layernorm.
I0123 20:41:26.295150 140446420529152 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:41:42.620046 140446420529152 alphageometry.py:566] LM output (score=-1.670909): "o : D k m m o 22 ;"
I0123 20:41:42.620317 140446420529152 alphageometry.py:567] Translation: "o = on_circle o m k"

I0123 20:41:42.620373 140446420529152 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o m k ? perp l n n e"
I0123 20:41:42.620547 140446420529152 graph.py:498] 
I0123 20:41:42.620608 140446420529152 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o m k ? perp l n n e
I0123 20:41:45.776765 140446420529152 ddar.py:60] Depth 1/1000 time = 3.120868444442749
I0123 20:41:49.428384 140446420529152 ddar.py:60] Depth 2/1000 time = 3.6514453887939453
I0123 20:41:53.713282 140446420529152 ddar.py:60] Depth 3/1000 time = 4.284724235534668
I0123 20:41:58.083640 140446420529152 ddar.py:60] Depth 4/1000 time = 4.370192527770996
I0123 20:42:02.394655 140446420529152 ddar.py:60] Depth 5/1000 time = 4.310820579528809
I0123 20:42:07.609204 140446420529152 ddar.py:60] Depth 6/1000 time = 5.20250391960144
I0123 20:42:13.614920 140446420529152 ddar.py:60] Depth 7/1000 time = 6.005528926849365
I0123 20:42:19.771971 140446420529152 ddar.py:60] Depth 8/1000 time = 6.156875371932983
I0123 20:42:29.053121 140446420529152 ddar.py:60] Depth 9/1000 time = 9.280954599380493
I0123 20:42:38.389961 140446420529152 ddar.py:60] Depth 10/1000 time = 9.336611032485962
I0123 20:42:47.564474 140446420529152 ddar.py:60] Depth 11/1000 time = 9.150859117507935
I0123 20:42:56.979276 140446420529152 ddar.py:60] Depth 12/1000 time = 9.384766340255737
I0123 20:42:57.000765 140446420529152 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 20:42:57.000881 140446420529152 alphageometry.py:566] LM output (score=-1.676933): "o : D i o k o 22 ;"
I0123 20:42:57.000919 140446420529152 alphageometry.py:567] Translation: "o = on_bline o k i"

I0123 20:42:57.000961 140446420529152 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_bline o k i ? perp l n n e"
I0123 20:42:57.001148 140446420529152 graph.py:498] 
I0123 20:42:57.001200 140446420529152 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_bline o k i ? perp l n n e
I0123 20:42:59.965229 140446420529152 ddar.py:60] Depth 1/1000 time = 2.9309194087982178
I0123 20:43:03.607763 140446420529152 ddar.py:60] Depth 2/1000 time = 3.6423532962799072
I0123 20:43:07.670264 140446420529152 ddar.py:60] Depth 3/1000 time = 4.0622968673706055
I0123 20:43:11.897175 140446420529152 ddar.py:60] Depth 4/1000 time = 4.22672700881958
I0123 20:43:16.102939 140446420529152 ddar.py:60] Depth 5/1000 time = 4.205573081970215
I0123 20:43:21.006224 140446420529152 ddar.py:60] Depth 6/1000 time = 4.892387628555298
I0123 20:43:26.649848 140446420529152 ddar.py:60] Depth 7/1000 time = 5.643449068069458
I0123 20:43:32.721549 140446420529152 ddar.py:60] Depth 8/1000 time = 6.0715012550354
I0123 20:43:41.571475 140446420529152 ddar.py:60] Depth 9/1000 time = 8.849687337875366
I0123 20:43:50.765226 140446420529152 ddar.py:60] Depth 10/1000 time = 9.193463563919067
I0123 20:43:59.998334 140446420529152 ddar.py:60] Depth 11/1000 time = 9.225362777709961
I0123 20:44:09.433703 140446420529152 ddar.py:60] Depth 12/1000 time = 9.41615343093872
I0123 20:44:18.885674 140446420529152 ddar.py:60] Depth 13/1000 time = 9.42702865600586
I0123 20:44:18.907338 140446420529152 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 20:44:18.907443 140446420529152 alphageometry.py:566] LM output (score=-2.153012): "o : D f g i o 22 D f o g i 23 ;"
I0123 20:44:18.907480 140446420529152 alphageometry.py:567] Translation: "o = eqdistance o i f g, eqdistance o f g i"

I0123 20:44:18.907521 140446420529152 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = eqdistance o i f g, eqdistance o f g i ? perp l n n e"
I0123 20:44:18.907719 140446420529152 graph.py:498] 
I0123 20:44:18.907778 140446420529152 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = eqdistance o i f g, eqdistance o f g i ? perp l n n e
I0123 20:44:21.818846 140446420529152 ddar.py:60] Depth 1/1000 time = 2.870981454849243
I0123 20:44:25.540991 140446420529152 ddar.py:60] Depth 2/1000 time = 3.721961498260498
I0123 20:44:29.494935 140446420529152 ddar.py:60] Depth 3/1000 time = 3.9537713527679443
I0123 20:44:34.183139 140446420529152 ddar.py:60] Depth 4/1000 time = 4.687983751296997
I0123 20:44:38.446374 140446420529152 ddar.py:60] Depth 5/1000 time = 4.262943744659424
I0123 20:44:43.521183 140446420529152 ddar.py:60] Depth 6/1000 time = 5.063108682632446
I0123 20:44:49.513284 140446420529152 ddar.py:60] Depth 7/1000 time = 5.991927146911621
I0123 20:44:55.538155 140446420529152 ddar.py:60] Depth 8/1000 time = 6.024643659591675
I0123 20:45:04.115785 140446420529152 ddar.py:60] Depth 9/1000 time = 8.577309608459473
I0123 20:45:13.004312 140446420529152 ddar.py:60] Depth 10/1000 time = 8.888268947601318
I0123 20:45:22.160327 140446420529152 ddar.py:60] Depth 11/1000 time = 9.133493185043335
I0123 20:45:31.276740 140446420529152 ddar.py:60] Depth 12/1000 time = 9.092459917068481
I0123 20:45:31.299072 140446420529152 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 20:45:31.299180 140446420529152 alphageometry.py:566] LM output (score=-2.249321): "o : C c i o 22 D c o i o 23 ;"
I0123 20:45:31.299219 140446420529152 alphageometry.py:567] Translation: "o = on_line o c i, on_bline o i c"

I0123 20:45:31.299261 140446420529152 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_line o c i, on_bline o i c ? perp l n n e"
I0123 20:45:31.299457 140446420529152 graph.py:498] 
I0123 20:45:31.299517 140446420529152 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_line o c i, on_bline o i c ? perp l n n e
I0123 20:45:34.508803 140446420529152 ddar.py:60] Depth 1/1000 time = 3.171873092651367
I0123 20:45:37.937996 140446420529152 ddar.py:60] Depth 2/1000 time = 3.4289770126342773
I0123 20:45:41.944251 140446420529152 ddar.py:60] Depth 3/1000 time = 4.005971431732178
I0123 20:45:46.413686 140446420529152 ddar.py:60] Depth 4/1000 time = 4.469263553619385
I0123 20:45:50.421165 140446420529152 ddar.py:60] Depth 5/1000 time = 4.00730562210083
I0123 20:45:54.623330 140446420529152 ddar.py:60] Depth 6/1000 time = 4.201721906661987
I0123 20:45:59.688447 140446420529152 ddar.py:60] Depth 7/1000 time = 5.05390477180481
I0123 20:46:05.353839 140446420529152 ddar.py:60] Depth 8/1000 time = 5.665088891983032
I0123 20:46:11.126246 140446420529152 ddar.py:60] Depth 9/1000 time = 5.772221565246582
I0123 20:46:20.209262 140446420529152 ddar.py:60] Depth 10/1000 time = 9.082818746566772
I0123 20:46:29.306288 140446420529152 ddar.py:60] Depth 11/1000 time = 9.09674859046936
I0123 20:46:38.344631 140446420529152 ddar.py:60] Depth 12/1000 time = 9.01976752281189
I0123 20:46:47.423841 140446420529152 ddar.py:60] Depth 13/1000 time = 9.058313369750977
I0123 20:46:47.445724 140446420529152 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 20:46:47.445829 140446420529152 alphageometry.py:566] LM output (score=-2.281760): "o : D b o c o 22 ;"
I0123 20:46:47.445867 140446420529152 alphageometry.py:567] Translation: "o = on_bline o c b"

I0123 20:46:47.445908 140446420529152 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_bline o c b ? perp l n n e"
I0123 20:46:47.446098 140446420529152 graph.py:498] 
I0123 20:46:47.446158 140446420529152 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_bline o c b ? perp l n n e
I0123 20:46:50.426684 140446420529152 ddar.py:60] Depth 1/1000 time = 2.946596622467041
I0123 20:46:54.000762 140446420529152 ddar.py:60] Depth 2/1000 time = 3.5739052295684814
I0123 20:46:57.978290 140446420529152 ddar.py:60] Depth 3/1000 time = 3.9773459434509277
I0123 20:47:02.124620 140446420529152 ddar.py:60] Depth 4/1000 time = 4.146140098571777
I0123 20:47:06.505191 140446420529152 ddar.py:60] Depth 5/1000 time = 4.38035249710083
I0123 20:47:11.890086 140446420529152 ddar.py:60] Depth 6/1000 time = 5.373566150665283
I0123 20:47:17.405686 140446420529152 ddar.py:60] Depth 7/1000 time = 5.515415906906128
I0123 20:47:23.398963 140446420529152 ddar.py:60] Depth 8/1000 time = 5.993098974227905
I0123 20:47:32.221609 140446420529152 ddar.py:60] Depth 9/1000 time = 8.822429656982422
I0123 20:47:41.411649 140446420529152 ddar.py:60] Depth 10/1000 time = 9.189768075942993
I0123 20:47:50.696213 140446420529152 ddar.py:60] Depth 11/1000 time = 9.260919094085693
I0123 20:48:00.055457 140446420529152 ddar.py:60] Depth 12/1000 time = 9.335406303405762
I0123 20:48:00.077054 140446420529152 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 20:48:00.077162 140446420529152 alphageometry.py:566] LM output (score=-2.296424): "o : D a c a o 22 D a c c o 23 ;"
I0123 20:48:00.077198 140446420529152 alphageometry.py:567] Translation: "o = on_circle o a c, on_circle o c a"

I0123 20:48:00.077237 140446420529152 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o a c, on_circle o c a ? perp l n n e"
I0123 20:48:00.077445 140446420529152 graph.py:498] 
I0123 20:48:00.077507 140446420529152 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o a c, on_circle o c a ? perp l n n e
I0123 20:48:03.169946 140446420529152 ddar.py:60] Depth 1/1000 time = 3.047748565673828
I0123 20:48:06.908936 140446420529152 ddar.py:60] Depth 2/1000 time = 3.738816738128662
I0123 20:48:11.065547 140446420529152 ddar.py:60] Depth 3/1000 time = 4.156424522399902
I0123 20:48:15.642541 140446420529152 ddar.py:60] Depth 4/1000 time = 4.5768022537231445
I0123 20:48:20.262358 140446420529152 ddar.py:60] Depth 5/1000 time = 4.6196253299713135
I0123 20:48:24.657746 140446420529152 ddar.py:60] Depth 6/1000 time = 4.394129991531372
I0123 20:48:29.712369 140446420529152 ddar.py:60] Depth 7/1000 time = 5.04286527633667
I0123 20:48:36.026732 140446420529152 ddar.py:60] Depth 8/1000 time = 6.314068555831909
I0123 20:48:42.175647 140446420529152 ddar.py:60] Depth 9/1000 time = 6.148733139038086
I0123 20:48:51.519019 140446420529152 ddar.py:60] Depth 10/1000 time = 9.343126773834229
I0123 20:49:01.074328 140446420529152 ddar.py:60] Depth 11/1000 time = 9.554917097091675
I0123 20:49:10.416549 140446420529152 ddar.py:60] Depth 12/1000 time = 9.31956148147583
I0123 20:49:20.333274 140446420529152 ddar.py:60] Depth 13/1000 time = 9.89315915107727
I0123 20:49:20.355396 140446420529152 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 20:49:20.355496 140446420529152 alphageometry.py:566] LM output (score=-2.339439): "o : D i o j o 22 ;"
I0123 20:49:20.355533 140446420529152 alphageometry.py:567] Translation: "o = on_bline o j i"

I0123 20:49:20.355572 140446420529152 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_bline o j i ? perp l n n e"
I0123 20:49:20.355761 140446420529152 graph.py:498] 
I0123 20:49:20.355821 140446420529152 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_bline o j i ? perp l n n e
I0123 20:49:23.151710 140446420529152 ddar.py:60] Depth 1/1000 time = 2.7615199089050293
I0123 20:49:27.086227 140446420529152 ddar.py:60] Depth 2/1000 time = 3.9343442916870117
I0123 20:49:31.166764 140446420529152 ddar.py:60] Depth 3/1000 time = 4.080348968505859
I0123 20:49:35.479166 140446420529152 ddar.py:60] Depth 4/1000 time = 4.312204599380493
I0123 20:49:40.642451 140446420529152 ddar.py:60] Depth 5/1000 time = 5.151731729507446
I0123 20:49:46.639476 140446420529152 ddar.py:60] Depth 6/1000 time = 5.996837377548218
I0123 20:49:52.492919 140446420529152 ddar.py:60] Depth 7/1000 time = 5.853280067443848
I0123 20:50:01.844810 140446420529152 ddar.py:60] Depth 8/1000 time = 9.351656198501587
I0123 20:50:11.088590 140446420529152 ddar.py:60] Depth 9/1000 time = 9.24351167678833
I0123 20:50:20.399917 140446420529152 ddar.py:60] Depth 10/1000 time = 9.303878545761108
I0123 20:50:30.009234 140446420529152 ddar.py:60] Depth 11/1000 time = 9.59037733078003
I0123 20:50:39.388605 140446420529152 ddar.py:60] Depth 12/1000 time = 9.355503797531128
I0123 20:50:39.409383 140446420529152 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 20:50:39.409519 140446420529152 alphageometry.py:566] LM output (score=-2.359380): "o : D l m m o 22 T l m m o 23 ;"
I0123 20:50:39.409557 140446420529152 alphageometry.py:567] Translation: "o = on_circle o m l, on_tline o m l m"

I0123 20:50:39.409612 140446420529152 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o m l, on_tline o m l m ? perp l n n e"
I0123 20:50:39.409836 140446420529152 graph.py:498] 
I0123 20:50:39.409897 140446420529152 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o m l, on_tline o m l m ? perp l n n e
I0123 20:50:42.252770 140446420529152 ddar.py:60] Depth 1/1000 time = 2.8049473762512207
I0123 20:50:45.873462 140446420529152 ddar.py:60] Depth 2/1000 time = 3.620520830154419
I0123 20:50:50.207356 140446420529152 ddar.py:60] Depth 3/1000 time = 4.333730936050415
I0123 20:50:54.531889 140446420529152 ddar.py:60] Depth 4/1000 time = 4.324354887008667
I0123 20:50:58.895489 140446420529152 ddar.py:60] Depth 5/1000 time = 4.363361835479736
I0123 20:51:03.260590 140446420529152 ddar.py:60] Depth 6/1000 time = 4.364323616027832
I0123 20:51:08.279313 140446420529152 ddar.py:60] Depth 7/1000 time = 5.005825042724609
I0123 20:51:14.214931 140446420529152 ddar.py:60] Depth 8/1000 time = 5.935432195663452
I0123 20:51:20.430168 140446420529152 ddar.py:60] Depth 9/1000 time = 6.215041399002075
I0123 20:51:29.756537 140446420529152 ddar.py:60] Depth 10/1000 time = 9.326130151748657
I0123 20:51:38.956028 140446420529152 ddar.py:60] Depth 11/1000 time = 9.19923734664917
I0123 20:51:48.664500 140446420529152 ddar.py:60] Depth 12/1000 time = 9.68498420715332
I0123 20:51:58.000633 140446420529152 ddar.py:60] Depth 13/1000 time = 9.307278871536255
I0123 20:51:58.023049 140446420529152 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 20:51:58.023162 140446420529152 alphageometry.py:566] LM output (score=-2.425440): "o : D b c b o 22 D b c c o 23 ;"
I0123 20:51:58.023199 140446420529152 alphageometry.py:567] Translation: "o = on_circle o b c, on_circle o c b"

I0123 20:51:58.023239 140446420529152 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o b c, on_circle o c b ? perp l n n e"
I0123 20:51:58.023434 140446420529152 graph.py:498] 
I0123 20:51:58.023495 140446420529152 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o b c, on_circle o c b ? perp l n n e
I0123 20:52:01.315461 140446420529152 ddar.py:60] Depth 1/1000 time = 3.250561237335205
I0123 20:52:04.970256 140446420529152 ddar.py:60] Depth 2/1000 time = 3.6546056270599365
I0123 20:52:09.583515 140446420529152 ddar.py:60] Depth 3/1000 time = 4.613054513931274
I0123 20:52:14.131622 140446420529152 ddar.py:60] Depth 4/1000 time = 4.547925233840942
I0123 20:52:18.378859 140446420529152 ddar.py:60] Depth 5/1000 time = 4.245967864990234
I0123 20:52:23.595102 140446420529152 ddar.py:60] Depth 6/1000 time = 5.203810691833496
I0123 20:52:29.700390 140446420529152 ddar.py:60] Depth 7/1000 time = 6.105084180831909
I0123 20:52:36.047045 140446420529152 ddar.py:60] Depth 8/1000 time = 6.3464672565460205
I0123 20:52:45.281421 140446420529152 ddar.py:60] Depth 9/1000 time = 9.234174013137817
I0123 20:52:54.528666 140446420529152 ddar.py:60] Depth 10/1000 time = 9.246966361999512
I0123 20:53:03.715741 140446420529152 ddar.py:60] Depth 11/1000 time = 9.163204908370972
I0123 20:53:13.022206 140446420529152 ddar.py:60] Depth 12/1000 time = 9.282768726348877
I0123 20:53:13.042849 140446420529152 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 20:53:13.042950 140446420529152 alphageometry.py:566] LM output (score=-2.519872): "o : D a o d o 22 D c o d o 23 ;"
I0123 20:53:13.042987 140446420529152 alphageometry.py:567] Translation: "o = on_bline o d a, on_bline o d c"

I0123 20:53:13.043027 140446420529152 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_bline o d a, on_bline o d c ? perp l n n e"
I0123 20:53:13.043223 140446420529152 graph.py:498] 
I0123 20:53:13.043285 140446420529152 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_bline o d a, on_bline o d c ? perp l n n e
I0123 20:53:16.153926 140446420529152 ddar.py:60] Depth 1/1000 time = 3.069274663925171
I0123 20:53:20.086108 140446420529152 ddar.py:60] Depth 2/1000 time = 3.9320054054260254
I0123 20:53:24.439376 140446420529152 ddar.py:60] Depth 3/1000 time = 4.353092908859253
I0123 20:53:29.376402 140446420529152 ddar.py:60] Depth 4/1000 time = 4.93678879737854
I0123 20:53:33.723815 140446420529152 ddar.py:60] Depth 5/1000 time = 4.347099542617798
I0123 20:53:39.032687 140446420529152 ddar.py:60] Depth 6/1000 time = 5.297603607177734
I0123 20:53:45.376801 140446420529152 ddar.py:60] Depth 7/1000 time = 6.343939781188965
I0123 20:53:51.622211 140446420529152 ddar.py:60] Depth 8/1000 time = 6.245176315307617
I0123 20:54:01.019957 140446420529152 ddar.py:60] Depth 9/1000 time = 9.39743185043335
I0123 20:54:11.012686 140446420529152 ddar.py:60] Depth 10/1000 time = 9.992467880249023
I0123 20:54:20.833887 140446420529152 ddar.py:60] Depth 11/1000 time = 9.797041893005371
I0123 20:54:30.592788 140446420529152 ddar.py:60] Depth 12/1000 time = 9.730925559997559
I0123 20:54:30.614656 140446420529152 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 20:54:30.614766 140446420529152 alphageometry.py:566] LM output (score=-2.581655): "o : D j m m o 22 ;"
I0123 20:54:30.614803 140446420529152 alphageometry.py:567] Translation: "o = on_circle o m j"

I0123 20:54:30.614843 140446420529152 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o m j ? perp l n n e"
I0123 20:54:30.615044 140446420529152 graph.py:498] 
I0123 20:54:30.615108 140446420529152 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o m j ? perp l n n e
I0123 20:54:33.610409 140446420529152 ddar.py:60] Depth 1/1000 time = 2.9659366607666016
I0123 20:54:37.660529 140446420529152 ddar.py:60] Depth 2/1000 time = 4.049838542938232
I0123 20:54:41.754250 140446420529152 ddar.py:60] Depth 3/1000 time = 4.093541860580444
I0123 20:54:46.416849 140446420529152 ddar.py:60] Depth 4/1000 time = 4.662426948547363
I0123 20:54:51.075264 140446420529152 ddar.py:60] Depth 5/1000 time = 4.6582231521606445
I0123 20:54:56.807188 140446420529152 ddar.py:60] Depth 6/1000 time = 5.720321178436279
I0123 20:55:02.846818 140446420529152 ddar.py:60] Depth 7/1000 time = 6.039395332336426
I0123 20:55:09.191714 140446420529152 ddar.py:60] Depth 8/1000 time = 6.34471583366394
I0123 20:55:19.020652 140446420529152 ddar.py:60] Depth 9/1000 time = 9.82871699333191
I0123 20:55:28.624078 140446420529152 ddar.py:60] Depth 10/1000 time = 9.60315203666687
I0123 20:55:38.379907 140446420529152 ddar.py:60] Depth 11/1000 time = 9.732451677322388
I0123 20:55:48.432855 140446420529152 ddar.py:60] Depth 12/1000 time = 10.025180339813232
I0123 20:55:48.455507 140446420529152 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 20:55:48.455613 140446420529152 alphageometry.py:566] LM output (score=-2.698877): "o : D f g h o 22 D f o g h 23 ;"
I0123 20:55:48.455651 140446420529152 alphageometry.py:567] Translation: "o = eqdistance o h f g, eqdistance o f g h"

I0123 20:55:48.455692 140446420529152 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = eqdistance o h f g, eqdistance o f g h ? perp l n n e"
I0123 20:55:48.455884 140446420529152 graph.py:498] 
I0123 20:55:48.455946 140446420529152 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = eqdistance o h f g, eqdistance o f g h ? perp l n n e
I0123 20:55:51.889661 140446420529152 ddar.py:60] Depth 1/1000 time = 3.3917877674102783
I0123 20:55:55.703276 140446420529152 ddar.py:60] Depth 2/1000 time = 3.8134572505950928
I0123 20:56:00.322087 140446420529152 ddar.py:60] Depth 3/1000 time = 4.6186230182647705
I0123 20:56:04.769266 140446420529152 ddar.py:60] Depth 4/1000 time = 4.446988105773926
I0123 20:56:09.491357 140446420529152 ddar.py:60] Depth 5/1000 time = 4.721898078918457
I0123 20:56:14.837538 140446420529152 ddar.py:60] Depth 6/1000 time = 5.3346850872039795
I0123 20:56:20.952889 140446420529152 ddar.py:60] Depth 7/1000 time = 6.1151039600372314
I0123 20:56:27.582622 140446420529152 ddar.py:60] Depth 8/1000 time = 6.629445314407349
I0123 20:56:37.293712 140446420529152 ddar.py:60] Depth 9/1000 time = 9.710888147354126
I0123 20:56:47.111115 140446420529152 ddar.py:60] Depth 10/1000 time = 9.81711745262146
I0123 20:56:56.943067 140446420529152 ddar.py:60] Depth 11/1000 time = 9.80878210067749
I0123 20:57:06.763396 140446420529152 ddar.py:60] Depth 12/1000 time = 9.79576563835144
I0123 20:57:06.786135 140446420529152 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 20:57:06.786234 140446420529152 alphageometry.py:566] LM output (score=-2.711867): "o : D f g f o 22 D g h h o 23 ;"
I0123 20:57:06.786272 140446420529152 alphageometry.py:567] Translation: "o = on_circle o f g, on_circle o h g"

I0123 20:57:06.786313 140446420529152 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o f g, on_circle o h g ? perp l n n e"
I0123 20:57:06.786505 140446420529152 graph.py:498] 
I0123 20:57:06.786566 140446420529152 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o f g, on_circle o h g ? perp l n n e
I0123 20:57:09.887867 140446420529152 ddar.py:60] Depth 1/1000 time = 3.0608930587768555
I0123 20:57:14.030211 140446420529152 ddar.py:60] Depth 2/1000 time = 4.1421661376953125
I0123 20:57:18.292219 140446420529152 ddar.py:60] Depth 3/1000 time = 4.261831998825073
I0123 20:57:22.748939 140446420529152 ddar.py:60] Depth 4/1000 time = 4.456543207168579
I0123 20:57:27.564108 140446420529152 ddar.py:60] Depth 5/1000 time = 4.814959287643433
I0123 20:57:33.189101 140446420529152 ddar.py:60] Depth 6/1000 time = 5.611860275268555
I0123 20:57:39.477161 140446420529152 ddar.py:60] Depth 7/1000 time = 6.2878806591033936
I0123 20:57:45.928470 140446420529152 ddar.py:60] Depth 8/1000 time = 6.4510838985443115
I0123 20:57:55.192975 140446420529152 ddar.py:60] Depth 9/1000 time = 9.264177322387695
I0123 20:58:04.886104 140446420529152 ddar.py:60] Depth 10/1000 time = 9.692870855331421
I0123 20:58:14.801203 140446420529152 ddar.py:60] Depth 11/1000 time = 9.890230655670166
I0123 20:58:24.625088 140446420529152 ddar.py:60] Depth 12/1000 time = 9.794040441513062
I0123 20:58:24.648513 140446420529152 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 20:58:24.648629 140446420529152 alphageometry.py:566] LM output (score=-2.720277): "o : C b i o 22 D b o i o 23 ;"
I0123 20:58:24.648665 140446420529152 alphageometry.py:567] Translation: "o = on_line o b i, on_bline o i b"

I0123 20:58:24.648706 140446420529152 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_line o b i, on_bline o i b ? perp l n n e"
I0123 20:58:24.648898 140446420529152 graph.py:498] 
I0123 20:58:24.648959 140446420529152 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_line o b i, on_bline o i b ? perp l n n e
I0123 20:58:29.646007 140446420529152 ddar.py:60] Depth 1/1000 time = 4.962887525558472
I0123 20:58:34.703010 140446420529152 ddar.py:60] Depth 2/1000 time = 5.056804180145264
I0123 20:58:40.512067 140446420529152 ddar.py:60] Depth 3/1000 time = 5.808873176574707
I0123 20:58:46.515827 140446420529152 ddar.py:60] Depth 4/1000 time = 6.0035765171051025
I0123 20:58:52.581917 140446420529152 ddar.py:60] Depth 5/1000 time = 6.065598011016846
I0123 20:58:59.539879 140446420529152 ddar.py:60] Depth 6/1000 time = 6.947041273117065
I0123 20:59:07.152512 140446420529152 ddar.py:60] Depth 7/1000 time = 7.612448453903198
I0123 20:59:15.058954 140446420529152 ddar.py:60] Depth 8/1000 time = 7.906236410140991
I0123 20:59:26.398794 140446420529152 ddar.py:60] Depth 9/1000 time = 11.33961272239685
I0123 20:59:37.654019 140446420529152 ddar.py:60] Depth 10/1000 time = 11.25493597984314
I0123 20:59:49.274410 140446420529152 ddar.py:60] Depth 11/1000 time = 11.601489782333374
I0123 21:00:01.311419 140446420529152 ddar.py:60] Depth 12/1000 time = 12.014325618743896
I0123 21:00:01.331641 140446420529152 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 21:00:01.331746 140446420529152 alphageometry.py:566] LM output (score=-2.723216): "o : D b o c o 22 D b o f o 23 ;"
I0123 21:00:01.331781 140446420529152 alphageometry.py:567] Translation: "o = on_bline o c b, on_bline o f b"

I0123 21:00:01.331822 140446420529152 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_bline o c b, on_bline o f b ? perp l n n e"
I0123 21:00:01.332014 140446420529152 graph.py:498] 
I0123 21:00:01.332075 140446420529152 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_bline o c b, on_bline o f b ? perp l n n e
I0123 21:00:04.628424 140446420529152 ddar.py:60] Depth 1/1000 time = 3.253756284713745
I0123 21:00:08.570398 140446420529152 ddar.py:60] Depth 2/1000 time = 3.9417951107025146
I0123 21:00:12.856167 140446420529152 ddar.py:60] Depth 3/1000 time = 4.2855916023254395
I0123 21:00:17.784939 140446420529152 ddar.py:60] Depth 4/1000 time = 4.928564786911011
I0123 21:00:24.961509 140446420529152 ddar.py:60] Depth 5/1000 time = 7.171447038650513
I0123 21:00:31.585565 140446420529152 ddar.py:60] Depth 6/1000 time = 6.623864650726318
I0123 21:00:39.193594 140446420529152 ddar.py:60] Depth 7/1000 time = 7.60777735710144
I0123 21:00:45.993587 140446420529152 ddar.py:60] Depth 8/1000 time = 6.7996532917022705
I0123 21:00:54.869173 140446420529152 ddar.py:60] Depth 9/1000 time = 8.857276439666748
I0123 21:01:04.313702 140446420529152 ddar.py:60] Depth 10/1000 time = 9.444310188293457
I0123 21:01:13.575486 140446420529152 ddar.py:60] Depth 11/1000 time = 9.26157522201538
I0123 21:01:26.020211 140446420529152 ddar.py:60] Depth 12/1000 time = 12.444479703903198
I0123 21:01:38.426738 140446420529152 ddar.py:60] Depth 13/1000 time = 12.406232118606567
I0123 21:01:51.677295 140446420529152 ddar.py:60] Depth 14/1000 time = 13.219235181808472
I0123 21:02:03.863956 140446420529152 ddar.py:60] Depth 15/1000 time = 12.157726049423218
I0123 21:02:03.887491 140446420529152 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 21:02:03.887628 140446420529152 alphageometry.py:566] LM output (score=-2.723280): "o : D a o c o 22 ;"
I0123 21:02:03.887664 140446420529152 alphageometry.py:567] Translation: "o = on_bline o c a"

I0123 21:02:03.887712 140446420529152 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_bline o c a ? perp l n n e"
I0123 21:02:03.887930 140446420529152 graph.py:498] 
I0123 21:02:03.887989 140446420529152 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_bline o c a ? perp l n n e
I0123 21:02:07.096158 140446420529152 ddar.py:60] Depth 1/1000 time = 3.1744799613952637
I0123 21:02:10.838854 140446420529152 ddar.py:60] Depth 2/1000 time = 3.7425169944763184
I0123 21:02:15.310263 140446420529152 ddar.py:60] Depth 3/1000 time = 4.471247434616089
I0123 21:02:19.636537 140446420529152 ddar.py:60] Depth 4/1000 time = 4.326109409332275
I0123 21:02:24.367157 140446420529152 ddar.py:60] Depth 5/1000 time = 4.730376720428467
I0123 21:02:29.473850 140446420529152 ddar.py:60] Depth 6/1000 time = 5.095009088516235
I0123 21:02:35.121328 140446420529152 ddar.py:60] Depth 7/1000 time = 5.647278547286987
I0123 21:02:41.700545 140446420529152 ddar.py:60] Depth 8/1000 time = 6.579034805297852
I0123 21:02:50.874585 140446420529152 ddar.py:60] Depth 9/1000 time = 9.173801898956299
I0123 21:03:00.296748 140446420529152 ddar.py:60] Depth 10/1000 time = 9.421870231628418
I0123 21:03:09.837687 140446420529152 ddar.py:60] Depth 11/1000 time = 9.51723861694336
I0123 21:03:19.739156 140446420529152 ddar.py:60] Depth 12/1000 time = 9.876914024353027
I0123 21:03:19.762206 140446420529152 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 21:03:19.762330 140446420529152 alphageometry.py:566] LM output (score=-2.742906): "o : D f o h o 22 ;"
I0123 21:03:19.762367 140446420529152 alphageometry.py:567] Translation: "o = on_bline o h f"

I0123 21:03:19.762408 140446420529152 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_bline o h f ? perp l n n e"
I0123 21:03:19.762597 140446420529152 graph.py:498] 
I0123 21:03:19.762658 140446420529152 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_bline o h f ? perp l n n e
I0123 21:03:22.592007 140446420529152 ddar.py:60] Depth 1/1000 time = 2.796292304992676
I0123 21:03:26.224349 140446420529152 ddar.py:60] Depth 2/1000 time = 3.6321678161621094
I0123 21:03:30.244887 140446420529152 ddar.py:60] Depth 3/1000 time = 4.020361423492432
I0123 21:03:34.381006 140446420529152 ddar.py:60] Depth 4/1000 time = 4.135930776596069
I0123 21:03:38.904111 140446420529152 ddar.py:60] Depth 5/1000 time = 4.5229058265686035
I0123 21:03:43.667158 140446420529152 ddar.py:60] Depth 6/1000 time = 4.75146746635437
I0123 21:03:49.449067 140446420529152 ddar.py:60] Depth 7/1000 time = 5.781723260879517
I0123 21:03:54.967761 140446420529152 ddar.py:60] Depth 8/1000 time = 5.51850700378418
I0123 21:04:03.467713 140446420529152 ddar.py:60] Depth 9/1000 time = 8.49974250793457
I0123 21:04:12.685678 140446420529152 ddar.py:60] Depth 10/1000 time = 9.217649698257446
I0123 21:04:21.583421 140446420529152 ddar.py:60] Depth 11/1000 time = 8.879424571990967
I0123 21:04:30.523477 140446420529152 ddar.py:60] Depth 12/1000 time = 8.918735027313232
I0123 21:04:30.545191 140446420529152 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 21:04:30.545305 140446420529152 alphageometry.py:566] LM output (score=-2.839750): "o : D j o k o 22 ;"
I0123 21:04:30.545355 140446420529152 alphageometry.py:567] Translation: "o = on_bline o k j"

I0123 21:04:30.545396 140446420529152 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_bline o k j ? perp l n n e"
I0123 21:04:30.545597 140446420529152 graph.py:498] 
I0123 21:04:30.545662 140446420529152 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_bline o k j ? perp l n n e
I0123 21:04:33.870305 140446420529152 ddar.py:60] Depth 1/1000 time = 3.284579038619995
I0123 21:04:37.669349 140446420529152 ddar.py:60] Depth 2/1000 time = 3.7988548278808594
I0123 21:04:41.953448 140446420529152 ddar.py:60] Depth 3/1000 time = 4.283933877944946
I0123 21:04:46.397311 140446420529152 ddar.py:60] Depth 4/1000 time = 4.443678617477417
I0123 21:04:52.142298 140446420529152 ddar.py:60] Depth 5/1000 time = 5.730875730514526
I0123 21:04:58.058740 140446420529152 ddar.py:60] Depth 6/1000 time = 5.916221857070923
I0123 21:05:06.987441 140446420529152 ddar.py:60] Depth 7/1000 time = 8.92847728729248
I0123 21:05:15.908254 140446420529152 ddar.py:60] Depth 8/1000 time = 8.920517206192017
I0123 21:05:25.392874 140446420529152 ddar.py:60] Depth 9/1000 time = 9.459231853485107
I0123 21:05:35.239681 140446420529152 ddar.py:60] Depth 10/1000 time = 9.814631462097168
I0123 21:05:35.261469 140446420529152 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 21:05:35.261589 140446420529152 alphageometry.py:566] LM output (score=-2.868104): "o : D b d d o 22 ;"
I0123 21:05:35.261627 140446420529152 alphageometry.py:567] Translation: "o = on_circle o d b"

I0123 21:05:35.261674 140446420529152 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o d b ? perp l n n e"
I0123 21:05:35.261873 140446420529152 graph.py:498] 
I0123 21:05:35.261931 140446420529152 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o d b ? perp l n n e
I0123 21:05:38.643660 140446420529152 ddar.py:60] Depth 1/1000 time = 3.352735996246338
I0123 21:05:42.248097 140446420529152 ddar.py:60] Depth 2/1000 time = 3.604252576828003
I0123 21:05:47.009633 140446420529152 ddar.py:60] Depth 3/1000 time = 4.7613489627838135
I0123 21:05:51.209289 140446420529152 ddar.py:60] Depth 4/1000 time = 4.1994712352752686
I0123 21:05:56.879345 140446420529152 ddar.py:60] Depth 5/1000 time = 5.6582746505737305
I0123 21:06:02.877879 140446420529152 ddar.py:60] Depth 6/1000 time = 5.998321294784546
I0123 21:06:09.402074 140446420529152 ddar.py:60] Depth 7/1000 time = 6.523982286453247
I0123 21:06:18.671812 140446420529152 ddar.py:60] Depth 8/1000 time = 9.269538640975952
I0123 21:06:28.270054 140446420529152 ddar.py:60] Depth 9/1000 time = 9.597978115081787
I0123 21:06:38.022776 140446420529152 ddar.py:60] Depth 10/1000 time = 9.728843450546265
I0123 21:06:47.874332 140446420529152 ddar.py:60] Depth 11/1000 time = 9.824673414230347
I0123 21:06:47.895550 140446420529152 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 21:06:47.895647 140446420529152 alphageometry.py:566] LM output (score=-2.899803): "o : D f o h o 22 D h o l o 23 ;"
I0123 21:06:47.895685 140446420529152 alphageometry.py:567] Translation: "o = on_bline o h f, on_bline o l h"

I0123 21:06:47.895726 140446420529152 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_bline o h f, on_bline o l h ? perp l n n e"
I0123 21:06:47.895915 140446420529152 graph.py:498] 
I0123 21:06:47.895975 140446420529152 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_bline o h f, on_bline o l h ? perp l n n e
I0123 21:06:51.373148 140446420529152 ddar.py:60] Depth 1/1000 time = 3.437460422515869
I0123 21:06:55.079856 140446420529152 ddar.py:60] Depth 2/1000 time = 3.7065229415893555
I0123 21:06:59.352193 140446420529152 ddar.py:60] Depth 3/1000 time = 4.272159576416016
I0123 21:07:04.331535 140446420529152 ddar.py:60] Depth 4/1000 time = 4.979143142700195
I0123 21:07:09.528253 140446420529152 ddar.py:60] Depth 5/1000 time = 5.184737682342529
I0123 21:07:15.347238 140446420529152 ddar.py:60] Depth 6/1000 time = 5.818779706954956
I0123 21:07:21.346384 140446420529152 ddar.py:60] Depth 7/1000 time = 5.998965740203857
I0123 21:07:30.633135 140446420529152 ddar.py:60] Depth 8/1000 time = 9.286526679992676
I0123 21:07:40.245434 140446420529152 ddar.py:60] Depth 9/1000 time = 9.612029075622559
I0123 21:07:49.753419 140446420529152 ddar.py:60] Depth 10/1000 time = 9.487473249435425
I0123 21:07:59.253096 140446420529152 ddar.py:60] Depth 11/1000 time = 9.474563121795654
I0123 21:07:59.274362 140446420529152 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 21:07:59.274472 140446420529152 alphageometry.py:566] LM output (score=-2.905009): "o : D d o i o 22 D g o i o 23 ;"
I0123 21:07:59.274508 140446420529152 alphageometry.py:567] Translation: "o = on_bline o i d, on_bline o i g"

I0123 21:07:59.274546 140446420529152 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_bline o i d, on_bline o i g ? perp l n n e"
I0123 21:07:59.274736 140446420529152 graph.py:498] 
I0123 21:07:59.274793 140446420529152 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_bline o i d, on_bline o i g ? perp l n n e
I0123 21:08:02.421855 140446420529152 ddar.py:60] Depth 1/1000 time = 3.1025073528289795
I0123 21:08:06.484743 140446420529152 ddar.py:60] Depth 2/1000 time = 4.062727689743042
I0123 21:08:10.965335 140446420529152 ddar.py:60] Depth 3/1000 time = 4.480411052703857
I0123 21:08:15.175491 140446420529152 ddar.py:60] Depth 4/1000 time = 4.209968090057373
I0123 21:08:19.758708 140446420529152 ddar.py:60] Depth 5/1000 time = 4.583014011383057
I0123 21:08:25.429073 140446420529152 ddar.py:60] Depth 6/1000 time = 5.658999919891357
I0123 21:08:31.352383 140446420529152 ddar.py:60] Depth 7/1000 time = 5.923099040985107
I0123 21:08:37.394047 140446420529152 ddar.py:60] Depth 8/1000 time = 6.041455507278442
I0123 21:08:46.474689 140446420529152 ddar.py:60] Depth 9/1000 time = 9.080440044403076
I0123 21:08:55.968865 140446420529152 ddar.py:60] Depth 10/1000 time = 9.493896484375
I0123 21:09:05.401258 140446420529152 ddar.py:60] Depth 11/1000 time = 9.424458503723145
I0123 21:09:15.157240 140446420529152 ddar.py:60] Depth 12/1000 time = 9.73635745048523
I0123 21:09:24.849965 140446420529152 ddar.py:60] Depth 13/1000 time = 9.66829538345337
I0123 21:09:24.873365 140446420529152 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 21:09:24.873477 140446420529152 alphageometry.py:566] LM output (score=-2.906250): "o : D f g f o 22 T f g f o 23 ;"
I0123 21:09:24.873513 140446420529152 alphageometry.py:567] Translation: "o = on_circle o f g, on_tline o f f g"

I0123 21:09:24.873553 140446420529152 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o f g, on_tline o f f g ? perp l n n e"
I0123 21:09:24.873753 140446420529152 graph.py:498] 
I0123 21:09:24.873816 140446420529152 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o f g, on_tline o f f g ? perp l n n e
I0123 21:09:27.843764 140446420529152 ddar.py:60] Depth 1/1000 time = 2.9276366233825684
I0123 21:09:31.820779 140446420529152 ddar.py:60] Depth 2/1000 time = 3.9768426418304443
I0123 21:09:36.258169 140446420529152 ddar.py:60] Depth 3/1000 time = 4.437211275100708
I0123 21:09:40.905210 140446420529152 ddar.py:60] Depth 4/1000 time = 4.646842956542969
I0123 21:09:45.081492 140446420529152 ddar.py:60] Depth 5/1000 time = 4.176085948944092
I0123 21:09:49.659911 140446420529152 ddar.py:60] Depth 6/1000 time = 4.577773094177246
I0123 21:09:54.974496 140446420529152 ddar.py:60] Depth 7/1000 time = 5.3010571002960205
I0123 21:10:00.863155 140446420529152 ddar.py:60] Depth 8/1000 time = 5.888352394104004
I0123 21:10:07.390216 140446420529152 ddar.py:60] Depth 9/1000 time = 6.526868104934692
I0123 21:10:16.849930 140446420529152 ddar.py:60] Depth 10/1000 time = 9.459493637084961
I0123 21:10:26.608621 140446420529152 ddar.py:60] Depth 11/1000 time = 9.758413791656494
I0123 21:10:35.880792 140446420529152 ddar.py:60] Depth 12/1000 time = 9.247978210449219
I0123 21:10:45.649056 140446420529152 ddar.py:60] Depth 13/1000 time = 9.737297773361206
I0123 21:10:45.672085 140446420529152 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 21:10:45.672186 140446420529152 alphageometry.py:566] LM output (score=-2.948152): "o : D b d d o 22 T b d d o 23 ;"
I0123 21:10:45.672223 140446420529152 alphageometry.py:567] Translation: "o = on_circle o d b, on_tline o d b d"

I0123 21:10:45.672265 140446420529152 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o d b, on_tline o d b d ? perp l n n e"
I0123 21:10:45.672471 140446420529152 graph.py:498] 
I0123 21:10:45.672533 140446420529152 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o d b, on_tline o d b d ? perp l n n e
I0123 21:10:49.167808 140446420529152 ddar.py:60] Depth 1/1000 time = 3.4579274654388428
I0123 21:10:52.985025 140446420529152 ddar.py:60] Depth 2/1000 time = 3.8170411586761475
I0123 21:10:57.702964 140446420529152 ddar.py:60] Depth 3/1000 time = 4.717760801315308
I0123 21:11:02.683471 140446420529152 ddar.py:60] Depth 4/1000 time = 4.980309963226318
I0123 21:11:07.615058 140446420529152 ddar.py:60] Depth 5/1000 time = 4.931373357772827
I0123 21:11:12.155520 140446420529152 ddar.py:60] Depth 6/1000 time = 4.5394206047058105
I0123 21:11:17.283559 140446420529152 ddar.py:60] Depth 7/1000 time = 5.127856969833374
I0123 21:11:22.557203 140446420529152 ddar.py:60] Depth 8/1000 time = 5.273423671722412
I0123 21:11:28.748409 140446420529152 ddar.py:60] Depth 9/1000 time = 6.178216457366943
I0123 21:11:35.847431 140446420529152 ddar.py:60] Depth 10/1000 time = 7.098830223083496
I0123 21:11:42.721961 140446420529152 ddar.py:60] Depth 11/1000 time = 6.874299764633179
I0123 21:11:52.609031 140446420529152 ddar.py:60] Depth 12/1000 time = 9.886757612228394
I0123 21:12:02.447145 140446420529152 ddar.py:60] Depth 13/1000 time = 9.837863683700562
I0123 21:12:13.300928 140446420529152 ddar.py:60] Depth 14/1000 time = 10.820558071136475
I0123 21:12:23.263479 140446420529152 ddar.py:60] Depth 15/1000 time = 9.928393125534058
I0123 21:12:23.287139 140446420529152 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 21:12:23.287257 140446420529152 alphageometry.py:566] LM output (score=-2.975823): "o : D d o i o 22 D f o i o 23 ;"
I0123 21:12:23.287294 140446420529152 alphageometry.py:567] Translation: "o = on_bline o i d, on_bline o i f"

I0123 21:12:23.287337 140446420529152 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_bline o i d, on_bline o i f ? perp l n n e"
I0123 21:12:23.287533 140446420529152 graph.py:498] 
I0123 21:12:23.287594 140446420529152 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_bline o i d, on_bline o i f ? perp l n n e
I0123 21:12:27.014334 140446420529152 ddar.py:60] Depth 1/1000 time = 3.6874005794525146
I0123 21:12:30.770691 140446420529152 ddar.py:60] Depth 2/1000 time = 3.7561540603637695
I0123 21:12:35.367848 140446420529152 ddar.py:60] Depth 3/1000 time = 4.596988916397095
I0123 21:12:39.729100 140446420529152 ddar.py:60] Depth 4/1000 time = 4.361080169677734
I0123 21:12:44.551298 140446420529152 ddar.py:60] Depth 5/1000 time = 4.822019338607788
I0123 21:12:50.088605 140446420529152 ddar.py:60] Depth 6/1000 time = 5.525554418563843
I0123 21:12:56.345942 140446420529152 ddar.py:60] Depth 7/1000 time = 6.257122993469238
I0123 21:13:02.770911 140446420529152 ddar.py:60] Depth 8/1000 time = 6.424788475036621
I0123 21:13:11.981433 140446420529152 ddar.py:60] Depth 9/1000 time = 9.210284233093262
I0123 21:13:21.939063 140446420529152 ddar.py:60] Depth 10/1000 time = 9.95730996131897
I0123 21:13:32.092080 140446420529152 ddar.py:60] Depth 11/1000 time = 10.132967710494995
I0123 21:13:41.568944 140446420529152 ddar.py:60] Depth 12/1000 time = 9.452528715133667
I0123 21:13:41.591990 140446420529152 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 21:13:41.592094 140446420529152 alphageometry.py:566] LM output (score=-3.002385): "o : D f i i o 22 D f j j o 23 ;"
I0123 21:13:41.592130 140446420529152 alphageometry.py:567] Translation: "o = on_circle o i f, on_circle o j f"

I0123 21:13:41.592170 140446420529152 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o i f, on_circle o j f ? perp l n n e"
I0123 21:13:41.592359 140446420529152 graph.py:498] 
I0123 21:13:41.592420 140446420529152 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o i f, on_circle o j f ? perp l n n e
I0123 21:13:44.749674 140446420529152 ddar.py:60] Depth 1/1000 time = 3.1171913146972656
I0123 21:13:49.174042 140446420529152 ddar.py:60] Depth 2/1000 time = 4.4241907596588135
I0123 21:13:54.659001 140446420529152 ddar.py:60] Depth 3/1000 time = 5.484762668609619
I0123 21:13:59.946375 140446420529152 ddar.py:60] Depth 4/1000 time = 5.287177562713623
I0123 21:14:06.344262 140446420529152 ddar.py:60] Depth 5/1000 time = 6.397698402404785
I0123 21:14:13.766583 140446420529152 ddar.py:60] Depth 6/1000 time = 7.409407138824463
I0123 21:14:21.636885 140446420529152 ddar.py:60] Depth 7/1000 time = 7.869978427886963
I0123 21:14:30.177112 140446420529152 ddar.py:60] Depth 8/1000 time = 8.540040254592896
I0123 21:14:42.265414 140446420529152 ddar.py:60] Depth 9/1000 time = 12.088073253631592
I0123 21:14:54.323571 140446420529152 ddar.py:60] Depth 10/1000 time = 12.057906866073608
I0123 21:15:06.604254 140446420529152 ddar.py:60] Depth 11/1000 time = 12.245015621185303
I0123 21:15:18.912321 140446420529152 ddar.py:60] Depth 12/1000 time = 12.254004955291748
I0123 21:15:18.936433 140446420529152 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 21:15:18.936573 140446420529152 alphageometry.py:566] LM output (score=-3.007937): "o : D a o d o 22 D b o d o 23 ;"
I0123 21:15:18.936611 140446420529152 alphageometry.py:567] Translation: "o = on_bline o d a, on_bline o d b"

I0123 21:15:18.936667 140446420529152 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_bline o d a, on_bline o d b ? perp l n n e"
I0123 21:15:18.936885 140446420529152 graph.py:498] 
I0123 21:15:18.936945 140446420529152 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_bline o d a, on_bline o d b ? perp l n n e
I0123 21:15:22.305981 140446420529152 ddar.py:60] Depth 1/1000 time = 3.3248767852783203
I0123 21:15:26.073352 140446420529152 ddar.py:60] Depth 2/1000 time = 3.7672011852264404
I0123 21:15:30.804271 140446420529152 ddar.py:60] Depth 3/1000 time = 4.730746507644653
I0123 21:15:35.270778 140446420529152 ddar.py:60] Depth 4/1000 time = 4.466317653656006
I0123 21:15:40.253911 140446420529152 ddar.py:60] Depth 5/1000 time = 4.982898950576782
I0123 21:15:45.492295 140446420529152 ddar.py:60] Depth 6/1000 time = 5.225511074066162
I0123 21:15:51.902586 140446420529152 ddar.py:60] Depth 7/1000 time = 6.410112142562866
I0123 21:15:59.071502 140446420529152 ddar.py:60] Depth 8/1000 time = 7.168680667877197
I0123 21:16:08.980125 140446420529152 ddar.py:60] Depth 9/1000 time = 9.908304214477539
I0123 21:16:18.730315 140446420529152 ddar.py:60] Depth 10/1000 time = 9.749948740005493
I0123 21:16:28.579061 140446420529152 ddar.py:60] Depth 11/1000 time = 9.821261405944824
I0123 21:16:38.338633 140446420529152 ddar.py:60] Depth 12/1000 time = 9.732845544815063
I0123 21:16:38.360500 140446420529152 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 21:16:38.360610 140446420529152 alphageometry.py:566] LM output (score=-3.066371): "o : D c o i o 22 D f o i o 23 ;"
I0123 21:16:38.360647 140446420529152 alphageometry.py:567] Translation: "o = on_bline o i c, on_bline o i f"

I0123 21:16:38.360686 140446420529152 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_bline o i c, on_bline o i f ? perp l n n e"
I0123 21:16:38.360873 140446420529152 graph.py:498] 
I0123 21:16:38.360932 140446420529152 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_bline o i c, on_bline o i f ? perp l n n e
I0123 21:16:42.241842 140446420529152 ddar.py:60] Depth 1/1000 time = 3.8390419483184814
I0123 21:16:46.121616 140446420529152 ddar.py:60] Depth 2/1000 time = 3.8795828819274902
I0123 21:16:50.284950 140446420529152 ddar.py:60] Depth 3/1000 time = 4.163125991821289
I0123 21:16:55.129733 140446420529152 ddar.py:60] Depth 4/1000 time = 4.844602108001709
I0123 21:16:59.496346 140446420529152 ddar.py:60] Depth 5/1000 time = 4.366406679153442
I0123 21:17:05.012489 140446420529152 ddar.py:60] Depth 6/1000 time = 5.50499701499939
I0123 21:17:10.677886 140446420529152 ddar.py:60] Depth 7/1000 time = 5.665097951889038
I0123 21:17:16.949563 140446420529152 ddar.py:60] Depth 8/1000 time = 6.271504878997803
I0123 21:17:26.446833 140446420529152 ddar.py:60] Depth 9/1000 time = 9.496983528137207
I0123 21:17:36.411605 140446420529152 ddar.py:60] Depth 10/1000 time = 9.964366436004639
I0123 21:17:46.207407 140446420529152 ddar.py:60] Depth 11/1000 time = 9.772645950317383
I0123 21:17:56.068954 140446420529152 ddar.py:60] Depth 12/1000 time = 9.86118197441101
I0123 21:18:06.379999 140446420529152 ddar.py:60] Depth 13/1000 time = 10.310734987258911
I0123 21:18:16.236569 140446420529152 ddar.py:60] Depth 14/1000 time = 9.817062377929688
I0123 21:18:16.262037 140446420529152 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 21:18:16.262140 140446420529152 alphageometry.py:566] LM output (score=-3.090465): "o : D b o c o 22 D b o d o 23 ;"
I0123 21:18:16.262177 140446420529152 alphageometry.py:567] Translation: "o = on_bline o c b, on_bline o d b"

I0123 21:18:16.262218 140446420529152 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_bline o c b, on_bline o d b ? perp l n n e"
I0123 21:18:16.262423 140446420529152 graph.py:498] 
I0123 21:18:16.262485 140446420529152 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_bline o c b, on_bline o d b ? perp l n n e
I0123 21:18:19.546794 140446420529152 ddar.py:60] Depth 1/1000 time = 3.2448856830596924
I0123 21:18:23.424786 140446420529152 ddar.py:60] Depth 2/1000 time = 3.8778138160705566
I0123 21:18:28.177751 140446420529152 ddar.py:60] Depth 3/1000 time = 4.752742528915405
I0123 21:18:32.656491 140446420529152 ddar.py:60] Depth 4/1000 time = 4.478443384170532
I0123 21:18:38.231922 140446420529152 ddar.py:60] Depth 5/1000 time = 5.563441753387451
I0123 21:18:44.561839 140446420529152 ddar.py:60] Depth 6/1000 time = 6.329713821411133
I0123 21:18:51.630161 140446420529152 ddar.py:60] Depth 7/1000 time = 7.068121671676636
I0123 21:19:01.539191 140446420529152 ddar.py:60] Depth 8/1000 time = 9.908800840377808
I0123 21:19:11.421490 140446420529152 ddar.py:60] Depth 9/1000 time = 9.881959915161133
I0123 21:19:21.322588 140446420529152 ddar.py:60] Depth 10/1000 time = 9.875659704208374
I0123 21:19:31.728409 140446420529152 ddar.py:60] Depth 11/1000 time = 10.377079010009766
I0123 21:19:31.748909 140446420529152 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 21:19:31.749017 140446420529152 alphageometry.py:566] LM output (score=-3.107044): "o : D b o i o 22 D c o i o 23 ;"
I0123 21:19:31.749054 140446420529152 alphageometry.py:567] Translation: "o = on_bline o i b, on_bline o i c"

I0123 21:19:31.749097 140446420529152 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_bline o i b, on_bline o i c ? perp l n n e"
I0123 21:19:31.749297 140446420529152 graph.py:498] 
I0123 21:19:31.749359 140446420529152 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_bline o i b, on_bline o i c ? perp l n n e
I0123 21:19:35.194509 140446420529152 ddar.py:60] Depth 1/1000 time = 3.4043185710906982
I0123 21:19:39.275761 140446420529152 ddar.py:60] Depth 2/1000 time = 4.081082344055176
I0123 21:19:44.113837 140446420529152 ddar.py:60] Depth 3/1000 time = 4.837899208068848
I0123 21:19:48.737343 140446420529152 ddar.py:60] Depth 4/1000 time = 4.623340606689453
I0123 21:19:53.294703 140446420529152 ddar.py:60] Depth 5/1000 time = 4.557146787643433
I0123 21:20:01.248905 140446420529152 ddar.py:60] Depth 6/1000 time = 7.936378240585327
I0123 21:20:11.535485 140446420529152 ddar.py:60] Depth 7/1000 time = 10.286340475082397
I0123 21:20:23.364162 140446420529152 ddar.py:60] Depth 8/1000 time = 11.828370094299316
I0123 21:20:39.651400 140446420529152 ddar.py:60] Depth 9/1000 time = 16.286989450454712
I0123 21:20:55.860995 140446420529152 ddar.py:60] Depth 10/1000 time = 16.209289073944092
I0123 21:21:12.232862 140446420529152 ddar.py:60] Depth 11/1000 time = 16.263245344161987
I0123 21:21:12.257809 140446420529152 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 21:21:12.257964 140446420529152 alphageometry.py:566] LM output (score=-3.133842): "o : D f o h o 22 D g o h o 23 ;"
I0123 21:21:12.258014 140446420529152 alphageometry.py:567] Translation: "o = on_bline o h f, on_bline o h g"

I0123 21:21:12.258058 140446420529152 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_bline o h f, on_bline o h g ? perp l n n e"
I0123 21:21:12.258271 140446420529152 graph.py:498] 
I0123 21:21:12.258331 140446420529152 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_bline o h f, on_bline o h g ? perp l n n e
I0123 21:21:15.247333 140446420529152 ddar.py:60] Depth 1/1000 time = 2.950063705444336
I0123 21:21:19.674265 140446420529152 ddar.py:60] Depth 2/1000 time = 4.426758766174316
I0123 21:21:23.990383 140446420529152 ddar.py:60] Depth 3/1000 time = 4.315953969955444
I0123 21:21:28.480083 140446420529152 ddar.py:60] Depth 4/1000 time = 4.4895172119140625
I0123 21:21:32.994198 140446420529152 ddar.py:60] Depth 5/1000 time = 4.51392388343811
I0123 21:21:38.120158 140446420529152 ddar.py:60] Depth 6/1000 time = 5.1144819259643555
I0123 21:21:44.291872 140446420529152 ddar.py:60] Depth 7/1000 time = 6.171492099761963
I0123 21:21:50.715434 140446420529152 ddar.py:60] Depth 8/1000 time = 6.4233808517456055
I0123 21:22:00.024082 140446420529152 ddar.py:60] Depth 9/1000 time = 9.308424234390259
I0123 21:22:09.663815 140446420529152 ddar.py:60] Depth 10/1000 time = 9.639473676681519
I0123 21:22:19.350291 140446420529152 ddar.py:60] Depth 11/1000 time = 9.67980670928955
I0123 21:22:29.213345 140446420529152 ddar.py:60] Depth 12/1000 time = 9.845776796340942
I0123 21:22:39.134199 140446420529152 ddar.py:60] Depth 13/1000 time = 9.920517921447754
I0123 21:22:49.483108 140446420529152 ddar.py:60] Depth 14/1000 time = 10.320618152618408
I0123 21:22:49.507577 140446420529152 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 21:22:49.507696 140446420529152 alphageometry.py:566] LM output (score=-3.151493): "o : D i o k o 22 P b e b o 23 ;"
I0123 21:22:49.507733 140446420529152 alphageometry.py:567] Translation: "ERROR: Invalid predicate P b e b o"

I0123 21:22:49.507770 140446420529152 alphageometry.py:566] LM output (score=-3.155298): "o : D f i f o 22 D h i h o 23 ;"
I0123 21:22:49.507796 140446420529152 alphageometry.py:567] Translation: "o = on_circle o f i, on_circle o h i"

I0123 21:22:49.507827 140446420529152 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o f i, on_circle o h i ? perp l n n e"
I0123 21:22:49.508021 140446420529152 graph.py:498] 
I0123 21:22:49.508081 140446420529152 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o f i, on_circle o h i ? perp l n n e
I0123 21:22:52.877427 140446420529152 ddar.py:60] Depth 1/1000 time = 3.3282406330108643
I0123 21:22:56.957137 140446420529152 ddar.py:60] Depth 2/1000 time = 4.0795488357543945
I0123 21:23:01.524523 140446420529152 ddar.py:60] Depth 3/1000 time = 4.567185878753662
I0123 21:23:06.262350 140446420529152 ddar.py:60] Depth 4/1000 time = 4.737651109695435
I0123 21:23:10.979418 140446420529152 ddar.py:60] Depth 5/1000 time = 4.716869354248047
I0123 21:23:16.417837 140446420529152 ddar.py:60] Depth 6/1000 time = 5.424792528152466
I0123 21:23:23.174556 140446420529152 ddar.py:60] Depth 7/1000 time = 6.756515026092529
I0123 21:23:30.025025 140446420529152 ddar.py:60] Depth 8/1000 time = 6.850276947021484
I0123 21:23:40.103342 140446420529152 ddar.py:60] Depth 9/1000 time = 10.07810926437378
I0123 21:23:50.037626 140446420529152 ddar.py:60] Depth 10/1000 time = 9.9340078830719
I0123 21:24:00.189963 140446420529152 ddar.py:60] Depth 11/1000 time = 10.128289461135864
I0123 21:24:10.371537 140446420529152 ddar.py:60] Depth 12/1000 time = 10.153986692428589
I0123 21:24:10.393312 140446420529152 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 21:24:10.393430 140446420529152 alphageometry.py:540] Depth 1. There are 31 nodes to expand:
I0123 21:24:10.393471 140446420529152 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a c e 02 ^ b a b e b e b c 03 ; f : C a c f 04 T a c b f 05 ; g : C b c g 06 ^ a b a g a g a c 07 ; h : C b c h 08 T a h b c 09 ; i : C a g i 10 C b e i 11 ; j : C g i j 12 T b i b j 13 ; k : C e i k 14 T a i a k 15 ; l : C f j l 16 C h k l 17 ; m : D i m j m 18 D j m k m 19 ; n : C d m n 20 C e g n 21 ? T l n n e {F1} x00 o : D k m m o 22 ; x00
I0123 21:24:10.393504 140446420529152 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a c e 02 ^ b a b e b e b c 03 ; f : C a c f 04 T a c b f 05 ; g : C b c g 06 ^ a b a g a g a c 07 ; h : C b c h 08 T a h b c 09 ; i : C a g i 10 C b e i 11 ; j : C g i j 12 T b i b j 13 ; k : C e i k 14 T a i a k 15 ; l : C f j l 16 C h k l 17 ; m : D i m j m 18 D j m k m 19 ; n : C d m n 20 C e g n 21 ? T l n n e {F1} x00 o : D i o k o 22 ; x00
I0123 21:24:10.393532 140446420529152 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a c e 02 ^ b a b e b e b c 03 ; f : C a c f 04 T a c b f 05 ; g : C b c g 06 ^ a b a g a g a c 07 ; h : C b c h 08 T a h b c 09 ; i : C a g i 10 C b e i 11 ; j : C g i j 12 T b i b j 13 ; k : C e i k 14 T a i a k 15 ; l : C f j l 16 C h k l 17 ; m : D i m j m 18 D j m k m 19 ; n : C d m n 20 C e g n 21 ? T l n n e {F1} x00 o : D f g i o 22 D f o g i 23 ; x00
I0123 21:24:10.393559 140446420529152 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a c e 02 ^ b a b e b e b c 03 ; f : C a c f 04 T a c b f 05 ; g : C b c g 06 ^ a b a g a g a c 07 ; h : C b c h 08 T a h b c 09 ; i : C a g i 10 C b e i 11 ; j : C g i j 12 T b i b j 13 ; k : C e i k 14 T a i a k 15 ; l : C f j l 16 C h k l 17 ; m : D i m j m 18 D j m k m 19 ; n : C d m n 20 C e g n 21 ? T l n n e {F1} x00 o : C c i o 22 D c o i o 23 ; x00
I0123 21:24:10.393585 140446420529152 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a c e 02 ^ b a b e b e b c 03 ; f : C a c f 04 T a c b f 05 ; g : C b c g 06 ^ a b a g a g a c 07 ; h : C b c h 08 T a h b c 09 ; i : C a g i 10 C b e i 11 ; j : C g i j 12 T b i b j 13 ; k : C e i k 14 T a i a k 15 ; l : C f j l 16 C h k l 17 ; m : D i m j m 18 D j m k m 19 ; n : C d m n 20 C e g n 21 ? T l n n e {F1} x00 o : D b o c o 22 ; x00
I0123 21:24:10.393610 140446420529152 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a c e 02 ^ b a b e b e b c 03 ; f : C a c f 04 T a c b f 05 ; g : C b c g 06 ^ a b a g a g a c 07 ; h : C b c h 08 T a h b c 09 ; i : C a g i 10 C b e i 11 ; j : C g i j 12 T b i b j 13 ; k : C e i k 14 T a i a k 15 ; l : C f j l 16 C h k l 17 ; m : D i m j m 18 D j m k m 19 ; n : C d m n 20 C e g n 21 ? T l n n e {F1} x00 o : D a c a o 22 D a c c o 23 ; x00
I0123 21:24:10.393635 140446420529152 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a c e 02 ^ b a b e b e b c 03 ; f : C a c f 04 T a c b f 05 ; g : C b c g 06 ^ a b a g a g a c 07 ; h : C b c h 08 T a h b c 09 ; i : C a g i 10 C b e i 11 ; j : C g i j 12 T b i b j 13 ; k : C e i k 14 T a i a k 15 ; l : C f j l 16 C h k l 17 ; m : D i m j m 18 D j m k m 19 ; n : C d m n 20 C e g n 21 ? T l n n e {F1} x00 o : D i o j o 22 ; x00
I0123 21:24:10.393679 140446420529152 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a c e 02 ^ b a b e b e b c 03 ; f : C a c f 04 T a c b f 05 ; g : C b c g 06 ^ a b a g a g a c 07 ; h : C b c h 08 T a h b c 09 ; i : C a g i 10 C b e i 11 ; j : C g i j 12 T b i b j 13 ; k : C e i k 14 T a i a k 15 ; l : C f j l 16 C h k l 17 ; m : D i m j m 18 D j m k m 19 ; n : C d m n 20 C e g n 21 ? T l n n e {F1} x00 o : D l m m o 22 T l m m o 23 ; x00
I0123 21:24:10.393707 140446420529152 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a c e 02 ^ b a b e b e b c 03 ; f : C a c f 04 T a c b f 05 ; g : C b c g 06 ^ a b a g a g a c 07 ; h : C b c h 08 T a h b c 09 ; i : C a g i 10 C b e i 11 ; j : C g i j 12 T b i b j 13 ; k : C e i k 14 T a i a k 15 ; l : C f j l 16 C h k l 17 ; m : D i m j m 18 D j m k m 19 ; n : C d m n 20 C e g n 21 ? T l n n e {F1} x00 o : D b c b o 22 D b c c o 23 ; x00
I0123 21:24:10.393733 140446420529152 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a c e 02 ^ b a b e b e b c 03 ; f : C a c f 04 T a c b f 05 ; g : C b c g 06 ^ a b a g a g a c 07 ; h : C b c h 08 T a h b c 09 ; i : C a g i 10 C b e i 11 ; j : C g i j 12 T b i b j 13 ; k : C e i k 14 T a i a k 15 ; l : C f j l 16 C h k l 17 ; m : D i m j m 18 D j m k m 19 ; n : C d m n 20 C e g n 21 ? T l n n e {F1} x00 o : D a o d o 22 D c o d o 23 ; x00
I0123 21:24:10.393758 140446420529152 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a c e 02 ^ b a b e b e b c 03 ; f : C a c f 04 T a c b f 05 ; g : C b c g 06 ^ a b a g a g a c 07 ; h : C b c h 08 T a h b c 09 ; i : C a g i 10 C b e i 11 ; j : C g i j 12 T b i b j 13 ; k : C e i k 14 T a i a k 15 ; l : C f j l 16 C h k l 17 ; m : D i m j m 18 D j m k m 19 ; n : C d m n 20 C e g n 21 ? T l n n e {F1} x00 o : D j m m o 22 ; x00
I0123 21:24:10.393782 140446420529152 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a c e 02 ^ b a b e b e b c 03 ; f : C a c f 04 T a c b f 05 ; g : C b c g 06 ^ a b a g a g a c 07 ; h : C b c h 08 T a h b c 09 ; i : C a g i 10 C b e i 11 ; j : C g i j 12 T b i b j 13 ; k : C e i k 14 T a i a k 15 ; l : C f j l 16 C h k l 17 ; m : D i m j m 18 D j m k m 19 ; n : C d m n 20 C e g n 21 ? T l n n e {F1} x00 o : D f g h o 22 D f o g h 23 ; x00
I0123 21:24:10.393806 140446420529152 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a c e 02 ^ b a b e b e b c 03 ; f : C a c f 04 T a c b f 05 ; g : C b c g 06 ^ a b a g a g a c 07 ; h : C b c h 08 T a h b c 09 ; i : C a g i 10 C b e i 11 ; j : C g i j 12 T b i b j 13 ; k : C e i k 14 T a i a k 15 ; l : C f j l 16 C h k l 17 ; m : D i m j m 18 D j m k m 19 ; n : C d m n 20 C e g n 21 ? T l n n e {F1} x00 o : D f g f o 22 D g h h o 23 ; x00
I0123 21:24:10.393830 140446420529152 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a c e 02 ^ b a b e b e b c 03 ; f : C a c f 04 T a c b f 05 ; g : C b c g 06 ^ a b a g a g a c 07 ; h : C b c h 08 T a h b c 09 ; i : C a g i 10 C b e i 11 ; j : C g i j 12 T b i b j 13 ; k : C e i k 14 T a i a k 15 ; l : C f j l 16 C h k l 17 ; m : D i m j m 18 D j m k m 19 ; n : C d m n 20 C e g n 21 ? T l n n e {F1} x00 o : C b i o 22 D b o i o 23 ; x00
I0123 21:24:10.393854 140446420529152 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a c e 02 ^ b a b e b e b c 03 ; f : C a c f 04 T a c b f 05 ; g : C b c g 06 ^ a b a g a g a c 07 ; h : C b c h 08 T a h b c 09 ; i : C a g i 10 C b e i 11 ; j : C g i j 12 T b i b j 13 ; k : C e i k 14 T a i a k 15 ; l : C f j l 16 C h k l 17 ; m : D i m j m 18 D j m k m 19 ; n : C d m n 20 C e g n 21 ? T l n n e {F1} x00 o : D b o c o 22 D b o f o 23 ; x00
I0123 21:24:10.393882 140446420529152 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a c e 02 ^ b a b e b e b c 03 ; f : C a c f 04 T a c b f 05 ; g : C b c g 06 ^ a b a g a g a c 07 ; h : C b c h 08 T a h b c 09 ; i : C a g i 10 C b e i 11 ; j : C g i j 12 T b i b j 13 ; k : C e i k 14 T a i a k 15 ; l : C f j l 16 C h k l 17 ; m : D i m j m 18 D j m k m 19 ; n : C d m n 20 C e g n 21 ? T l n n e {F1} x00 o : D a o c o 22 ; x00
I0123 21:24:10.393908 140446420529152 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a c e 02 ^ b a b e b e b c 03 ; f : C a c f 04 T a c b f 05 ; g : C b c g 06 ^ a b a g a g a c 07 ; h : C b c h 08 T a h b c 09 ; i : C a g i 10 C b e i 11 ; j : C g i j 12 T b i b j 13 ; k : C e i k 14 T a i a k 15 ; l : C f j l 16 C h k l 17 ; m : D i m j m 18 D j m k m 19 ; n : C d m n 20 C e g n 21 ? T l n n e {F1} x00 o : D f o h o 22 ; x00
I0123 21:24:10.393933 140446420529152 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a c e 02 ^ b a b e b e b c 03 ; f : C a c f 04 T a c b f 05 ; g : C b c g 06 ^ a b a g a g a c 07 ; h : C b c h 08 T a h b c 09 ; i : C a g i 10 C b e i 11 ; j : C g i j 12 T b i b j 13 ; k : C e i k 14 T a i a k 15 ; l : C f j l 16 C h k l 17 ; m : D i m j m 18 D j m k m 19 ; n : C d m n 20 C e g n 21 ? T l n n e {F1} x00 o : D j o k o 22 ; x00
I0123 21:24:10.393957 140446420529152 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a c e 02 ^ b a b e b e b c 03 ; f : C a c f 04 T a c b f 05 ; g : C b c g 06 ^ a b a g a g a c 07 ; h : C b c h 08 T a h b c 09 ; i : C a g i 10 C b e i 11 ; j : C g i j 12 T b i b j 13 ; k : C e i k 14 T a i a k 15 ; l : C f j l 16 C h k l 17 ; m : D i m j m 18 D j m k m 19 ; n : C d m n 20 C e g n 21 ? T l n n e {F1} x00 o : D b d d o 22 ; x00
I0123 21:24:10.393981 140446420529152 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a c e 02 ^ b a b e b e b c 03 ; f : C a c f 04 T a c b f 05 ; g : C b c g 06 ^ a b a g a g a c 07 ; h : C b c h 08 T a h b c 09 ; i : C a g i 10 C b e i 11 ; j : C g i j 12 T b i b j 13 ; k : C e i k 14 T a i a k 15 ; l : C f j l 16 C h k l 17 ; m : D i m j m 18 D j m k m 19 ; n : C d m n 20 C e g n 21 ? T l n n e {F1} x00 o : D f o h o 22 D h o l o 23 ; x00
I0123 21:24:10.394005 140446420529152 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a c e 02 ^ b a b e b e b c 03 ; f : C a c f 04 T a c b f 05 ; g : C b c g 06 ^ a b a g a g a c 07 ; h : C b c h 08 T a h b c 09 ; i : C a g i 10 C b e i 11 ; j : C g i j 12 T b i b j 13 ; k : C e i k 14 T a i a k 15 ; l : C f j l 16 C h k l 17 ; m : D i m j m 18 D j m k m 19 ; n : C d m n 20 C e g n 21 ? T l n n e {F1} x00 o : D d o i o 22 D g o i o 23 ; x00
I0123 21:24:10.394029 140446420529152 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a c e 02 ^ b a b e b e b c 03 ; f : C a c f 04 T a c b f 05 ; g : C b c g 06 ^ a b a g a g a c 07 ; h : C b c h 08 T a h b c 09 ; i : C a g i 10 C b e i 11 ; j : C g i j 12 T b i b j 13 ; k : C e i k 14 T a i a k 15 ; l : C f j l 16 C h k l 17 ; m : D i m j m 18 D j m k m 19 ; n : C d m n 20 C e g n 21 ? T l n n e {F1} x00 o : D f g f o 22 T f g f o 23 ; x00
I0123 21:24:10.394051 140446420529152 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a c e 02 ^ b a b e b e b c 03 ; f : C a c f 04 T a c b f 05 ; g : C b c g 06 ^ a b a g a g a c 07 ; h : C b c h 08 T a h b c 09 ; i : C a g i 10 C b e i 11 ; j : C g i j 12 T b i b j 13 ; k : C e i k 14 T a i a k 15 ; l : C f j l 16 C h k l 17 ; m : D i m j m 18 D j m k m 19 ; n : C d m n 20 C e g n 21 ? T l n n e {F1} x00 o : D b d d o 22 T b d d o 23 ; x00
I0123 21:24:10.394074 140446420529152 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a c e 02 ^ b a b e b e b c 03 ; f : C a c f 04 T a c b f 05 ; g : C b c g 06 ^ a b a g a g a c 07 ; h : C b c h 08 T a h b c 09 ; i : C a g i 10 C b e i 11 ; j : C g i j 12 T b i b j 13 ; k : C e i k 14 T a i a k 15 ; l : C f j l 16 C h k l 17 ; m : D i m j m 18 D j m k m 19 ; n : C d m n 20 C e g n 21 ? T l n n e {F1} x00 o : D d o i o 22 D f o i o 23 ; x00
I0123 21:24:10.394102 140446420529152 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a c e 02 ^ b a b e b e b c 03 ; f : C a c f 04 T a c b f 05 ; g : C b c g 06 ^ a b a g a g a c 07 ; h : C b c h 08 T a h b c 09 ; i : C a g i 10 C b e i 11 ; j : C g i j 12 T b i b j 13 ; k : C e i k 14 T a i a k 15 ; l : C f j l 16 C h k l 17 ; m : D i m j m 18 D j m k m 19 ; n : C d m n 20 C e g n 21 ? T l n n e {F1} x00 o : D f i i o 22 D f j j o 23 ; x00
I0123 21:24:10.394127 140446420529152 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a c e 02 ^ b a b e b e b c 03 ; f : C a c f 04 T a c b f 05 ; g : C b c g 06 ^ a b a g a g a c 07 ; h : C b c h 08 T a h b c 09 ; i : C a g i 10 C b e i 11 ; j : C g i j 12 T b i b j 13 ; k : C e i k 14 T a i a k 15 ; l : C f j l 16 C h k l 17 ; m : D i m j m 18 D j m k m 19 ; n : C d m n 20 C e g n 21 ? T l n n e {F1} x00 o : D a o d o 22 D b o d o 23 ; x00
I0123 21:24:10.394151 140446420529152 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a c e 02 ^ b a b e b e b c 03 ; f : C a c f 04 T a c b f 05 ; g : C b c g 06 ^ a b a g a g a c 07 ; h : C b c h 08 T a h b c 09 ; i : C a g i 10 C b e i 11 ; j : C g i j 12 T b i b j 13 ; k : C e i k 14 T a i a k 15 ; l : C f j l 16 C h k l 17 ; m : D i m j m 18 D j m k m 19 ; n : C d m n 20 C e g n 21 ? T l n n e {F1} x00 o : D c o i o 22 D f o i o 23 ; x00
I0123 21:24:10.394174 140446420529152 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a c e 02 ^ b a b e b e b c 03 ; f : C a c f 04 T a c b f 05 ; g : C b c g 06 ^ a b a g a g a c 07 ; h : C b c h 08 T a h b c 09 ; i : C a g i 10 C b e i 11 ; j : C g i j 12 T b i b j 13 ; k : C e i k 14 T a i a k 15 ; l : C f j l 16 C h k l 17 ; m : D i m j m 18 D j m k m 19 ; n : C d m n 20 C e g n 21 ? T l n n e {F1} x00 o : D b o c o 22 D b o d o 23 ; x00
I0123 21:24:10.394198 140446420529152 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a c e 02 ^ b a b e b e b c 03 ; f : C a c f 04 T a c b f 05 ; g : C b c g 06 ^ a b a g a g a c 07 ; h : C b c h 08 T a h b c 09 ; i : C a g i 10 C b e i 11 ; j : C g i j 12 T b i b j 13 ; k : C e i k 14 T a i a k 15 ; l : C f j l 16 C h k l 17 ; m : D i m j m 18 D j m k m 19 ; n : C d m n 20 C e g n 21 ? T l n n e {F1} x00 o : D b o i o 22 D c o i o 23 ; x00
I0123 21:24:10.394221 140446420529152 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a c e 02 ^ b a b e b e b c 03 ; f : C a c f 04 T a c b f 05 ; g : C b c g 06 ^ a b a g a g a c 07 ; h : C b c h 08 T a h b c 09 ; i : C a g i 10 C b e i 11 ; j : C g i j 12 T b i b j 13 ; k : C e i k 14 T a i a k 15 ; l : C f j l 16 C h k l 17 ; m : D i m j m 18 D j m k m 19 ; n : C d m n 20 C e g n 21 ? T l n n e {F1} x00 o : D f o h o 22 D g o h o 23 ; x00
I0123 21:24:10.394244 140446420529152 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a c e 02 ^ b a b e b e b c 03 ; f : C a c f 04 T a c b f 05 ; g : C b c g 06 ^ a b a g a g a c 07 ; h : C b c h 08 T a h b c 09 ; i : C a g i 10 C b e i 11 ; j : C g i j 12 T b i b j 13 ; k : C e i k 14 T a i a k 15 ; l : C f j l 16 C h k l 17 ; m : D i m j m 18 D j m k m 19 ; n : C d m n 20 C e g n 21 ? T l n n e {F1} x00 o : D f i f o 22 D h i h o 23 ; x00
I0123 21:24:10.394270 140446420529152 alphageometry.py:549] Decoding from {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a c e 02 ^ b a b e b e b c 03 ; f : C a c f 04 T a c b f 05 ; g : C b c g 06 ^ a b a g a g a c 07 ; h : C b c h 08 T a h b c 09 ; i : C a g i 10 C b e i 11 ; j : C g i j 12 T b i b j 13 ; k : C e i k 14 T a i a k 15 ; l : C f j l 16 C h k l 17 ; m : D i m j m 18 D j m k m 19 ; n : C d m n 20 C e g n 21 ? T l n n e {F1} x00 o : D k m m o 22 ; x00
I0123 21:24:20.464625 140446420529152 alphageometry.py:566] LM output (score=-1.234286): "p : C c i p 23 D c p i p 24 ;"
I0123 21:24:20.464780 140446420529152 alphageometry.py:567] Translation: "p = on_line p c i, on_bline p i c"

I0123 21:24:20.464824 140446420529152 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o m k; p = on_line p c i, on_bline p i c ? perp l n n e"
I0123 21:24:20.465017 140446420529152 graph.py:498] 
I0123 21:24:20.465078 140446420529152 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o m k; p = on_line p c i, on_bline p i c ? perp l n n e
I0123 21:24:24.288200 140446420529152 ddar.py:60] Depth 1/1000 time = 3.7850162982940674
I0123 21:24:28.856492 140446420529152 ddar.py:60] Depth 2/1000 time = 4.568100690841675
I0123 21:24:34.171786 140446420529152 ddar.py:60] Depth 3/1000 time = 5.315078020095825
I0123 21:24:39.194560 140446420529152 ddar.py:60] Depth 4/1000 time = 5.022592782974243
I0123 21:24:44.745408 140446420529152 ddar.py:60] Depth 5/1000 time = 5.550653457641602
I0123 21:24:49.761207 140446420529152 ddar.py:60] Depth 6/1000 time = 5.015375137329102
I0123 21:24:56.010235 140446420529152 ddar.py:60] Depth 7/1000 time = 6.237013339996338
I0123 21:25:03.086307 140446420529152 ddar.py:60] Depth 8/1000 time = 7.075880765914917
I0123 21:25:10.915008 140446420529152 ddar.py:60] Depth 9/1000 time = 7.828506231307983
I0123 21:25:21.772425 140446420529152 ddar.py:60] Depth 10/1000 time = 10.857182025909424
I0123 21:25:33.249433 140446420529152 ddar.py:60] Depth 11/1000 time = 11.476717710494995
I0123 21:25:44.221069 140446420529152 ddar.py:60] Depth 12/1000 time = 10.946719646453857
I0123 21:25:55.754637 140446420529152 ddar.py:60] Depth 13/1000 time = 11.503931999206543
I0123 21:25:55.778372 140446420529152 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 21:25:55.778441 140446420529152 alphageometry.py:566] LM output (score=-1.907028): "p : D c o c p 23 D c o o p 24 ;"
I0123 21:25:55.778478 140446420529152 alphageometry.py:567] Translation: "p = on_circle p c o, on_circle p o c"

I0123 21:25:55.778518 140446420529152 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o m k; p = on_circle p c o, on_circle p o c ? perp l n n e"
I0123 21:25:55.778723 140446420529152 graph.py:498] 
I0123 21:25:55.778785 140446420529152 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o m k; p = on_circle p c o, on_circle p o c ? perp l n n e
I0123 21:25:59.665277 140446420529152 ddar.py:60] Depth 1/1000 time = 3.842824697494507
I0123 21:26:03.618867 140446420529152 ddar.py:60] Depth 2/1000 time = 3.953413963317871
I0123 21:26:09.157786 140446420529152 ddar.py:60] Depth 3/1000 time = 5.538727283477783
I0123 21:26:14.395765 140446420529152 ddar.py:60] Depth 4/1000 time = 5.2377541065216064
I0123 21:26:19.593663 140446420529152 ddar.py:60] Depth 5/1000 time = 5.197684288024902
I0123 21:26:25.374635 140446420529152 ddar.py:60] Depth 6/1000 time = 5.780132293701172
I0123 21:26:31.378312 140446420529152 ddar.py:60] Depth 7/1000 time = 5.991286993026733
I0123 21:26:38.640735 140446420529152 ddar.py:60] Depth 8/1000 time = 7.262240648269653
I0123 21:26:46.188676 140446420529152 ddar.py:60] Depth 9/1000 time = 7.5477094650268555
I0123 21:26:56.622245 140446420529152 ddar.py:60] Depth 10/1000 time = 10.433250904083252
I0123 21:27:07.496408 140446420529152 ddar.py:60] Depth 11/1000 time = 10.873865842819214
I0123 21:27:18.432978 140446420529152 ddar.py:60] Depth 12/1000 time = 10.910600900650024
I0123 21:27:29.454750 140446420529152 ddar.py:60] Depth 13/1000 time = 10.991254091262817
I0123 21:27:29.477094 140446420529152 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 21:27:29.477161 140446420529152 alphageometry.py:566] LM output (score=-2.087111): "p : D f g i p 23 D f p g i 24 ;"
I0123 21:27:29.477197 140446420529152 alphageometry.py:567] Translation: "p = eqdistance p i f g, eqdistance p f g i"

I0123 21:27:29.477239 140446420529152 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o m k; p = eqdistance p i f g, eqdistance p f g i ? perp l n n e"
I0123 21:27:29.477445 140446420529152 graph.py:498] 
I0123 21:27:29.477508 140446420529152 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o m k; p = eqdistance p i f g, eqdistance p f g i ? perp l n n e
I0123 21:27:33.391952 140446420529152 ddar.py:60] Depth 1/1000 time = 3.8661742210388184
I0123 21:27:37.467832 140446420529152 ddar.py:60] Depth 2/1000 time = 4.075698137283325
I0123 21:27:43.253632 140446420529152 ddar.py:60] Depth 3/1000 time = 5.785603284835815
I0123 21:27:48.724756 140446420529152 ddar.py:60] Depth 4/1000 time = 5.470917224884033
I0123 21:27:54.210570 140446420529152 ddar.py:60] Depth 5/1000 time = 5.48561429977417
I0123 21:28:00.570241 140446420529152 ddar.py:60] Depth 6/1000 time = 6.347116470336914
I0123 21:28:08.249853 140446420529152 ddar.py:60] Depth 7/1000 time = 7.679393768310547
I0123 21:28:16.197531 140446420529152 ddar.py:60] Depth 8/1000 time = 7.947447299957275
I0123 21:28:27.079735 140446420529152 ddar.py:60] Depth 9/1000 time = 10.88186240196228
I0123 21:28:38.408812 140446420529152 ddar.py:60] Depth 10/1000 time = 11.328758478164673
I0123 21:28:49.728752 140446420529152 ddar.py:60] Depth 11/1000 time = 11.290332317352295
I0123 21:29:01.020549 140446420529152 ddar.py:60] Depth 12/1000 time = 11.260814189910889
I0123 21:29:01.043697 140446420529152 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 21:29:01.043762 140446420529152 alphageometry.py:566] LM output (score=-2.090140): "p : D b c b p 23 D b c c p 24 ;"
I0123 21:29:01.043798 140446420529152 alphageometry.py:567] Translation: "p = on_circle p b c, on_circle p c b"

I0123 21:29:01.043839 140446420529152 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o m k; p = on_circle p b c, on_circle p c b ? perp l n n e"
I0123 21:29:01.044053 140446420529152 graph.py:498] 
I0123 21:29:01.044118 140446420529152 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o m k; p = on_circle p b c, on_circle p c b ? perp l n n e
I0123 21:29:04.385763 140446420529152 ddar.py:60] Depth 1/1000 time = 3.298534393310547
I0123 21:29:09.608924 140446420529152 ddar.py:60] Depth 2/1000 time = 5.22298526763916
I0123 21:29:14.885553 140446420529152 ddar.py:60] Depth 3/1000 time = 5.276439428329468
I0123 21:29:20.326982 140446420529152 ddar.py:60] Depth 4/1000 time = 5.441224098205566
I0123 21:29:25.789510 140446420529152 ddar.py:60] Depth 5/1000 time = 5.462337493896484
I0123 21:29:31.320744 140446420529152 ddar.py:60] Depth 6/1000 time = 5.529658555984497
I0123 21:29:37.636222 140446420529152 ddar.py:60] Depth 7/1000 time = 6.302865982055664
I0123 21:29:44.646305 140446420529152 ddar.py:60] Depth 8/1000 time = 7.009884834289551
I0123 21:29:52.490654 140446420529152 ddar.py:60] Depth 9/1000 time = 7.844140291213989
I0123 21:30:03.730249 140446420529152 ddar.py:60] Depth 10/1000 time = 11.239359378814697
I0123 21:30:15.073777 140446420529152 ddar.py:60] Depth 11/1000 time = 11.343241691589355
I0123 21:30:26.476732 140446420529152 ddar.py:60] Depth 12/1000 time = 11.373298168182373
I0123 21:30:37.283262 140446420529152 ddar.py:60] Depth 13/1000 time = 10.775543451309204
I0123 21:30:37.307199 140446420529152 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 21:30:37.307267 140446420529152 alphageometry.py:566] LM output (score=-2.117189): "p : D i p k p 23 ;"
I0123 21:30:37.307301 140446420529152 alphageometry.py:567] Translation: "p = on_bline p k i"

I0123 21:30:37.307341 140446420529152 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o m k; p = on_bline p k i ? perp l n n e"
I0123 21:30:37.307540 140446420529152 graph.py:498] 
I0123 21:30:37.307603 140446420529152 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o m k; p = on_bline p k i ? perp l n n e
I0123 21:30:41.140227 140446420529152 ddar.py:60] Depth 1/1000 time = 3.793611764907837
I0123 21:30:46.039871 140446420529152 ddar.py:60] Depth 2/1000 time = 4.899456739425659
I0123 21:30:51.074332 140446420529152 ddar.py:60] Depth 3/1000 time = 5.0342583656311035
I0123 21:30:56.348617 140446420529152 ddar.py:60] Depth 4/1000 time = 5.2741005420684814
I0123 21:31:02.286931 140446420529152 ddar.py:60] Depth 5/1000 time = 5.938068389892578
I0123 21:31:08.506998 140446420529152 ddar.py:60] Depth 6/1000 time = 6.207589864730835
I0123 21:31:15.515465 140446420529152 ddar.py:60] Depth 7/1000 time = 7.0082831382751465
I0123 21:31:23.324064 140446420529152 ddar.py:60] Depth 8/1000 time = 7.808352708816528
I0123 21:31:33.825052 140446420529152 ddar.py:60] Depth 9/1000 time = 10.500654458999634
I0123 21:31:44.916222 140446420529152 ddar.py:60] Depth 10/1000 time = 11.090874671936035
I0123 21:31:55.920488 140446420529152 ddar.py:60] Depth 11/1000 time = 10.995806217193604
I0123 21:32:07.171945 140446420529152 ddar.py:60] Depth 12/1000 time = 11.225579261779785
I0123 21:32:18.470340 140446420529152 ddar.py:60] Depth 13/1000 time = 11.268201112747192
I0123 21:32:18.493584 140446420529152 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 21:32:18.493692 140446420529152 alphageometry.py:566] LM output (score=-2.183442): "p : D f o f p 23 D f o o p 24 ;"
I0123 21:32:18.493730 140446420529152 alphageometry.py:567] Translation: "p = on_circle p f o, on_circle p o f"

I0123 21:32:18.493786 140446420529152 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o m k; p = on_circle p f o, on_circle p o f ? perp l n n e"
I0123 21:32:18.494010 140446420529152 graph.py:498] 
I0123 21:32:18.494071 140446420529152 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o m k; p = on_circle p f o, on_circle p o f ? perp l n n e
I0123 21:32:22.548857 140446420529152 ddar.py:60] Depth 1/1000 time = 4.006272315979004
I0123 21:32:26.523106 140446420529152 ddar.py:60] Depth 2/1000 time = 3.9740676879882812
I0123 21:32:32.092367 140446420529152 ddar.py:60] Depth 3/1000 time = 5.569082498550415
I0123 21:32:37.454832 140446420529152 ddar.py:60] Depth 4/1000 time = 5.362245559692383
I0123 21:32:42.794744 140446420529152 ddar.py:60] Depth 5/1000 time = 5.339656829833984
I0123 21:32:48.108618 140446420529152 ddar.py:60] Depth 6/1000 time = 5.313104152679443
I0123 21:32:54.226038 140446420529152 ddar.py:60] Depth 7/1000 time = 6.10570764541626
I0123 21:33:01.119285 140446420529152 ddar.py:60] Depth 8/1000 time = 6.892940282821655
I0123 21:33:08.774598 140446420529152 ddar.py:60] Depth 9/1000 time = 7.655121088027954
I0123 21:33:19.319967 140446420529152 ddar.py:60] Depth 10/1000 time = 10.545119047164917
I0123 21:33:30.199031 140446420529152 ddar.py:60] Depth 11/1000 time = 10.878786087036133
I0123 21:33:41.272569 140446420529152 ddar.py:60] Depth 12/1000 time = 11.048691272735596
I0123 21:33:52.391069 140446420529152 ddar.py:60] Depth 13/1000 time = 11.089980363845825
I0123 21:33:52.415077 140446420529152 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 21:33:52.415147 140446420529152 alphageometry.py:566] LM output (score=-2.196220): "p : D b o b p 23 D b o o p 24 ;"
I0123 21:33:52.415183 140446420529152 alphageometry.py:567] Translation: "p = on_circle p b o, on_circle p o b"

I0123 21:33:52.415223 140446420529152 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o m k; p = on_circle p b o, on_circle p o b ? perp l n n e"
I0123 21:33:52.415423 140446420529152 graph.py:498] 
I0123 21:33:52.415484 140446420529152 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o m k; p = on_circle p b o, on_circle p o b ? perp l n n e
I0123 21:33:55.819201 140446420529152 ddar.py:60] Depth 1/1000 time = 3.3568837642669678
I0123 21:34:00.650794 140446420529152 ddar.py:60] Depth 2/1000 time = 4.831402063369751
I0123 21:34:05.745713 140446420529152 ddar.py:60] Depth 3/1000 time = 5.094737768173218
I0123 21:34:11.185806 140446420529152 ddar.py:60] Depth 4/1000 time = 5.4398932456970215
I0123 21:34:16.524713 140446420529152 ddar.py:60] Depth 5/1000 time = 5.3386828899383545
I0123 21:34:22.503162 140446420529152 ddar.py:60] Depth 6/1000 time = 5.977671384811401
I0123 21:34:28.785614 140446420529152 ddar.py:60] Depth 7/1000 time = 6.270698308944702
I0123 21:34:35.781666 140446420529152 ddar.py:60] Depth 8/1000 time = 6.995794057846069
I0123 21:34:43.003239 140446420529152 ddar.py:60] Depth 9/1000 time = 7.2213850021362305
I0123 21:34:53.634503 140446420529152 ddar.py:60] Depth 10/1000 time = 10.631009817123413
I0123 21:35:04.636425 140446420529152 ddar.py:60] Depth 11/1000 time = 11.001601696014404
I0123 21:35:15.718663 140446420529152 ddar.py:60] Depth 12/1000 time = 11.057981252670288
I0123 21:35:26.919714 140446420529152 ddar.py:60] Depth 13/1000 time = 11.171688795089722
I0123 21:35:26.944244 140446420529152 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 21:35:26.944313 140446420529152 alphageometry.py:566] LM output (score=-2.231070): "p : D c i i p 23 T c i i p 24 ;"
I0123 21:35:26.944348 140446420529152 alphageometry.py:567] Translation: "p = on_circle p i c, on_tline p i c i"

I0123 21:35:26.944392 140446420529152 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o m k; p = on_circle p i c, on_tline p i c i ? perp l n n e"
I0123 21:35:26.944591 140446420529152 graph.py:498] 
I0123 21:35:26.944651 140446420529152 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o m k; p = on_circle p i c, on_tline p i c i ? perp l n n e
I0123 21:35:30.267533 140446420529152 ddar.py:60] Depth 1/1000 time = 3.2802748680114746
I0123 21:35:35.153101 140446420529152 ddar.py:60] Depth 2/1000 time = 4.885395765304565
I0123 21:35:40.260078 140446420529152 ddar.py:60] Depth 3/1000 time = 5.10680627822876
I0123 21:35:45.662971 140446420529152 ddar.py:60] Depth 4/1000 time = 5.402710199356079
I0123 21:35:50.993413 140446420529152 ddar.py:60] Depth 5/1000 time = 5.330220460891724
I0123 21:35:56.349506 140446420529152 ddar.py:60] Depth 6/1000 time = 5.355401039123535
I0123 21:36:03.508672 140446420529152 ddar.py:60] Depth 7/1000 time = 7.145159482955933
I0123 21:36:10.960545 140446420529152 ddar.py:60] Depth 8/1000 time = 7.4516282081604
I0123 21:36:19.305653 140446420529152 ddar.py:60] Depth 9/1000 time = 8.34485936164856
I0123 21:36:29.955696 140446420529152 ddar.py:60] Depth 10/1000 time = 10.649713277816772
I0123 21:36:41.325972 140446420529152 ddar.py:60] Depth 11/1000 time = 11.36996841430664
I0123 21:36:52.038104 140446420529152 ddar.py:60] Depth 12/1000 time = 10.684201955795288
I0123 21:37:03.421172 140446420529152 ddar.py:60] Depth 13/1000 time = 11.34913945198059
I0123 21:37:03.445480 140446420529152 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 21:37:03.445546 140446420529152 alphageometry.py:566] LM output (score=-2.262429): "p : C i k p 23 D i p k p 24 ;"
I0123 21:37:03.445580 140446420529152 alphageometry.py:567] Translation: "p = on_line p i k, on_bline p k i"

I0123 21:37:03.445621 140446420529152 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o m k; p = on_line p i k, on_bline p k i ? perp l n n e"
I0123 21:37:03.445842 140446420529152 graph.py:498] 
I0123 21:37:03.445905 140446420529152 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o m k; p = on_line p i k, on_bline p k i ? perp l n n e
I0123 21:37:09.304895 140446420529152 ddar.py:60] Depth 1/1000 time = 5.82076621055603
I0123 21:37:16.775900 140446420529152 ddar.py:60] Depth 2/1000 time = 7.470804691314697
I0123 21:37:24.718672 140446420529152 ddar.py:60] Depth 3/1000 time = 7.942566871643066
I0123 21:37:34.581623 140446420529152 ddar.py:60] Depth 4/1000 time = 9.862704515457153
I0123 21:37:43.886430 140446420529152 ddar.py:60] Depth 5/1000 time = 9.304484844207764
I0123 21:37:54.871954 140446420529152 ddar.py:60] Depth 6/1000 time = 10.98531723022461
I0123 21:38:10.036200 140446420529152 ddar.py:60] Depth 7/1000 time = 15.163964033126831
I0123 21:38:24.926245 140446420529152 ddar.py:60] Depth 8/1000 time = 14.889715909957886
I0123 21:38:39.833453 140446420529152 ddar.py:60] Depth 9/1000 time = 14.906961917877197
I0123 21:38:54.742689 140446420529152 ddar.py:60] Depth 10/1000 time = 14.908255338668823
I0123 21:39:09.662535 140446420529152 ddar.py:60] Depth 11/1000 time = 14.916808843612671
I0123 21:39:26.262571 140446420529152 ddar.py:60] Depth 12/1000 time = 16.599624156951904
I0123 21:39:43.013146 140446420529152 ddar.py:60] Depth 13/1000 time = 16.750157117843628
I0123 21:39:58.606364 140446420529152 ddar.py:60] Depth 14/1000 time = 15.592840433120728
I0123 21:40:14.903274 140446420529152 ddar.py:60] Depth 15/1000 time = 16.164080381393433
I0123 21:40:14.911178 140446420529152 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 21:40:14.911241 140446420529152 alphageometry.py:566] LM output (score=-2.303465): "p : D f g f p 23 T f g f p 24 ;"
I0123 21:40:14.911275 140446420529152 alphageometry.py:567] Translation: "p = on_circle p f g, on_tline p f f g"

I0123 21:40:14.911315 140446420529152 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o m k; p = on_circle p f g, on_tline p f f g ? perp l n n e"
I0123 21:40:14.911505 140446420529152 graph.py:498] 
I0123 21:40:14.911565 140446420529152 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o m k; p = on_circle p f g, on_tline p f f g ? perp l n n e
I0123 21:40:18.966539 140446420529152 ddar.py:60] Depth 1/1000 time = 4.0115966796875
I0123 21:40:24.033468 140446420529152 ddar.py:60] Depth 2/1000 time = 5.066632032394409
I0123 21:40:29.274059 140446420529152 ddar.py:60] Depth 3/1000 time = 5.240407943725586
I0123 21:40:34.853436 140446420529152 ddar.py:60] Depth 4/1000 time = 5.579178333282471
I0123 21:40:39.857347 140446420529152 ddar.py:60] Depth 5/1000 time = 5.003699064254761
I0123 21:40:45.391674 140446420529152 ddar.py:60] Depth 6/1000 time = 5.5335845947265625
I0123 21:40:51.767881 140446420529152 ddar.py:60] Depth 7/1000 time = 6.363293409347534
I0123 21:40:59.652522 140446420529152 ddar.py:60] Depth 8/1000 time = 7.884437322616577
I0123 21:41:07.103878 140446420529152 ddar.py:60] Depth 9/1000 time = 7.451150894165039
I0123 21:41:17.437595 140446420529152 ddar.py:60] Depth 10/1000 time = 10.33343768119812
I0123 21:41:29.017997 140446420529152 ddar.py:60] Depth 11/1000 time = 11.580002069473267
I0123 21:41:40.058300 140446420529152 ddar.py:60] Depth 12/1000 time = 11.010638952255249
I0123 21:41:51.618532 140446420529152 ddar.py:60] Depth 13/1000 time = 11.523305654525757
I0123 21:41:51.643994 140446420529152 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 21:41:51.644086 140446420529152 alphageometry.py:566] LM output (score=-2.445546): "p : C b i p 23 D b p i p 24 ;"
I0123 21:41:51.644121 140446420529152 alphageometry.py:567] Translation: "p = on_line p b i, on_bline p i b"

I0123 21:41:51.644176 140446420529152 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o m k; p = on_line p b i, on_bline p i b ? perp l n n e"
I0123 21:41:51.644399 140446420529152 graph.py:498] 
I0123 21:41:51.644461 140446420529152 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o m k; p = on_line p b i, on_bline p i b ? perp l n n e
I0123 21:41:57.057315 140446420529152 ddar.py:60] Depth 1/1000 time = 5.3739073276519775
I0123 21:42:03.991264 140446420529152 ddar.py:60] Depth 2/1000 time = 6.933768033981323
I0123 21:42:10.567439 140446420529152 ddar.py:60] Depth 3/1000 time = 6.575943470001221
I0123 21:42:18.050978 140446420529152 ddar.py:60] Depth 4/1000 time = 7.483243942260742
I0123 21:42:25.543519 140446420529152 ddar.py:60] Depth 5/1000 time = 7.492340564727783
I0123 21:42:33.111385 140446420529152 ddar.py:60] Depth 6/1000 time = 7.567422389984131
I0123 21:42:40.640401 140446420529152 ddar.py:60] Depth 7/1000 time = 7.517437934875488
I0123 21:42:50.459727 140446420529152 ddar.py:60] Depth 8/1000 time = 9.81911301612854
I0123 21:42:59.091366 140446420529152 ddar.py:60] Depth 9/1000 time = 8.631429433822632
I0123 21:43:12.090608 140446420529152 ddar.py:60] Depth 10/1000 time = 12.999027729034424
I0123 21:43:25.608151 140446420529152 ddar.py:60] Depth 11/1000 time = 13.517210245132446
I0123 21:43:39.332483 140446420529152 ddar.py:60] Depth 12/1000 time = 13.699627161026001
I0123 21:43:52.452968 140446420529152 ddar.py:60] Depth 13/1000 time = 13.0910165309906
I0123 21:43:52.475150 140446420529152 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 21:43:52.475218 140446420529152 alphageometry.py:566] LM output (score=-2.532509): "p : ^ d c d p d p d f 23 ^ f c f p f p f d 24 ;"
I0123 21:43:52.475253 140446420529152 alphageometry.py:567] Translation: "ERROR: Invalid predicate ^ d c d p d p d f"

I0123 21:43:52.475289 140446420529152 alphageometry.py:566] LM output (score=-2.564382): "p : D f l f p 23 ;"
I0123 21:43:52.475316 140446420529152 alphageometry.py:567] Translation: "p = on_circle p f l"

I0123 21:43:52.475347 140446420529152 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o m k; p = on_circle p f l ? perp l n n e"
I0123 21:43:52.475552 140446420529152 graph.py:498] 
I0123 21:43:52.475612 140446420529152 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o m k; p = on_circle p f l ? perp l n n e
I0123 21:43:55.878573 140446420529152 ddar.py:60] Depth 1/1000 time = 3.365588665008545
I0123 21:44:00.567396 140446420529152 ddar.py:60] Depth 2/1000 time = 4.688639879226685
I0123 21:44:05.124729 140446420529152 ddar.py:60] Depth 3/1000 time = 4.557161569595337
I0123 21:44:10.508805 140446420529152 ddar.py:60] Depth 4/1000 time = 5.383881092071533
I0123 21:44:15.890889 140446420529152 ddar.py:60] Depth 5/1000 time = 5.381882905960083
I0123 21:44:22.168667 140446420529152 ddar.py:60] Depth 6/1000 time = 6.265483617782593
I0123 21:44:29.031086 140446420529152 ddar.py:60] Depth 7/1000 time = 6.862107276916504
I0123 21:44:36.032201 140446420529152 ddar.py:60] Depth 8/1000 time = 7.0009307861328125
I0123 21:44:46.602212 140446420529152 ddar.py:60] Depth 9/1000 time = 10.569774150848389
I0123 21:44:56.927381 140446420529152 ddar.py:60] Depth 10/1000 time = 10.32483696937561
I0123 21:45:08.110327 140446420529152 ddar.py:60] Depth 11/1000 time = 11.15805983543396
I0123 21:45:18.606782 140446420529152 ddar.py:60] Depth 12/1000 time = 10.468980073928833
I0123 21:45:18.630038 140446420529152 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 21:45:18.630104 140446420529152 alphageometry.py:566] LM output (score=-2.592275): "p : D f k f p 23 T f k f p 24 ;"
I0123 21:45:18.630139 140446420529152 alphageometry.py:567] Translation: "p = on_circle p f k, on_tline p f f k"

I0123 21:45:18.630179 140446420529152 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o m k; p = on_circle p f k, on_tline p f f k ? perp l n n e"
I0123 21:45:18.630381 140446420529152 graph.py:498] 
I0123 21:45:18.630446 140446420529152 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o m k; p = on_circle p f k, on_tline p f f k ? perp l n n e
I0123 21:45:22.047129 140446420529152 ddar.py:60] Depth 1/1000 time = 3.3699631690979004
I0123 21:45:27.070847 140446420529152 ddar.py:60] Depth 2/1000 time = 5.023550987243652
I0123 21:45:32.365560 140446420529152 ddar.py:60] Depth 3/1000 time = 5.2945396900177
I0123 21:45:37.330459 140446420529152 ddar.py:60] Depth 4/1000 time = 4.9646875858306885
I0123 21:45:42.859930 140446420529152 ddar.py:60] Depth 5/1000 time = 5.529242515563965
I0123 21:45:48.416726 140446420529152 ddar.py:60] Depth 6/1000 time = 5.556098699569702
I0123 21:45:54.892923 140446420529152 ddar.py:60] Depth 7/1000 time = 6.462829828262329
I0123 21:46:02.880307 140446420529152 ddar.py:60] Depth 8/1000 time = 7.987048387527466
I0123 21:46:10.308378 140446420529152 ddar.py:60] Depth 9/1000 time = 7.427895784378052
I0123 21:46:21.371500 140446420529152 ddar.py:60] Depth 10/1000 time = 11.06287670135498
I0123 21:46:32.323026 140446420529152 ddar.py:60] Depth 11/1000 time = 10.951167345046997
I0123 21:46:43.351681 140446420529152 ddar.py:60] Depth 12/1000 time = 10.99850058555603
I0123 21:46:54.385685 140446420529152 ddar.py:60] Depth 13/1000 time = 10.99843692779541
I0123 21:46:54.410948 140446420529152 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 21:46:54.411014 140446420529152 alphageometry.py:566] LM output (score=-2.634831): "p : D b p c p 23 ;"
I0123 21:46:54.411048 140446420529152 alphageometry.py:567] Translation: "p = on_bline p c b"

I0123 21:46:54.411090 140446420529152 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o m k; p = on_bline p c b ? perp l n n e"
I0123 21:46:54.411286 140446420529152 graph.py:498] 
I0123 21:46:54.411349 140446420529152 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o m k; p = on_bline p c b ? perp l n n e
I0123 21:46:58.530092 140446420529152 ddar.py:60] Depth 1/1000 time = 4.080177068710327
I0123 21:47:03.004442 140446420529152 ddar.py:60] Depth 2/1000 time = 4.4741644859313965
I0123 21:47:08.311433 140446420529152 ddar.py:60] Depth 3/1000 time = 5.306733131408691
I0123 21:47:13.863630 140446420529152 ddar.py:60] Depth 4/1000 time = 5.551896810531616
I0123 21:47:19.411005 140446420529152 ddar.py:60] Depth 5/1000 time = 5.5471882820129395
I0123 21:47:25.821189 140446420529152 ddar.py:60] Depth 6/1000 time = 6.398245573043823
I0123 21:47:32.929741 140446420529152 ddar.py:60] Depth 7/1000 time = 7.108248472213745
I0123 21:47:40.938630 140446420529152 ddar.py:60] Depth 8/1000 time = 8.008695602416992
I0123 21:47:51.294646 140446420529152 ddar.py:60] Depth 9/1000 time = 10.35578727722168
I0123 21:48:02.832481 140446420529152 ddar.py:60] Depth 10/1000 time = 11.53750205039978
I0123 21:48:13.887065 140446420529152 ddar.py:60] Depth 11/1000 time = 11.02397894859314
I0123 21:48:24.848057 140446420529152 ddar.py:60] Depth 12/1000 time = 10.927706718444824
I0123 21:48:24.869663 140446420529152 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 21:48:24.869731 140446420529152 alphageometry.py:566] LM output (score=-2.685565): "p : D j p k p 23 ;"
I0123 21:48:24.869766 140446420529152 alphageometry.py:567] Translation: "p = on_bline p k j"

I0123 21:48:24.869806 140446420529152 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o m k; p = on_bline p k j ? perp l n n e"
I0123 21:48:24.870011 140446420529152 graph.py:498] 
I0123 21:48:24.870073 140446420529152 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o m k; p = on_bline p k j ? perp l n n e
I0123 21:48:28.379359 140446420529152 ddar.py:60] Depth 1/1000 time = 3.470735788345337
I0123 21:48:33.126349 140446420529152 ddar.py:60] Depth 2/1000 time = 4.746816873550415
I0123 21:48:37.916517 140446420529152 ddar.py:60] Depth 3/1000 time = 4.789993047714233
I0123 21:48:43.687814 140446420529152 ddar.py:60] Depth 4/1000 time = 5.771097421646118
I0123 21:48:49.395929 140446420529152 ddar.py:60] Depth 5/1000 time = 5.707888841629028
I0123 21:48:56.199715 140446420529152 ddar.py:60] Depth 6/1000 time = 6.788853645324707
I0123 21:49:03.666536 140446420529152 ddar.py:60] Depth 7/1000 time = 7.466639518737793
I0123 21:49:14.717872 140446420529152 ddar.py:60] Depth 8/1000 time = 11.051130533218384
I0123 21:49:25.822741 140446420529152 ddar.py:60] Depth 9/1000 time = 11.104571342468262
I0123 21:49:36.853662 140446420529152 ddar.py:60] Depth 10/1000 time = 10.999448776245117
I0123 21:49:47.808024 140446420529152 ddar.py:60] Depth 11/1000 time = 10.915245771408081
I0123 21:49:47.831825 140446420529152 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 21:49:47.831899 140446420529152 alphageometry.py:566] LM output (score=-2.699770): "p : D b o b p 23 T b o b p 24 ;"
I0123 21:49:47.831934 140446420529152 alphageometry.py:567] Translation: "p = on_circle p b o, on_tline p b b o"

I0123 21:49:47.831974 140446420529152 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o m k; p = on_circle p b o, on_tline p b b o ? perp l n n e"
I0123 21:49:47.832178 140446420529152 graph.py:498] 
I0123 21:49:47.832237 140446420529152 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o m k; p = on_circle p b o, on_tline p b b o ? perp l n n e
I0123 21:49:51.298326 140446420529152 ddar.py:60] Depth 1/1000 time = 3.4234864711761475
I0123 21:49:56.536351 140446420529152 ddar.py:60] Depth 2/1000 time = 5.23784065246582
I0123 21:50:01.267034 140446420529152 ddar.py:60] Depth 3/1000 time = 4.730471134185791
I0123 21:50:06.885131 140446420529152 ddar.py:60] Depth 4/1000 time = 5.617917537689209
I0123 21:50:12.679256 140446420529152 ddar.py:60] Depth 5/1000 time = 5.793889760971069
I0123 21:50:17.706692 140446420529152 ddar.py:60] Depth 6/1000 time = 5.0266358852386475
I0123 21:50:24.198493 140446420529152 ddar.py:60] Depth 7/1000 time = 6.478312253952026
I0123 21:50:31.467400 140446420529152 ddar.py:60] Depth 8/1000 time = 7.268671274185181
I0123 21:50:38.964836 140446420529152 ddar.py:60] Depth 9/1000 time = 7.4971559047698975
I0123 21:50:50.113657 140446420529152 ddar.py:60] Depth 10/1000 time = 11.148526906967163
I0123 21:51:01.110767 140446420529152 ddar.py:60] Depth 11/1000 time = 10.996700048446655
I0123 21:51:12.189011 140446420529152 ddar.py:60] Depth 12/1000 time = 11.048521280288696
I0123 21:51:23.283060 140446420529152 ddar.py:60] Depth 13/1000 time = 11.05765700340271
I0123 21:51:23.308352 140446420529152 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 21:51:23.308431 140446420529152 alphageometry.py:566] LM output (score=-2.727546): "p : D f o f p 23 T f o f p 24 ;"
I0123 21:51:23.308466 140446420529152 alphageometry.py:567] Translation: "p = on_circle p f o, on_tline p f f o"

I0123 21:51:23.308509 140446420529152 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o m k; p = on_circle p f o, on_tline p f f o ? perp l n n e"
I0123 21:51:23.308715 140446420529152 graph.py:498] 
I0123 21:51:23.308788 140446420529152 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o m k; p = on_circle p f o, on_tline p f f o ? perp l n n e
I0123 21:51:27.579837 140446420529152 ddar.py:60] Depth 1/1000 time = 4.228190898895264
I0123 21:51:31.827823 140446420529152 ddar.py:60] Depth 2/1000 time = 4.24778938293457
I0123 21:51:36.534466 140446420529152 ddar.py:60] Depth 3/1000 time = 4.706469535827637
I0123 21:51:42.222018 140446420529152 ddar.py:60] Depth 4/1000 time = 5.687377452850342
I0123 21:51:48.013740 140446420529152 ddar.py:60] Depth 5/1000 time = 5.791510581970215
I0123 21:51:53.026636 140446420529152 ddar.py:60] Depth 6/1000 time = 5.012204647064209
I0123 21:51:59.563247 140446420529152 ddar.py:60] Depth 7/1000 time = 6.523028373718262
I0123 21:52:07.040593 140446420529152 ddar.py:60] Depth 8/1000 time = 7.477142095565796
I0123 21:52:13.853441 140446420529152 ddar.py:60] Depth 9/1000 time = 6.812640428543091
I0123 21:52:24.876649 140446420529152 ddar.py:60] Depth 10/1000 time = 11.022907257080078
I0123 21:52:35.840007 140446420529152 ddar.py:60] Depth 11/1000 time = 10.962939023971558
I0123 21:52:47.481922 140446420529152 ddar.py:60] Depth 12/1000 time = 11.6116361618042
I0123 21:52:58.490439 140446420529152 ddar.py:60] Depth 13/1000 time = 10.971675395965576
I0123 21:52:58.516024 140446420529152 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 21:52:58.516111 140446420529152 alphageometry.py:566] LM output (score=-2.739339): "p : D d o d p 23 D d o o p 24 ;"
I0123 21:52:58.516146 140446420529152 alphageometry.py:567] Translation: "p = on_circle p d o, on_circle p o d"

I0123 21:52:58.516189 140446420529152 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o m k; p = on_circle p d o, on_circle p o d ? perp l n n e"
I0123 21:52:58.516406 140446420529152 graph.py:498] 
I0123 21:52:58.516469 140446420529152 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o m k; p = on_circle p d o, on_circle p o d ? perp l n n e
I0123 21:53:02.199782 140446420529152 ddar.py:60] Depth 1/1000 time = 3.623669147491455
I0123 21:53:07.170881 140446420529152 ddar.py:60] Depth 2/1000 time = 4.970911026000977
I0123 21:53:11.908164 140446420529152 ddar.py:60] Depth 3/1000 time = 4.7371134757995605
I0123 21:53:16.881064 140446420529152 ddar.py:60] Depth 4/1000 time = 4.972733497619629
I0123 21:53:22.564761 140446420529152 ddar.py:60] Depth 5/1000 time = 5.683468580245972
I0123 21:53:28.186339 140446420529152 ddar.py:60] Depth 6/1000 time = 5.620774745941162
I0123 21:53:34.695521 140446420529152 ddar.py:60] Depth 7/1000 time = 6.497585296630859
I0123 21:53:41.199815 140446420529152 ddar.py:60] Depth 8/1000 time = 6.504075765609741
I0123 21:53:48.501061 140446420529152 ddar.py:60] Depth 9/1000 time = 7.301042795181274
I0123 21:53:59.459059 140446420529152 ddar.py:60] Depth 10/1000 time = 10.95772385597229
I0123 21:54:10.249766 140446420529152 ddar.py:60] Depth 11/1000 time = 10.790367126464844
I0123 21:54:21.208108 140446420529152 ddar.py:60] Depth 12/1000 time = 10.933140754699707
I0123 21:54:32.165648 140446420529152 ddar.py:60] Depth 13/1000 time = 10.927941083908081
I0123 21:54:32.188649 140446420529152 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 21:54:32.188767 140446420529152 alphageometry.py:566] LM output (score=-2.874189): "p : D c i c p 23 T c i c p 24 ;"
I0123 21:54:32.188805 140446420529152 alphageometry.py:567] Translation: "p = on_circle p c i, on_tline p c c i"

I0123 21:54:32.188863 140446420529152 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o m k; p = on_circle p c i, on_tline p c c i ? perp l n n e"
I0123 21:54:32.189098 140446420529152 graph.py:498] 
I0123 21:54:32.189168 140446420529152 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o m k; p = on_circle p c i, on_tline p c c i ? perp l n n e
I0123 21:54:35.818308 140446420529152 ddar.py:60] Depth 1/1000 time = 3.5846078395843506
I0123 21:54:41.039711 140446420529152 ddar.py:60] Depth 2/1000 time = 5.221210241317749
I0123 21:54:45.749696 140446420529152 ddar.py:60] Depth 3/1000 time = 4.709808349609375
I0123 21:54:50.711988 140446420529152 ddar.py:60] Depth 4/1000 time = 4.962053298950195
I0123 21:54:56.462425 140446420529152 ddar.py:60] Depth 5/1000 time = 5.750091791152954
I0123 21:55:02.179600 140446420529152 ddar.py:60] Depth 6/1000 time = 5.7164835929870605
I0123 21:55:08.140824 140446420529152 ddar.py:60] Depth 7/1000 time = 5.946127891540527
I0123 21:55:16.561634 140446420529152 ddar.py:60] Depth 8/1000 time = 8.420575857162476
I0123 21:55:24.490993 140446420529152 ddar.py:60] Depth 9/1000 time = 7.929126024246216
I0123 21:55:35.022985 140446420529152 ddar.py:60] Depth 10/1000 time = 10.531739234924316
I0123 21:55:48.382934 140446420529152 ddar.py:60] Depth 11/1000 time = 13.359591722488403
I0123 21:56:01.938836 140446420529152 ddar.py:60] Depth 12/1000 time = 13.52738642692566
I0123 21:56:14.801101 140446420529152 ddar.py:60] Depth 13/1000 time = 12.82854700088501
I0123 21:56:14.824925 140446420529152 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 21:56:14.824998 140446420529152 alphageometry.py:566] LM output (score=-2.886746): "p : D f m f p 23 T f m f p 24 ;"
I0123 21:56:14.825032 140446420529152 alphageometry.py:567] Translation: "p = on_circle p f m, on_tline p f f m"

I0123 21:56:14.825070 140446420529152 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o m k; p = on_circle p f m, on_tline p f f m ? perp l n n e"
I0123 21:56:14.825260 140446420529152 graph.py:498] 
I0123 21:56:14.825320 140446420529152 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o m k; p = on_circle p f m, on_tline p f f m ? perp l n n e
I0123 21:56:19.139935 140446420529152 ddar.py:60] Depth 1/1000 time = 4.270802736282349
I0123 21:56:23.825216 140446420529152 ddar.py:60] Depth 2/1000 time = 4.6850481033325195
I0123 21:56:28.647710 140446420529152 ddar.py:60] Depth 3/1000 time = 4.822216272354126
I0123 21:56:34.397899 140446420529152 ddar.py:60] Depth 4/1000 time = 5.750004291534424
I0123 21:56:39.475791 140446420529152 ddar.py:60] Depth 5/1000 time = 5.077684164047241
I0123 21:56:45.334991 140446420529152 ddar.py:60] Depth 6/1000 time = 5.858482599258423
I0123 21:56:51.966300 140446420529152 ddar.py:60] Depth 7/1000 time = 6.617988348007202
I0123 21:56:58.793431 140446420529152 ddar.py:60] Depth 8/1000 time = 6.826935052871704
I0123 21:57:06.582304 140446420529152 ddar.py:60] Depth 9/1000 time = 7.788670539855957
I0123 21:57:17.830056 140446420529152 ddar.py:60] Depth 10/1000 time = 11.24754548072815
I0123 21:57:28.952666 140446420529152 ddar.py:60] Depth 11/1000 time = 11.122315168380737
I0123 21:57:39.445742 140446420529152 ddar.py:60] Depth 12/1000 time = 10.461503028869629
I0123 21:57:51.353473 140446420529152 ddar.py:60] Depth 13/1000 time = 11.867702960968018
I0123 21:57:51.378316 140446420529152 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 21:57:51.378395 140446420529152 alphageometry.py:566] LM output (score=-2.892206): "p : ^ b i b p b p b m 23 ;"
I0123 21:57:51.378430 140446420529152 alphageometry.py:567] Translation: "ERROR: Invalid predicate ^ b i b p b p b m"

I0123 21:57:51.378465 140446420529152 alphageometry.py:566] LM output (score=-2.899467): "p : ^ d b d p d p d g 23 ^ g b g p g p g d 24 ;"
I0123 21:57:51.378491 140446420529152 alphageometry.py:567] Translation: "ERROR: Invalid predicate ^ d b d p d p d g"

I0123 21:57:51.378519 140446420529152 alphageometry.py:566] LM output (score=-2.908814): "p : ^ d g d p d p d e 23 ;"
I0123 21:57:51.378545 140446420529152 alphageometry.py:567] Translation: "ERROR: Invalid predicate ^ d g d p d p d e"

I0123 21:57:51.378571 140446420529152 alphageometry.py:566] LM output (score=-2.942392): "p : D k m m p 23 ;"
I0123 21:57:51.378596 140446420529152 alphageometry.py:567] Translation: "p = on_circle p m k"

I0123 21:57:51.378625 140446420529152 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o m k; p = on_circle p m k ? perp l n n e"
I0123 21:57:51.378816 140446420529152 graph.py:498] 
I0123 21:57:51.378879 140446420529152 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o m k; p = on_circle p m k ? perp l n n e
I0123 21:57:55.576868 140446420529152 ddar.py:60] Depth 1/1000 time = 4.164586067199707
I0123 21:58:00.518860 140446420529152 ddar.py:60] Depth 2/1000 time = 4.941725492477417
I0123 21:58:06.040072 140446420529152 ddar.py:60] Depth 3/1000 time = 5.5210487842559814
I0123 21:58:12.572901 140446420529152 ddar.py:60] Depth 4/1000 time = 6.532644271850586
I0123 21:58:19.208408 140446420529152 ddar.py:60] Depth 5/1000 time = 6.635290622711182
I0123 21:58:25.998077 140446420529152 ddar.py:60] Depth 6/1000 time = 6.776660442352295
I0123 21:58:34.338811 140446420529152 ddar.py:60] Depth 7/1000 time = 8.34046745300293
I0123 21:58:42.882100 140446420529152 ddar.py:60] Depth 8/1000 time = 8.54297685623169
I0123 21:58:55.194685 140446420529152 ddar.py:60] Depth 9/1000 time = 12.3123037815094
I0123 21:59:07.469306 140446420529152 ddar.py:60] Depth 10/1000 time = 12.27422285079956
I0123 21:59:19.823511 140446420529152 ddar.py:60] Depth 11/1000 time = 12.322151184082031
I0123 21:59:32.121223 140446420529152 ddar.py:60] Depth 12/1000 time = 12.260190486907959
I0123 21:59:32.144840 140446420529152 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 21:59:32.144906 140446420529152 alphageometry.py:566] LM output (score=-2.960743): "p : D d p i p 23 D f p i p 24 ;"
I0123 21:59:32.144940 140446420529152 alphageometry.py:567] Translation: "p = on_bline p i d, on_bline p i f"

I0123 21:59:32.144980 140446420529152 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o m k; p = on_bline p i d, on_bline p i f ? perp l n n e"
I0123 21:59:32.145174 140446420529152 graph.py:498] 
I0123 21:59:32.145234 140446420529152 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o m k; p = on_bline p i d, on_bline p i f ? perp l n n e
I0123 21:59:36.030515 140446420529152 ddar.py:60] Depth 1/1000 time = 3.8362436294555664
I0123 21:59:40.647915 140446420529152 ddar.py:60] Depth 2/1000 time = 4.61721134185791
I0123 21:59:46.464968 140446420529152 ddar.py:60] Depth 3/1000 time = 5.816879987716675
I0123 21:59:51.814903 140446420529152 ddar.py:60] Depth 4/1000 time = 5.34973931312561
I0123 21:59:57.800154 140446420529152 ddar.py:60] Depth 5/1000 time = 5.985053062438965
I0123 22:00:03.910964 140446420529152 ddar.py:60] Depth 6/1000 time = 6.099115371704102
I0123 22:00:11.606173 140446420529152 ddar.py:60] Depth 7/1000 time = 7.695014238357544
I0123 22:00:19.439600 140446420529152 ddar.py:60] Depth 8/1000 time = 7.833242416381836
I0123 22:00:31.198046 140446420529152 ddar.py:60] Depth 9/1000 time = 11.758201122283936
I0123 22:00:41.928878 140446420529152 ddar.py:60] Depth 10/1000 time = 10.730546951293945
I0123 22:00:53.500709 140446420529152 ddar.py:60] Depth 11/1000 time = 11.545629024505615
I0123 22:01:05.038472 140446420529152 ddar.py:60] Depth 12/1000 time = 11.508111000061035
I0123 22:01:05.062086 140446420529152 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 22:01:05.062179 140446420529152 alphageometry.py:566] LM output (score=-2.975215): "p : D f l f p 23 T f l f p 24 ;"
I0123 22:01:05.062214 140446420529152 alphageometry.py:567] Translation: "p = on_circle p f l, on_tline p f f l"

I0123 22:01:05.062266 140446420529152 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o m k; p = on_circle p f l, on_tline p f f l ? perp l n n e"
I0123 22:01:05.062483 140446420529152 graph.py:498] 
I0123 22:01:05.062543 140446420529152 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o m k; p = on_circle p f l, on_tline p f f l ? perp l n n e
I0123 22:01:09.480511 140446420529152 ddar.py:60] Depth 1/1000 time = 4.375958442687988
I0123 22:01:13.837481 140446420529152 ddar.py:60] Depth 2/1000 time = 4.35679030418396
I0123 22:01:18.740570 140446420529152 ddar.py:60] Depth 3/1000 time = 4.902921676635742
I0123 22:01:24.682261 140446420529152 ddar.py:60] Depth 4/1000 time = 5.941495418548584
I0123 22:01:29.804971 140446420529152 ddar.py:60] Depth 5/1000 time = 5.122493267059326
I0123 22:01:34.969346 140446420529152 ddar.py:60] Depth 6/1000 time = 5.163677215576172
I0123 22:01:41.764835 140446420529152 ddar.py:60] Depth 7/1000 time = 6.781600475311279
I0123 22:01:49.421444 140446420529152 ddar.py:60] Depth 8/1000 time = 7.656381130218506
I0123 22:01:56.438752 140446420529152 ddar.py:60] Depth 9/1000 time = 7.017065048217773
I0123 22:02:07.857671 140446420529152 ddar.py:60] Depth 10/1000 time = 11.418593883514404
I0123 22:02:18.460210 140446420529152 ddar.py:60] Depth 11/1000 time = 10.602190971374512
I0123 22:02:29.784886 140446420529152 ddar.py:60] Depth 12/1000 time = 11.294578790664673
I0123 22:02:41.264121 140446420529152 ddar.py:60] Depth 13/1000 time = 11.441935539245605
I0123 22:02:41.288670 140446420529152 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 22:02:41.288737 140446420529152 alphageometry.py:566] LM output (score=-2.981347): "p : D f o f p 23 D i o i p 24 ;"
I0123 22:02:41.288773 140446420529152 alphageometry.py:567] Translation: "p = on_circle p f o, on_circle p i o"

I0123 22:02:41.288814 140446420529152 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o m k; p = on_circle p f o, on_circle p i o ? perp l n n e"
I0123 22:02:41.289014 140446420529152 graph.py:498] 
I0123 22:02:41.289076 140446420529152 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o m k; p = on_circle p f o, on_circle p i o ? perp l n n e
I0123 22:02:45.078433 140446420529152 ddar.py:60] Depth 1/1000 time = 3.744032621383667
I0123 22:02:49.920764 140446420529152 ddar.py:60] Depth 2/1000 time = 4.842155456542969
I0123 22:02:55.086168 140446420529152 ddar.py:60] Depth 3/1000 time = 5.165194988250732
I0123 22:03:01.267147 140446420529152 ddar.py:60] Depth 4/1000 time = 6.180767774581909
I0123 22:03:06.670066 140446420529152 ddar.py:60] Depth 5/1000 time = 5.402734041213989
I0123 22:03:13.776487 140446420529152 ddar.py:60] Depth 6/1000 time = 7.092628002166748
I0123 22:03:21.785782 140446420529152 ddar.py:60] Depth 7/1000 time = 8.008979797363281
I0123 22:03:30.091059 140446420529152 ddar.py:60] Depth 8/1000 time = 8.305046319961548
I0123 22:03:41.310493 140446420529152 ddar.py:60] Depth 9/1000 time = 11.219107866287231
I0123 22:03:53.073223 140446420529152 ddar.py:60] Depth 10/1000 time = 11.762450218200684
I0123 22:04:04.789166 140446420529152 ddar.py:60] Depth 11/1000 time = 11.68349027633667
I0123 22:04:16.596252 140446420529152 ddar.py:60] Depth 12/1000 time = 11.766234159469604
I0123 22:04:16.619469 140446420529152 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 22:04:16.619537 140446420529152 alphageometry.py:566] LM output (score=-2.985927): "p : D b d d p 23 ;"
I0123 22:04:16.619572 140446420529152 alphageometry.py:567] Translation: "p = on_circle p d b"

I0123 22:04:16.619611 140446420529152 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o m k; p = on_circle p d b ? perp l n n e"
I0123 22:04:16.619820 140446420529152 graph.py:498] 
I0123 22:04:16.619882 140446420529152 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o m k; p = on_circle p d b ? perp l n n e
I0123 22:04:20.310424 140446420529152 ddar.py:60] Depth 1/1000 time = 3.6537580490112305
I0123 22:04:24.842244 140446420529152 ddar.py:60] Depth 2/1000 time = 4.53159236907959
I0123 22:04:30.735786 140446420529152 ddar.py:60] Depth 3/1000 time = 5.893247127532959
I0123 22:04:36.061066 140446420529152 ddar.py:60] Depth 4/1000 time = 5.325098991394043
I0123 22:04:41.417120 140446420529152 ddar.py:60] Depth 5/1000 time = 5.355867624282837
I0123 22:04:48.516646 140446420529152 ddar.py:60] Depth 6/1000 time = 7.0859763622283936
I0123 22:04:55.577095 140446420529152 ddar.py:60] Depth 7/1000 time = 7.060253858566284
I0123 22:05:03.642251 140446420529152 ddar.py:60] Depth 8/1000 time = 8.064927101135254
I0123 22:05:14.523426 140446420529152 ddar.py:60] Depth 9/1000 time = 10.880849123001099
I0123 22:05:26.111309 140446420529152 ddar.py:60] Depth 10/1000 time = 11.587617635726929
I0123 22:05:36.887964 140446420529152 ddar.py:60] Depth 11/1000 time = 10.747220754623413
I0123 22:05:48.516108 140446420529152 ddar.py:60] Depth 12/1000 time = 11.593804359436035
I0123 22:05:48.539615 140446420529152 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 22:05:48.539686 140446420529152 alphageometry.py:566] LM output (score=-3.033006): "p : D i p j p 23 ;"
I0123 22:05:48.539721 140446420529152 alphageometry.py:567] Translation: "p = on_bline p j i"

I0123 22:05:48.539763 140446420529152 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o m k; p = on_bline p j i ? perp l n n e"
I0123 22:05:48.539968 140446420529152 graph.py:498] 
I0123 22:05:48.540029 140446420529152 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o m k; p = on_bline p j i ? perp l n n e
I0123 22:05:52.241865 140446420529152 ddar.py:60] Depth 1/1000 time = 3.6623928546905518
I0123 22:05:56.647351 140446420529152 ddar.py:60] Depth 2/1000 time = 4.405308723449707
I0123 22:06:02.368296 140446420529152 ddar.py:60] Depth 3/1000 time = 5.720756530761719
I0123 22:06:07.464381 140446420529152 ddar.py:60] Depth 4/1000 time = 5.095905542373657
I0123 22:06:13.380192 140446420529152 ddar.py:60] Depth 5/1000 time = 5.915604114532471
I0123 22:06:19.414748 140446420529152 ddar.py:60] Depth 6/1000 time = 6.022174119949341
I0123 22:06:27.072231 140446420529152 ddar.py:60] Depth 7/1000 time = 7.657244443893433
I0123 22:06:34.990898 140446420529152 ddar.py:60] Depth 8/1000 time = 7.9184410572052
I0123 22:06:45.662792 140446420529152 ddar.py:60] Depth 9/1000 time = 10.671575546264648
I0123 22:06:56.998180 140446420529152 ddar.py:60] Depth 10/1000 time = 11.33512258529663
I0123 22:07:07.471891 140446420529152 ddar.py:60] Depth 11/1000 time = 10.465082883834839
I0123 22:07:18.957673 140446420529152 ddar.py:60] Depth 12/1000 time = 11.460815668106079
I0123 22:07:30.406978 140446420529152 ddar.py:60] Depth 13/1000 time = 11.417616605758667
I0123 22:07:30.429849 140446420529152 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 22:07:30.429961 140446420529152 alphageometry.py:566] LM output (score=-3.039339): "p : D f g f p 23 D g h h p 24 ;"
I0123 22:07:30.429997 140446420529152 alphageometry.py:567] Translation: "p = on_circle p f g, on_circle p h g"

I0123 22:07:30.430051 140446420529152 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o m k; p = on_circle p f g, on_circle p h g ? perp l n n e"
I0123 22:07:30.430282 140446420529152 graph.py:498] 
I0123 22:07:30.430346 140446420529152 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o m k; p = on_circle p f g, on_circle p h g ? perp l n n e
I0123 22:07:34.324259 140446420529152 ddar.py:60] Depth 1/1000 time = 3.847964286804199
I0123 22:07:39.231099 140446420529152 ddar.py:60] Depth 2/1000 time = 4.906660556793213
I0123 22:07:44.354630 140446420529152 ddar.py:60] Depth 3/1000 time = 5.123361110687256
I0123 22:07:50.657214 140446420529152 ddar.py:60] Depth 4/1000 time = 6.302333116531372
I0123 22:07:56.166023 140446420529152 ddar.py:60] Depth 5/1000 time = 5.508484601974487
I0123 22:08:03.369255 140446420529152 ddar.py:60] Depth 6/1000 time = 7.188697814941406
I0123 22:08:10.569927 140446420529152 ddar.py:60] Depth 7/1000 time = 7.200422286987305
I0123 22:08:18.697271 140446420529152 ddar.py:60] Depth 8/1000 time = 8.127030611038208
I0123 22:08:30.465019 140446420529152 ddar.py:60] Depth 9/1000 time = 11.767528057098389
I0123 22:08:41.315217 140446420529152 ddar.py:60] Depth 10/1000 time = 10.84992790222168
I0123 22:08:53.154324 140446420529152 ddar.py:60] Depth 11/1000 time = 11.80732798576355
I0123 22:09:04.985584 140446420529152 ddar.py:60] Depth 12/1000 time = 11.794816732406616
I0123 22:09:05.010627 140446420529152 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 22:09:05.010731 140446420529152 alphageometry.py:566] LM output (score=-3.062089): "p : D f l l p 23 T f l l p 24 ;"
I0123 22:09:05.010765 140446420529152 alphageometry.py:567] Translation: "p = on_circle p l f, on_tline p l f l"

I0123 22:09:05.010818 140446420529152 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o m k; p = on_circle p l f, on_tline p l f l ? perp l n n e"
I0123 22:09:05.011052 140446420529152 graph.py:498] 
I0123 22:09:05.011113 140446420529152 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e c b a, on_line e c a; f = foot f b c a; g = angle_bisector g c a b, on_line g c b; h = foot h a c b; i = on_line i b e, on_line i a g; j = lc_tangent j b i, on_line j g i; k = lc_tangent k a i, on_line k e i; l = on_line l h k, on_line l j f; m = circle m k j i; n = on_line n m d, on_line n g e; o = on_circle o m k; p = on_circle p l f, on_tline p l f l ? perp l n n e
I0123 22:09:08.006095 140446420529152 ddar.py:60] Depth 1/1000 time = 2.9528746604919434
I0123 22:09:13.627920 140446420529152 ddar.py:60] Depth 2/1000 time = 5.6216371059417725
I0123 22:09:18.623389 140446420529152 ddar.py:60] Depth 3/1000 time = 4.9952802658081055
I0123 22:09:23.972833 140446420529152 ddar.py:60] Depth 4/1000 time = 5.349210739135742
I0123 22:09:29.267574 140446420529152 ddar.py:60] Depth 5/1000 time = 5.294442176818848
I0123 22:09:35.354753 140446420529152 ddar.py:60] Depth 6/1000 time = 6.086417198181152
I0123 22:09:41.516398 140446420529152 ddar.py:60] Depth 7/1000 time = 6.147627592086792
I0123 22:09:48.400851 140446420529152 ddar.py:60] Depth 8/1000 time = 6.8841235637664795
I0123 22:09:56.269994 140446420529152 ddar.py:60] Depth 9/1000 time = 7.86895227432251
I0123 22:10:07.975037 140446420529152 ddar.py:60] Depth 10/1000 time = 11.704797983169556
I0123 22:10:18.819931 140446420529152 ddar.py:60] Depth 11/1000 time = 10.844542503356934
I0123 22:10:30.655968 140446420529152 ddar.py:60] Depth 12/1000 time = 11.805670261383057
I0123 22:10:30.693779 140446420529152 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 22:10:30.693833 140446420529152 alphageometry.py:585] Timeout.
