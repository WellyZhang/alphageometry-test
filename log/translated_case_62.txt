I0123 20:56:45.945784 139822468673536 inference_utils.py:69] Parsing gin configuration.
I0123 20:56:45.945882 139822468673536 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0123 20:56:45.946077 139822468673536 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0123 20:56:45.946110 139822468673536 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0123 20:56:45.946139 139822468673536 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0123 20:56:45.946167 139822468673536 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0123 20:56:45.946194 139822468673536 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0123 20:56:45.946220 139822468673536 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0123 20:56:45.946245 139822468673536 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0123 20:56:45.946271 139822468673536 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=32
I0123 20:56:45.946297 139822468673536 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0123 20:56:45.946326 139822468673536 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0123 20:56:45.946371 139822468673536 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0123 20:56:45.946503 139822468673536 resource_reader.py:55] Path not found: base_htrans.gin
I0123 20:56:45.946700 139822468673536 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0123 20:56:45.946797 139822468673536 resource_reader.py:55] Path not found: trainer_configuration.gin
I0123 20:56:45.953071 139822468673536 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0123 20:56:45.953189 139822468673536 resource_reader.py:55] Path not found: size/medium_150M.gin
I0123 20:56:45.953513 139822468673536 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0123 20:56:45.953616 139822468673536 resource_reader.py:55] Path not found: options/positions_t5.gin
I0123 20:56:45.953919 139822468673536 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0123 20:56:45.954020 139822468673536 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0123 20:56:45.954428 139822468673536 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0123 20:56:45.954526 139822468673536 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0123 20:56:45.958215 139822468673536 training_loop.py:334] ==== Training loop: initializing model ====
I0123 20:56:46.067638 139822468673536 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0123 20:56:46.068391 139822468673536 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0123 20:56:46.075143 139822468673536 training_loop.py:335] Process 0 of 1
I0123 20:56:46.075196 139822468673536 training_loop.py:336] Local device count = 1
I0123 20:56:46.075236 139822468673536 training_loop.py:337] Number of replicas = 1
I0123 20:56:46.075267 139822468673536 training_loop.py:339] Using random number seed 42
I0123 20:56:46.577854 139822468673536 training_loop.py:359] Initializing the model.
I0123 20:56:46.985701 139822468673536 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:46.985982 139822468673536 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 20:56:46.986086 139822468673536 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 20:56:46.986166 139822468673536 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 20:56:46.986244 139822468673536 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 20:56:46.986325 139822468673536 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 20:56:46.986396 139822468673536 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 20:56:46.986465 139822468673536 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 20:56:46.986533 139822468673536 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 20:56:46.986600 139822468673536 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 20:56:46.986666 139822468673536 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 20:56:46.986732 139822468673536 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 20:56:46.986798 139822468673536 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 20:56:46.986864 139822468673536 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 20:56:46.986902 139822468673536 attention.py:418] Single window, no scan.
I0123 20:56:46.986947 139822468673536 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 20:56:46.987059 139822468673536 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:56:46.987098 139822468673536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:56:46.987128 139822468673536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:56:46.989120 139822468673536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:46.994425 139822468673536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:56:47.005039 139822468673536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.005324 139822468673536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:56:47.009807 139822468673536 transformer_layer.py:213] tlayer: windowed attention.
I0123 20:56:47.020361 139822468673536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:56:47.020418 139822468673536 attention.py:418] Single window, no scan.
I0123 20:56:47.020457 139822468673536 transformer_layer.py:389] tlayer: self-attention.
I0123 20:56:47.020489 139822468673536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.020551 139822468673536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.021733 139822468673536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.021811 139822468673536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.022519 139822468673536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.024965 139822468673536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.030691 139822468673536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.032394 139822468673536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.032475 139822468673536 transformer_layer.py:468] tlayer: End windows.
I0123 20:56:47.032510 139822468673536 transformer_layer.py:472] tlayer: final FFN.
I0123 20:56:47.032569 139822468673536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.032695 139822468673536 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:56:47.033018 139822468673536 nn_components.py:325] mlp: activation = None
I0123 20:56:47.033066 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 20:56:47.034966 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.035067 139822468673536 nn_components.py:261] mlp: residual
I0123 20:56:47.037950 139822468673536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.038030 139822468673536 transformer_base.py:443] tbase: final FFN
I0123 20:56:47.038523 139822468673536 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:56:47.048651 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 20:56:47.057414 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.057512 139822468673536 nn_components.py:261] mlp: residual
I0123 20:56:47.057820 139822468673536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.057902 139822468673536 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 20:56:47.058012 139822468673536 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:56:47.058051 139822468673536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:56:47.058082 139822468673536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:56:47.059935 139822468673536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.062408 139822468673536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:56:47.067935 139822468673536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.068196 139822468673536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:56:47.070825 139822468673536 transformer_layer.py:213] tlayer: windowed attention.
I0123 20:56:47.074585 139822468673536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:56:47.074639 139822468673536 attention.py:418] Single window, no scan.
I0123 20:56:47.074675 139822468673536 transformer_layer.py:389] tlayer: self-attention.
I0123 20:56:47.074705 139822468673536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.074767 139822468673536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.075333 139822468673536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.075407 139822468673536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.075768 139822468673536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.076530 139822468673536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.079008 139822468673536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.079628 139822468673536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.079708 139822468673536 transformer_layer.py:468] tlayer: End windows.
I0123 20:56:47.079744 139822468673536 transformer_layer.py:472] tlayer: final FFN.
I0123 20:56:47.079801 139822468673536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.079927 139822468673536 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:56:47.080246 139822468673536 nn_components.py:325] mlp: activation = None
I0123 20:56:47.080289 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 20:56:47.082219 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.082314 139822468673536 nn_components.py:261] mlp: residual
I0123 20:56:47.084830 139822468673536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.084909 139822468673536 transformer_base.py:443] tbase: final FFN
I0123 20:56:47.085334 139822468673536 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:56:47.087655 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 20:56:47.089540 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.089638 139822468673536 nn_components.py:261] mlp: residual
I0123 20:56:47.089933 139822468673536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.090012 139822468673536 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 20:56:47.090121 139822468673536 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:56:47.090159 139822468673536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:56:47.090189 139822468673536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:56:47.092087 139822468673536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.094444 139822468673536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:56:47.100352 139822468673536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.100608 139822468673536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:56:47.103285 139822468673536 transformer_layer.py:213] tlayer: windowed attention.
I0123 20:56:47.107091 139822468673536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:56:47.107146 139822468673536 attention.py:418] Single window, no scan.
I0123 20:56:47.107181 139822468673536 transformer_layer.py:389] tlayer: self-attention.
I0123 20:56:47.107212 139822468673536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.107277 139822468673536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.107827 139822468673536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.107902 139822468673536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.108259 139822468673536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.109015 139822468673536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.111512 139822468673536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.112184 139822468673536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.112262 139822468673536 transformer_layer.py:468] tlayer: End windows.
I0123 20:56:47.112297 139822468673536 transformer_layer.py:472] tlayer: final FFN.
I0123 20:56:47.112356 139822468673536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.112482 139822468673536 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:56:47.112802 139822468673536 nn_components.py:325] mlp: activation = None
I0123 20:56:47.112844 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 20:56:47.114756 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.114850 139822468673536 nn_components.py:261] mlp: residual
I0123 20:56:47.117330 139822468673536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.117414 139822468673536 transformer_base.py:443] tbase: final FFN
I0123 20:56:47.117906 139822468673536 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:56:47.120166 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 20:56:47.122080 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.122177 139822468673536 nn_components.py:261] mlp: residual
I0123 20:56:47.122462 139822468673536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.122541 139822468673536 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 20:56:47.122649 139822468673536 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:56:47.122687 139822468673536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:56:47.122717 139822468673536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:56:47.124611 139822468673536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.127000 139822468673536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:56:47.132579 139822468673536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.132837 139822468673536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:56:47.135459 139822468673536 transformer_layer.py:213] tlayer: windowed attention.
I0123 20:56:47.139215 139822468673536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:56:47.139270 139822468673536 attention.py:418] Single window, no scan.
I0123 20:56:47.139309 139822468673536 transformer_layer.py:389] tlayer: self-attention.
I0123 20:56:47.139339 139822468673536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.139399 139822468673536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.139952 139822468673536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.140030 139822468673536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.140389 139822468673536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.141148 139822468673536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.143714 139822468673536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.144334 139822468673536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.144415 139822468673536 transformer_layer.py:468] tlayer: End windows.
I0123 20:56:47.144449 139822468673536 transformer_layer.py:472] tlayer: final FFN.
I0123 20:56:47.144507 139822468673536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.144639 139822468673536 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:56:47.144958 139822468673536 nn_components.py:325] mlp: activation = None
I0123 20:56:47.145000 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 20:56:47.146893 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.146989 139822468673536 nn_components.py:261] mlp: residual
I0123 20:56:47.149533 139822468673536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.149616 139822468673536 transformer_base.py:443] tbase: final FFN
I0123 20:56:47.150047 139822468673536 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:56:47.152287 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 20:56:47.154181 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.154281 139822468673536 nn_components.py:261] mlp: residual
I0123 20:56:47.154573 139822468673536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.154653 139822468673536 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 20:56:47.154761 139822468673536 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:56:47.154800 139822468673536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:56:47.154830 139822468673536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:56:47.156712 139822468673536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.159068 139822468673536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:56:47.164673 139822468673536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.164941 139822468673536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:56:47.167667 139822468673536 transformer_layer.py:213] tlayer: windowed attention.
I0123 20:56:47.171369 139822468673536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:56:47.171424 139822468673536 attention.py:418] Single window, no scan.
I0123 20:56:47.171459 139822468673536 transformer_layer.py:389] tlayer: self-attention.
I0123 20:56:47.171489 139822468673536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.171550 139822468673536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.172116 139822468673536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.172192 139822468673536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.172546 139822468673536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.173320 139822468673536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.176210 139822468673536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.176835 139822468673536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.176927 139822468673536 transformer_layer.py:468] tlayer: End windows.
I0123 20:56:47.176962 139822468673536 transformer_layer.py:472] tlayer: final FFN.
I0123 20:56:47.177021 139822468673536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.177160 139822468673536 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:56:47.177480 139822468673536 nn_components.py:325] mlp: activation = None
I0123 20:56:47.177523 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 20:56:47.179403 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.179496 139822468673536 nn_components.py:261] mlp: residual
I0123 20:56:47.182024 139822468673536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.182102 139822468673536 transformer_base.py:443] tbase: final FFN
I0123 20:56:47.182523 139822468673536 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:56:47.184758 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 20:56:47.186705 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.186801 139822468673536 nn_components.py:261] mlp: residual
I0123 20:56:47.187097 139822468673536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.187177 139822468673536 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 20:56:47.187286 139822468673536 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:56:47.187324 139822468673536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:56:47.187355 139822468673536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:56:47.189180 139822468673536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.191541 139822468673536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:56:47.197098 139822468673536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.197352 139822468673536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:56:47.200024 139822468673536 transformer_layer.py:213] tlayer: windowed attention.
I0123 20:56:47.203719 139822468673536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:56:47.203775 139822468673536 attention.py:418] Single window, no scan.
I0123 20:56:47.203810 139822468673536 transformer_layer.py:389] tlayer: self-attention.
I0123 20:56:47.203842 139822468673536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.203904 139822468673536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.204509 139822468673536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.204585 139822468673536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.204944 139822468673536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.205729 139822468673536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.208205 139822468673536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.208818 139822468673536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.208895 139822468673536 transformer_layer.py:468] tlayer: End windows.
I0123 20:56:47.208930 139822468673536 transformer_layer.py:472] tlayer: final FFN.
I0123 20:56:47.208993 139822468673536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.209118 139822468673536 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:56:47.209435 139822468673536 nn_components.py:325] mlp: activation = None
I0123 20:56:47.209477 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 20:56:47.212182 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.212339 139822468673536 nn_components.py:261] mlp: residual
I0123 20:56:47.214966 139822468673536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.215047 139822468673536 transformer_base.py:443] tbase: final FFN
I0123 20:56:47.215479 139822468673536 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:56:47.217835 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 20:56:47.219772 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.219868 139822468673536 nn_components.py:261] mlp: residual
I0123 20:56:47.220161 139822468673536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.220244 139822468673536 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 20:56:47.220357 139822468673536 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:56:47.220396 139822468673536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:56:47.220427 139822468673536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:56:47.222288 139822468673536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.224770 139822468673536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:56:47.230440 139822468673536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.230711 139822468673536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:56:47.233363 139822468673536 transformer_layer.py:213] tlayer: windowed attention.
I0123 20:56:47.237234 139822468673536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:56:47.237292 139822468673536 attention.py:418] Single window, no scan.
I0123 20:56:47.237329 139822468673536 transformer_layer.py:389] tlayer: self-attention.
I0123 20:56:47.237360 139822468673536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.237421 139822468673536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.238008 139822468673536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.238089 139822468673536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.238465 139822468673536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.239227 139822468673536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.241731 139822468673536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.242352 139822468673536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.242433 139822468673536 transformer_layer.py:468] tlayer: End windows.
I0123 20:56:47.242468 139822468673536 transformer_layer.py:472] tlayer: final FFN.
I0123 20:56:47.242526 139822468673536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.242655 139822468673536 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:56:47.242975 139822468673536 nn_components.py:325] mlp: activation = None
I0123 20:56:47.243019 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 20:56:47.244979 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.245074 139822468673536 nn_components.py:261] mlp: residual
I0123 20:56:47.247590 139822468673536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.247671 139822468673536 transformer_base.py:443] tbase: final FFN
I0123 20:56:47.248099 139822468673536 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:56:47.250759 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 20:56:47.252676 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.252780 139822468673536 nn_components.py:261] mlp: residual
I0123 20:56:47.253075 139822468673536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.253161 139822468673536 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 20:56:47.253273 139822468673536 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:56:47.253313 139822468673536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:56:47.253345 139822468673536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:56:47.390925 139822468673536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.394015 139822468673536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:56:47.399857 139822468673536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.400149 139822468673536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:56:47.402842 139822468673536 transformer_layer.py:213] tlayer: windowed attention.
I0123 20:56:47.406722 139822468673536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:56:47.406782 139822468673536 attention.py:418] Single window, no scan.
I0123 20:56:47.406820 139822468673536 transformer_layer.py:389] tlayer: self-attention.
I0123 20:56:47.406853 139822468673536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.406920 139822468673536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.407537 139822468673536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.407614 139822468673536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.407982 139822468673536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.408774 139822468673536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.411433 139822468673536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.412079 139822468673536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.412158 139822468673536 transformer_layer.py:468] tlayer: End windows.
I0123 20:56:47.412195 139822468673536 transformer_layer.py:472] tlayer: final FFN.
I0123 20:56:47.412276 139822468673536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.412408 139822468673536 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:56:47.412748 139822468673536 nn_components.py:325] mlp: activation = None
I0123 20:56:47.412792 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 20:56:47.414742 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.414840 139822468673536 nn_components.py:261] mlp: residual
I0123 20:56:47.417439 139822468673536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.417519 139822468673536 transformer_base.py:443] tbase: final FFN
I0123 20:56:47.417973 139822468673536 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:56:47.420304 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 20:56:47.422250 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.422357 139822468673536 nn_components.py:261] mlp: residual
I0123 20:56:47.422656 139822468673536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.422740 139822468673536 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 20:56:47.422853 139822468673536 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:56:47.422893 139822468673536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:56:47.422925 139822468673536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:56:47.424890 139822468673536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.427307 139822468673536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:56:47.432980 139822468673536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.433251 139822468673536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:56:47.435987 139822468673536 transformer_layer.py:213] tlayer: windowed attention.
I0123 20:56:47.439773 139822468673536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:56:47.439829 139822468673536 attention.py:418] Single window, no scan.
I0123 20:56:47.439866 139822468673536 transformer_layer.py:389] tlayer: self-attention.
I0123 20:56:47.439898 139822468673536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.439961 139822468673536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.440528 139822468673536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.440604 139822468673536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.440970 139822468673536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.441749 139822468673536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.444361 139822468673536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.444988 139822468673536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.445065 139822468673536 transformer_layer.py:468] tlayer: End windows.
I0123 20:56:47.445100 139822468673536 transformer_layer.py:472] tlayer: final FFN.
I0123 20:56:47.445160 139822468673536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.445287 139822468673536 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:56:47.445608 139822468673536 nn_components.py:325] mlp: activation = None
I0123 20:56:47.445658 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 20:56:47.447609 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.447704 139822468673536 nn_components.py:261] mlp: residual
I0123 20:56:47.450272 139822468673536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.450351 139822468673536 transformer_base.py:443] tbase: final FFN
I0123 20:56:47.450778 139822468673536 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:56:47.453040 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 20:56:47.454998 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.455093 139822468673536 nn_components.py:261] mlp: residual
I0123 20:56:47.455381 139822468673536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.455467 139822468673536 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 20:56:47.455580 139822468673536 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:56:47.455620 139822468673536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:56:47.455651 139822468673536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:56:47.457469 139822468673536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.459905 139822468673536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:56:47.465608 139822468673536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.465881 139822468673536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:56:47.468887 139822468673536 transformer_layer.py:213] tlayer: windowed attention.
I0123 20:56:47.472596 139822468673536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:56:47.472652 139822468673536 attention.py:418] Single window, no scan.
I0123 20:56:47.472688 139822468673536 transformer_layer.py:389] tlayer: self-attention.
I0123 20:56:47.472718 139822468673536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.472779 139822468673536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.473382 139822468673536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.473460 139822468673536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.473830 139822468673536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.474599 139822468673536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.477077 139822468673536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.477708 139822468673536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.477786 139822468673536 transformer_layer.py:468] tlayer: End windows.
I0123 20:56:47.477820 139822468673536 transformer_layer.py:472] tlayer: final FFN.
I0123 20:56:47.477878 139822468673536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.478003 139822468673536 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:56:47.478322 139822468673536 nn_components.py:325] mlp: activation = None
I0123 20:56:47.478365 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 20:56:47.480261 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.480354 139822468673536 nn_components.py:261] mlp: residual
I0123 20:56:47.482887 139822468673536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.482966 139822468673536 transformer_base.py:443] tbase: final FFN
I0123 20:56:47.483398 139822468673536 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:56:47.485710 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 20:56:47.487618 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.487714 139822468673536 nn_components.py:261] mlp: residual
I0123 20:56:47.488009 139822468673536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.488095 139822468673536 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 20:56:47.488208 139822468673536 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:56:47.488249 139822468673536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:56:47.488280 139822468673536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:56:47.490111 139822468673536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.492548 139822468673536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:56:47.498128 139822468673536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.498387 139822468673536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:56:47.501017 139822468673536 transformer_layer.py:213] tlayer: windowed attention.
I0123 20:56:47.504758 139822468673536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:56:47.504813 139822468673536 attention.py:418] Single window, no scan.
I0123 20:56:47.504848 139822468673536 transformer_layer.py:389] tlayer: self-attention.
I0123 20:56:47.504880 139822468673536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.504942 139822468673536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.505508 139822468673536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.505584 139822468673536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.505949 139822468673536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.506710 139822468673536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.509180 139822468673536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.509816 139822468673536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.509895 139822468673536 transformer_layer.py:468] tlayer: End windows.
I0123 20:56:47.509929 139822468673536 transformer_layer.py:472] tlayer: final FFN.
I0123 20:56:47.509988 139822468673536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.510117 139822468673536 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:56:47.510432 139822468673536 nn_components.py:325] mlp: activation = None
I0123 20:56:47.510478 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 20:56:47.512410 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.512507 139822468673536 nn_components.py:261] mlp: residual
I0123 20:56:47.515272 139822468673536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.515353 139822468673536 transformer_base.py:443] tbase: final FFN
I0123 20:56:47.515770 139822468673536 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:56:47.518098 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 20:56:47.520027 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.520121 139822468673536 nn_components.py:261] mlp: residual
I0123 20:56:47.520416 139822468673536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.520496 139822468673536 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 20:56:47.520611 139822468673536 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:56:47.520651 139822468673536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:56:47.520682 139822468673536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:56:47.522567 139822468673536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.524942 139822468673536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:56:47.530514 139822468673536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.530777 139822468673536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:56:47.533388 139822468673536 transformer_layer.py:213] tlayer: windowed attention.
I0123 20:56:47.537151 139822468673536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:56:47.537209 139822468673536 attention.py:418] Single window, no scan.
I0123 20:56:47.537246 139822468673536 transformer_layer.py:389] tlayer: self-attention.
I0123 20:56:47.537277 139822468673536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.537339 139822468673536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.537909 139822468673536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.537986 139822468673536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.538340 139822468673536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.539110 139822468673536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.541567 139822468673536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.542552 139822468673536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.542635 139822468673536 transformer_layer.py:468] tlayer: End windows.
I0123 20:56:47.542672 139822468673536 transformer_layer.py:472] tlayer: final FFN.
I0123 20:56:47.542734 139822468673536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.542861 139822468673536 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:56:47.543183 139822468673536 nn_components.py:325] mlp: activation = None
I0123 20:56:47.543228 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 20:56:47.545146 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.545241 139822468673536 nn_components.py:261] mlp: residual
I0123 20:56:47.547791 139822468673536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.547872 139822468673536 transformer_base.py:443] tbase: final FFN
I0123 20:56:47.548357 139822468673536 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:56:47.550598 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 20:56:47.552476 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.552569 139822468673536 nn_components.py:261] mlp: residual
I0123 20:56:47.552854 139822468673536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.553135 139822468673536 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 20:56:47.553205 139822468673536 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 20:56:47.553270 139822468673536 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 20:56:47.553326 139822468673536 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 20:56:47.553381 139822468673536 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 20:56:47.553434 139822468673536 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 20:56:47.553486 139822468673536 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 20:56:47.553538 139822468673536 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 20:56:47.553590 139822468673536 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 20:56:47.553647 139822468673536 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 20:56:47.553702 139822468673536 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 20:56:47.553756 139822468673536 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 20:56:47.553794 139822468673536 decoder_stack.py:344] dstack: Final layernorm.
I0123 20:56:47.557289 139822468673536 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:56:47.604859 139822468673536 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.604945 139822468673536 decoder_stack.py:333] dstack: autoregressive generator.
I0123 20:56:47.605000 139822468673536 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 20:56:47.605105 139822468673536 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:56:47.605144 139822468673536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:56:47.605174 139822468673536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:56:47.605238 139822468673536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.607681 139822468673536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:56:47.613164 139822468673536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.613424 139822468673536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:56:47.616067 139822468673536 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:56:47.632597 139822468673536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:56:47.632653 139822468673536 attention.py:418] Single window, no scan.
I0123 20:56:47.632689 139822468673536 transformer_layer.py:389] tlayer: self-attention.
I0123 20:56:47.632721 139822468673536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.632784 139822468673536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.633921 139822468673536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.634000 139822468673536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.634713 139822468673536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.636694 139822468673536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.641443 139822468673536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.642760 139822468673536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.642849 139822468673536 transformer_layer.py:468] tlayer: End windows.
I0123 20:56:47.642886 139822468673536 transformer_layer.py:472] tlayer: final FFN.
I0123 20:56:47.642947 139822468673536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.643079 139822468673536 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:56:47.643188 139822468673536 nn_components.py:325] mlp: activation = None
I0123 20:56:47.643228 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 20:56:47.645106 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.645200 139822468673536 nn_components.py:261] mlp: residual
I0123 20:56:47.647638 139822468673536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.647718 139822468673536 transformer_base.py:443] tbase: final FFN
I0123 20:56:47.647825 139822468673536 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:56:47.650052 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 20:56:47.652002 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.652099 139822468673536 nn_components.py:261] mlp: residual
I0123 20:56:47.652389 139822468673536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.652472 139822468673536 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 20:56:47.652581 139822468673536 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:56:47.652621 139822468673536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:56:47.652652 139822468673536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:56:47.652718 139822468673536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.654982 139822468673536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:56:47.660453 139822468673536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.660715 139822468673536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:56:47.663400 139822468673536 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:56:47.676472 139822468673536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:56:47.676528 139822468673536 attention.py:418] Single window, no scan.
I0123 20:56:47.676564 139822468673536 transformer_layer.py:389] tlayer: self-attention.
I0123 20:56:47.676596 139822468673536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.676661 139822468673536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.677219 139822468673536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.677296 139822468673536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.677652 139822468673536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.678336 139822468673536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.680817 139822468673536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.681434 139822468673536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.681510 139822468673536 transformer_layer.py:468] tlayer: End windows.
I0123 20:56:47.681550 139822468673536 transformer_layer.py:472] tlayer: final FFN.
I0123 20:56:47.681609 139822468673536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.681747 139822468673536 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:56:47.681856 139822468673536 nn_components.py:325] mlp: activation = None
I0123 20:56:47.681895 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 20:56:47.683795 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.683887 139822468673536 nn_components.py:261] mlp: residual
I0123 20:56:47.686318 139822468673536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.686398 139822468673536 transformer_base.py:443] tbase: final FFN
I0123 20:56:47.686505 139822468673536 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:56:47.688697 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 20:56:47.690616 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.690711 139822468673536 nn_components.py:261] mlp: residual
I0123 20:56:47.690998 139822468673536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.691079 139822468673536 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 20:56:47.691186 139822468673536 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:56:47.691224 139822468673536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:56:47.691255 139822468673536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:56:47.691320 139822468673536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.693557 139822468673536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:56:47.698947 139822468673536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.699202 139822468673536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:56:47.701869 139822468673536 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:56:47.714371 139822468673536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:56:47.714431 139822468673536 attention.py:418] Single window, no scan.
I0123 20:56:47.714468 139822468673536 transformer_layer.py:389] tlayer: self-attention.
I0123 20:56:47.714500 139822468673536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.714562 139822468673536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.715106 139822468673536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.715181 139822468673536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.715541 139822468673536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.716220 139822468673536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.718703 139822468673536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.719315 139822468673536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.719394 139822468673536 transformer_layer.py:468] tlayer: End windows.
I0123 20:56:47.719429 139822468673536 transformer_layer.py:472] tlayer: final FFN.
I0123 20:56:47.719491 139822468673536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.719617 139822468673536 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:56:47.719723 139822468673536 nn_components.py:325] mlp: activation = None
I0123 20:56:47.719761 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 20:56:47.721669 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.721764 139822468673536 nn_components.py:261] mlp: residual
I0123 20:56:47.724152 139822468673536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.724230 139822468673536 transformer_base.py:443] tbase: final FFN
I0123 20:56:47.724336 139822468673536 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:56:47.726535 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 20:56:47.728428 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.728524 139822468673536 nn_components.py:261] mlp: residual
I0123 20:56:47.728807 139822468673536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.728888 139822468673536 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 20:56:47.728997 139822468673536 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:56:47.729037 139822468673536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:56:47.729068 139822468673536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:56:47.729132 139822468673536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.731363 139822468673536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:56:47.736783 139822468673536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.737038 139822468673536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:56:47.739705 139822468673536 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:56:47.752291 139822468673536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:56:47.752346 139822468673536 attention.py:418] Single window, no scan.
I0123 20:56:47.752382 139822468673536 transformer_layer.py:389] tlayer: self-attention.
I0123 20:56:47.752412 139822468673536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.752474 139822468673536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.753030 139822468673536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.753105 139822468673536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.753454 139822468673536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.754145 139822468673536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.756600 139822468673536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.757219 139822468673536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.757296 139822468673536 transformer_layer.py:468] tlayer: End windows.
I0123 20:56:47.757330 139822468673536 transformer_layer.py:472] tlayer: final FFN.
I0123 20:56:47.757388 139822468673536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.757525 139822468673536 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:56:47.757634 139822468673536 nn_components.py:325] mlp: activation = None
I0123 20:56:47.757679 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 20:56:47.759599 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.759692 139822468673536 nn_components.py:261] mlp: residual
I0123 20:56:47.762079 139822468673536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.762158 139822468673536 transformer_base.py:443] tbase: final FFN
I0123 20:56:47.762266 139822468673536 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:56:47.764469 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 20:56:47.766328 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.766421 139822468673536 nn_components.py:261] mlp: residual
I0123 20:56:47.766705 139822468673536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.766784 139822468673536 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 20:56:47.766893 139822468673536 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:56:47.766931 139822468673536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:56:47.766962 139822468673536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:56:47.767024 139822468673536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.769598 139822468673536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:56:47.775008 139822468673536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.775269 139822468673536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:56:47.777863 139822468673536 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:56:47.790698 139822468673536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:56:47.790754 139822468673536 attention.py:418] Single window, no scan.
I0123 20:56:47.790790 139822468673536 transformer_layer.py:389] tlayer: self-attention.
I0123 20:56:47.790821 139822468673536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.790887 139822468673536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.791445 139822468673536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.791523 139822468673536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.791885 139822468673536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.792576 139822468673536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.795445 139822468673536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.796225 139822468673536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.796301 139822468673536 transformer_layer.py:468] tlayer: End windows.
I0123 20:56:47.796335 139822468673536 transformer_layer.py:472] tlayer: final FFN.
I0123 20:56:47.796392 139822468673536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.796526 139822468673536 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:56:47.796640 139822468673536 nn_components.py:325] mlp: activation = None
I0123 20:56:47.796678 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 20:56:47.798551 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.798645 139822468673536 nn_components.py:261] mlp: residual
I0123 20:56:47.801045 139822468673536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.801124 139822468673536 transformer_base.py:443] tbase: final FFN
I0123 20:56:47.801233 139822468673536 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:56:47.803490 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 20:56:47.805332 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.805427 139822468673536 nn_components.py:261] mlp: residual
I0123 20:56:47.805719 139822468673536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.805800 139822468673536 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 20:56:47.805909 139822468673536 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:56:47.805949 139822468673536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:56:47.805980 139822468673536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:56:47.806044 139822468673536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.808274 139822468673536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:56:47.813632 139822468673536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.813891 139822468673536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:56:47.816542 139822468673536 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:56:47.829007 139822468673536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:56:47.829062 139822468673536 attention.py:418] Single window, no scan.
I0123 20:56:47.829097 139822468673536 transformer_layer.py:389] tlayer: self-attention.
I0123 20:56:47.829127 139822468673536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.829187 139822468673536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.829750 139822468673536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.829826 139822468673536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.830187 139822468673536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.830880 139822468673536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.833348 139822468673536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.833962 139822468673536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.834043 139822468673536 transformer_layer.py:468] tlayer: End windows.
I0123 20:56:47.834077 139822468673536 transformer_layer.py:472] tlayer: final FFN.
I0123 20:56:47.834135 139822468673536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.834266 139822468673536 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:56:47.834381 139822468673536 nn_components.py:325] mlp: activation = None
I0123 20:56:47.834419 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 20:56:47.836325 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.836417 139822468673536 nn_components.py:261] mlp: residual
I0123 20:56:47.838835 139822468673536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.838916 139822468673536 transformer_base.py:443] tbase: final FFN
I0123 20:56:47.839022 139822468673536 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:56:47.841216 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 20:56:47.843044 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.843140 139822468673536 nn_components.py:261] mlp: residual
I0123 20:56:47.843429 139822468673536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.843510 139822468673536 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 20:56:47.843621 139822468673536 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:56:47.843660 139822468673536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:56:47.843692 139822468673536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:56:47.843755 139822468673536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.845985 139822468673536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:56:47.851454 139822468673536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.851709 139822468673536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:56:47.854293 139822468673536 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:56:47.872153 139822468673536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:56:47.872235 139822468673536 attention.py:418] Single window, no scan.
I0123 20:56:47.872275 139822468673536 transformer_layer.py:389] tlayer: self-attention.
I0123 20:56:47.872308 139822468673536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.872389 139822468673536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.873005 139822468673536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.873080 139822468673536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.873460 139822468673536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.874173 139822468673536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.876721 139822468673536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.877727 139822468673536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.877805 139822468673536 transformer_layer.py:468] tlayer: End windows.
I0123 20:56:47.877840 139822468673536 transformer_layer.py:472] tlayer: final FFN.
I0123 20:56:47.877904 139822468673536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.878031 139822468673536 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:56:47.878144 139822468673536 nn_components.py:325] mlp: activation = None
I0123 20:56:47.878186 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 20:56:47.880181 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.880275 139822468673536 nn_components.py:261] mlp: residual
I0123 20:56:47.882754 139822468673536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.882834 139822468673536 transformer_base.py:443] tbase: final FFN
I0123 20:56:47.882945 139822468673536 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:56:47.885198 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 20:56:47.887217 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.887315 139822468673536 nn_components.py:261] mlp: residual
I0123 20:56:47.887600 139822468673536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.887684 139822468673536 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 20:56:47.887796 139822468673536 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:56:47.887837 139822468673536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:56:47.887868 139822468673536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:56:47.887933 139822468673536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.890194 139822468673536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:56:47.895856 139822468673536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.896124 139822468673536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:56:47.898839 139822468673536 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:56:47.911469 139822468673536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:56:47.911524 139822468673536 attention.py:418] Single window, no scan.
I0123 20:56:47.911560 139822468673536 transformer_layer.py:389] tlayer: self-attention.
I0123 20:56:47.911590 139822468673536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.911652 139822468673536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.912250 139822468673536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.912325 139822468673536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.912681 139822468673536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.913369 139822468673536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.915864 139822468673536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.916471 139822468673536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.916546 139822468673536 transformer_layer.py:468] tlayer: End windows.
I0123 20:56:47.916580 139822468673536 transformer_layer.py:472] tlayer: final FFN.
I0123 20:56:47.916637 139822468673536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.916765 139822468673536 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:56:47.916871 139822468673536 nn_components.py:325] mlp: activation = None
I0123 20:56:47.916915 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 20:56:47.918801 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.918894 139822468673536 nn_components.py:261] mlp: residual
I0123 20:56:47.921345 139822468673536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.921423 139822468673536 transformer_base.py:443] tbase: final FFN
I0123 20:56:47.921528 139822468673536 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:56:47.923732 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 20:56:47.925586 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.925688 139822468673536 nn_components.py:261] mlp: residual
I0123 20:56:47.925975 139822468673536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.926057 139822468673536 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 20:56:47.926165 139822468673536 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:56:47.926203 139822468673536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:56:47.926234 139822468673536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:56:47.926297 139822468673536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.928531 139822468673536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:56:47.934027 139822468673536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.934285 139822468673536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:56:47.936919 139822468673536 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:56:47.949541 139822468673536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:56:47.949596 139822468673536 attention.py:418] Single window, no scan.
I0123 20:56:47.949631 139822468673536 transformer_layer.py:389] tlayer: self-attention.
I0123 20:56:47.949670 139822468673536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.949733 139822468673536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.950293 139822468673536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.950370 139822468673536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.950729 139822468673536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.951421 139822468673536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.953914 139822468673536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.954572 139822468673536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.954650 139822468673536 transformer_layer.py:468] tlayer: End windows.
I0123 20:56:47.954686 139822468673536 transformer_layer.py:472] tlayer: final FFN.
I0123 20:56:47.954744 139822468673536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.954875 139822468673536 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:56:47.954984 139822468673536 nn_components.py:325] mlp: activation = None
I0123 20:56:47.955021 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 20:56:47.956880 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.956973 139822468673536 nn_components.py:261] mlp: residual
I0123 20:56:47.959368 139822468673536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.959447 139822468673536 transformer_base.py:443] tbase: final FFN
I0123 20:56:47.959553 139822468673536 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:56:47.961750 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 20:56:47.963672 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.963767 139822468673536 nn_components.py:261] mlp: residual
I0123 20:56:47.964052 139822468673536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.964133 139822468673536 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 20:56:47.964242 139822468673536 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:56:47.964281 139822468673536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:56:47.964312 139822468673536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:56:47.964375 139822468673536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.966748 139822468673536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:56:47.972116 139822468673536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.972375 139822468673536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:56:47.975038 139822468673536 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:56:47.987865 139822468673536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:56:47.987920 139822468673536 attention.py:418] Single window, no scan.
I0123 20:56:47.987956 139822468673536 transformer_layer.py:389] tlayer: self-attention.
I0123 20:56:47.987986 139822468673536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.988047 139822468673536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.988651 139822468673536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.988728 139822468673536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.989084 139822468673536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.989803 139822468673536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.992302 139822468673536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.992933 139822468673536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.993011 139822468673536 transformer_layer.py:468] tlayer: End windows.
I0123 20:56:47.993046 139822468673536 transformer_layer.py:472] tlayer: final FFN.
I0123 20:56:47.993105 139822468673536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.993235 139822468673536 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:56:47.993343 139822468673536 nn_components.py:325] mlp: activation = None
I0123 20:56:47.993382 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 20:56:47.995274 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.995373 139822468673536 nn_components.py:261] mlp: residual
I0123 20:56:47.997840 139822468673536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:47.997924 139822468673536 transformer_base.py:443] tbase: final FFN
I0123 20:56:47.998033 139822468673536 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:56:48.000265 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 20:56:48.002142 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:48.002238 139822468673536 nn_components.py:261] mlp: residual
I0123 20:56:48.002527 139822468673536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:48.002609 139822468673536 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 20:56:48.002718 139822468673536 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:56:48.002757 139822468673536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:56:48.002788 139822468673536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:56:48.002850 139822468673536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:48.005111 139822468673536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:56:48.010612 139822468673536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:48.010867 139822468673536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:56:48.013521 139822468673536 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:56:48.026192 139822468673536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:56:48.026247 139822468673536 attention.py:418] Single window, no scan.
I0123 20:56:48.026284 139822468673536 transformer_layer.py:389] tlayer: self-attention.
I0123 20:56:48.026314 139822468673536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:48.026377 139822468673536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:48.026925 139822468673536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:48.027000 139822468673536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:48.027356 139822468673536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:48.028043 139822468673536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:48.030569 139822468673536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:48.031227 139822468673536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:48.031303 139822468673536 transformer_layer.py:468] tlayer: End windows.
I0123 20:56:48.031338 139822468673536 transformer_layer.py:472] tlayer: final FFN.
I0123 20:56:48.031396 139822468673536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:48.031527 139822468673536 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:56:48.031639 139822468673536 nn_components.py:325] mlp: activation = None
I0123 20:56:48.031678 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 20:56:48.033553 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:48.033656 139822468673536 nn_components.py:261] mlp: residual
I0123 20:56:48.036119 139822468673536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:48.036198 139822468673536 transformer_base.py:443] tbase: final FFN
I0123 20:56:48.036306 139822468673536 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:56:48.038538 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 20:56:48.040468 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:48.040564 139822468673536 nn_components.py:261] mlp: residual
I0123 20:56:48.040853 139822468673536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:48.040935 139822468673536 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 20:56:48.041045 139822468673536 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:56:48.041085 139822468673536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:56:48.041117 139822468673536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:56:48.041179 139822468673536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:48.043424 139822468673536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:56:48.048869 139822468673536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:48.049127 139822468673536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:56:48.051864 139822468673536 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:56:48.064679 139822468673536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:56:48.064734 139822468673536 attention.py:418] Single window, no scan.
I0123 20:56:48.064770 139822468673536 transformer_layer.py:389] tlayer: self-attention.
I0123 20:56:48.064800 139822468673536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:48.064863 139822468673536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:48.065425 139822468673536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:48.065503 139822468673536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:48.066006 139822468673536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:48.066759 139822468673536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:48.069313 139822468673536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:48.069945 139822468673536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:48.070024 139822468673536 transformer_layer.py:468] tlayer: End windows.
I0123 20:56:48.070059 139822468673536 transformer_layer.py:472] tlayer: final FFN.
I0123 20:56:48.070118 139822468673536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:48.070250 139822468673536 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:56:48.070367 139822468673536 nn_components.py:325] mlp: activation = None
I0123 20:56:48.070406 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 20:56:48.072309 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:48.072403 139822468673536 nn_components.py:261] mlp: residual
I0123 20:56:48.074833 139822468673536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:48.074913 139822468673536 transformer_base.py:443] tbase: final FFN
I0123 20:56:48.075027 139822468673536 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:56:48.077308 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 20:56:48.079172 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:48.079269 139822468673536 nn_components.py:261] mlp: residual
I0123 20:56:48.079552 139822468673536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:48.079640 139822468673536 decoder_stack.py:344] dstack: Final layernorm.
I0123 20:56:48.082531 139822468673536 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:56:48.137951 139822468673536 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.138035 139822468673536 decoder_stack.py:333] dstack: autoregressive generator.
I0123 20:56:48.138090 139822468673536 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 20:56:48.138195 139822468673536 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:56:48.138234 139822468673536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:56:48.138266 139822468673536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:56:48.138329 139822468673536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.140990 139822468673536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:56:48.146324 139822468673536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.146585 139822468673536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:56:48.149145 139822468673536 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:56:48.161423 139822468673536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:56:48.161478 139822468673536 attention.py:418] Single window, no scan.
I0123 20:56:48.161513 139822468673536 transformer_layer.py:389] tlayer: self-attention.
I0123 20:56:48.161543 139822468673536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.161604 139822468673536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.162160 139822468673536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.162236 139822468673536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.162590 139822468673536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.163259 139822468673536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.165754 139822468673536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.166368 139822468673536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.166447 139822468673536 transformer_layer.py:468] tlayer: End windows.
I0123 20:56:48.166483 139822468673536 transformer_layer.py:472] tlayer: final FFN.
I0123 20:56:48.166544 139822468673536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.166677 139822468673536 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:56:48.166793 139822468673536 nn_components.py:325] mlp: activation = None
I0123 20:56:48.166833 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 20:56:48.168682 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.168777 139822468673536 nn_components.py:261] mlp: residual
I0123 20:56:48.171186 139822468673536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.171267 139822468673536 transformer_base.py:443] tbase: final FFN
I0123 20:56:48.171376 139822468673536 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:56:48.173615 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 20:56:48.175503 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.175598 139822468673536 nn_components.py:261] mlp: residual
I0123 20:56:48.175883 139822468673536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.175965 139822468673536 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 20:56:48.176074 139822468673536 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:56:48.176113 139822468673536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:56:48.176146 139822468673536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:56:48.176209 139822468673536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.178473 139822468673536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:56:48.183889 139822468673536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.184150 139822468673536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:56:48.186840 139822468673536 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:56:48.199208 139822468673536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:56:48.199266 139822468673536 attention.py:418] Single window, no scan.
I0123 20:56:48.199303 139822468673536 transformer_layer.py:389] tlayer: self-attention.
I0123 20:56:48.199337 139822468673536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.199401 139822468673536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.199959 139822468673536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.200035 139822468673536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.200393 139822468673536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.201082 139822468673536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.203618 139822468673536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.204234 139822468673536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.204312 139822468673536 transformer_layer.py:468] tlayer: End windows.
I0123 20:56:48.204347 139822468673536 transformer_layer.py:472] tlayer: final FFN.
I0123 20:56:48.204408 139822468673536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.204536 139822468673536 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:56:48.204645 139822468673536 nn_components.py:325] mlp: activation = None
I0123 20:56:48.204688 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 20:56:48.206557 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.206651 139822468673536 nn_components.py:261] mlp: residual
I0123 20:56:48.209059 139822468673536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.209138 139822468673536 transformer_base.py:443] tbase: final FFN
I0123 20:56:48.209250 139822468673536 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:56:48.211537 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 20:56:48.213399 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.213496 139822468673536 nn_components.py:261] mlp: residual
I0123 20:56:48.213792 139822468673536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.213875 139822468673536 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 20:56:48.213984 139822468673536 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:56:48.214024 139822468673536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:56:48.214056 139822468673536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:56:48.214122 139822468673536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.216358 139822468673536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:56:48.221783 139822468673536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.222041 139822468673536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:56:48.224711 139822468673536 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:56:48.237132 139822468673536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:56:48.237188 139822468673536 attention.py:418] Single window, no scan.
I0123 20:56:48.237225 139822468673536 transformer_layer.py:389] tlayer: self-attention.
I0123 20:56:48.237258 139822468673536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.237320 139822468673536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.237887 139822468673536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.237966 139822468673536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.238329 139822468673536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.239024 139822468673536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.241544 139822468673536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.242173 139822468673536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.242254 139822468673536 transformer_layer.py:468] tlayer: End windows.
I0123 20:56:48.242290 139822468673536 transformer_layer.py:472] tlayer: final FFN.
I0123 20:56:48.242351 139822468673536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.242480 139822468673536 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:56:48.242589 139822468673536 nn_components.py:325] mlp: activation = None
I0123 20:56:48.242629 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 20:56:48.244482 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.244575 139822468673536 nn_components.py:261] mlp: residual
I0123 20:56:48.246991 139822468673536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.247072 139822468673536 transformer_base.py:443] tbase: final FFN
I0123 20:56:48.247182 139822468673536 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:56:48.249896 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 20:56:48.251771 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.251868 139822468673536 nn_components.py:261] mlp: residual
I0123 20:56:48.252158 139822468673536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.252241 139822468673536 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 20:56:48.252350 139822468673536 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:56:48.252390 139822468673536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:56:48.252423 139822468673536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:56:48.252488 139822468673536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.254745 139822468673536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:56:48.260176 139822468673536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.260437 139822468673536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:56:48.263134 139822468673536 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:56:48.275636 139822468673536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:56:48.275692 139822468673536 attention.py:418] Single window, no scan.
I0123 20:56:48.275730 139822468673536 transformer_layer.py:389] tlayer: self-attention.
I0123 20:56:48.275779 139822468673536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.275843 139822468673536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.276412 139822468673536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.276488 139822468673536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.276847 139822468673536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.277541 139822468673536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.280086 139822468673536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.280697 139822468673536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.280772 139822468673536 transformer_layer.py:468] tlayer: End windows.
I0123 20:56:48.280807 139822468673536 transformer_layer.py:472] tlayer: final FFN.
I0123 20:56:48.280866 139822468673536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.280996 139822468673536 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:56:48.281104 139822468673536 nn_components.py:325] mlp: activation = None
I0123 20:56:48.281143 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 20:56:48.283035 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.283126 139822468673536 nn_components.py:261] mlp: residual
I0123 20:56:48.285535 139822468673536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.285611 139822468673536 transformer_base.py:443] tbase: final FFN
I0123 20:56:48.285725 139822468673536 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:56:48.288021 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 20:56:48.289878 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.289973 139822468673536 nn_components.py:261] mlp: residual
I0123 20:56:48.290260 139822468673536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.290340 139822468673536 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 20:56:48.290448 139822468673536 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:56:48.290487 139822468673536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:56:48.290518 139822468673536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:56:48.290581 139822468673536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.292834 139822468673536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:56:48.298321 139822468673536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.298578 139822468673536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:56:48.301265 139822468673536 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:56:48.313819 139822468673536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:56:48.313871 139822468673536 attention.py:418] Single window, no scan.
I0123 20:56:48.313906 139822468673536 transformer_layer.py:389] tlayer: self-attention.
I0123 20:56:48.313937 139822468673536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.313999 139822468673536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.314555 139822468673536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.314628 139822468673536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.314984 139822468673536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.315669 139822468673536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.318252 139822468673536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.318874 139822468673536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.318950 139822468673536 transformer_layer.py:468] tlayer: End windows.
I0123 20:56:48.318985 139822468673536 transformer_layer.py:472] tlayer: final FFN.
I0123 20:56:48.319044 139822468673536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.319169 139822468673536 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:56:48.319276 139822468673536 nn_components.py:325] mlp: activation = None
I0123 20:56:48.319314 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 20:56:48.321180 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.321278 139822468673536 nn_components.py:261] mlp: residual
I0123 20:56:48.323702 139822468673536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.323780 139822468673536 transformer_base.py:443] tbase: final FFN
I0123 20:56:48.323892 139822468673536 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:56:48.326184 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 20:56:48.328048 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.328142 139822468673536 nn_components.py:261] mlp: residual
I0123 20:56:48.328425 139822468673536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.328505 139822468673536 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 20:56:48.328612 139822468673536 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:56:48.328650 139822468673536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:56:48.328680 139822468673536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:56:48.328744 139822468673536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.331000 139822468673536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:56:48.336420 139822468673536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.336674 139822468673536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:56:48.339378 139822468673536 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:56:48.351903 139822468673536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:56:48.351956 139822468673536 attention.py:418] Single window, no scan.
I0123 20:56:48.351990 139822468673536 transformer_layer.py:389] tlayer: self-attention.
I0123 20:56:48.352019 139822468673536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.352079 139822468673536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.352629 139822468673536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.352704 139822468673536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.353056 139822468673536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.353740 139822468673536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.356308 139822468673536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.356919 139822468673536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.356995 139822468673536 transformer_layer.py:468] tlayer: End windows.
I0123 20:56:48.357029 139822468673536 transformer_layer.py:472] tlayer: final FFN.
I0123 20:56:48.357086 139822468673536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.357217 139822468673536 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:56:48.357325 139822468673536 nn_components.py:325] mlp: activation = None
I0123 20:56:48.357363 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 20:56:48.359246 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.359344 139822468673536 nn_components.py:261] mlp: residual
I0123 20:56:48.361745 139822468673536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.361823 139822468673536 transformer_base.py:443] tbase: final FFN
I0123 20:56:48.361928 139822468673536 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:56:48.364579 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 20:56:48.366449 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.366543 139822468673536 nn_components.py:261] mlp: residual
I0123 20:56:48.366827 139822468673536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.366906 139822468673536 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 20:56:48.367011 139822468673536 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:56:48.367049 139822468673536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:56:48.367078 139822468673536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:56:48.367139 139822468673536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.369358 139822468673536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:56:48.374766 139822468673536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.375023 139822468673536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:56:48.377711 139822468673536 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:56:48.390170 139822468673536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:56:48.390222 139822468673536 attention.py:418] Single window, no scan.
I0123 20:56:48.390260 139822468673536 transformer_layer.py:389] tlayer: self-attention.
I0123 20:56:48.390290 139822468673536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.390350 139822468673536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.390906 139822468673536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.390981 139822468673536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.391336 139822468673536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.392015 139822468673536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.394551 139822468673536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.395179 139822468673536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.395256 139822468673536 transformer_layer.py:468] tlayer: End windows.
I0123 20:56:48.395291 139822468673536 transformer_layer.py:472] tlayer: final FFN.
I0123 20:56:48.395348 139822468673536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.395474 139822468673536 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:56:48.395580 139822468673536 nn_components.py:325] mlp: activation = None
I0123 20:56:48.395617 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 20:56:48.397459 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.397551 139822468673536 nn_components.py:261] mlp: residual
I0123 20:56:48.399935 139822468673536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.400013 139822468673536 transformer_base.py:443] tbase: final FFN
I0123 20:56:48.400119 139822468673536 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:56:48.402390 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 20:56:48.404233 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.404325 139822468673536 nn_components.py:261] mlp: residual
I0123 20:56:48.404612 139822468673536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.404692 139822468673536 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 20:56:48.404798 139822468673536 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:56:48.404836 139822468673536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:56:48.404865 139822468673536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:56:48.404926 139822468673536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.407147 139822468673536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:56:48.412532 139822468673536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.412789 139822468673536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:56:48.415472 139822468673536 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:56:48.427932 139822468673536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:56:48.427985 139822468673536 attention.py:418] Single window, no scan.
I0123 20:56:48.428019 139822468673536 transformer_layer.py:389] tlayer: self-attention.
I0123 20:56:48.428049 139822468673536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.428110 139822468673536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.428661 139822468673536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.428734 139822468673536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.429082 139822468673536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.429767 139822468673536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.432310 139822468673536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.432933 139822468673536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.433009 139822468673536 transformer_layer.py:468] tlayer: End windows.
I0123 20:56:48.433043 139822468673536 transformer_layer.py:472] tlayer: final FFN.
I0123 20:56:48.433100 139822468673536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.433227 139822468673536 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:56:48.433335 139822468673536 nn_components.py:325] mlp: activation = None
I0123 20:56:48.433373 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 20:56:48.435236 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.435327 139822468673536 nn_components.py:261] mlp: residual
I0123 20:56:48.437732 139822468673536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.437816 139822468673536 transformer_base.py:443] tbase: final FFN
I0123 20:56:48.437925 139822468673536 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:56:48.440180 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 20:56:48.442025 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.442119 139822468673536 nn_components.py:261] mlp: residual
I0123 20:56:48.442401 139822468673536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.442481 139822468673536 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 20:56:48.442587 139822468673536 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:56:48.442625 139822468673536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:56:48.442656 139822468673536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:56:48.442719 139822468673536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.444941 139822468673536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:56:48.450339 139822468673536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.450599 139822468673536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:56:48.453267 139822468673536 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:56:48.465791 139822468673536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:56:48.465848 139822468673536 attention.py:418] Single window, no scan.
I0123 20:56:48.465881 139822468673536 transformer_layer.py:389] tlayer: self-attention.
I0123 20:56:48.465911 139822468673536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.465971 139822468673536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.466523 139822468673536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.466597 139822468673536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.466956 139822468673536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.467639 139822468673536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.470207 139822468673536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.470818 139822468673536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.470894 139822468673536 transformer_layer.py:468] tlayer: End windows.
I0123 20:56:48.470927 139822468673536 transformer_layer.py:472] tlayer: final FFN.
I0123 20:56:48.470983 139822468673536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.471108 139822468673536 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:56:48.471214 139822468673536 nn_components.py:325] mlp: activation = None
I0123 20:56:48.471251 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 20:56:48.473115 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.473206 139822468673536 nn_components.py:261] mlp: residual
I0123 20:56:48.475594 139822468673536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.475678 139822468673536 transformer_base.py:443] tbase: final FFN
I0123 20:56:48.475786 139822468673536 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:56:48.478420 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 20:56:48.480254 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.480346 139822468673536 nn_components.py:261] mlp: residual
I0123 20:56:48.480629 139822468673536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.480708 139822468673536 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 20:56:48.480814 139822468673536 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:56:48.480851 139822468673536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:56:48.480881 139822468673536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:56:48.480942 139822468673536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.483164 139822468673536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:56:48.488533 139822468673536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.488785 139822468673536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:56:48.491466 139822468673536 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:56:48.503819 139822468673536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:56:48.503872 139822468673536 attention.py:418] Single window, no scan.
I0123 20:56:48.503906 139822468673536 transformer_layer.py:389] tlayer: self-attention.
I0123 20:56:48.503935 139822468673536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.503995 139822468673536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.504550 139822468673536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.504625 139822468673536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.504974 139822468673536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.505651 139822468673536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.508151 139822468673536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.508779 139822468673536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.508857 139822468673536 transformer_layer.py:468] tlayer: End windows.
I0123 20:56:48.508890 139822468673536 transformer_layer.py:472] tlayer: final FFN.
I0123 20:56:48.508946 139822468673536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.509070 139822468673536 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:56:48.509175 139822468673536 nn_components.py:325] mlp: activation = None
I0123 20:56:48.509212 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 20:56:48.511593 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.511686 139822468673536 nn_components.py:261] mlp: residual
I0123 20:56:48.514049 139822468673536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.514126 139822468673536 transformer_base.py:443] tbase: final FFN
I0123 20:56:48.514241 139822468673536 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:56:48.516435 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 20:56:48.518250 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.518343 139822468673536 nn_components.py:261] mlp: residual
I0123 20:56:48.518623 139822468673536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.518702 139822468673536 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 20:56:48.518806 139822468673536 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:56:48.518842 139822468673536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:56:48.518871 139822468673536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:56:48.518931 139822468673536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.521131 139822468673536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:56:48.526471 139822468673536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.526729 139822468673536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:56:48.529362 139822468673536 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:56:48.541660 139822468673536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:56:48.541713 139822468673536 attention.py:418] Single window, no scan.
I0123 20:56:48.541748 139822468673536 transformer_layer.py:389] tlayer: self-attention.
I0123 20:56:48.541777 139822468673536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.541839 139822468673536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.542391 139822468673536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.542465 139822468673536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.542821 139822468673536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.543512 139822468673536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.546047 139822468673536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.546664 139822468673536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.546740 139822468673536 transformer_layer.py:468] tlayer: End windows.
I0123 20:56:48.546774 139822468673536 transformer_layer.py:472] tlayer: final FFN.
I0123 20:56:48.546829 139822468673536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.546954 139822468673536 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:56:48.547060 139822468673536 nn_components.py:325] mlp: activation = None
I0123 20:56:48.547097 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 20:56:48.548946 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.549036 139822468673536 nn_components.py:261] mlp: residual
I0123 20:56:48.551411 139822468673536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.551489 139822468673536 transformer_base.py:443] tbase: final FFN
I0123 20:56:48.551595 139822468673536 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:56:48.553833 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 20:56:48.555644 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.555737 139822468673536 nn_components.py:261] mlp: residual
I0123 20:56:48.556020 139822468673536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.556100 139822468673536 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 20:56:48.556205 139822468673536 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:56:48.556243 139822468673536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:56:48.556273 139822468673536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:56:48.556334 139822468673536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.558550 139822468673536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:56:48.563941 139822468673536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.564194 139822468673536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:56:48.566889 139822468673536 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:56:48.579293 139822468673536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:56:48.579346 139822468673536 attention.py:418] Single window, no scan.
I0123 20:56:48.579380 139822468673536 transformer_layer.py:389] tlayer: self-attention.
I0123 20:56:48.579409 139822468673536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.579484 139822468673536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.580037 139822468673536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.580111 139822468673536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.580463 139822468673536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.581145 139822468673536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.583662 139822468673536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.584275 139822468673536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.584353 139822468673536 transformer_layer.py:468] tlayer: End windows.
I0123 20:56:48.584386 139822468673536 transformer_layer.py:472] tlayer: final FFN.
I0123 20:56:48.584442 139822468673536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.584569 139822468673536 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:56:48.584674 139822468673536 nn_components.py:325] mlp: activation = None
I0123 20:56:48.584711 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 20:56:48.586556 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.586647 139822468673536 nn_components.py:261] mlp: residual
I0123 20:56:48.589014 139822468673536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.589091 139822468673536 transformer_base.py:443] tbase: final FFN
I0123 20:56:48.589197 139822468673536 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:56:48.591795 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 20:56:48.593623 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.593721 139822468673536 nn_components.py:261] mlp: residual
I0123 20:56:48.593999 139822468673536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:48.594083 139822468673536 decoder_stack.py:344] dstack: Final layernorm.
I0123 20:56:48.596867 139822468673536 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:56:53.009943 139822468673536 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
I0123 20:56:53.538187 139822468673536 training_loop.py:409] No working directory specified.
I0123 20:56:53.538310 139822468673536 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0123 20:56:53.539077 139822468673536 checkpoints.py:1062] Restoring legacy Flax checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0123 20:56:57.025910 139822468673536 training_loop.py:447] Only restoring trainable parameters.
I0123 20:56:57.026499 139822468673536 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0123 20:56:57.026576 139822468673536 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0123 20:56:57.026627 139822468673536 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 20:56:57.026672 139822468673536 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 20:56:57.026715 139822468673536 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:56:57.026756 139822468673536 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 20:56:57.026794 139822468673536 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:56:57.026832 139822468673536 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:56:57.026870 139822468673536 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 20:56:57.026908 139822468673536 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 20:56:57.026945 139822468673536 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:56:57.026982 139822468673536 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 20:56:57.027018 139822468673536 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 20:56:57.027055 139822468673536 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 20:56:57.027093 139822468673536 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:56:57.027129 139822468673536 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 20:56:57.027165 139822468673536 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:56:57.027201 139822468673536 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:56:57.027236 139822468673536 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 20:56:57.027271 139822468673536 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 20:56:57.027319 139822468673536 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:56:57.027357 139822468673536 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 20:56:57.027394 139822468673536 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 20:56:57.027429 139822468673536 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 20:56:57.027463 139822468673536 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:56:57.027498 139822468673536 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 20:56:57.027534 139822468673536 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:56:57.027570 139822468673536 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:56:57.027605 139822468673536 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 20:56:57.027642 139822468673536 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 20:56:57.027679 139822468673536 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:56:57.027715 139822468673536 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 20:56:57.027751 139822468673536 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 20:56:57.027786 139822468673536 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 20:56:57.027823 139822468673536 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:56:57.027859 139822468673536 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 20:56:57.027896 139822468673536 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:56:57.027933 139822468673536 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:56:57.027968 139822468673536 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 20:56:57.028004 139822468673536 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 20:56:57.028039 139822468673536 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:56:57.028074 139822468673536 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 20:56:57.028110 139822468673536 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 20:56:57.028144 139822468673536 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 20:56:57.028180 139822468673536 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:56:57.028214 139822468673536 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 20:56:57.028255 139822468673536 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:56:57.028292 139822468673536 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:56:57.028328 139822468673536 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 20:56:57.028363 139822468673536 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 20:56:57.028398 139822468673536 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:56:57.028433 139822468673536 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 20:56:57.028469 139822468673536 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 20:56:57.028504 139822468673536 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 20:56:57.028540 139822468673536 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:56:57.028576 139822468673536 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 20:56:57.028612 139822468673536 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:56:57.028647 139822468673536 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:56:57.028682 139822468673536 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 20:56:57.028717 139822468673536 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 20:56:57.028753 139822468673536 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:56:57.028788 139822468673536 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 20:56:57.028824 139822468673536 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 20:56:57.028860 139822468673536 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 20:56:57.028894 139822468673536 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:56:57.028929 139822468673536 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 20:56:57.028965 139822468673536 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:56:57.029001 139822468673536 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:56:57.029036 139822468673536 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 20:56:57.029071 139822468673536 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 20:56:57.029106 139822468673536 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:56:57.029141 139822468673536 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 20:56:57.029175 139822468673536 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 20:56:57.029215 139822468673536 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 20:56:57.029252 139822468673536 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:56:57.029287 139822468673536 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 20:56:57.029322 139822468673536 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:56:57.029357 139822468673536 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:56:57.029392 139822468673536 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 20:56:57.029426 139822468673536 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 20:56:57.029460 139822468673536 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:56:57.029495 139822468673536 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 20:56:57.029529 139822468673536 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 20:56:57.029563 139822468673536 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 20:56:57.029598 139822468673536 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:56:57.029632 139822468673536 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 20:56:57.029678 139822468673536 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:56:57.029715 139822468673536 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:56:57.029754 139822468673536 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 20:56:57.029789 139822468673536 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 20:56:57.029823 139822468673536 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:56:57.029858 139822468673536 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 20:56:57.029892 139822468673536 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 20:56:57.029927 139822468673536 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 20:56:57.029961 139822468673536 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:56:57.029996 139822468673536 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 20:56:57.030031 139822468673536 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:56:57.030065 139822468673536 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:56:57.030098 139822468673536 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 20:56:57.030133 139822468673536 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 20:56:57.030172 139822468673536 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:56:57.030209 139822468673536 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 20:56:57.030245 139822468673536 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 20:56:57.030279 139822468673536 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 20:56:57.030313 139822468673536 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:56:57.030349 139822468673536 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 20:56:57.030386 139822468673536 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:56:57.030421 139822468673536 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:56:57.030457 139822468673536 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 20:56:57.030492 139822468673536 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 20:56:57.030527 139822468673536 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:56:57.030562 139822468673536 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 20:56:57.030596 139822468673536 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 20:56:57.030630 139822468673536 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 20:56:57.030664 139822468673536 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:56:57.030699 139822468673536 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 20:56:57.030733 139822468673536 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:56:57.030768 139822468673536 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:56:57.030802 139822468673536 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 20:56:57.030835 139822468673536 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 20:56:57.030869 139822468673536 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:56:57.030904 139822468673536 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 20:56:57.030931 139822468673536 training_loop.py:725] Total parameters: 152072288
I0123 20:56:57.031138 139822468673536 training_loop.py:739] Total state size: 0
I0123 20:56:57.052195 139822468673536 training_loop.py:492] Training loop: creating task for mode beam_search
I0123 20:56:57.052463 139822468673536 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0123 20:56:57.052801 139822468673536 training_loop.py:652] Compiling mode beam_search with jit.
I0123 20:56:57.053136 139822468673536 training_loop.py:89] registering functions: dict_keys([])
I0123 20:56:57.069816 139822468673536 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e c b a; f = foot f b c a; g = on_line g c e, on_line g b f; h = on_circle h d b; i = on_circle i d h, on_line i g h; j = foot j h b a; k = foot k h c b; l = on_pline l i j k, on_line l k c; m = on_circle m d i, on_line m l i; n = midpoint n b a; o = foot o h d n; p = mirror p h o; q = on_line q m p, on_line q b a ? cong q m q i
I0123 20:57:05.124730 139822468673536 ddar.py:60] Depth 1/1000 time = 7.991002559661865
I0123 20:57:27.461520 139822468673536 ddar.py:60] Depth 2/1000 time = 22.336543083190918
I0123 20:58:11.965429 139822468673536 ddar.py:60] Depth 3/1000 time = 44.50349998474121
I0123 20:58:54.581247 139822468673536 ddar.py:60] Depth 4/1000 time = 42.615373373031616
I0123 20:59:37.025087 139822468673536 ddar.py:60] Depth 5/1000 time = 42.44338393211365
I0123 21:00:20.090246 139822468673536 ddar.py:60] Depth 6/1000 time = 43.06400680541992
I0123 21:01:03.352287 139822468673536 ddar.py:60] Depth 7/1000 time = 43.183610916137695
I0123 21:01:47.114682 139822468673536 ddar.py:60] Depth 8/1000 time = 43.463966369628906
I0123 21:01:47.121768 139822468673536 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 21:01:47.121884 139822468673536 alphageometry.py:540] Depth 0. There are 1 nodes to expand:
I0123 21:01:47.121922 139822468673536 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a b e 02 T a b c e 03 ; f : C a c f 04 T a c b f 05 ; g : C b f g 06 C c e g 07 ; h : D b d d h 08 ; i : C g h i 09 D d h d i 10 ; j : C a b j 11 T a b h j 12 ; k : C b c k 13 T b c h k 14 ; l : C c k l 15 P i l j k 16 ; m : C i l m 17 D d i d m 18 ; n : C a b n 19 D a n b n 20 ; o : C d n o 21 T d n h o 22 ; p : C h o p 23 D h o o p 24 ; q : C a b q 25 C m p q 26 ? D q m q i {F1} x00
I0123 21:01:47.121954 139822468673536 alphageometry.py:549] Decoding from {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a b e 02 T a b c e 03 ; f : C a c f 04 T a c b f 05 ; g : C b f g 06 C c e g 07 ; h : D b d d h 08 ; i : C g h i 09 D d h d i 10 ; j : C a b j 11 T a b h j 12 ; k : C b c k 13 T b c h k 14 ; l : C c k l 15 P i l j k 16 ; m : C i l m 17 D d i d m 18 ; n : C a b n 19 D a n b n 20 ; o : C d n o 21 T d n h o 22 ; p : C h o p 23 D h o o p 24 ; q : C a b q 25 C m p q 26 ? D q m q i {F1} x00
I0123 21:01:47.271858 139822468673536 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.272066 139822468673536 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 21:01:47.272176 139822468673536 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 21:01:47.272253 139822468673536 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 21:01:47.272326 139822468673536 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 21:01:47.272395 139822468673536 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 21:01:47.272464 139822468673536 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 21:01:47.272531 139822468673536 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 21:01:47.272598 139822468673536 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 21:01:47.272663 139822468673536 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 21:01:47.272729 139822468673536 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 21:01:47.272794 139822468673536 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 21:01:47.272859 139822468673536 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 21:01:47.272925 139822468673536 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 21:01:47.272968 139822468673536 attention.py:418] Single window, no scan.
I0123 21:01:47.273024 139822468673536 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 21:01:47.273137 139822468673536 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:01:47.273175 139822468673536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:01:47.273205 139822468673536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:01:47.275184 139822468673536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.277757 139822468673536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:01:47.283555 139822468673536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.283833 139822468673536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:01:47.286561 139822468673536 transformer_layer.py:213] tlayer: windowed attention.
I0123 21:01:47.290557 139822468673536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:01:47.290614 139822468673536 attention.py:418] Single window, no scan.
I0123 21:01:47.290652 139822468673536 transformer_layer.py:389] tlayer: self-attention.
I0123 21:01:47.290685 139822468673536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.290749 139822468673536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.291404 139822468673536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.291480 139822468673536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.291856 139822468673536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.292655 139822468673536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.295306 139822468673536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.295952 139822468673536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.296030 139822468673536 transformer_layer.py:468] tlayer: End windows.
I0123 21:01:47.296065 139822468673536 transformer_layer.py:472] tlayer: final FFN.
I0123 21:01:47.296128 139822468673536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.296255 139822468673536 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:01:47.296583 139822468673536 nn_components.py:325] mlp: activation = None
I0123 21:01:47.296627 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 21:01:47.298560 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.298654 139822468673536 nn_components.py:261] mlp: residual
I0123 21:01:47.301151 139822468673536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.301228 139822468673536 transformer_base.py:443] tbase: final FFN
I0123 21:01:47.301657 139822468673536 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:01:47.304115 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 21:01:47.306049 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.306144 139822468673536 nn_components.py:261] mlp: residual
I0123 21:01:47.306434 139822468673536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.306514 139822468673536 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 21:01:47.306628 139822468673536 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:01:47.306667 139822468673536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:01:47.306698 139822468673536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:01:47.308476 139822468673536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.310793 139822468673536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:01:47.316386 139822468673536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.316638 139822468673536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:01:47.319173 139822468673536 transformer_layer.py:213] tlayer: windowed attention.
I0123 21:01:47.322796 139822468673536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:01:47.322849 139822468673536 attention.py:418] Single window, no scan.
I0123 21:01:47.322883 139822468673536 transformer_layer.py:389] tlayer: self-attention.
I0123 21:01:47.322912 139822468673536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.322973 139822468673536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.323571 139822468673536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.323645 139822468673536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.323993 139822468673536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.324749 139822468673536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.327194 139822468673536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.327805 139822468673536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.327879 139822468673536 transformer_layer.py:468] tlayer: End windows.
I0123 21:01:47.327913 139822468673536 transformer_layer.py:472] tlayer: final FFN.
I0123 21:01:47.327969 139822468673536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.328093 139822468673536 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:01:47.328401 139822468673536 nn_components.py:325] mlp: activation = None
I0123 21:01:47.328442 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 21:01:47.330383 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.330475 139822468673536 nn_components.py:261] mlp: residual
I0123 21:01:47.332887 139822468673536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.332964 139822468673536 transformer_base.py:443] tbase: final FFN
I0123 21:01:47.333378 139822468673536 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:01:47.335604 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 21:01:47.337484 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.337576 139822468673536 nn_components.py:261] mlp: residual
I0123 21:01:47.337868 139822468673536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.337948 139822468673536 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 21:01:47.338055 139822468673536 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:01:47.338099 139822468673536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:01:47.338130 139822468673536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:01:47.339974 139822468673536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.342288 139822468673536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:01:47.347794 139822468673536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.348043 139822468673536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:01:47.350617 139822468673536 transformer_layer.py:213] tlayer: windowed attention.
I0123 21:01:47.354288 139822468673536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:01:47.354342 139822468673536 attention.py:418] Single window, no scan.
I0123 21:01:47.354377 139822468673536 transformer_layer.py:389] tlayer: self-attention.
I0123 21:01:47.354406 139822468673536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.354466 139822468673536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.355016 139822468673536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.355089 139822468673536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.355436 139822468673536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.356183 139822468673536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.358638 139822468673536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.359248 139822468673536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.359323 139822468673536 transformer_layer.py:468] tlayer: End windows.
I0123 21:01:47.359356 139822468673536 transformer_layer.py:472] tlayer: final FFN.
I0123 21:01:47.359412 139822468673536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.359535 139822468673536 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:01:47.359891 139822468673536 nn_components.py:325] mlp: activation = None
I0123 21:01:47.359933 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 21:01:47.361807 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.361896 139822468673536 nn_components.py:261] mlp: residual
I0123 21:01:47.364312 139822468673536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.364389 139822468673536 transformer_base.py:443] tbase: final FFN
I0123 21:01:47.364802 139822468673536 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:01:47.367034 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 21:01:47.369351 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.369445 139822468673536 nn_components.py:261] mlp: residual
I0123 21:01:47.369741 139822468673536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.369821 139822468673536 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 21:01:47.369927 139822468673536 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:01:47.369965 139822468673536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:01:47.370000 139822468673536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:01:47.371754 139822468673536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.374078 139822468673536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:01:47.379709 139822468673536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.379963 139822468673536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:01:47.382516 139822468673536 transformer_layer.py:213] tlayer: windowed attention.
I0123 21:01:47.386133 139822468673536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:01:47.386187 139822468673536 attention.py:418] Single window, no scan.
I0123 21:01:47.386220 139822468673536 transformer_layer.py:389] tlayer: self-attention.
I0123 21:01:47.386250 139822468673536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.386311 139822468673536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.386915 139822468673536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.386990 139822468673536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.387344 139822468673536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.388105 139822468673536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.390550 139822468673536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.391168 139822468673536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.391245 139822468673536 transformer_layer.py:468] tlayer: End windows.
I0123 21:01:47.391279 139822468673536 transformer_layer.py:472] tlayer: final FFN.
I0123 21:01:47.391335 139822468673536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.391461 139822468673536 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:01:47.391769 139822468673536 nn_components.py:325] mlp: activation = None
I0123 21:01:47.391810 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 21:01:47.393784 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.393877 139822468673536 nn_components.py:261] mlp: residual
I0123 21:01:47.396320 139822468673536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.396397 139822468673536 transformer_base.py:443] tbase: final FFN
I0123 21:01:47.396815 139822468673536 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:01:47.399045 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 21:01:47.400922 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.401014 139822468673536 nn_components.py:261] mlp: residual
I0123 21:01:47.401297 139822468673536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.401376 139822468673536 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 21:01:47.401483 139822468673536 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:01:47.401520 139822468673536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:01:47.401551 139822468673536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:01:47.403392 139822468673536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.405685 139822468673536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:01:47.411190 139822468673536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.411441 139822468673536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:01:47.414025 139822468673536 transformer_layer.py:213] tlayer: windowed attention.
I0123 21:01:47.417596 139822468673536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:01:47.417652 139822468673536 attention.py:418] Single window, no scan.
I0123 21:01:47.417689 139822468673536 transformer_layer.py:389] tlayer: self-attention.
I0123 21:01:47.417718 139822468673536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.417779 139822468673536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.418330 139822468673536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.418404 139822468673536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.418750 139822468673536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.419505 139822468673536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.421941 139822468673536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.422594 139822468673536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.422670 139822468673536 transformer_layer.py:468] tlayer: End windows.
I0123 21:01:47.422703 139822468673536 transformer_layer.py:472] tlayer: final FFN.
I0123 21:01:47.422759 139822468673536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.422883 139822468673536 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:01:47.423185 139822468673536 nn_components.py:325] mlp: activation = None
I0123 21:01:47.423226 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 21:01:47.425085 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.425176 139822468673536 nn_components.py:261] mlp: residual
I0123 21:01:47.427585 139822468673536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.427663 139822468673536 transformer_base.py:443] tbase: final FFN
I0123 21:01:47.428079 139822468673536 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:01:47.430377 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 21:01:47.432251 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.432343 139822468673536 nn_components.py:261] mlp: residual
I0123 21:01:47.432630 139822468673536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.432710 139822468673536 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 21:01:47.432815 139822468673536 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:01:47.432852 139822468673536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:01:47.432881 139822468673536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:01:47.434631 139822468673536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.436904 139822468673536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:01:47.442480 139822468673536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.442730 139822468673536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:01:47.445255 139822468673536 transformer_layer.py:213] tlayer: windowed attention.
I0123 21:01:47.448815 139822468673536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:01:47.448869 139822468673536 attention.py:418] Single window, no scan.
I0123 21:01:47.448904 139822468673536 transformer_layer.py:389] tlayer: self-attention.
I0123 21:01:47.448933 139822468673536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.449041 139822468673536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.449594 139822468673536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.449674 139822468673536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.450027 139822468673536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.450773 139822468673536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.453218 139822468673536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.453830 139822468673536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.453906 139822468673536 transformer_layer.py:468] tlayer: End windows.
I0123 21:01:47.453939 139822468673536 transformer_layer.py:472] tlayer: final FFN.
I0123 21:01:47.453994 139822468673536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.454136 139822468673536 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:01:47.454441 139822468673536 nn_components.py:325] mlp: activation = None
I0123 21:01:47.454481 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 21:01:47.456404 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.456494 139822468673536 nn_components.py:261] mlp: residual
I0123 21:01:47.458922 139822468673536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.458999 139822468673536 transformer_base.py:443] tbase: final FFN
I0123 21:01:47.459417 139822468673536 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:01:47.461632 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 21:01:47.463497 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.463589 139822468673536 nn_components.py:261] mlp: residual
I0123 21:01:47.463871 139822468673536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.463951 139822468673536 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 21:01:47.464056 139822468673536 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:01:47.464093 139822468673536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:01:47.464122 139822468673536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:01:47.465951 139822468673536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.468217 139822468673536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:01:47.473699 139822468673536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.473950 139822468673536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:01:47.476857 139822468673536 transformer_layer.py:213] tlayer: windowed attention.
I0123 21:01:47.480434 139822468673536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:01:47.480488 139822468673536 attention.py:418] Single window, no scan.
I0123 21:01:47.480522 139822468673536 transformer_layer.py:389] tlayer: self-attention.
I0123 21:01:47.480551 139822468673536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.480612 139822468673536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.481163 139822468673536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.481239 139822468673536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.481592 139822468673536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.482355 139822468673536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.484797 139822468673536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.485462 139822468673536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.485538 139822468673536 transformer_layer.py:468] tlayer: End windows.
I0123 21:01:47.485572 139822468673536 transformer_layer.py:472] tlayer: final FFN.
I0123 21:01:47.485630 139822468673536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.485765 139822468673536 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:01:47.486075 139822468673536 nn_components.py:325] mlp: activation = None
I0123 21:01:47.486117 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 21:01:47.488011 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.488102 139822468673536 nn_components.py:261] mlp: residual
I0123 21:01:47.490556 139822468673536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.490634 139822468673536 transformer_base.py:443] tbase: final FFN
I0123 21:01:47.491055 139822468673536 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:01:47.493366 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 21:01:47.495286 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.495381 139822468673536 nn_components.py:261] mlp: residual
I0123 21:01:47.495670 139822468673536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.495749 139822468673536 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 21:01:47.495856 139822468673536 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:01:47.495893 139822468673536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:01:47.495924 139822468673536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:01:47.497714 139822468673536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.499988 139822468673536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:01:47.505566 139822468673536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.505831 139822468673536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:01:47.508383 139822468673536 transformer_layer.py:213] tlayer: windowed attention.
I0123 21:01:47.511992 139822468673536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:01:47.512046 139822468673536 attention.py:418] Single window, no scan.
I0123 21:01:47.512080 139822468673536 transformer_layer.py:389] tlayer: self-attention.
I0123 21:01:47.512109 139822468673536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.512218 139822468673536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.512775 139822468673536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.512850 139822468673536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.513207 139822468673536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.513975 139822468673536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.516440 139822468673536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.517054 139822468673536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.517131 139822468673536 transformer_layer.py:468] tlayer: End windows.
I0123 21:01:47.517165 139822468673536 transformer_layer.py:472] tlayer: final FFN.
I0123 21:01:47.517223 139822468673536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.517349 139822468673536 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:01:47.517664 139822468673536 nn_components.py:325] mlp: activation = None
I0123 21:01:47.517707 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 21:01:47.519674 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.519766 139822468673536 nn_components.py:261] mlp: residual
I0123 21:01:47.522216 139822468673536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.522294 139822468673536 transformer_base.py:443] tbase: final FFN
I0123 21:01:47.522706 139822468673536 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:01:47.524915 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 21:01:47.526833 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.526928 139822468673536 nn_components.py:261] mlp: residual
I0123 21:01:47.527217 139822468673536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.527297 139822468673536 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 21:01:47.527403 139822468673536 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:01:47.527441 139822468673536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:01:47.527471 139822468673536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:01:47.529315 139822468673536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.531614 139822468673536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:01:47.537099 139822468673536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.537353 139822468673536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:01:47.539979 139822468673536 transformer_layer.py:213] tlayer: windowed attention.
I0123 21:01:47.543586 139822468673536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:01:47.543641 139822468673536 attention.py:418] Single window, no scan.
I0123 21:01:47.543675 139822468673536 transformer_layer.py:389] tlayer: self-attention.
I0123 21:01:47.543706 139822468673536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.543768 139822468673536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.544322 139822468673536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.544396 139822468673536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.544747 139822468673536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.545506 139822468673536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.547981 139822468673536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.548599 139822468673536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.548675 139822468673536 transformer_layer.py:468] tlayer: End windows.
I0123 21:01:47.548709 139822468673536 transformer_layer.py:472] tlayer: final FFN.
I0123 21:01:47.548767 139822468673536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.548895 139822468673536 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:01:47.549264 139822468673536 nn_components.py:325] mlp: activation = None
I0123 21:01:47.549307 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 21:01:47.551228 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.551320 139822468673536 nn_components.py:261] mlp: residual
I0123 21:01:47.553901 139822468673536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.553979 139822468673536 transformer_base.py:443] tbase: final FFN
I0123 21:01:47.554397 139822468673536 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:01:47.556657 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 21:01:47.558557 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.558651 139822468673536 nn_components.py:261] mlp: residual
I0123 21:01:47.558938 139822468673536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.559018 139822468673536 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 21:01:47.559125 139822468673536 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:01:47.559163 139822468673536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:01:47.559193 139822468673536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:01:47.561022 139822468673536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.563322 139822468673536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:01:47.568824 139822468673536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.569077 139822468673536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:01:47.571628 139822468673536 transformer_layer.py:213] tlayer: windowed attention.
I0123 21:01:47.575245 139822468673536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:01:47.575297 139822468673536 attention.py:418] Single window, no scan.
I0123 21:01:47.575330 139822468673536 transformer_layer.py:389] tlayer: self-attention.
I0123 21:01:47.575359 139822468673536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.575420 139822468673536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.575967 139822468673536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.576041 139822468673536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.576390 139822468673536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.577138 139822468673536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.579566 139822468673536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.580173 139822468673536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.580249 139822468673536 transformer_layer.py:468] tlayer: End windows.
I0123 21:01:47.580282 139822468673536 transformer_layer.py:472] tlayer: final FFN.
I0123 21:01:47.580338 139822468673536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.580463 139822468673536 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:01:47.580770 139822468673536 nn_components.py:325] mlp: activation = None
I0123 21:01:47.580810 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 21:01:47.582754 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.582845 139822468673536 nn_components.py:261] mlp: residual
I0123 21:01:47.585245 139822468673536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.585321 139822468673536 transformer_base.py:443] tbase: final FFN
I0123 21:01:47.585742 139822468673536 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:01:47.587954 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 21:01:47.589847 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.589942 139822468673536 nn_components.py:261] mlp: residual
I0123 21:01:47.590228 139822468673536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.590306 139822468673536 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 21:01:47.590412 139822468673536 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:01:47.590449 139822468673536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:01:47.590478 139822468673536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:01:47.592647 139822468673536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.594933 139822468673536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:01:47.600448 139822468673536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.600698 139822468673536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:01:47.603240 139822468673536 transformer_layer.py:213] tlayer: windowed attention.
I0123 21:01:47.606874 139822468673536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:01:47.606927 139822468673536 attention.py:418] Single window, no scan.
I0123 21:01:47.606961 139822468673536 transformer_layer.py:389] tlayer: self-attention.
I0123 21:01:47.606990 139822468673536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.607051 139822468673536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.607599 139822468673536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.607673 139822468673536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.608022 139822468673536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.608774 139822468673536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.611259 139822468673536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.611875 139822468673536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.611952 139822468673536 transformer_layer.py:468] tlayer: End windows.
I0123 21:01:47.611987 139822468673536 transformer_layer.py:472] tlayer: final FFN.
I0123 21:01:47.612046 139822468673536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.612174 139822468673536 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:01:47.612484 139822468673536 nn_components.py:325] mlp: activation = None
I0123 21:01:47.612526 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 21:01:47.614521 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.614614 139822468673536 nn_components.py:261] mlp: residual
I0123 21:01:47.617084 139822468673536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.617162 139822468673536 transformer_base.py:443] tbase: final FFN
I0123 21:01:47.617582 139822468673536 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:01:47.619846 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 21:01:47.621764 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.621858 139822468673536 nn_components.py:261] mlp: residual
I0123 21:01:47.622149 139822468673536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.622229 139822468673536 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 21:01:47.622336 139822468673536 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:01:47.622374 139822468673536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:01:47.622405 139822468673536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:01:47.624265 139822468673536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.626591 139822468673536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:01:47.632154 139822468673536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.632405 139822468673536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:01:47.634977 139822468673536 transformer_layer.py:213] tlayer: windowed attention.
I0123 21:01:47.638639 139822468673536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:01:47.638697 139822468673536 attention.py:418] Single window, no scan.
I0123 21:01:47.638733 139822468673536 transformer_layer.py:389] tlayer: self-attention.
I0123 21:01:47.638762 139822468673536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.638824 139822468673536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.639379 139822468673536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.639456 139822468673536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.639806 139822468673536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.640558 139822468673536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.643013 139822468673536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.643625 139822468673536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.643702 139822468673536 transformer_layer.py:468] tlayer: End windows.
I0123 21:01:47.643737 139822468673536 transformer_layer.py:472] tlayer: final FFN.
I0123 21:01:47.643794 139822468673536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.643922 139822468673536 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:01:47.644233 139822468673536 nn_components.py:325] mlp: activation = None
I0123 21:01:47.644276 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 21:01:47.646256 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.646349 139822468673536 nn_components.py:261] mlp: residual
I0123 21:01:47.648795 139822468673536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.648873 139822468673536 transformer_base.py:443] tbase: final FFN
I0123 21:01:47.649293 139822468673536 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:01:47.651545 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 21:01:47.653625 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.653725 139822468673536 nn_components.py:261] mlp: residual
I0123 21:01:47.654014 139822468673536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.654260 139822468673536 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 21:01:47.654327 139822468673536 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 21:01:47.654384 139822468673536 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 21:01:47.654438 139822468673536 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 21:01:47.654491 139822468673536 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 21:01:47.654543 139822468673536 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 21:01:47.654595 139822468673536 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 21:01:47.654647 139822468673536 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 21:01:47.654699 139822468673536 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 21:01:47.654749 139822468673536 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 21:01:47.654807 139822468673536 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 21:01:47.654859 139822468673536 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 21:01:47.654895 139822468673536 decoder_stack.py:344] dstack: Final layernorm.
I0123 21:01:47.657847 139822468673536 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 21:01:47.702472 139822468673536 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.702556 139822468673536 decoder_stack.py:333] dstack: autoregressive generator.
I0123 21:01:47.702607 139822468673536 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 21:01:47.702708 139822468673536 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:01:47.702745 139822468673536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:01:47.702775 139822468673536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:01:47.702835 139822468673536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.705187 139822468673536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:01:47.710542 139822468673536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.710798 139822468673536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:01:47.713356 139822468673536 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:01:47.726057 139822468673536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:01:47.726110 139822468673536 attention.py:418] Single window, no scan.
I0123 21:01:47.726145 139822468673536 transformer_layer.py:389] tlayer: self-attention.
I0123 21:01:47.726176 139822468673536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.726237 139822468673536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.726799 139822468673536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.726873 139822468673536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.727231 139822468673536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.727912 139822468673536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.730522 139822468673536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.731134 139822468673536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.731210 139822468673536 transformer_layer.py:468] tlayer: End windows.
I0123 21:01:47.731243 139822468673536 transformer_layer.py:472] tlayer: final FFN.
I0123 21:01:47.731300 139822468673536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.731427 139822468673536 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:01:47.731532 139822468673536 nn_components.py:325] mlp: activation = None
I0123 21:01:47.731569 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 21:01:47.733411 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.733502 139822468673536 nn_components.py:261] mlp: residual
I0123 21:01:47.735919 139822468673536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.736004 139822468673536 transformer_base.py:443] tbase: final FFN
I0123 21:01:47.736114 139822468673536 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:01:47.738346 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 21:01:47.740172 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.740265 139822468673536 nn_components.py:261] mlp: residual
I0123 21:01:47.740552 139822468673536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.740632 139822468673536 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 21:01:47.740738 139822468673536 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:01:47.740777 139822468673536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:01:47.740806 139822468673536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:01:47.740867 139822468673536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.743088 139822468673536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:01:47.748417 139822468673536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.748672 139822468673536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:01:47.751332 139822468673536 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:01:47.763529 139822468673536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:01:47.763583 139822468673536 attention.py:418] Single window, no scan.
I0123 21:01:47.763617 139822468673536 transformer_layer.py:389] tlayer: self-attention.
I0123 21:01:47.763646 139822468673536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.763708 139822468673536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.764257 139822468673536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.764332 139822468673536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.764687 139822468673536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.765422 139822468673536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.767889 139822468673536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.768504 139822468673536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.768580 139822468673536 transformer_layer.py:468] tlayer: End windows.
I0123 21:01:47.768613 139822468673536 transformer_layer.py:472] tlayer: final FFN.
I0123 21:01:47.768670 139822468673536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.768797 139822468673536 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:01:47.768904 139822468673536 nn_components.py:325] mlp: activation = None
I0123 21:01:47.768941 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 21:01:47.770775 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.770867 139822468673536 nn_components.py:261] mlp: residual
I0123 21:01:47.773238 139822468673536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.773320 139822468673536 transformer_base.py:443] tbase: final FFN
I0123 21:01:47.773428 139822468673536 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:01:47.776072 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 21:01:47.777897 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.777989 139822468673536 nn_components.py:261] mlp: residual
I0123 21:01:47.778276 139822468673536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.778356 139822468673536 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 21:01:47.778463 139822468673536 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:01:47.778500 139822468673536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:01:47.778530 139822468673536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:01:47.778591 139822468673536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.780790 139822468673536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:01:47.786192 139822468673536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.786450 139822468673536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:01:47.789077 139822468673536 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:01:47.801249 139822468673536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:01:47.801303 139822468673536 attention.py:418] Single window, no scan.
I0123 21:01:47.801337 139822468673536 transformer_layer.py:389] tlayer: self-attention.
I0123 21:01:47.801366 139822468673536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.801426 139822468673536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.801982 139822468673536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.802058 139822468673536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.802410 139822468673536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.803296 139822468673536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.805723 139822468673536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.806332 139822468673536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.806410 139822468673536 transformer_layer.py:468] tlayer: End windows.
I0123 21:01:47.806442 139822468673536 transformer_layer.py:472] tlayer: final FFN.
I0123 21:01:47.806499 139822468673536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.806623 139822468673536 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:01:47.806729 139822468673536 nn_components.py:325] mlp: activation = None
I0123 21:01:47.806766 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 21:01:47.808580 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.808671 139822468673536 nn_components.py:261] mlp: residual
I0123 21:01:47.811067 139822468673536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.811146 139822468673536 transformer_base.py:443] tbase: final FFN
I0123 21:01:47.811263 139822468673536 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:01:47.813486 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 21:01:47.815299 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.815392 139822468673536 nn_components.py:261] mlp: residual
I0123 21:01:47.815676 139822468673536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.815754 139822468673536 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 21:01:47.815859 139822468673536 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:01:47.815896 139822468673536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:01:47.815926 139822468673536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:01:47.815986 139822468673536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.818180 139822468673536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:01:47.823448 139822468673536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.823699 139822468673536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:01:47.826333 139822468673536 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:01:47.838485 139822468673536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:01:47.838540 139822468673536 attention.py:418] Single window, no scan.
I0123 21:01:47.838574 139822468673536 transformer_layer.py:389] tlayer: self-attention.
I0123 21:01:47.838605 139822468673536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.838667 139822468673536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.839220 139822468673536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.839295 139822468673536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.839649 139822468673536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.840372 139822468673536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.842820 139822468673536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.843433 139822468673536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.843510 139822468673536 transformer_layer.py:468] tlayer: End windows.
I0123 21:01:47.843543 139822468673536 transformer_layer.py:472] tlayer: final FFN.
I0123 21:01:47.843600 139822468673536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.843727 139822468673536 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:01:47.843832 139822468673536 nn_components.py:325] mlp: activation = None
I0123 21:01:47.843869 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 21:01:47.845714 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.845806 139822468673536 nn_components.py:261] mlp: residual
I0123 21:01:47.848205 139822468673536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.848283 139822468673536 transformer_base.py:443] tbase: final FFN
I0123 21:01:47.848391 139822468673536 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:01:47.850631 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 21:01:47.852457 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.852548 139822468673536 nn_components.py:261] mlp: residual
I0123 21:01:47.852833 139822468673536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.852912 139822468673536 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 21:01:47.853018 139822468673536 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:01:47.853056 139822468673536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:01:47.853085 139822468673536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:01:47.853147 139822468673536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.855352 139822468673536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:01:47.860675 139822468673536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.860930 139822468673536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:01:47.863601 139822468673536 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:01:47.875823 139822468673536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:01:47.875877 139822468673536 attention.py:418] Single window, no scan.
I0123 21:01:47.875912 139822468673536 transformer_layer.py:389] tlayer: self-attention.
I0123 21:01:47.875942 139822468673536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.876004 139822468673536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.876555 139822468673536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.876630 139822468673536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.876983 139822468673536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.877729 139822468673536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.880189 139822468673536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.880801 139822468673536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.880877 139822468673536 transformer_layer.py:468] tlayer: End windows.
I0123 21:01:47.880911 139822468673536 transformer_layer.py:472] tlayer: final FFN.
I0123 21:01:47.880967 139822468673536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.881095 139822468673536 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:01:47.881201 139822468673536 nn_components.py:325] mlp: activation = None
I0123 21:01:47.881238 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 21:01:47.883080 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.883172 139822468673536 nn_components.py:261] mlp: residual
I0123 21:01:47.885550 139822468673536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.885627 139822468673536 transformer_base.py:443] tbase: final FFN
I0123 21:01:47.885746 139822468673536 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:01:47.888410 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 21:01:47.890269 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.890363 139822468673536 nn_components.py:261] mlp: residual
I0123 21:01:47.890653 139822468673536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.890732 139822468673536 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 21:01:47.890839 139822468673536 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:01:47.890877 139822468673536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:01:47.890906 139822468673536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:01:47.890966 139822468673536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.893175 139822468673536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:01:47.898490 139822468673536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.898746 139822468673536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:01:47.901372 139822468673536 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:01:47.913911 139822468673536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:01:47.913964 139822468673536 attention.py:418] Single window, no scan.
I0123 21:01:47.913998 139822468673536 transformer_layer.py:389] tlayer: self-attention.
I0123 21:01:47.914028 139822468673536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.914089 139822468673536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.914640 139822468673536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.914713 139822468673536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.915064 139822468673536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.915788 139822468673536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.918238 139822468673536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.918847 139822468673536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.918923 139822468673536 transformer_layer.py:468] tlayer: End windows.
I0123 21:01:47.918957 139822468673536 transformer_layer.py:472] tlayer: final FFN.
I0123 21:01:47.919014 139822468673536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.919141 139822468673536 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:01:47.919246 139822468673536 nn_components.py:325] mlp: activation = None
I0123 21:01:47.919283 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 21:01:47.921094 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.921186 139822468673536 nn_components.py:261] mlp: residual
I0123 21:01:47.923573 139822468673536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.923651 139822468673536 transformer_base.py:443] tbase: final FFN
I0123 21:01:47.923757 139822468673536 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:01:47.926019 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 21:01:47.927823 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.927922 139822468673536 nn_components.py:261] mlp: residual
I0123 21:01:47.928212 139822468673536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.928290 139822468673536 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 21:01:47.928397 139822468673536 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:01:47.928434 139822468673536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:01:47.928464 139822468673536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:01:47.928524 139822468673536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.930742 139822468673536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:01:47.936023 139822468673536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.936276 139822468673536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:01:47.938906 139822468673536 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:01:47.951014 139822468673536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:01:47.951068 139822468673536 attention.py:418] Single window, no scan.
I0123 21:01:47.951102 139822468673536 transformer_layer.py:389] tlayer: self-attention.
I0123 21:01:47.951131 139822468673536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.951192 139822468673536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.951740 139822468673536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.951814 139822468673536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.952171 139822468673536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.952891 139822468673536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.955352 139822468673536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.955971 139822468673536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.956049 139822468673536 transformer_layer.py:468] tlayer: End windows.
I0123 21:01:47.956083 139822468673536 transformer_layer.py:472] tlayer: final FFN.
I0123 21:01:47.956141 139822468673536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.956269 139822468673536 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:01:47.956378 139822468673536 nn_components.py:325] mlp: activation = None
I0123 21:01:47.956415 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 21:01:47.958271 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.958364 139822468673536 nn_components.py:261] mlp: residual
I0123 21:01:47.960798 139822468673536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.960874 139822468673536 transformer_base.py:443] tbase: final FFN
I0123 21:01:47.960981 139822468673536 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:01:47.963255 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 21:01:47.965095 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.965195 139822468673536 nn_components.py:261] mlp: residual
I0123 21:01:47.965488 139822468673536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.965569 139822468673536 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 21:01:47.965683 139822468673536 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:01:47.965723 139822468673536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:01:47.965755 139822468673536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:01:47.965819 139822468673536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.968061 139822468673536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:01:47.973439 139822468673536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.973709 139822468673536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:01:47.976364 139822468673536 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:01:47.988749 139822468673536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:01:47.988804 139822468673536 attention.py:418] Single window, no scan.
I0123 21:01:47.988839 139822468673536 transformer_layer.py:389] tlayer: self-attention.
I0123 21:01:47.988870 139822468673536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.988933 139822468673536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.989491 139822468673536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.989567 139822468673536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.989936 139822468673536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.990684 139822468673536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.993165 139822468673536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.993788 139822468673536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.993865 139822468673536 transformer_layer.py:468] tlayer: End windows.
I0123 21:01:47.993900 139822468673536 transformer_layer.py:472] tlayer: final FFN.
I0123 21:01:47.993957 139822468673536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.994086 139822468673536 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:01:47.994194 139822468673536 nn_components.py:325] mlp: activation = None
I0123 21:01:47.994231 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 21:01:47.996073 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.996164 139822468673536 nn_components.py:261] mlp: residual
I0123 21:01:47.998570 139822468673536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:47.998650 139822468673536 transformer_base.py:443] tbase: final FFN
I0123 21:01:47.998758 139822468673536 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:01:48.001423 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 21:01:48.003272 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:48.003367 139822468673536 nn_components.py:261] mlp: residual
I0123 21:01:48.003666 139822468673536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:48.003901 139822468673536 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 21:01:48.004010 139822468673536 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:01:48.004049 139822468673536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:01:48.004079 139822468673536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:01:48.004143 139822468673536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:48.006563 139822468673536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:01:48.011995 139822468673536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:48.012255 139822468673536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:01:48.014935 139822468673536 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:01:48.027380 139822468673536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:01:48.027433 139822468673536 attention.py:418] Single window, no scan.
I0123 21:01:48.027468 139822468673536 transformer_layer.py:389] tlayer: self-attention.
I0123 21:01:48.027497 139822468673536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:48.027559 139822468673536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:48.028114 139822468673536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:48.028191 139822468673536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:48.028547 139822468673536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:48.029237 139822468673536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:48.031772 139822468673536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:48.032392 139822468673536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:48.032469 139822468673536 transformer_layer.py:468] tlayer: End windows.
I0123 21:01:48.032504 139822468673536 transformer_layer.py:472] tlayer: final FFN.
I0123 21:01:48.032562 139822468673536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:48.032690 139822468673536 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:01:48.032797 139822468673536 nn_components.py:325] mlp: activation = None
I0123 21:01:48.032835 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 21:01:48.034692 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:48.034785 139822468673536 nn_components.py:261] mlp: residual
I0123 21:01:48.037204 139822468673536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:48.037283 139822468673536 transformer_base.py:443] tbase: final FFN
I0123 21:01:48.037393 139822468673536 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:01:48.039662 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 21:01:48.041510 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:48.041604 139822468673536 nn_components.py:261] mlp: residual
I0123 21:01:48.041902 139822468673536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:48.041993 139822468673536 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 21:01:48.042102 139822468673536 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:01:48.042140 139822468673536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:01:48.042171 139822468673536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:01:48.042234 139822468673536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:48.044469 139822468673536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:01:48.049836 139822468673536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:48.050092 139822468673536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:01:48.052745 139822468673536 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:01:48.064946 139822468673536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:01:48.065000 139822468673536 attention.py:418] Single window, no scan.
I0123 21:01:48.065035 139822468673536 transformer_layer.py:389] tlayer: self-attention.
I0123 21:01:48.065063 139822468673536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:48.065122 139822468673536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:48.065680 139822468673536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:48.065756 139822468673536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:48.066110 139822468673536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:48.066779 139822468673536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:48.069254 139822468673536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:48.069867 139822468673536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:48.069944 139822468673536 transformer_layer.py:468] tlayer: End windows.
I0123 21:01:48.069977 139822468673536 transformer_layer.py:472] tlayer: final FFN.
I0123 21:01:48.070033 139822468673536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:48.070159 139822468673536 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:01:48.070264 139822468673536 nn_components.py:325] mlp: activation = None
I0123 21:01:48.070301 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 21:01:48.072125 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:48.072216 139822468673536 nn_components.py:261] mlp: residual
I0123 21:01:48.074588 139822468673536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:48.074666 139822468673536 transformer_base.py:443] tbase: final FFN
I0123 21:01:48.074774 139822468673536 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:01:48.076993 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 21:01:48.078829 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:48.078923 139822468673536 nn_components.py:261] mlp: residual
I0123 21:01:48.079216 139822468673536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:48.079303 139822468673536 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 21:01:48.079412 139822468673536 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:01:48.079451 139822468673536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:01:48.079481 139822468673536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:01:48.079543 139822468673536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:48.081757 139822468673536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:01:48.087082 139822468673536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:48.087340 139822468673536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:01:48.089975 139822468673536 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:01:48.102149 139822468673536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:01:48.102202 139822468673536 attention.py:418] Single window, no scan.
I0123 21:01:48.102237 139822468673536 transformer_layer.py:389] tlayer: self-attention.
I0123 21:01:48.102267 139822468673536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:48.102328 139822468673536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:48.102879 139822468673536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:48.102954 139822468673536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:48.103314 139822468673536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:48.103991 139822468673536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:48.106744 139822468673536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:48.107355 139822468673536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:48.107432 139822468673536 transformer_layer.py:468] tlayer: End windows.
I0123 21:01:48.107466 139822468673536 transformer_layer.py:472] tlayer: final FFN.
I0123 21:01:48.107522 139822468673536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:48.107650 139822468673536 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:01:48.107757 139822468673536 nn_components.py:325] mlp: activation = None
I0123 21:01:48.107793 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 21:01:48.109627 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:48.109725 139822468673536 nn_components.py:261] mlp: residual
I0123 21:01:48.112112 139822468673536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:48.112191 139822468673536 transformer_base.py:443] tbase: final FFN
I0123 21:01:48.112298 139822468673536 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:01:48.114983 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 21:01:48.116841 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:48.116934 139822468673536 nn_components.py:261] mlp: residual
I0123 21:01:48.117224 139822468673536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:48.117305 139822468673536 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 21:01:48.117423 139822468673536 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:01:48.117463 139822468673536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:01:48.117495 139822468673536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:01:48.117558 139822468673536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:48.119814 139822468673536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:01:48.125168 139822468673536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:48.125418 139822468673536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:01:48.128062 139822468673536 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:01:48.140357 139822468673536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:01:48.140412 139822468673536 attention.py:418] Single window, no scan.
I0123 21:01:48.140448 139822468673536 transformer_layer.py:389] tlayer: self-attention.
I0123 21:01:48.140478 139822468673536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:48.140540 139822468673536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:48.141096 139822468673536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:48.141171 139822468673536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:48.141526 139822468673536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:48.142217 139822468673536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:48.144763 139822468673536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:48.145382 139822468673536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:48.145461 139822468673536 transformer_layer.py:468] tlayer: End windows.
I0123 21:01:48.145495 139822468673536 transformer_layer.py:472] tlayer: final FFN.
I0123 21:01:48.145553 139822468673536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:48.145690 139822468673536 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:01:48.145800 139822468673536 nn_components.py:325] mlp: activation = None
I0123 21:01:48.145838 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 21:01:48.147701 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:48.147794 139822468673536 nn_components.py:261] mlp: residual
I0123 21:01:48.150239 139822468673536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:48.150319 139822468673536 transformer_base.py:443] tbase: final FFN
I0123 21:01:48.150429 139822468673536 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:01:48.152700 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 21:01:48.154571 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:48.154665 139822468673536 nn_components.py:261] mlp: residual
I0123 21:01:48.154961 139822468673536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:48.155047 139822468673536 decoder_stack.py:344] dstack: Final layernorm.
I0123 21:01:48.157907 139822468673536 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 21:01:48.207748 139822468673536 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.207834 139822468673536 decoder_stack.py:333] dstack: autoregressive generator.
I0123 21:01:48.207887 139822468673536 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 21:01:48.207988 139822468673536 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:01:48.208025 139822468673536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:01:48.208055 139822468673536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:01:48.208117 139822468673536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.210388 139822468673536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:01:48.215826 139822468673536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.216086 139822468673536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:01:48.218663 139822468673536 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:01:48.230961 139822468673536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:01:48.231013 139822468673536 attention.py:418] Single window, no scan.
I0123 21:01:48.231047 139822468673536 transformer_layer.py:389] tlayer: self-attention.
I0123 21:01:48.231076 139822468673536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.231138 139822468673536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.231686 139822468673536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.231761 139822468673536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.232116 139822468673536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.232788 139822468673536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.235227 139822468673536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.235824 139822468673536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.235899 139822468673536 transformer_layer.py:468] tlayer: End windows.
I0123 21:01:48.235933 139822468673536 transformer_layer.py:472] tlayer: final FFN.
I0123 21:01:48.235989 139822468673536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.236116 139822468673536 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:01:48.236223 139822468673536 nn_components.py:325] mlp: activation = None
I0123 21:01:48.236260 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 21:01:48.238154 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.238245 139822468673536 nn_components.py:261] mlp: residual
I0123 21:01:48.240616 139822468673536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.240694 139822468673536 transformer_base.py:443] tbase: final FFN
I0123 21:01:48.240801 139822468673536 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:01:48.242980 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 21:01:48.244811 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.244912 139822468673536 nn_components.py:261] mlp: residual
I0123 21:01:48.245203 139822468673536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.245283 139822468673536 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 21:01:48.245389 139822468673536 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:01:48.245425 139822468673536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:01:48.245453 139822468673536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:01:48.245514 139822468673536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.247736 139822468673536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:01:48.253128 139822468673536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.253387 139822468673536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:01:48.255926 139822468673536 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:01:48.268160 139822468673536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:01:48.268213 139822468673536 attention.py:418] Single window, no scan.
I0123 21:01:48.268246 139822468673536 transformer_layer.py:389] tlayer: self-attention.
I0123 21:01:48.268276 139822468673536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.268338 139822468673536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.268890 139822468673536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.268964 139822468673536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.269316 139822468673536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.269992 139822468673536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.272433 139822468673536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.273037 139822468673536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.273113 139822468673536 transformer_layer.py:468] tlayer: End windows.
I0123 21:01:48.273149 139822468673536 transformer_layer.py:472] tlayer: final FFN.
I0123 21:01:48.273206 139822468673536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.273333 139822468673536 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:01:48.273438 139822468673536 nn_components.py:325] mlp: activation = None
I0123 21:01:48.273475 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 21:01:48.275375 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.275468 139822468673536 nn_components.py:261] mlp: residual
I0123 21:01:48.277843 139822468673536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.277922 139822468673536 transformer_base.py:443] tbase: final FFN
I0123 21:01:48.278029 139822468673536 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:01:48.280174 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 21:01:48.281992 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.282093 139822468673536 nn_components.py:261] mlp: residual
I0123 21:01:48.282381 139822468673536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.282460 139822468673536 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 21:01:48.282565 139822468673536 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:01:48.282603 139822468673536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:01:48.282632 139822468673536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:01:48.282692 139822468673536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.284888 139822468673536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:01:48.290272 139822468673536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.290529 139822468673536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:01:48.293090 139822468673536 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:01:48.305864 139822468673536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:01:48.305917 139822468673536 attention.py:418] Single window, no scan.
I0123 21:01:48.305952 139822468673536 transformer_layer.py:389] tlayer: self-attention.
I0123 21:01:48.305982 139822468673536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.306044 139822468673536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.306599 139822468673536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.306675 139822468673536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.307030 139822468673536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.307696 139822468673536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.310134 139822468673536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.310751 139822468673536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.310828 139822468673536 transformer_layer.py:468] tlayer: End windows.
I0123 21:01:48.310861 139822468673536 transformer_layer.py:472] tlayer: final FFN.
I0123 21:01:48.310918 139822468673536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.311046 139822468673536 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:01:48.311153 139822468673536 nn_components.py:325] mlp: activation = None
I0123 21:01:48.311191 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 21:01:48.313099 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.313190 139822468673536 nn_components.py:261] mlp: residual
I0123 21:01:48.315578 139822468673536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.315656 139822468673536 transformer_base.py:443] tbase: final FFN
I0123 21:01:48.315762 139822468673536 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:01:48.317937 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 21:01:48.319777 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.319869 139822468673536 nn_components.py:261] mlp: residual
I0123 21:01:48.320162 139822468673536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.320242 139822468673536 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 21:01:48.320348 139822468673536 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:01:48.320386 139822468673536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:01:48.320416 139822468673536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:01:48.320477 139822468673536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.322698 139822468673536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:01:48.328131 139822468673536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.328388 139822468673536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:01:48.330959 139822468673536 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:01:48.343259 139822468673536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:01:48.343314 139822468673536 attention.py:418] Single window, no scan.
I0123 21:01:48.343348 139822468673536 transformer_layer.py:389] tlayer: self-attention.
I0123 21:01:48.343378 139822468673536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.343440 139822468673536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.343993 139822468673536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.344068 139822468673536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.344425 139822468673536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.345103 139822468673536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.347555 139822468673536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.348163 139822468673536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.348239 139822468673536 transformer_layer.py:468] tlayer: End windows.
I0123 21:01:48.348273 139822468673536 transformer_layer.py:472] tlayer: final FFN.
I0123 21:01:48.348330 139822468673536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.348457 139822468673536 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:01:48.348564 139822468673536 nn_components.py:325] mlp: activation = None
I0123 21:01:48.348601 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 21:01:48.350532 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.350624 139822468673536 nn_components.py:261] mlp: residual
I0123 21:01:48.352992 139822468673536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.353069 139822468673536 transformer_base.py:443] tbase: final FFN
I0123 21:01:48.353177 139822468673536 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:01:48.355359 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 21:01:48.357201 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.357295 139822468673536 nn_components.py:261] mlp: residual
I0123 21:01:48.357583 139822468673536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.357674 139822468673536 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 21:01:48.357790 139822468673536 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:01:48.357827 139822468673536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:01:48.357857 139822468673536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:01:48.357918 139822468673536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.360155 139822468673536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:01:48.365590 139822468673536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.365852 139822468673536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:01:48.368419 139822468673536 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:01:48.380672 139822468673536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:01:48.380725 139822468673536 attention.py:418] Single window, no scan.
I0123 21:01:48.380759 139822468673536 transformer_layer.py:389] tlayer: self-attention.
I0123 21:01:48.380788 139822468673536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.380849 139822468673536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.381397 139822468673536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.381472 139822468673536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.381837 139822468673536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.382513 139822468673536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.384939 139822468673536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.385542 139822468673536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.385617 139822468673536 transformer_layer.py:468] tlayer: End windows.
I0123 21:01:48.385657 139822468673536 transformer_layer.py:472] tlayer: final FFN.
I0123 21:01:48.385715 139822468673536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.385841 139822468673536 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:01:48.385947 139822468673536 nn_components.py:325] mlp: activation = None
I0123 21:01:48.385985 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 21:01:48.387868 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.387959 139822468673536 nn_components.py:261] mlp: residual
I0123 21:01:48.390316 139822468673536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.390394 139822468673536 transformer_base.py:443] tbase: final FFN
I0123 21:01:48.390500 139822468673536 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:01:48.392652 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 21:01:48.394474 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.394569 139822468673536 nn_components.py:261] mlp: residual
I0123 21:01:48.394856 139822468673536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.394943 139822468673536 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 21:01:48.395053 139822468673536 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:01:48.395092 139822468673536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:01:48.395121 139822468673536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:01:48.395181 139822468673536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.397397 139822468673536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:01:48.402825 139822468673536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.403083 139822468673536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:01:48.405632 139822468673536 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:01:48.418379 139822468673536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:01:48.418432 139822468673536 attention.py:418] Single window, no scan.
I0123 21:01:48.418466 139822468673536 transformer_layer.py:389] tlayer: self-attention.
I0123 21:01:48.418496 139822468673536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.418557 139822468673536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.419106 139822468673536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.419180 139822468673536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.419532 139822468673536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.420205 139822468673536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.422644 139822468673536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.423248 139822468673536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.423323 139822468673536 transformer_layer.py:468] tlayer: End windows.
I0123 21:01:48.423357 139822468673536 transformer_layer.py:472] tlayer: final FFN.
I0123 21:01:48.423413 139822468673536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.423538 139822468673536 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:01:48.423642 139822468673536 nn_components.py:325] mlp: activation = None
I0123 21:01:48.423678 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 21:01:48.425559 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.425659 139822468673536 nn_components.py:261] mlp: residual
I0123 21:01:48.428017 139822468673536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.428094 139822468673536 transformer_base.py:443] tbase: final FFN
I0123 21:01:48.428200 139822468673536 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:01:48.430359 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 21:01:48.432162 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.432255 139822468673536 nn_components.py:261] mlp: residual
I0123 21:01:48.432543 139822468673536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.432622 139822468673536 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 21:01:48.432734 139822468673536 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:01:48.432773 139822468673536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:01:48.432803 139822468673536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:01:48.432865 139822468673536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.435063 139822468673536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:01:48.440434 139822468673536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.440687 139822468673536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:01:48.443224 139822468673536 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:01:48.455420 139822468673536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:01:48.455473 139822468673536 attention.py:418] Single window, no scan.
I0123 21:01:48.455507 139822468673536 transformer_layer.py:389] tlayer: self-attention.
I0123 21:01:48.455537 139822468673536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.455598 139822468673536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.456141 139822468673536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.456216 139822468673536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.456566 139822468673536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.457259 139822468673536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.459713 139822468673536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.460319 139822468673536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.460395 139822468673536 transformer_layer.py:468] tlayer: End windows.
I0123 21:01:48.460429 139822468673536 transformer_layer.py:472] tlayer: final FFN.
I0123 21:01:48.460485 139822468673536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.460613 139822468673536 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:01:48.460719 139822468673536 nn_components.py:325] mlp: activation = None
I0123 21:01:48.460756 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 21:01:48.462666 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.462758 139822468673536 nn_components.py:261] mlp: residual
I0123 21:01:48.465139 139822468673536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.465216 139822468673536 transformer_base.py:443] tbase: final FFN
I0123 21:01:48.465322 139822468673536 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:01:48.467469 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 21:01:48.469288 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.469381 139822468673536 nn_components.py:261] mlp: residual
I0123 21:01:48.469670 139822468673536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.469750 139822468673536 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 21:01:48.469856 139822468673536 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:01:48.469900 139822468673536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:01:48.469931 139822468673536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:01:48.469993 139822468673536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.472190 139822468673536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:01:48.477584 139822468673536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.477847 139822468673536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:01:48.480376 139822468673536 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:01:48.492511 139822468673536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:01:48.492565 139822468673536 attention.py:418] Single window, no scan.
I0123 21:01:48.492599 139822468673536 transformer_layer.py:389] tlayer: self-attention.
I0123 21:01:48.492627 139822468673536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.492687 139822468673536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.493230 139822468673536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.493305 139822468673536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.493662 139822468673536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.494324 139822468673536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.496725 139822468673536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.497332 139822468673536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.497406 139822468673536 transformer_layer.py:468] tlayer: End windows.
I0123 21:01:48.497440 139822468673536 transformer_layer.py:472] tlayer: final FFN.
I0123 21:01:48.497496 139822468673536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.497620 139822468673536 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:01:48.497735 139822468673536 nn_components.py:325] mlp: activation = None
I0123 21:01:48.497772 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 21:01:48.499659 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.499748 139822468673536 nn_components.py:261] mlp: residual
I0123 21:01:48.502112 139822468673536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.502188 139822468673536 transformer_base.py:443] tbase: final FFN
I0123 21:01:48.502293 139822468673536 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:01:48.504446 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 21:01:48.506279 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.506372 139822468673536 nn_components.py:261] mlp: residual
I0123 21:01:48.506658 139822468673536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.506735 139822468673536 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 21:01:48.506839 139822468673536 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:01:48.506883 139822468673536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:01:48.506914 139822468673536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:01:48.506975 139822468673536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.509178 139822468673536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:01:48.514581 139822468673536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.514837 139822468673536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:01:48.517401 139822468673536 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:01:48.529978 139822468673536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:01:48.530032 139822468673536 attention.py:418] Single window, no scan.
I0123 21:01:48.530067 139822468673536 transformer_layer.py:389] tlayer: self-attention.
I0123 21:01:48.530096 139822468673536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.530155 139822468673536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.530701 139822468673536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.530775 139822468673536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.531130 139822468673536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.531797 139822468673536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.534270 139822468673536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.534881 139822468673536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.534957 139822468673536 transformer_layer.py:468] tlayer: End windows.
I0123 21:01:48.534990 139822468673536 transformer_layer.py:472] tlayer: final FFN.
I0123 21:01:48.535046 139822468673536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.535173 139822468673536 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:01:48.535279 139822468673536 nn_components.py:325] mlp: activation = None
I0123 21:01:48.535315 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 21:01:48.537205 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.537296 139822468673536 nn_components.py:261] mlp: residual
I0123 21:01:48.539669 139822468673536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.539745 139822468673536 transformer_base.py:443] tbase: final FFN
I0123 21:01:48.539851 139822468673536 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:01:48.542003 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 21:01:48.543820 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.543911 139822468673536 nn_components.py:261] mlp: residual
I0123 21:01:48.544197 139822468673536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.544276 139822468673536 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 21:01:48.544383 139822468673536 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:01:48.544420 139822468673536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:01:48.544456 139822468673536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:01:48.544520 139822468673536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.546726 139822468673536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:01:48.552138 139822468673536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.552392 139822468673536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:01:48.554954 139822468673536 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:01:48.567330 139822468673536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:01:48.567383 139822468673536 attention.py:418] Single window, no scan.
I0123 21:01:48.567417 139822468673536 transformer_layer.py:389] tlayer: self-attention.
I0123 21:01:48.567447 139822468673536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.567506 139822468673536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.568051 139822468673536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.568125 139822468673536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.568474 139822468673536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.569146 139822468673536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.571603 139822468673536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.572218 139822468673536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.572293 139822468673536 transformer_layer.py:468] tlayer: End windows.
I0123 21:01:48.572327 139822468673536 transformer_layer.py:472] tlayer: final FFN.
I0123 21:01:48.572383 139822468673536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.572509 139822468673536 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:01:48.572617 139822468673536 nn_components.py:325] mlp: activation = None
I0123 21:01:48.572654 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 21:01:48.574555 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.574645 139822468673536 nn_components.py:261] mlp: residual
I0123 21:01:48.577020 139822468673536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.577095 139822468673536 transformer_base.py:443] tbase: final FFN
I0123 21:01:48.577201 139822468673536 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:01:48.579357 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 21:01:48.581180 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.581272 139822468673536 nn_components.py:261] mlp: residual
I0123 21:01:48.581561 139822468673536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.581645 139822468673536 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 21:01:48.581755 139822468673536 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:01:48.581793 139822468673536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:01:48.581823 139822468673536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:01:48.581890 139822468673536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.584110 139822468673536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:01:48.589512 139822468673536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.589776 139822468673536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:01:48.592323 139822468673536 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:01:48.604545 139822468673536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:01:48.604598 139822468673536 attention.py:418] Single window, no scan.
I0123 21:01:48.604632 139822468673536 transformer_layer.py:389] tlayer: self-attention.
I0123 21:01:48.604663 139822468673536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.604723 139822468673536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.605275 139822468673536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.605348 139822468673536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.605710 139822468673536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.606379 139822468673536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.608804 139822468673536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.609409 139822468673536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.609484 139822468673536 transformer_layer.py:468] tlayer: End windows.
I0123 21:01:48.609518 139822468673536 transformer_layer.py:472] tlayer: final FFN.
I0123 21:01:48.609573 139822468673536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.609704 139822468673536 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:01:48.609817 139822468673536 nn_components.py:325] mlp: activation = None
I0123 21:01:48.609853 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 21:01:48.611743 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.611833 139822468673536 nn_components.py:261] mlp: residual
I0123 21:01:48.614204 139822468673536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.614282 139822468673536 transformer_base.py:443] tbase: final FFN
I0123 21:01:48.614388 139822468673536 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:01:48.616552 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 21:01:48.618381 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.618474 139822468673536 nn_components.py:261] mlp: residual
I0123 21:01:48.618759 139822468673536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.618838 139822468673536 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 21:01:48.618944 139822468673536 transformer_layer.py:154] tlayer: recurrent = False
I0123 21:01:48.618981 139822468673536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 21:01:48.619010 139822468673536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 21:01:48.619071 139822468673536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.621264 139822468673536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 21:01:48.626702 139822468673536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.626971 139822468673536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 21:01:48.629508 139822468673536 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 21:01:48.642125 139822468673536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 21:01:48.642179 139822468673536 attention.py:418] Single window, no scan.
I0123 21:01:48.642212 139822468673536 transformer_layer.py:389] tlayer: self-attention.
I0123 21:01:48.642242 139822468673536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.642301 139822468673536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.642847 139822468673536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.642920 139822468673536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.643273 139822468673536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.643942 139822468673536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.646364 139822468673536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.646970 139822468673536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.647045 139822468673536 transformer_layer.py:468] tlayer: End windows.
I0123 21:01:48.647079 139822468673536 transformer_layer.py:472] tlayer: final FFN.
I0123 21:01:48.647135 139822468673536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.647259 139822468673536 transformer_base.py:410] tbase: post-attention MLP.
I0123 21:01:48.647365 139822468673536 nn_components.py:325] mlp: activation = None
I0123 21:01:48.647402 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 21:01:48.649292 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.649383 139822468673536 nn_components.py:261] mlp: residual
I0123 21:01:48.651757 139822468673536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.651834 139822468673536 transformer_base.py:443] tbase: final FFN
I0123 21:01:48.651939 139822468673536 nn_components.py:320] mlp: hidden 4096, relu
I0123 21:01:48.654100 139822468673536 nn_components.py:329] mlp: final activation = None
I0123 21:01:48.655922 139822468673536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.656013 139822468673536 nn_components.py:261] mlp: residual
I0123 21:01:48.656301 139822468673536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:01:48.656383 139822468673536 decoder_stack.py:344] dstack: Final layernorm.
I0123 21:01:48.659210 139822468673536 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 21:02:06.390466 139822468673536 alphageometry.py:566] LM output (score=-1.436601): "r : T b g g r 27 ;"
I0123 21:02:06.390652 139822468673536 alphageometry.py:567] Translation: "r = on_tline r g b g"

I0123 21:02:06.390705 139822468673536 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e c b a; f = foot f b c a; g = on_line g c e, on_line g b f; h = on_circle h d b; i = on_circle i d h, on_line i g h; j = foot j h b a; k = foot k h c b; l = on_pline l i j k, on_line l k c; m = on_circle m d i, on_line m l i; n = midpoint n b a; o = foot o h d n; p = mirror p h o; q = on_line q m p, on_line q b a; r = on_tline r g b g ? cong q m q i"
I0123 21:02:06.390991 139822468673536 graph.py:498] 
I0123 21:02:06.391053 139822468673536 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e c b a; f = foot f b c a; g = on_line g c e, on_line g b f; h = on_circle h d b; i = on_circle i d h, on_line i g h; j = foot j h b a; k = foot k h c b; l = on_pline l i j k, on_line l k c; m = on_circle m d i, on_line m l i; n = midpoint n b a; o = foot o h d n; p = mirror p h o; q = on_line q m p, on_line q b a; r = on_tline r g b g ? cong q m q i
I0123 21:02:15.074425 139822468673536 ddar.py:60] Depth 1/1000 time = 8.617007493972778
I0123 21:02:39.533953 139822468673536 ddar.py:60] Depth 2/1000 time = 24.45934224128723
I0123 21:03:27.254771 139822468673536 ddar.py:60] Depth 3/1000 time = 47.72057771682739
I0123 21:04:13.073453 139822468673536 ddar.py:60] Depth 4/1000 time = 45.81830716133118
I0123 21:04:58.067136 139822468673536 ddar.py:60] Depth 5/1000 time = 44.993186950683594
I0123 21:05:43.500825 139822468673536 ddar.py:60] Depth 6/1000 time = 45.43249535560608
I0123 21:06:29.615772 139822468673536 ddar.py:60] Depth 7/1000 time = 46.03189969062805
I0123 21:07:15.201655 139822468673536 ddar.py:60] Depth 8/1000 time = 45.273380279541016
I0123 21:07:15.210748 139822468673536 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 21:07:15.210857 139822468673536 alphageometry.py:566] LM output (score=-1.592241): "r : T f g g r 27 ;"
I0123 21:07:15.210896 139822468673536 alphageometry.py:567] Translation: "r = on_tline r g f g"

I0123 21:07:15.210937 139822468673536 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e c b a; f = foot f b c a; g = on_line g c e, on_line g b f; h = on_circle h d b; i = on_circle i d h, on_line i g h; j = foot j h b a; k = foot k h c b; l = on_pline l i j k, on_line l k c; m = on_circle m d i, on_line m l i; n = midpoint n b a; o = foot o h d n; p = mirror p h o; q = on_line q m p, on_line q b a; r = on_tline r g f g ? cong q m q i"
I0123 21:07:15.211137 139822468673536 graph.py:498] 
I0123 21:07:15.211196 139822468673536 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e c b a; f = foot f b c a; g = on_line g c e, on_line g b f; h = on_circle h d b; i = on_circle i d h, on_line i g h; j = foot j h b a; k = foot k h c b; l = on_pline l i j k, on_line l k c; m = on_circle m d i, on_line m l i; n = midpoint n b a; o = foot o h d n; p = mirror p h o; q = on_line q m p, on_line q b a; r = on_tline r g f g ? cong q m q i
I0123 21:07:23.928229 139822468673536 ddar.py:60] Depth 1/1000 time = 8.649204969406128
I0123 21:07:48.317280 139822468673536 ddar.py:60] Depth 2/1000 time = 24.38884663581848
I0123 21:08:36.660812 139822468673536 ddar.py:60] Depth 3/1000 time = 48.34322118759155
I0123 21:09:23.095399 139822468673536 ddar.py:60] Depth 4/1000 time = 46.43411684036255
I0123 21:10:08.337388 139822468673536 ddar.py:60] Depth 5/1000 time = 45.241535663604736
I0123 21:10:54.958644 139822468673536 ddar.py:60] Depth 6/1000 time = 46.620197772979736
I0123 21:11:40.573235 139822468673536 ddar.py:60] Depth 7/1000 time = 45.52876019477844
I0123 21:12:27.178261 139822468673536 ddar.py:60] Depth 8/1000 time = 46.29866313934326
I0123 21:12:27.187343 139822468673536 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 21:12:27.187486 139822468673536 alphageometry.py:566] LM output (score=-1.602929): "r : T b f f r 27 ;"
I0123 21:12:27.187526 139822468673536 alphageometry.py:567] Translation: "r = on_tline r f b f"

I0123 21:12:27.187586 139822468673536 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e c b a; f = foot f b c a; g = on_line g c e, on_line g b f; h = on_circle h d b; i = on_circle i d h, on_line i g h; j = foot j h b a; k = foot k h c b; l = on_pline l i j k, on_line l k c; m = on_circle m d i, on_line m l i; n = midpoint n b a; o = foot o h d n; p = mirror p h o; q = on_line q m p, on_line q b a; r = on_tline r f b f ? cong q m q i"
I0123 21:12:27.187816 139822468673536 graph.py:498] 
I0123 21:12:27.187879 139822468673536 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e c b a; f = foot f b c a; g = on_line g c e, on_line g b f; h = on_circle h d b; i = on_circle i d h, on_line i g h; j = foot j h b a; k = foot k h c b; l = on_pline l i j k, on_line l k c; m = on_circle m d i, on_line m l i; n = midpoint n b a; o = foot o h d n; p = mirror p h o; q = on_line q m p, on_line q b a; r = on_tline r f b f ? cong q m q i
I0123 21:12:36.023614 139822468673536 ddar.py:60] Depth 1/1000 time = 8.771430253982544
I0123 21:13:03.135036 139822468673536 ddar.py:60] Depth 2/1000 time = 27.11117720603943
I0123 21:13:51.127324 139822468673536 ddar.py:60] Depth 3/1000 time = 47.99190664291382
I0123 21:14:38.975776 139822468673536 ddar.py:60] Depth 4/1000 time = 47.84800052642822
I0123 21:15:28.814781 139822468673536 ddar.py:60] Depth 5/1000 time = 49.838494300842285
I0123 21:16:16.720708 139822468673536 ddar.py:60] Depth 6/1000 time = 47.90485882759094
I0123 21:17:05.894028 139822468673536 ddar.py:60] Depth 7/1000 time = 49.09616708755493
I0123 21:17:54.376208 139822468673536 ddar.py:60] Depth 8/1000 time = 48.180745124816895
I0123 21:17:54.385014 139822468673536 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 21:17:54.385153 139822468673536 alphageometry.py:566] LM output (score=-1.944018): "r : T f g f r 27 ;"
I0123 21:17:54.385193 139822468673536 alphageometry.py:567] Translation: "r = on_tline r f f g"

I0123 21:17:54.385252 139822468673536 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e c b a; f = foot f b c a; g = on_line g c e, on_line g b f; h = on_circle h d b; i = on_circle i d h, on_line i g h; j = foot j h b a; k = foot k h c b; l = on_pline l i j k, on_line l k c; m = on_circle m d i, on_line m l i; n = midpoint n b a; o = foot o h d n; p = mirror p h o; q = on_line q m p, on_line q b a; r = on_tline r f f g ? cong q m q i"
I0123 21:17:54.385477 139822468673536 graph.py:498] 
I0123 21:17:54.385540 139822468673536 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e c b a; f = foot f b c a; g = on_line g c e, on_line g b f; h = on_circle h d b; i = on_circle i d h, on_line i g h; j = foot j h b a; k = foot k h c b; l = on_pline l i j k, on_line l k c; m = on_circle m d i, on_line m l i; n = midpoint n b a; o = foot o h d n; p = mirror p h o; q = on_line q m p, on_line q b a; r = on_tline r f f g ? cong q m q i
I0123 21:18:03.505845 139822468673536 ddar.py:60] Depth 1/1000 time = 9.052372932434082
I0123 21:18:30.381188 139822468673536 ddar.py:60] Depth 2/1000 time = 26.87514042854309
I0123 21:19:20.309204 139822468673536 ddar.py:60] Depth 3/1000 time = 49.92772626876831
I0123 21:20:08.886889 139822468673536 ddar.py:60] Depth 4/1000 time = 48.57721018791199
I0123 21:20:57.739824 139822468673536 ddar.py:60] Depth 5/1000 time = 48.852415561676025
I0123 21:21:46.724984 139822468673536 ddar.py:60] Depth 6/1000 time = 48.984097480773926
I0123 21:22:36.479393 139822468673536 ddar.py:60] Depth 7/1000 time = 49.675464391708374
I0123 21:23:25.488129 139822468673536 ddar.py:60] Depth 8/1000 time = 48.705321311950684
I0123 21:23:25.496757 139822468673536 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 21:23:25.496888 139822468673536 alphageometry.py:566] LM output (score=-1.999530): "r : T b g k r 27 ;"
I0123 21:23:25.496926 139822468673536 alphageometry.py:567] Translation: "r = on_tline r k b g"

I0123 21:23:25.496982 139822468673536 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e c b a; f = foot f b c a; g = on_line g c e, on_line g b f; h = on_circle h d b; i = on_circle i d h, on_line i g h; j = foot j h b a; k = foot k h c b; l = on_pline l i j k, on_line l k c; m = on_circle m d i, on_line m l i; n = midpoint n b a; o = foot o h d n; p = mirror p h o; q = on_line q m p, on_line q b a; r = on_tline r k b g ? cong q m q i"
I0123 21:23:25.497205 139822468673536 graph.py:498] 
I0123 21:23:25.497267 139822468673536 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e c b a; f = foot f b c a; g = on_line g c e, on_line g b f; h = on_circle h d b; i = on_circle i d h, on_line i g h; j = foot j h b a; k = foot k h c b; l = on_pline l i j k, on_line l k c; m = on_circle m d i, on_line m l i; n = midpoint n b a; o = foot o h d n; p = mirror p h o; q = on_line q m p, on_line q b a; r = on_tline r k b g ? cong q m q i
I0123 21:23:33.471318 139822468673536 ddar.py:60] Depth 1/1000 time = 7.90817928314209
I0123 21:23:58.657650 139822468673536 ddar.py:60] Depth 2/1000 time = 25.186138153076172
I0123 21:24:44.540262 139822468673536 ddar.py:60] Depth 3/1000 time = 45.88230490684509
I0123 21:25:31.112086 139822468673536 ddar.py:60] Depth 4/1000 time = 46.57134556770325
I0123 21:26:16.547086 139822468673536 ddar.py:60] Depth 5/1000 time = 45.43449878692627
I0123 21:27:02.471094 139822468673536 ddar.py:60] Depth 6/1000 time = 45.92301845550537
I0123 21:27:48.760259 139822468673536 ddar.py:60] Depth 7/1000 time = 46.20535850524902
I0123 21:28:35.848573 139822468673536 ddar.py:60] Depth 8/1000 time = 46.7866415977478
I0123 21:28:35.856552 139822468673536 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 21:28:35.856693 139822468673536 alphageometry.py:566] LM output (score=-2.220363): "r : T b k g r 27 ;"
I0123 21:28:35.856734 139822468673536 alphageometry.py:567] Translation: "r = on_tline r g b k"

I0123 21:28:35.856794 139822468673536 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e c b a; f = foot f b c a; g = on_line g c e, on_line g b f; h = on_circle h d b; i = on_circle i d h, on_line i g h; j = foot j h b a; k = foot k h c b; l = on_pline l i j k, on_line l k c; m = on_circle m d i, on_line m l i; n = midpoint n b a; o = foot o h d n; p = mirror p h o; q = on_line q m p, on_line q b a; r = on_tline r g b k ? cong q m q i"
I0123 21:28:35.857016 139822468673536 graph.py:498] 
I0123 21:28:35.857077 139822468673536 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e c b a; f = foot f b c a; g = on_line g c e, on_line g b f; h = on_circle h d b; i = on_circle i d h, on_line i g h; j = foot j h b a; k = foot k h c b; l = on_pline l i j k, on_line l k c; m = on_circle m d i, on_line m l i; n = midpoint n b a; o = foot o h d n; p = mirror p h o; q = on_line q m p, on_line q b a; r = on_tline r g b k ? cong q m q i
I0123 21:28:43.733449 139822468673536 ddar.py:60] Depth 1/1000 time = 7.801116228103638
I0123 21:29:08.306169 139822468673536 ddar.py:60] Depth 2/1000 time = 24.57253074645996
I0123 21:29:56.413808 139822468673536 ddar.py:60] Depth 3/1000 time = 48.10734677314758
I0123 21:30:43.040216 139822468673536 ddar.py:60] Depth 4/1000 time = 46.625959157943726
I0123 21:31:30.102737 139822468673536 ddar.py:60] Depth 5/1000 time = 47.06203317642212
I0123 21:32:17.447538 139822468673536 ddar.py:60] Depth 6/1000 time = 47.34365248680115
I0123 21:33:05.930005 139822468673536 ddar.py:60] Depth 7/1000 time = 48.392932415008545
I0123 21:33:54.245872 139822468673536 ddar.py:60] Depth 8/1000 time = 48.016950845718384
I0123 21:33:54.254031 139822468673536 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 21:33:54.254168 139822468673536 alphageometry.py:566] LM output (score=-2.316493): "r : T b k k r 27 ;"
I0123 21:33:54.254207 139822468673536 alphageometry.py:567] Translation: "r = on_tline r k b k"

I0123 21:33:54.254265 139822468673536 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e c b a; f = foot f b c a; g = on_line g c e, on_line g b f; h = on_circle h d b; i = on_circle i d h, on_line i g h; j = foot j h b a; k = foot k h c b; l = on_pline l i j k, on_line l k c; m = on_circle m d i, on_line m l i; n = midpoint n b a; o = foot o h d n; p = mirror p h o; q = on_line q m p, on_line q b a; r = on_tline r k b k ? cong q m q i"
I0123 21:33:54.254488 139822468673536 graph.py:498] 
I0123 21:33:54.254551 139822468673536 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e c b a; f = foot f b c a; g = on_line g c e, on_line g b f; h = on_circle h d b; i = on_circle i d h, on_line i g h; j = foot j h b a; k = foot k h c b; l = on_pline l i j k, on_line l k c; m = on_circle m d i, on_line m l i; n = midpoint n b a; o = foot o h d n; p = mirror p h o; q = on_line q m p, on_line q b a; r = on_tline r k b k ? cong q m q i
I0123 21:34:03.526915 139822468673536 ddar.py:60] Depth 1/1000 time = 9.209611415863037
I0123 21:34:30.611081 139822468673536 ddar.py:60] Depth 2/1000 time = 27.083889484405518
I0123 21:35:19.142819 139822468673536 ddar.py:60] Depth 3/1000 time = 48.531322956085205
I0123 21:36:07.000853 139822468673536 ddar.py:60] Depth 4/1000 time = 47.85756206512451
I0123 21:36:55.736938 139822468673536 ddar.py:60] Depth 5/1000 time = 48.73560404777527
I0123 21:37:43.956748 139822468673536 ddar.py:60] Depth 6/1000 time = 48.21872329711914
I0123 21:38:31.976242 139822468673536 ddar.py:60] Depth 7/1000 time = 47.93987274169922
I0123 21:39:21.435972 139822468673536 ddar.py:60] Depth 8/1000 time = 48.749507665634155
I0123 21:39:21.444555 139822468673536 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 21:39:21.444689 139822468673536 alphageometry.py:566] LM output (score=-2.407501): "r : T b h k r 27 ;"
I0123 21:39:21.444726 139822468673536 alphageometry.py:567] Translation: "r = on_tline r k b h"

I0123 21:39:21.444783 139822468673536 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e c b a; f = foot f b c a; g = on_line g c e, on_line g b f; h = on_circle h d b; i = on_circle i d h, on_line i g h; j = foot j h b a; k = foot k h c b; l = on_pline l i j k, on_line l k c; m = on_circle m d i, on_line m l i; n = midpoint n b a; o = foot o h d n; p = mirror p h o; q = on_line q m p, on_line q b a; r = on_tline r k b h ? cong q m q i"
I0123 21:39:21.444996 139822468673536 graph.py:498] 
I0123 21:39:21.445056 139822468673536 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e c b a; f = foot f b c a; g = on_line g c e, on_line g b f; h = on_circle h d b; i = on_circle i d h, on_line i g h; j = foot j h b a; k = foot k h c b; l = on_pline l i j k, on_line l k c; m = on_circle m d i, on_line m l i; n = midpoint n b a; o = foot o h d n; p = mirror p h o; q = on_line q m p, on_line q b a; r = on_tline r k b h ? cong q m q i
I0123 21:39:29.877287 139822468673536 ddar.py:60] Depth 1/1000 time = 8.361603498458862
I0123 21:39:55.306404 139822468673536 ddar.py:60] Depth 2/1000 time = 25.428768634796143
I0123 21:40:42.898559 139822468673536 ddar.py:60] Depth 3/1000 time = 47.59172582626343
I0123 21:41:31.178547 139822468673536 ddar.py:60] Depth 4/1000 time = 48.279528856277466
I0123 21:42:20.655729 139822468673536 ddar.py:60] Depth 5/1000 time = 49.476675271987915
I0123 21:43:10.285664 139822468673536 ddar.py:60] Depth 6/1000 time = 49.628862619400024
I0123 21:44:00.127301 139822468673536 ddar.py:60] Depth 7/1000 time = 49.7475905418396
I0123 21:44:49.479202 139822468673536 ddar.py:60] Depth 8/1000 time = 49.01068043708801
I0123 21:44:49.486590 139822468673536 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 21:44:49.486777 139822468673536 alphageometry.py:566] LM output (score=-2.429547): "r : T f g k r 27 ;"
I0123 21:44:49.486821 139822468673536 alphageometry.py:567] Translation: "r = on_tline r k f g"

I0123 21:44:49.486882 139822468673536 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e c b a; f = foot f b c a; g = on_line g c e, on_line g b f; h = on_circle h d b; i = on_circle i d h, on_line i g h; j = foot j h b a; k = foot k h c b; l = on_pline l i j k, on_line l k c; m = on_circle m d i, on_line m l i; n = midpoint n b a; o = foot o h d n; p = mirror p h o; q = on_line q m p, on_line q b a; r = on_tline r k f g ? cong q m q i"
I0123 21:44:49.487119 139822468673536 graph.py:498] 
I0123 21:44:49.487184 139822468673536 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e c b a; f = foot f b c a; g = on_line g c e, on_line g b f; h = on_circle h d b; i = on_circle i d h, on_line i g h; j = foot j h b a; k = foot k h c b; l = on_pline l i j k, on_line l k c; m = on_circle m d i, on_line m l i; n = midpoint n b a; o = foot o h d n; p = mirror p h o; q = on_line q m p, on_line q b a; r = on_tline r k f g ? cong q m q i
I0123 21:44:59.278798 139822468673536 ddar.py:60] Depth 1/1000 time = 9.718583345413208
I0123 21:45:23.966405 139822468673536 ddar.py:60] Depth 2/1000 time = 24.68723750114441
I0123 21:46:08.893028 139822468673536 ddar.py:60] Depth 3/1000 time = 44.92622089385986
I0123 21:46:56.218504 139822468673536 ddar.py:60] Depth 4/1000 time = 47.325016260147095
I0123 21:47:44.496869 139822468673536 ddar.py:60] Depth 5/1000 time = 48.277888774871826
I0123 21:48:32.894918 139822468673536 ddar.py:60] Depth 6/1000 time = 48.39692687988281
I0123 21:49:21.569109 139822468673536 ddar.py:60] Depth 7/1000 time = 48.587836027145386
I0123 21:50:08.399745 139822468673536 ddar.py:60] Depth 8/1000 time = 46.5221529006958
I0123 21:50:08.407673 139822468673536 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 21:50:08.407810 139822468673536 alphageometry.py:566] LM output (score=-2.443819): "r : T b f b r 27 ;"
I0123 21:50:08.407851 139822468673536 alphageometry.py:567] Translation: "r = on_tline r b b f"

I0123 21:50:08.407910 139822468673536 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e c b a; f = foot f b c a; g = on_line g c e, on_line g b f; h = on_circle h d b; i = on_circle i d h, on_line i g h; j = foot j h b a; k = foot k h c b; l = on_pline l i j k, on_line l k c; m = on_circle m d i, on_line m l i; n = midpoint n b a; o = foot o h d n; p = mirror p h o; q = on_line q m p, on_line q b a; r = on_tline r b b f ? cong q m q i"
I0123 21:50:08.408132 139822468673536 graph.py:498] 
I0123 21:50:08.408195 139822468673536 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e c b a; f = foot f b c a; g = on_line g c e, on_line g b f; h = on_circle h d b; i = on_circle i d h, on_line i g h; j = foot j h b a; k = foot k h c b; l = on_pline l i j k, on_line l k c; m = on_circle m d i, on_line m l i; n = midpoint n b a; o = foot o h d n; p = mirror p h o; q = on_line q m p, on_line q b a; r = on_tline r b b f ? cong q m q i
I0123 21:50:17.548940 139822468673536 ddar.py:60] Depth 1/1000 time = 9.071725130081177
I0123 21:50:42.637530 139822468673536 ddar.py:60] Depth 2/1000 time = 25.088411569595337
I0123 21:51:32.189190 139822468673536 ddar.py:60] Depth 3/1000 time = 49.5513129234314
I0123 21:52:19.091749 139822468673536 ddar.py:60] Depth 4/1000 time = 46.90212082862854
I0123 21:53:06.424103 139822468673536 ddar.py:60] Depth 5/1000 time = 47.33188509941101
I0123 21:53:53.283245 139822468673536 ddar.py:60] Depth 6/1000 time = 46.858057737350464
I0123 21:54:40.023179 139822468673536 ddar.py:60] Depth 7/1000 time = 46.653526067733765
I0123 21:55:28.405310 139822468673536 ddar.py:60] Depth 8/1000 time = 48.07510781288147
I0123 21:55:28.413252 139822468673536 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 21:55:28.413377 139822468673536 alphageometry.py:566] LM output (score=-2.450398): "r : T b g o r 27 ;"
I0123 21:55:28.413419 139822468673536 alphageometry.py:567] Translation: "r = on_tline r o b g"

I0123 21:55:28.413473 139822468673536 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e c b a; f = foot f b c a; g = on_line g c e, on_line g b f; h = on_circle h d b; i = on_circle i d h, on_line i g h; j = foot j h b a; k = foot k h c b; l = on_pline l i j k, on_line l k c; m = on_circle m d i, on_line m l i; n = midpoint n b a; o = foot o h d n; p = mirror p h o; q = on_line q m p, on_line q b a; r = on_tline r o b g ? cong q m q i"
I0123 21:55:28.413700 139822468673536 graph.py:498] 
I0123 21:55:28.413783 139822468673536 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e c b a; f = foot f b c a; g = on_line g c e, on_line g b f; h = on_circle h d b; i = on_circle i d h, on_line i g h; j = foot j h b a; k = foot k h c b; l = on_pline l i j k, on_line l k c; m = on_circle m d i, on_line m l i; n = midpoint n b a; o = foot o h d n; p = mirror p h o; q = on_line q m p, on_line q b a; r = on_tline r o b g ? cong q m q i
I0123 21:55:36.337929 139822468673536 ddar.py:60] Depth 1/1000 time = 7.852504253387451
I0123 21:56:01.892782 139822468673536 ddar.py:60] Depth 2/1000 time = 25.554579257965088
I0123 21:56:48.542910 139822468673536 ddar.py:60] Depth 3/1000 time = 46.64973306655884
I0123 21:57:34.618523 139822468673536 ddar.py:60] Depth 4/1000 time = 46.0751690864563
I0123 21:58:22.166905 139822468673536 ddar.py:60] Depth 5/1000 time = 47.547903060913086
I0123 21:59:08.455027 139822468673536 ddar.py:60] Depth 6/1000 time = 46.28702402114868
I0123 21:59:55.557644 139822468673536 ddar.py:60] Depth 7/1000 time = 47.018630504608154
I0123 22:00:43.155061 139822468673536 ddar.py:60] Depth 8/1000 time = 47.2909460067749
I0123 22:00:43.162382 139822468673536 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 22:00:43.162508 139822468673536 alphageometry.py:566] LM output (score=-2.529531): "r : T b k l r 27 ;"
I0123 22:00:43.162547 139822468673536 alphageometry.py:567] Translation: "r = on_tline r l b k"

I0123 22:00:43.162598 139822468673536 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e c b a; f = foot f b c a; g = on_line g c e, on_line g b f; h = on_circle h d b; i = on_circle i d h, on_line i g h; j = foot j h b a; k = foot k h c b; l = on_pline l i j k, on_line l k c; m = on_circle m d i, on_line m l i; n = midpoint n b a; o = foot o h d n; p = mirror p h o; q = on_line q m p, on_line q b a; r = on_tline r l b k ? cong q m q i"
I0123 22:00:43.162818 139822468673536 graph.py:498] 
I0123 22:00:43.162879 139822468673536 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e c b a; f = foot f b c a; g = on_line g c e, on_line g b f; h = on_circle h d b; i = on_circle i d h, on_line i g h; j = foot j h b a; k = foot k h c b; l = on_pline l i j k, on_line l k c; m = on_circle m d i, on_line m l i; n = midpoint n b a; o = foot o h d n; p = mirror p h o; q = on_line q m p, on_line q b a; r = on_tline r l b k ? cong q m q i
I0123 22:00:52.565649 139822468673536 ddar.py:60] Depth 1/1000 time = 9.330005407333374
I0123 22:01:18.195964 139822468673536 ddar.py:60] Depth 2/1000 time = 25.630089044570923
I0123 22:02:09.099665 139822468673536 ddar.py:60] Depth 3/1000 time = 50.90336537361145
I0123 22:02:56.310348 139822468673536 ddar.py:60] Depth 4/1000 time = 47.210254430770874
I0123 22:03:44.560769 139822468673536 ddar.py:60] Depth 5/1000 time = 48.25005030632019
I0123 22:04:31.492676 139822468673536 ddar.py:60] Depth 6/1000 time = 46.930886030197144
I0123 22:05:19.107964 139822468673536 ddar.py:60] Depth 7/1000 time = 47.52788519859314
I0123 22:06:06.804298 139822468673536 ddar.py:60] Depth 8/1000 time = 47.38684582710266
I0123 22:06:06.811088 139822468673536 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 22:06:06.811231 139822468673536 alphageometry.py:566] LM output (score=-2.560319): "r : T b d b r 27 ;"
I0123 22:06:06.811271 139822468673536 alphageometry.py:567] Translation: "r = on_tline r b b d"

I0123 22:06:06.811327 139822468673536 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e c b a; f = foot f b c a; g = on_line g c e, on_line g b f; h = on_circle h d b; i = on_circle i d h, on_line i g h; j = foot j h b a; k = foot k h c b; l = on_pline l i j k, on_line l k c; m = on_circle m d i, on_line m l i; n = midpoint n b a; o = foot o h d n; p = mirror p h o; q = on_line q m p, on_line q b a; r = on_tline r b b d ? cong q m q i"
I0123 22:06:06.811553 139822468673536 graph.py:498] 
I0123 22:06:06.811617 139822468673536 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e c b a; f = foot f b c a; g = on_line g c e, on_line g b f; h = on_circle h d b; i = on_circle i d h, on_line i g h; j = foot j h b a; k = foot k h c b; l = on_pline l i j k, on_line l k c; m = on_circle m d i, on_line m l i; n = midpoint n b a; o = foot o h d n; p = mirror p h o; q = on_line q m p, on_line q b a; r = on_tline r b b d ? cong q m q i
I0123 22:06:16.405615 139822468673536 ddar.py:60] Depth 1/1000 time = 9.524198532104492
I0123 22:06:42.868376 139822468673536 ddar.py:60] Depth 2/1000 time = 26.46256184577942
I0123 22:07:33.410995 139822468673536 ddar.py:60] Depth 3/1000 time = 50.542314529418945
I0123 22:08:24.195206 139822468673536 ddar.py:60] Depth 4/1000 time = 50.783815145492554
I0123 22:09:14.711224 139822468673536 ddar.py:60] Depth 5/1000 time = 50.515660524368286
I0123 22:10:04.413311 139822468673536 ddar.py:60] Depth 6/1000 time = 49.70104217529297
I0123 22:10:53.678808 139822468673536 ddar.py:60] Depth 7/1000 time = 49.1786003112793
I0123 22:11:44.385101 139822468673536 ddar.py:60] Depth 8/1000 time = 50.33287191390991
I0123 22:11:44.394588 139822468673536 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 22:11:44.394742 139822468673536 alphageometry.py:566] LM output (score=-2.592833): "r : T i p i r 27 ;"
I0123 22:11:44.394781 139822468673536 alphageometry.py:567] Translation: "r = on_tline r i i p"

I0123 22:11:44.394840 139822468673536 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e c b a; f = foot f b c a; g = on_line g c e, on_line g b f; h = on_circle h d b; i = on_circle i d h, on_line i g h; j = foot j h b a; k = foot k h c b; l = on_pline l i j k, on_line l k c; m = on_circle m d i, on_line m l i; n = midpoint n b a; o = foot o h d n; p = mirror p h o; q = on_line q m p, on_line q b a; r = on_tline r i i p ? cong q m q i"
I0123 22:11:44.395067 139822468673536 graph.py:498] 
I0123 22:11:44.395130 139822468673536 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e c b a; f = foot f b c a; g = on_line g c e, on_line g b f; h = on_circle h d b; i = on_circle i d h, on_line i g h; j = foot j h b a; k = foot k h c b; l = on_pline l i j k, on_line l k c; m = on_circle m d i, on_line m l i; n = midpoint n b a; o = foot o h d n; p = mirror p h o; q = on_line q m p, on_line q b a; r = on_tline r i i p ? cong q m q i
I0123 22:11:52.667283 139822468673536 ddar.py:60] Depth 1/1000 time = 8.20312786102295
I0123 22:12:18.996762 139822468673536 ddar.py:60] Depth 2/1000 time = 26.32926917076111
I0123 22:13:10.083706 139822468673536 ddar.py:60] Depth 3/1000 time = 51.08658766746521
I0123 22:13:59.334328 139822468673536 ddar.py:60] Depth 4/1000 time = 49.25013303756714
I0123 22:14:47.441708 139822468673536 ddar.py:60] Depth 5/1000 time = 48.10687041282654
I0123 22:15:36.640385 139822468673536 ddar.py:60] Depth 6/1000 time = 49.19760203361511
I0123 22:16:27.328135 139822468673536 ddar.py:60] Depth 7/1000 time = 50.598389863967896
I0123 22:17:16.555158 139822468673536 ddar.py:60] Depth 8/1000 time = 48.86818289756775
I0123 22:17:16.563249 139822468673536 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 22:17:16.563395 139822468673536 alphageometry.py:566] LM output (score=-2.619724): "r : T b g b r 27 ;"
I0123 22:17:16.563436 139822468673536 alphageometry.py:567] Translation: "r = on_tline r b b g"

I0123 22:17:16.563497 139822468673536 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e c b a; f = foot f b c a; g = on_line g c e, on_line g b f; h = on_circle h d b; i = on_circle i d h, on_line i g h; j = foot j h b a; k = foot k h c b; l = on_pline l i j k, on_line l k c; m = on_circle m d i, on_line m l i; n = midpoint n b a; o = foot o h d n; p = mirror p h o; q = on_line q m p, on_line q b a; r = on_tline r b b g ? cong q m q i"
I0123 22:17:16.563744 139822468673536 graph.py:498] 
I0123 22:17:16.563813 139822468673536 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e c b a; f = foot f b c a; g = on_line g c e, on_line g b f; h = on_circle h d b; i = on_circle i d h, on_line i g h; j = foot j h b a; k = foot k h c b; l = on_pline l i j k, on_line l k c; m = on_circle m d i, on_line m l i; n = midpoint n b a; o = foot o h d n; p = mirror p h o; q = on_line q m p, on_line q b a; r = on_tline r b b g ? cong q m q i
I0123 22:17:25.633989 139822468673536 ddar.py:60] Depth 1/1000 time = 9.00236439704895
I0123 22:17:51.360781 139822468673536 ddar.py:60] Depth 2/1000 time = 25.726595640182495
I0123 22:18:39.384544 139822468673536 ddar.py:60] Depth 3/1000 time = 48.02342629432678
I0123 22:19:27.367299 139822468673536 ddar.py:60] Depth 4/1000 time = 47.98229146003723
I0123 22:20:16.417405 139822468673536 ddar.py:60] Depth 5/1000 time = 49.04964327812195
I0123 22:21:04.797818 139822468673536 ddar.py:60] Depth 6/1000 time = 48.3792622089386
I0123 22:21:53.759299 139822468673536 ddar.py:60] Depth 7/1000 time = 48.87724995613098
I0123 22:22:41.045966 139822468673536 ddar.py:60] Depth 8/1000 time = 46.97947835922241
I0123 22:22:41.053716 139822468673536 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 22:22:41.053838 139822468673536 alphageometry.py:566] LM output (score=-2.693026): "r : T g h h r 27 ;"
I0123 22:22:41.053877 139822468673536 alphageometry.py:567] Translation: "r = on_tline r h g h"

I0123 22:22:41.053927 139822468673536 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = foot e c b a; f = foot f b c a; g = on_line g c e, on_line g b f; h = on_circle h d b; i = on_circle i d h, on_line i g h; j = foot j h b a; k = foot k h c b; l = on_pline l i j k, on_line l k c; m = on_circle m d i, on_line m l i; n = midpoint n b a; o = foot o h d n; p = mirror p h o; q = on_line q m p, on_line q b a; r = on_tline r h g h ? cong q m q i"
I0123 22:22:41.054138 139822468673536 graph.py:498] 
I0123 22:22:41.054202 139822468673536 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e c b a; f = foot f b c a; g = on_line g c e, on_line g b f; h = on_circle h d b; i = on_circle i d h, on_line i g h; j = foot j h b a; k = foot k h c b; l = on_pline l i j k, on_line l k c; m = on_circle m d i, on_line m l i; n = midpoint n b a; o = foot o h d n; p = mirror p h o; q = on_line q m p, on_line q b a; r = on_tline r h g h ? cong q m q i
I0123 22:22:50.689599 139822468673536 ddar.py:60] Depth 1/1000 time = 9.563538551330566
I0123 22:23:17.042762 139822468673536 ddar.py:60] Depth 2/1000 time = 26.352943658828735
I0123 22:24:06.850402 139822468673536 ddar.py:60] Depth 3/1000 time = 49.80730628967285
I0123 22:24:56.610829 139822468673536 ddar.py:60] Depth 4/1000 time = 49.76000475883484
I0123 22:25:47.245141 139822468673536 ddar.py:60] Depth 5/1000 time = 50.63395094871521
I0123 22:26:36.722160 139822468673536 ddar.py:60] Depth 6/1000 time = 49.4758837223053
I0123 22:27:27.727245 139822468673536 ddar.py:60] Depth 7/1000 time = 50.90044665336609
I0123 22:27:28.058125 139822468673536 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 22:27:28.058241 139822468673536 alphageometry.py:585] Timeout.
