I0123 12:31:58.764280 140045223440384 inference_utils.py:69] Parsing gin configuration.
I0123 12:31:58.764377 140045223440384 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0123 12:31:58.764569 140045223440384 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0123 12:31:58.764605 140045223440384 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0123 12:31:58.764634 140045223440384 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0123 12:31:58.764662 140045223440384 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0123 12:31:58.764688 140045223440384 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0123 12:31:58.764714 140045223440384 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0123 12:31:58.764741 140045223440384 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0123 12:31:58.764768 140045223440384 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=32
I0123 12:31:58.764795 140045223440384 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0123 12:31:58.764822 140045223440384 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0123 12:31:58.764867 140045223440384 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0123 12:31:58.764991 140045223440384 resource_reader.py:55] Path not found: base_htrans.gin
I0123 12:31:58.765187 140045223440384 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0123 12:31:58.765291 140045223440384 resource_reader.py:55] Path not found: trainer_configuration.gin
I0123 12:31:58.771630 140045223440384 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0123 12:31:58.771753 140045223440384 resource_reader.py:55] Path not found: size/medium_150M.gin
I0123 12:31:58.772068 140045223440384 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0123 12:31:58.772171 140045223440384 resource_reader.py:55] Path not found: options/positions_t5.gin
I0123 12:31:58.772444 140045223440384 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0123 12:31:58.772542 140045223440384 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0123 12:31:58.772944 140045223440384 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0123 12:31:58.773044 140045223440384 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0123 12:31:58.776690 140045223440384 training_loop.py:334] ==== Training loop: initializing model ====
I0123 12:31:58.866106 140045223440384 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0123 12:31:58.866819 140045223440384 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0123 12:31:58.873517 140045223440384 training_loop.py:335] Process 0 of 1
I0123 12:31:58.873571 140045223440384 training_loop.py:336] Local device count = 1
I0123 12:31:58.873610 140045223440384 training_loop.py:337] Number of replicas = 1
I0123 12:31:58.873646 140045223440384 training_loop.py:339] Using random number seed 42
I0123 12:31:59.333184 140045223440384 training_loop.py:359] Initializing the model.
I0123 12:31:59.746377 140045223440384 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.746625 140045223440384 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 12:31:59.746728 140045223440384 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:31:59.746808 140045223440384 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:31:59.746884 140045223440384 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:31:59.746963 140045223440384 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:31:59.747035 140045223440384 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:31:59.747106 140045223440384 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:31:59.747174 140045223440384 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:31:59.747242 140045223440384 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:31:59.747312 140045223440384 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:31:59.747381 140045223440384 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:31:59.747449 140045223440384 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:31:59.747519 140045223440384 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:31:59.747558 140045223440384 attention.py:418] Single window, no scan.
I0123 12:31:59.747604 140045223440384 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 12:31:59.747717 140045223440384 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:59.747756 140045223440384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:59.747785 140045223440384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:59.749813 140045223440384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.755176 140045223440384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:59.765897 140045223440384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.766173 140045223440384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:59.770560 140045223440384 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:31:59.781270 140045223440384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:59.781328 140045223440384 attention.py:418] Single window, no scan.
I0123 12:31:59.781366 140045223440384 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:59.781399 140045223440384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.781461 140045223440384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.782658 140045223440384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.782736 140045223440384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.783458 140045223440384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.785979 140045223440384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.792389 140045223440384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.793723 140045223440384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.793810 140045223440384 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:59.793846 140045223440384 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:59.793907 140045223440384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.794039 140045223440384 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:59.794374 140045223440384 nn_components.py:325] mlp: activation = None
I0123 12:31:59.794421 140045223440384 nn_components.py:329] mlp: final activation = None
I0123 12:31:59.796339 140045223440384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.796439 140045223440384 nn_components.py:261] mlp: residual
I0123 12:31:59.799362 140045223440384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.799448 140045223440384 transformer_base.py:443] tbase: final FFN
I0123 12:31:59.799948 140045223440384 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:59.810372 140045223440384 nn_components.py:329] mlp: final activation = None
I0123 12:31:59.819281 140045223440384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.819380 140045223440384 nn_components.py:261] mlp: residual
I0123 12:31:59.819680 140045223440384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.819762 140045223440384 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 12:31:59.819872 140045223440384 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:59.819911 140045223440384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:59.819941 140045223440384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:59.821792 140045223440384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.824276 140045223440384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:59.829875 140045223440384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.830146 140045223440384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:59.832782 140045223440384 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:31:59.836620 140045223440384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:59.836677 140045223440384 attention.py:418] Single window, no scan.
I0123 12:31:59.836713 140045223440384 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:59.836745 140045223440384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.836806 140045223440384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.837374 140045223440384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.837451 140045223440384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.837833 140045223440384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.838616 140045223440384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.841152 140045223440384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.842296 140045223440384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.842443 140045223440384 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:59.842479 140045223440384 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:59.842542 140045223440384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.842694 140045223440384 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:59.843063 140045223440384 nn_components.py:325] mlp: activation = None
I0123 12:31:59.843108 140045223440384 nn_components.py:329] mlp: final activation = None
I0123 12:31:59.845203 140045223440384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.845299 140045223440384 nn_components.py:261] mlp: residual
I0123 12:31:59.847847 140045223440384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.847927 140045223440384 transformer_base.py:443] tbase: final FFN
I0123 12:31:59.848357 140045223440384 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:59.850690 140045223440384 nn_components.py:329] mlp: final activation = None
I0123 12:31:59.852597 140045223440384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.852691 140045223440384 nn_components.py:261] mlp: residual
I0123 12:31:59.852990 140045223440384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.853073 140045223440384 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 12:31:59.853183 140045223440384 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:59.853221 140045223440384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:59.853251 140045223440384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:59.855523 140045223440384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.857923 140045223440384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:59.863592 140045223440384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.863852 140045223440384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:59.866558 140045223440384 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:31:59.870457 140045223440384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:59.870515 140045223440384 attention.py:418] Single window, no scan.
I0123 12:31:59.870551 140045223440384 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:59.870582 140045223440384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.870643 140045223440384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.871218 140045223440384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.871294 140045223440384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.871659 140045223440384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.872440 140045223440384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.874974 140045223440384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.875650 140045223440384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.875726 140045223440384 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:59.875761 140045223440384 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:59.875820 140045223440384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.875951 140045223440384 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:59.876278 140045223440384 nn_components.py:325] mlp: activation = None
I0123 12:31:59.876322 140045223440384 nn_components.py:329] mlp: final activation = None
I0123 12:31:59.878248 140045223440384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.878344 140045223440384 nn_components.py:261] mlp: residual
I0123 12:31:59.880877 140045223440384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.880962 140045223440384 transformer_base.py:443] tbase: final FFN
I0123 12:31:59.881449 140045223440384 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:59.883747 140045223440384 nn_components.py:329] mlp: final activation = None
I0123 12:31:59.885719 140045223440384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.885827 140045223440384 nn_components.py:261] mlp: residual
I0123 12:31:59.886137 140045223440384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.886220 140045223440384 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 12:31:59.886333 140045223440384 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:59.886373 140045223440384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:59.886404 140045223440384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:59.888303 140045223440384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.890715 140045223440384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:59.896433 140045223440384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.896707 140045223440384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:59.899424 140045223440384 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:31:59.903278 140045223440384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:59.903334 140045223440384 attention.py:418] Single window, no scan.
I0123 12:31:59.903370 140045223440384 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:59.903402 140045223440384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.903468 140045223440384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.904037 140045223440384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.904112 140045223440384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.904480 140045223440384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.905261 140045223440384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.907870 140045223440384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.908501 140045223440384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.908577 140045223440384 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:59.908612 140045223440384 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:59.908672 140045223440384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.908804 140045223440384 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:59.909132 140045223440384 nn_components.py:325] mlp: activation = None
I0123 12:31:59.909177 140045223440384 nn_components.py:329] mlp: final activation = None
I0123 12:31:59.911242 140045223440384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.911336 140045223440384 nn_components.py:261] mlp: residual
I0123 12:31:59.913960 140045223440384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.914046 140045223440384 transformer_base.py:443] tbase: final FFN
I0123 12:31:59.914487 140045223440384 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:59.916765 140045223440384 nn_components.py:329] mlp: final activation = None
I0123 12:31:59.918710 140045223440384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.918806 140045223440384 nn_components.py:261] mlp: residual
I0123 12:31:59.919099 140045223440384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.919179 140045223440384 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 12:31:59.919288 140045223440384 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:59.919327 140045223440384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:59.919357 140045223440384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:59.921267 140045223440384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.923703 140045223440384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:59.929416 140045223440384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.929682 140045223440384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:59.932735 140045223440384 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:31:59.936550 140045223440384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:59.936606 140045223440384 attention.py:418] Single window, no scan.
I0123 12:31:59.936642 140045223440384 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:59.936675 140045223440384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.936737 140045223440384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.937312 140045223440384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.937389 140045223440384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.937763 140045223440384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.938552 140045223440384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.941146 140045223440384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.941792 140045223440384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.941873 140045223440384 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:59.941910 140045223440384 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:59.941971 140045223440384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.942112 140045223440384 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:59.942443 140045223440384 nn_components.py:325] mlp: activation = None
I0123 12:31:59.942488 140045223440384 nn_components.py:329] mlp: final activation = None
I0123 12:31:59.944423 140045223440384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.944518 140045223440384 nn_components.py:261] mlp: residual
I0123 12:31:59.947153 140045223440384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.947233 140045223440384 transformer_base.py:443] tbase: final FFN
I0123 12:31:59.947667 140045223440384 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:59.950002 140045223440384 nn_components.py:329] mlp: final activation = None
I0123 12:31:59.952007 140045223440384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.952103 140045223440384 nn_components.py:261] mlp: residual
I0123 12:31:59.952398 140045223440384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.952481 140045223440384 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 12:31:59.952593 140045223440384 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:59.952634 140045223440384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:59.952667 140045223440384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:59.954563 140045223440384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.956988 140045223440384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:59.962731 140045223440384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.962994 140045223440384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:59.965739 140045223440384 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:31:59.969540 140045223440384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:31:59.969595 140045223440384 attention.py:418] Single window, no scan.
I0123 12:31:59.969631 140045223440384 transformer_layer.py:389] tlayer: self-attention.
I0123 12:31:59.969672 140045223440384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.969735 140045223440384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.970352 140045223440384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.970430 140045223440384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.970802 140045223440384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.971587 140045223440384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.974156 140045223440384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.974782 140045223440384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.974860 140045223440384 transformer_layer.py:468] tlayer: End windows.
I0123 12:31:59.974896 140045223440384 transformer_layer.py:472] tlayer: final FFN.
I0123 12:31:59.974957 140045223440384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.975089 140045223440384 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:31:59.975418 140045223440384 nn_components.py:325] mlp: activation = None
I0123 12:31:59.975464 140045223440384 nn_components.py:329] mlp: final activation = None
I0123 12:31:59.977404 140045223440384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.977498 140045223440384 nn_components.py:261] mlp: residual
I0123 12:31:59.980149 140045223440384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.980230 140045223440384 transformer_base.py:443] tbase: final FFN
I0123 12:31:59.980664 140045223440384 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:31:59.983050 140045223440384 nn_components.py:329] mlp: final activation = None
I0123 12:31:59.985010 140045223440384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.985107 140045223440384 nn_components.py:261] mlp: residual
I0123 12:31:59.985409 140045223440384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.985491 140045223440384 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 12:31:59.985604 140045223440384 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:31:59.985649 140045223440384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:31:59.985683 140045223440384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:31:59.987550 140045223440384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.990058 140045223440384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:31:59.995887 140045223440384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:31:59.996157 140045223440384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:31:59.998860 140045223440384 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:32:00.002734 140045223440384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:00.002790 140045223440384 attention.py:418] Single window, no scan.
I0123 12:32:00.002828 140045223440384 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:00.002860 140045223440384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.002925 140045223440384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.003499 140045223440384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.003577 140045223440384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.003945 140045223440384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.004742 140045223440384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.007277 140045223440384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.007910 140045223440384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.007990 140045223440384 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:00.008026 140045223440384 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:00.008085 140045223440384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.008222 140045223440384 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:00.008553 140045223440384 nn_components.py:325] mlp: activation = None
I0123 12:32:00.008598 140045223440384 nn_components.py:329] mlp: final activation = None
I0123 12:32:00.010927 140045223440384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.011023 140045223440384 nn_components.py:261] mlp: residual
I0123 12:32:00.013570 140045223440384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.013660 140045223440384 transformer_base.py:443] tbase: final FFN
I0123 12:32:00.014093 140045223440384 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:00.157886 140045223440384 nn_components.py:329] mlp: final activation = None
I0123 12:32:00.160161 140045223440384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.160320 140045223440384 nn_components.py:261] mlp: residual
I0123 12:32:00.160648 140045223440384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.160741 140045223440384 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 12:32:00.160859 140045223440384 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:00.160900 140045223440384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:00.160932 140045223440384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:00.163031 140045223440384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.165612 140045223440384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:00.171450 140045223440384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.171727 140045223440384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:00.174485 140045223440384 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:32:00.178506 140045223440384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:00.178564 140045223440384 attention.py:418] Single window, no scan.
I0123 12:32:00.178602 140045223440384 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:00.178636 140045223440384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.178704 140045223440384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.179329 140045223440384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.179407 140045223440384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.179784 140045223440384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.180588 140045223440384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.183266 140045223440384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.183923 140045223440384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.184002 140045223440384 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:00.184038 140045223440384 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:00.184102 140045223440384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.184233 140045223440384 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:00.184568 140045223440384 nn_components.py:325] mlp: activation = None
I0123 12:32:00.184613 140045223440384 nn_components.py:329] mlp: final activation = None
I0123 12:32:00.186580 140045223440384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.186676 140045223440384 nn_components.py:261] mlp: residual
I0123 12:32:00.189256 140045223440384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.189336 140045223440384 transformer_base.py:443] tbase: final FFN
I0123 12:32:00.189835 140045223440384 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:00.192178 140045223440384 nn_components.py:329] mlp: final activation = None
I0123 12:32:00.194166 140045223440384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.194272 140045223440384 nn_components.py:261] mlp: residual
I0123 12:32:00.194573 140045223440384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.194658 140045223440384 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 12:32:00.194770 140045223440384 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:00.194809 140045223440384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:00.194841 140045223440384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:00.196805 140045223440384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.199444 140045223440384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:00.205214 140045223440384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.205479 140045223440384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:00.208234 140045223440384 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:32:00.212131 140045223440384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:00.212188 140045223440384 attention.py:418] Single window, no scan.
I0123 12:32:00.212224 140045223440384 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:00.212257 140045223440384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.212320 140045223440384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.212902 140045223440384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.212980 140045223440384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.213346 140045223440384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.214143 140045223440384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.216753 140045223440384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.217391 140045223440384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.217470 140045223440384 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:00.217506 140045223440384 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:00.217567 140045223440384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.217713 140045223440384 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:00.218048 140045223440384 nn_components.py:325] mlp: activation = None
I0123 12:32:00.218092 140045223440384 nn_components.py:329] mlp: final activation = None
I0123 12:32:00.220045 140045223440384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.220140 140045223440384 nn_components.py:261] mlp: residual
I0123 12:32:00.222768 140045223440384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.222849 140045223440384 transformer_base.py:443] tbase: final FFN
I0123 12:32:00.223289 140045223440384 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:00.225628 140045223440384 nn_components.py:329] mlp: final activation = None
I0123 12:32:00.227580 140045223440384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.227675 140045223440384 nn_components.py:261] mlp: residual
I0123 12:32:00.227973 140045223440384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.228064 140045223440384 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 12:32:00.228177 140045223440384 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:00.228217 140045223440384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:00.228249 140045223440384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:00.230174 140045223440384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.232598 140045223440384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:00.238694 140045223440384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.238966 140045223440384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:00.241738 140045223440384 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:32:00.245547 140045223440384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:00.245603 140045223440384 attention.py:418] Single window, no scan.
I0123 12:32:00.245647 140045223440384 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:00.245682 140045223440384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.245745 140045223440384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.246325 140045223440384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.246401 140045223440384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.246763 140045223440384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.247593 140045223440384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.250179 140045223440384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.250812 140045223440384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.250894 140045223440384 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:00.250930 140045223440384 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:00.250991 140045223440384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.251120 140045223440384 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:00.251449 140045223440384 nn_components.py:325] mlp: activation = None
I0123 12:32:00.251494 140045223440384 nn_components.py:329] mlp: final activation = None
I0123 12:32:00.253456 140045223440384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.253551 140045223440384 nn_components.py:261] mlp: residual
I0123 12:32:00.256173 140045223440384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.256258 140045223440384 transformer_base.py:443] tbase: final FFN
I0123 12:32:00.256699 140045223440384 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:00.259026 140045223440384 nn_components.py:329] mlp: final activation = None
I0123 12:32:00.261048 140045223440384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.261148 140045223440384 nn_components.py:261] mlp: residual
I0123 12:32:00.261445 140045223440384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.261534 140045223440384 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 12:32:00.261657 140045223440384 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:00.261698 140045223440384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:00.261731 140045223440384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:00.263601 140045223440384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.266113 140045223440384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:00.271811 140045223440384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.272086 140045223440384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:00.274814 140045223440384 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:32:00.278623 140045223440384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:00.278680 140045223440384 attention.py:418] Single window, no scan.
I0123 12:32:00.278716 140045223440384 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:00.278749 140045223440384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.278853 140045223440384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.279422 140045223440384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.279500 140045223440384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.279862 140045223440384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.280654 140045223440384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.283220 140045223440384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.283853 140045223440384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.283931 140045223440384 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:00.283967 140045223440384 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:00.284028 140045223440384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.284160 140045223440384 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:00.284488 140045223440384 nn_components.py:325] mlp: activation = None
I0123 12:32:00.284533 140045223440384 nn_components.py:329] mlp: final activation = None
I0123 12:32:00.286540 140045223440384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.286636 140045223440384 nn_components.py:261] mlp: residual
I0123 12:32:00.289438 140045223440384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.289517 140045223440384 transformer_base.py:443] tbase: final FFN
I0123 12:32:00.289960 140045223440384 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:00.292320 140045223440384 nn_components.py:329] mlp: final activation = None
I0123 12:32:00.294266 140045223440384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.294361 140045223440384 nn_components.py:261] mlp: residual
I0123 12:32:00.294656 140045223440384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.294739 140045223440384 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 12:32:00.294858 140045223440384 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:00.294899 140045223440384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:00.294931 140045223440384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:00.296766 140045223440384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.299267 140045223440384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:00.305197 140045223440384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.305465 140045223440384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:00.308172 140045223440384 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:32:00.312349 140045223440384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:00.312405 140045223440384 attention.py:418] Single window, no scan.
I0123 12:32:00.312441 140045223440384 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:00.312473 140045223440384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.312536 140045223440384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.313108 140045223440384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.313185 140045223440384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.313551 140045223440384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.314348 140045223440384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.316891 140045223440384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.317537 140045223440384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.317616 140045223440384 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:00.317660 140045223440384 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:00.317723 140045223440384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.317856 140045223440384 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:00.318187 140045223440384 nn_components.py:325] mlp: activation = None
I0123 12:32:00.318232 140045223440384 nn_components.py:329] mlp: final activation = None
I0123 12:32:00.320220 140045223440384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.320314 140045223440384 nn_components.py:261] mlp: residual
I0123 12:32:00.322899 140045223440384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.322981 140045223440384 transformer_base.py:443] tbase: final FFN
I0123 12:32:00.323419 140045223440384 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:00.325793 140045223440384 nn_components.py:329] mlp: final activation = None
I0123 12:32:00.327740 140045223440384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.327836 140045223440384 nn_components.py:261] mlp: residual
I0123 12:32:00.328132 140045223440384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.328419 140045223440384 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:32:00.328491 140045223440384 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:32:00.328558 140045223440384 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:32:00.328618 140045223440384 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:32:00.328674 140045223440384 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:32:00.328729 140045223440384 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:32:00.328785 140045223440384 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:32:00.328840 140045223440384 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:32:00.328893 140045223440384 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:32:00.328946 140045223440384 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:32:00.328999 140045223440384 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:32:00.329051 140045223440384 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:32:00.329090 140045223440384 decoder_stack.py:344] dstack: Final layernorm.
I0123 12:32:00.332707 140045223440384 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:00.381218 140045223440384 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.381303 140045223440384 decoder_stack.py:333] dstack: autoregressive generator.
I0123 12:32:00.381358 140045223440384 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 12:32:00.381463 140045223440384 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:00.381501 140045223440384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:00.381532 140045223440384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:00.381595 140045223440384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.384066 140045223440384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:00.389623 140045223440384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.389892 140045223440384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:00.392564 140045223440384 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:00.409471 140045223440384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:00.409528 140045223440384 attention.py:418] Single window, no scan.
I0123 12:32:00.409564 140045223440384 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:00.409594 140045223440384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.409666 140045223440384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.410947 140045223440384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.411026 140045223440384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.411745 140045223440384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.413786 140045223440384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.418605 140045223440384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.419928 140045223440384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.420013 140045223440384 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:00.420049 140045223440384 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:00.420110 140045223440384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.420240 140045223440384 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:00.420359 140045223440384 nn_components.py:325] mlp: activation = None
I0123 12:32:00.420399 140045223440384 nn_components.py:329] mlp: final activation = None
I0123 12:32:00.422320 140045223440384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.422416 140045223440384 nn_components.py:261] mlp: residual
I0123 12:32:00.424884 140045223440384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.424967 140045223440384 transformer_base.py:443] tbase: final FFN
I0123 12:32:00.425077 140045223440384 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:00.427360 140045223440384 nn_components.py:329] mlp: final activation = None
I0123 12:32:00.429346 140045223440384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.429443 140045223440384 nn_components.py:261] mlp: residual
I0123 12:32:00.429751 140045223440384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.429836 140045223440384 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 12:32:00.429947 140045223440384 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:00.429986 140045223440384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:00.430017 140045223440384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:00.430082 140045223440384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.432377 140045223440384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:00.437912 140045223440384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.438174 140045223440384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:00.440894 140045223440384 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:00.454040 140045223440384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:00.454095 140045223440384 attention.py:418] Single window, no scan.
I0123 12:32:00.454131 140045223440384 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:00.454162 140045223440384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.454229 140045223440384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.454799 140045223440384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.454876 140045223440384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.455240 140045223440384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.455942 140045223440384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.458475 140045223440384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.459097 140045223440384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.459175 140045223440384 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:00.459216 140045223440384 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:00.459278 140045223440384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.459412 140045223440384 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:00.459524 140045223440384 nn_components.py:325] mlp: activation = None
I0123 12:32:00.459568 140045223440384 nn_components.py:329] mlp: final activation = None
I0123 12:32:00.461513 140045223440384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.461606 140045223440384 nn_components.py:261] mlp: residual
I0123 12:32:00.464058 140045223440384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.464138 140045223440384 transformer_base.py:443] tbase: final FFN
I0123 12:32:00.464249 140045223440384 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:00.466493 140045223440384 nn_components.py:329] mlp: final activation = None
I0123 12:32:00.468449 140045223440384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.468544 140045223440384 nn_components.py:261] mlp: residual
I0123 12:32:00.468840 140045223440384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.468923 140045223440384 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 12:32:00.469034 140045223440384 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:00.469073 140045223440384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:00.469104 140045223440384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:00.469169 140045223440384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.471463 140045223440384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:00.476955 140045223440384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.477214 140045223440384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:00.479946 140045223440384 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:00.492872 140045223440384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:00.492929 140045223440384 attention.py:418] Single window, no scan.
I0123 12:32:00.492965 140045223440384 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:00.492995 140045223440384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.493058 140045223440384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.493626 140045223440384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.493714 140045223440384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.494079 140045223440384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.494778 140045223440384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.497308 140045223440384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.501444 140045223440384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.501562 140045223440384 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:00.501600 140045223440384 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:00.501689 140045223440384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.501835 140045223440384 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:00.501965 140045223440384 nn_components.py:325] mlp: activation = None
I0123 12:32:00.502005 140045223440384 nn_components.py:329] mlp: final activation = None
I0123 12:32:00.504160 140045223440384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.504255 140045223440384 nn_components.py:261] mlp: residual
I0123 12:32:00.506832 140045223440384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.506911 140045223440384 transformer_base.py:443] tbase: final FFN
I0123 12:32:00.507027 140045223440384 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:00.509304 140045223440384 nn_components.py:329] mlp: final activation = None
I0123 12:32:00.511308 140045223440384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.511404 140045223440384 nn_components.py:261] mlp: residual
I0123 12:32:00.511701 140045223440384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.511786 140045223440384 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 12:32:00.511900 140045223440384 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:00.511941 140045223440384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:00.511973 140045223440384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:00.512038 140045223440384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.514355 140045223440384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:00.519927 140045223440384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.520191 140045223440384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:00.522959 140045223440384 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:00.535958 140045223440384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:00.536015 140045223440384 attention.py:418] Single window, no scan.
I0123 12:32:00.536051 140045223440384 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:00.536082 140045223440384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.536145 140045223440384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.536741 140045223440384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.536817 140045223440384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.537179 140045223440384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.537890 140045223440384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.540454 140045223440384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.541079 140045223440384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.541156 140045223440384 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:00.541191 140045223440384 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:00.541250 140045223440384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.541387 140045223440384 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:00.541499 140045223440384 nn_components.py:325] mlp: activation = None
I0123 12:32:00.541538 140045223440384 nn_components.py:329] mlp: final activation = None
I0123 12:32:00.543814 140045223440384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.543909 140045223440384 nn_components.py:261] mlp: residual
I0123 12:32:00.546401 140045223440384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.546482 140045223440384 transformer_base.py:443] tbase: final FFN
I0123 12:32:00.546594 140045223440384 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:00.548846 140045223440384 nn_components.py:329] mlp: final activation = None
I0123 12:32:00.550760 140045223440384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.550855 140045223440384 nn_components.py:261] mlp: residual
I0123 12:32:00.551144 140045223440384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.551225 140045223440384 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 12:32:00.551333 140045223440384 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:00.551372 140045223440384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:00.551402 140045223440384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:00.551465 140045223440384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.553824 140045223440384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:00.559389 140045223440384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.559656 140045223440384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:00.562360 140045223440384 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:00.575441 140045223440384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:00.575498 140045223440384 attention.py:418] Single window, no scan.
I0123 12:32:00.575534 140045223440384 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:00.575566 140045223440384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.575630 140045223440384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.576194 140045223440384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.576270 140045223440384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.576639 140045223440384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.577342 140045223440384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.579946 140045223440384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.580576 140045223440384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.580654 140045223440384 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:00.580689 140045223440384 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:00.580748 140045223440384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.580890 140045223440384 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:00.581004 140045223440384 nn_components.py:325] mlp: activation = None
I0123 12:32:00.581044 140045223440384 nn_components.py:329] mlp: final activation = None
I0123 12:32:00.582985 140045223440384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.583080 140045223440384 nn_components.py:261] mlp: residual
I0123 12:32:00.585531 140045223440384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.585612 140045223440384 transformer_base.py:443] tbase: final FFN
I0123 12:32:00.585731 140045223440384 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:00.588058 140045223440384 nn_components.py:329] mlp: final activation = None
I0123 12:32:00.590059 140045223440384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.590156 140045223440384 nn_components.py:261] mlp: residual
I0123 12:32:00.590446 140045223440384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.590529 140045223440384 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 12:32:00.590640 140045223440384 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:00.590678 140045223440384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:00.590709 140045223440384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:00.590775 140045223440384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.593067 140045223440384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:00.598619 140045223440384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.598877 140045223440384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:00.601613 140045223440384 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:00.614477 140045223440384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:00.614533 140045223440384 attention.py:418] Single window, no scan.
I0123 12:32:00.614569 140045223440384 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:00.614600 140045223440384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.614663 140045223440384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.615237 140045223440384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.615317 140045223440384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.615686 140045223440384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.616389 140045223440384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.618926 140045223440384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.619551 140045223440384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.619630 140045223440384 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:00.619665 140045223440384 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:00.619724 140045223440384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.619852 140045223440384 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:00.619969 140045223440384 nn_components.py:325] mlp: activation = None
I0123 12:32:00.620008 140045223440384 nn_components.py:329] mlp: final activation = None
I0123 12:32:00.621993 140045223440384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.622087 140045223440384 nn_components.py:261] mlp: residual
I0123 12:32:00.624529 140045223440384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.624608 140045223440384 transformer_base.py:443] tbase: final FFN
I0123 12:32:00.624719 140045223440384 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:00.627008 140045223440384 nn_components.py:329] mlp: final activation = None
I0123 12:32:00.628943 140045223440384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.629038 140045223440384 nn_components.py:261] mlp: residual
I0123 12:32:00.629328 140045223440384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.629411 140045223440384 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 12:32:00.629521 140045223440384 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:00.629559 140045223440384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:00.629590 140045223440384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:00.629662 140045223440384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.631953 140045223440384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:00.637591 140045223440384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.637860 140045223440384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:00.640480 140045223440384 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:00.653836 140045223440384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:00.653892 140045223440384 attention.py:418] Single window, no scan.
I0123 12:32:00.653927 140045223440384 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:00.653958 140045223440384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.654021 140045223440384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.654592 140045223440384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.654668 140045223440384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.655028 140045223440384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.655731 140045223440384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.658272 140045223440384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.658947 140045223440384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.659025 140045223440384 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:00.659061 140045223440384 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:00.659122 140045223440384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.659255 140045223440384 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:00.659369 140045223440384 nn_components.py:325] mlp: activation = None
I0123 12:32:00.659413 140045223440384 nn_components.py:329] mlp: final activation = None
I0123 12:32:00.661311 140045223440384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.661405 140045223440384 nn_components.py:261] mlp: residual
I0123 12:32:00.663885 140045223440384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.663965 140045223440384 transformer_base.py:443] tbase: final FFN
I0123 12:32:00.664076 140045223440384 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:00.666326 140045223440384 nn_components.py:329] mlp: final activation = None
I0123 12:32:00.668282 140045223440384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.668377 140045223440384 nn_components.py:261] mlp: residual
I0123 12:32:00.668670 140045223440384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.668751 140045223440384 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 12:32:00.668863 140045223440384 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:00.668901 140045223440384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:00.668932 140045223440384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:00.668996 140045223440384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.671277 140045223440384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:00.676775 140045223440384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.677044 140045223440384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:00.679764 140045223440384 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:00.692476 140045223440384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:00.692532 140045223440384 attention.py:418] Single window, no scan.
I0123 12:32:00.692568 140045223440384 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:00.692599 140045223440384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.692662 140045223440384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.693274 140045223440384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.693351 140045223440384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.693725 140045223440384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.694419 140045223440384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.696928 140045223440384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.697572 140045223440384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.697656 140045223440384 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:00.697693 140045223440384 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:00.697757 140045223440384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.697887 140045223440384 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:00.698001 140045223440384 nn_components.py:325] mlp: activation = None
I0123 12:32:00.698046 140045223440384 nn_components.py:329] mlp: final activation = None
I0123 12:32:00.699938 140045223440384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.700031 140045223440384 nn_components.py:261] mlp: residual
I0123 12:32:00.702547 140045223440384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.702628 140045223440384 transformer_base.py:443] tbase: final FFN
I0123 12:32:00.702739 140045223440384 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:00.704976 140045223440384 nn_components.py:329] mlp: final activation = None
I0123 12:32:00.706862 140045223440384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.706958 140045223440384 nn_components.py:261] mlp: residual
I0123 12:32:00.707247 140045223440384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.707328 140045223440384 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 12:32:00.707437 140045223440384 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:00.707477 140045223440384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:00.707508 140045223440384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:00.707571 140045223440384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.709835 140045223440384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:00.715395 140045223440384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.715654 140045223440384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:00.718332 140045223440384 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:00.731121 140045223440384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:00.731176 140045223440384 attention.py:418] Single window, no scan.
I0123 12:32:00.731212 140045223440384 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:00.731242 140045223440384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.731304 140045223440384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.731874 140045223440384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.731952 140045223440384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.732313 140045223440384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.733026 140045223440384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.735592 140045223440384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.736278 140045223440384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.736357 140045223440384 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:00.736392 140045223440384 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:00.736453 140045223440384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.736586 140045223440384 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:00.736702 140045223440384 nn_components.py:325] mlp: activation = None
I0123 12:32:00.736741 140045223440384 nn_components.py:329] mlp: final activation = None
I0123 12:32:00.738663 140045223440384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.738758 140045223440384 nn_components.py:261] mlp: residual
I0123 12:32:00.741201 140045223440384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.741281 140045223440384 transformer_base.py:443] tbase: final FFN
I0123 12:32:00.741392 140045223440384 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:00.743664 140045223440384 nn_components.py:329] mlp: final activation = None
I0123 12:32:00.745608 140045223440384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.745712 140045223440384 nn_components.py:261] mlp: residual
I0123 12:32:00.746006 140045223440384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.746089 140045223440384 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 12:32:00.746199 140045223440384 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:00.746238 140045223440384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:00.746269 140045223440384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:00.746334 140045223440384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.748608 140045223440384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:00.754137 140045223440384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.754399 140045223440384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:00.757597 140045223440384 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:00.770296 140045223440384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:00.770353 140045223440384 attention.py:418] Single window, no scan.
I0123 12:32:00.770388 140045223440384 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:00.770419 140045223440384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.770482 140045223440384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.771094 140045223440384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.771172 140045223440384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.771538 140045223440384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.772238 140045223440384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.774770 140045223440384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.775390 140045223440384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.775469 140045223440384 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:00.775504 140045223440384 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:00.775566 140045223440384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.775699 140045223440384 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:00.775810 140045223440384 nn_components.py:325] mlp: activation = None
I0123 12:32:00.775848 140045223440384 nn_components.py:329] mlp: final activation = None
I0123 12:32:00.777748 140045223440384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.777848 140045223440384 nn_components.py:261] mlp: residual
I0123 12:32:00.780361 140045223440384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.780440 140045223440384 transformer_base.py:443] tbase: final FFN
I0123 12:32:00.780548 140045223440384 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:00.782810 140045223440384 nn_components.py:329] mlp: final activation = None
I0123 12:32:00.784680 140045223440384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.784774 140045223440384 nn_components.py:261] mlp: residual
I0123 12:32:00.785062 140045223440384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.785143 140045223440384 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 12:32:00.785253 140045223440384 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:00.785291 140045223440384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:00.785322 140045223440384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:00.785386 140045223440384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.787671 140045223440384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:00.793261 140045223440384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.793523 140045223440384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:00.796206 140045223440384 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:00.808989 140045223440384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:00.809044 140045223440384 attention.py:418] Single window, no scan.
I0123 12:32:00.809080 140045223440384 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:00.809110 140045223440384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.809173 140045223440384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.809743 140045223440384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.809821 140045223440384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.810190 140045223440384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.810889 140045223440384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.813409 140045223440384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.814096 140045223440384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.814175 140045223440384 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:00.814209 140045223440384 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:00.814268 140045223440384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.814402 140045223440384 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:00.814513 140045223440384 nn_components.py:325] mlp: activation = None
I0123 12:32:00.814553 140045223440384 nn_components.py:329] mlp: final activation = None
I0123 12:32:00.816464 140045223440384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.816565 140045223440384 nn_components.py:261] mlp: residual
I0123 12:32:00.819035 140045223440384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.819114 140045223440384 transformer_base.py:443] tbase: final FFN
I0123 12:32:00.819224 140045223440384 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:00.821464 140045223440384 nn_components.py:329] mlp: final activation = None
I0123 12:32:00.823418 140045223440384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.823515 140045223440384 nn_components.py:261] mlp: residual
I0123 12:32:00.823805 140045223440384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.823887 140045223440384 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 12:32:00.823996 140045223440384 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:00.824035 140045223440384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:00.824066 140045223440384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:00.824130 140045223440384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.826406 140045223440384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:00.831907 140045223440384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.832167 140045223440384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:00.834857 140045223440384 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:00.847566 140045223440384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:00.847622 140045223440384 attention.py:418] Single window, no scan.
I0123 12:32:00.847659 140045223440384 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:00.847690 140045223440384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.847754 140045223440384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.848315 140045223440384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.848392 140045223440384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.848750 140045223440384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.849452 140045223440384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.852040 140045223440384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.852680 140045223440384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.852757 140045223440384 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:00.852792 140045223440384 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:00.852851 140045223440384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.852982 140045223440384 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:00.853093 140045223440384 nn_components.py:325] mlp: activation = None
I0123 12:32:00.853131 140045223440384 nn_components.py:329] mlp: final activation = None
I0123 12:32:00.855036 140045223440384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.855131 140045223440384 nn_components.py:261] mlp: residual
I0123 12:32:00.857707 140045223440384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.857788 140045223440384 transformer_base.py:443] tbase: final FFN
I0123 12:32:00.857902 140045223440384 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:00.860759 140045223440384 nn_components.py:329] mlp: final activation = None
I0123 12:32:00.862665 140045223440384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.862762 140045223440384 nn_components.py:261] mlp: residual
I0123 12:32:00.863053 140045223440384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.863144 140045223440384 decoder_stack.py:344] dstack: Final layernorm.
I0123 12:32:00.866079 140045223440384 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:00.922363 140045223440384 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:00.922451 140045223440384 decoder_stack.py:333] dstack: autoregressive generator.
I0123 12:32:00.922507 140045223440384 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 12:32:00.922613 140045223440384 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:00.922652 140045223440384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:00.922682 140045223440384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:00.922745 140045223440384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:00.925198 140045223440384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:00.930685 140045223440384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:00.930950 140045223440384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:00.933587 140045223440384 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:00.946041 140045223440384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:00.946097 140045223440384 attention.py:418] Single window, no scan.
I0123 12:32:00.946132 140045223440384 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:00.946163 140045223440384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:00.946225 140045223440384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:00.946789 140045223440384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:00.946866 140045223440384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:00.947224 140045223440384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:00.947905 140045223440384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:00.950457 140045223440384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:00.951077 140045223440384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:00.951154 140045223440384 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:00.951189 140045223440384 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:00.951248 140045223440384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:00.951377 140045223440384 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:00.951494 140045223440384 nn_components.py:325] mlp: activation = None
I0123 12:32:00.951534 140045223440384 nn_components.py:329] mlp: final activation = None
I0123 12:32:00.953382 140045223440384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:00.953477 140045223440384 nn_components.py:261] mlp: residual
I0123 12:32:00.955910 140045223440384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:00.955990 140045223440384 transformer_base.py:443] tbase: final FFN
I0123 12:32:00.956098 140045223440384 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:00.958543 140045223440384 nn_components.py:329] mlp: final activation = None
I0123 12:32:00.960420 140045223440384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:00.960515 140045223440384 nn_components.py:261] mlp: residual
I0123 12:32:00.960807 140045223440384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:00.960888 140045223440384 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 12:32:00.960996 140045223440384 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:00.961035 140045223440384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:00.961065 140045223440384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:00.961128 140045223440384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:00.963410 140045223440384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:00.968853 140045223440384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:00.969114 140045223440384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:00.971808 140045223440384 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:00.984189 140045223440384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:00.984247 140045223440384 attention.py:418] Single window, no scan.
I0123 12:32:00.984283 140045223440384 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:00.984314 140045223440384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:00.984376 140045223440384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:00.984939 140045223440384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:00.985015 140045223440384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:00.985375 140045223440384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:00.986067 140045223440384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:00.988613 140045223440384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:00.989231 140045223440384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:00.989310 140045223440384 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:00.989345 140045223440384 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:00.989404 140045223440384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:00.989536 140045223440384 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:00.989650 140045223440384 nn_components.py:325] mlp: activation = None
I0123 12:32:00.989696 140045223440384 nn_components.py:329] mlp: final activation = None
I0123 12:32:00.991579 140045223440384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:00.991672 140045223440384 nn_components.py:261] mlp: residual
I0123 12:32:00.994108 140045223440384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:00.994188 140045223440384 transformer_base.py:443] tbase: final FFN
I0123 12:32:00.994300 140045223440384 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:00.996573 140045223440384 nn_components.py:329] mlp: final activation = None
I0123 12:32:00.998460 140045223440384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:00.998557 140045223440384 nn_components.py:261] mlp: residual
I0123 12:32:00.998846 140045223440384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:00.998928 140045223440384 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 12:32:00.999037 140045223440384 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:00.999076 140045223440384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:00.999106 140045223440384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:00.999170 140045223440384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.001432 140045223440384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:01.006866 140045223440384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.007128 140045223440384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:01.009812 140045223440384 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:01.022221 140045223440384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:01.022276 140045223440384 attention.py:418] Single window, no scan.
I0123 12:32:01.022312 140045223440384 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:01.022343 140045223440384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.022405 140045223440384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.022971 140045223440384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.023048 140045223440384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.023406 140045223440384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.024092 140045223440384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.027089 140045223440384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.027708 140045223440384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.027786 140045223440384 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:01.027822 140045223440384 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:01.027883 140045223440384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.028012 140045223440384 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:01.028122 140045223440384 nn_components.py:325] mlp: activation = None
I0123 12:32:01.028162 140045223440384 nn_components.py:329] mlp: final activation = None
I0123 12:32:01.030047 140045223440384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.030142 140045223440384 nn_components.py:261] mlp: residual
I0123 12:32:01.032568 140045223440384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.032648 140045223440384 transformer_base.py:443] tbase: final FFN
I0123 12:32:01.032760 140045223440384 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:01.035023 140045223440384 nn_components.py:329] mlp: final activation = None
I0123 12:32:01.036890 140045223440384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.036986 140045223440384 nn_components.py:261] mlp: residual
I0123 12:32:01.037275 140045223440384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.037357 140045223440384 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 12:32:01.037465 140045223440384 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:01.037503 140045223440384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:01.037534 140045223440384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:01.037598 140045223440384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.039838 140045223440384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:01.045222 140045223440384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.045484 140045223440384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:01.048176 140045223440384 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:01.060728 140045223440384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:01.060784 140045223440384 attention.py:418] Single window, no scan.
I0123 12:32:01.060827 140045223440384 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:01.060869 140045223440384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.060934 140045223440384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.061510 140045223440384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.061586 140045223440384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.061952 140045223440384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.062656 140045223440384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.065216 140045223440384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.065845 140045223440384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.065922 140045223440384 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:01.065956 140045223440384 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:01.066015 140045223440384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.066143 140045223440384 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:01.066251 140045223440384 nn_components.py:325] mlp: activation = None
I0123 12:32:01.066291 140045223440384 nn_components.py:329] mlp: final activation = None
I0123 12:32:01.068163 140045223440384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.068256 140045223440384 nn_components.py:261] mlp: residual
I0123 12:32:01.070689 140045223440384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.070767 140045223440384 transformer_base.py:443] tbase: final FFN
I0123 12:32:01.070882 140045223440384 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:01.073184 140045223440384 nn_components.py:329] mlp: final activation = None
I0123 12:32:01.075065 140045223440384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.075160 140045223440384 nn_components.py:261] mlp: residual
I0123 12:32:01.075445 140045223440384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.075526 140045223440384 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 12:32:01.075634 140045223440384 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:01.075671 140045223440384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:01.075702 140045223440384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:01.075764 140045223440384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.078025 140045223440384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:01.083441 140045223440384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.083697 140045223440384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:01.086539 140045223440384 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:01.099159 140045223440384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:01.099213 140045223440384 attention.py:418] Single window, no scan.
I0123 12:32:01.099247 140045223440384 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:01.099275 140045223440384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.099336 140045223440384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.099904 140045223440384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.099979 140045223440384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.100345 140045223440384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.101039 140045223440384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.103603 140045223440384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.104233 140045223440384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.104310 140045223440384 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:01.104343 140045223440384 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:01.104401 140045223440384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.104529 140045223440384 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:01.104638 140045223440384 nn_components.py:325] mlp: activation = None
I0123 12:32:01.104676 140045223440384 nn_components.py:329] mlp: final activation = None
I0123 12:32:01.106561 140045223440384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.106660 140045223440384 nn_components.py:261] mlp: residual
I0123 12:32:01.109106 140045223440384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.109184 140045223440384 transformer_base.py:443] tbase: final FFN
I0123 12:32:01.109292 140045223440384 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:01.111593 140045223440384 nn_components.py:329] mlp: final activation = None
I0123 12:32:01.113474 140045223440384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.113567 140045223440384 nn_components.py:261] mlp: residual
I0123 12:32:01.113862 140045223440384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.113944 140045223440384 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 12:32:01.114052 140045223440384 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:01.114090 140045223440384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:01.114120 140045223440384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:01.114183 140045223440384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.116446 140045223440384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:01.121925 140045223440384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.122183 140045223440384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:01.124879 140045223440384 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:01.137469 140045223440384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:01.137521 140045223440384 attention.py:418] Single window, no scan.
I0123 12:32:01.137558 140045223440384 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:01.137588 140045223440384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.137655 140045223440384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.138220 140045223440384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.138296 140045223440384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.138653 140045223440384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.139348 140045223440384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.142338 140045223440384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.142960 140045223440384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.143038 140045223440384 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:01.143072 140045223440384 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:01.143129 140045223440384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.143256 140045223440384 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:01.143366 140045223440384 nn_components.py:325] mlp: activation = None
I0123 12:32:01.143403 140045223440384 nn_components.py:329] mlp: final activation = None
I0123 12:32:01.145296 140045223440384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.145395 140045223440384 nn_components.py:261] mlp: residual
I0123 12:32:01.147823 140045223440384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.147903 140045223440384 transformer_base.py:443] tbase: final FFN
I0123 12:32:01.148010 140045223440384 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:01.150330 140045223440384 nn_components.py:329] mlp: final activation = None
I0123 12:32:01.152217 140045223440384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.152310 140045223440384 nn_components.py:261] mlp: residual
I0123 12:32:01.152597 140045223440384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.152677 140045223440384 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 12:32:01.152784 140045223440384 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:01.152821 140045223440384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:01.152852 140045223440384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:01.152914 140045223440384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.155186 140045223440384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:01.160633 140045223440384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.160892 140045223440384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:01.163618 140045223440384 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:01.176065 140045223440384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:01.176120 140045223440384 attention.py:418] Single window, no scan.
I0123 12:32:01.176155 140045223440384 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:01.176185 140045223440384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.176246 140045223440384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.176805 140045223440384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.176880 140045223440384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.177239 140045223440384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.177939 140045223440384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.180478 140045223440384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.181118 140045223440384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.181194 140045223440384 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:01.181227 140045223440384 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:01.181285 140045223440384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.181411 140045223440384 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:01.181519 140045223440384 nn_components.py:325] mlp: activation = None
I0123 12:32:01.181556 140045223440384 nn_components.py:329] mlp: final activation = None
I0123 12:32:01.183439 140045223440384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.183531 140045223440384 nn_components.py:261] mlp: residual
I0123 12:32:01.185956 140045223440384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.186034 140045223440384 transformer_base.py:443] tbase: final FFN
I0123 12:32:01.186141 140045223440384 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:01.188453 140045223440384 nn_components.py:329] mlp: final activation = None
I0123 12:32:01.190314 140045223440384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.190408 140045223440384 nn_components.py:261] mlp: residual
I0123 12:32:01.190697 140045223440384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.190778 140045223440384 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 12:32:01.190885 140045223440384 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:01.190923 140045223440384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:01.190952 140045223440384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:01.191015 140045223440384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.193269 140045223440384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:01.198755 140045223440384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.199016 140045223440384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:01.201736 140045223440384 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:01.214434 140045223440384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:01.214489 140045223440384 attention.py:418] Single window, no scan.
I0123 12:32:01.214524 140045223440384 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:01.214554 140045223440384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.214616 140045223440384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.215184 140045223440384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.215262 140045223440384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.215625 140045223440384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.216325 140045223440384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.218910 140045223440384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.219542 140045223440384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.219619 140045223440384 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:01.219653 140045223440384 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:01.219712 140045223440384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.219838 140045223440384 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:01.219947 140045223440384 nn_components.py:325] mlp: activation = None
I0123 12:32:01.219984 140045223440384 nn_components.py:329] mlp: final activation = None
I0123 12:32:01.221872 140045223440384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.221965 140045223440384 nn_components.py:261] mlp: residual
I0123 12:32:01.224392 140045223440384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.224477 140045223440384 transformer_base.py:443] tbase: final FFN
I0123 12:32:01.224587 140045223440384 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:01.226993 140045223440384 nn_components.py:329] mlp: final activation = None
I0123 12:32:01.229039 140045223440384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.229134 140045223440384 nn_components.py:261] mlp: residual
I0123 12:32:01.229425 140045223440384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.229506 140045223440384 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 12:32:01.229615 140045223440384 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:01.229659 140045223440384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:01.229690 140045223440384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:01.229753 140045223440384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.232023 140045223440384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:01.237489 140045223440384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.237756 140045223440384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:01.240486 140045223440384 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:01.253112 140045223440384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:01.253166 140045223440384 attention.py:418] Single window, no scan.
I0123 12:32:01.253200 140045223440384 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:01.253230 140045223440384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.253290 140045223440384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.253884 140045223440384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.253962 140045223440384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.254328 140045223440384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.255028 140045223440384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.258083 140045223440384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.258711 140045223440384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.258788 140045223440384 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:01.258822 140045223440384 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:01.258880 140045223440384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.259013 140045223440384 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:01.259122 140045223440384 nn_components.py:325] mlp: activation = None
I0123 12:32:01.259159 140045223440384 nn_components.py:329] mlp: final activation = None
I0123 12:32:01.261058 140045223440384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.261149 140045223440384 nn_components.py:261] mlp: residual
I0123 12:32:01.263612 140045223440384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.263700 140045223440384 transformer_base.py:443] tbase: final FFN
I0123 12:32:01.263811 140045223440384 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:01.266145 140045223440384 nn_components.py:329] mlp: final activation = None
I0123 12:32:01.268026 140045223440384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.268121 140045223440384 nn_components.py:261] mlp: residual
I0123 12:32:01.268414 140045223440384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.268496 140045223440384 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 12:32:01.268604 140045223440384 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:01.268642 140045223440384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:01.268672 140045223440384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:01.268737 140045223440384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.271070 140045223440384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:01.276587 140045223440384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.276846 140045223440384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:01.279578 140045223440384 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:01.292362 140045223440384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:01.292416 140045223440384 attention.py:418] Single window, no scan.
I0123 12:32:01.292450 140045223440384 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:01.292480 140045223440384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.292546 140045223440384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.293118 140045223440384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.293194 140045223440384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.293553 140045223440384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.294285 140045223440384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.296875 140045223440384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.297500 140045223440384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.297579 140045223440384 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:01.297612 140045223440384 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:01.297677 140045223440384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.297813 140045223440384 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:01.297920 140045223440384 nn_components.py:325] mlp: activation = None
I0123 12:32:01.297959 140045223440384 nn_components.py:329] mlp: final activation = None
I0123 12:32:01.300412 140045223440384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.300507 140045223440384 nn_components.py:261] mlp: residual
I0123 12:32:01.302938 140045223440384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.303018 140045223440384 transformer_base.py:443] tbase: final FFN
I0123 12:32:01.303133 140045223440384 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:01.305394 140045223440384 nn_components.py:329] mlp: final activation = None
I0123 12:32:01.307259 140045223440384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.307355 140045223440384 nn_components.py:261] mlp: residual
I0123 12:32:01.307647 140045223440384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.307730 140045223440384 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 12:32:01.307837 140045223440384 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:01.307875 140045223440384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:01.307903 140045223440384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:01.307964 140045223440384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.310216 140045223440384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:01.315643 140045223440384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.315901 140045223440384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:01.318609 140045223440384 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:01.331185 140045223440384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:01.331239 140045223440384 attention.py:418] Single window, no scan.
I0123 12:32:01.331273 140045223440384 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:01.331302 140045223440384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.331365 140045223440384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.331924 140045223440384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.332000 140045223440384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.332362 140045223440384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.333059 140045223440384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.335640 140045223440384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.336271 140045223440384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.336348 140045223440384 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:01.336382 140045223440384 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:01.336438 140045223440384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.336562 140045223440384 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:01.336668 140045223440384 nn_components.py:325] mlp: activation = None
I0123 12:32:01.336706 140045223440384 nn_components.py:329] mlp: final activation = None
I0123 12:32:01.338597 140045223440384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.338690 140045223440384 nn_components.py:261] mlp: residual
I0123 12:32:01.341081 140045223440384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.341157 140045223440384 transformer_base.py:443] tbase: final FFN
I0123 12:32:01.341262 140045223440384 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:01.343569 140045223440384 nn_components.py:329] mlp: final activation = None
I0123 12:32:01.345619 140045223440384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.345720 140045223440384 nn_components.py:261] mlp: residual
I0123 12:32:01.346012 140045223440384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.346093 140045223440384 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 12:32:01.346199 140045223440384 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:01.346238 140045223440384 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:01.346267 140045223440384 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:01.346328 140045223440384 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.348587 140045223440384 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:01.354034 140045223440384 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.354290 140045223440384 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:01.356999 140045223440384 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:01.369567 140045223440384 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:01.369621 140045223440384 attention.py:418] Single window, no scan.
I0123 12:32:01.369663 140045223440384 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:01.369694 140045223440384 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.369756 140045223440384 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.370316 140045223440384 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.370391 140045223440384 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.370756 140045223440384 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.371455 140045223440384 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.374405 140045223440384 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.375030 140045223440384 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.375106 140045223440384 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:01.375140 140045223440384 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:01.375197 140045223440384 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.375329 140045223440384 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:01.375437 140045223440384 nn_components.py:325] mlp: activation = None
I0123 12:32:01.375474 140045223440384 nn_components.py:329] mlp: final activation = None
I0123 12:32:01.377353 140045223440384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.377445 140045223440384 nn_components.py:261] mlp: residual
I0123 12:32:01.379873 140045223440384 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.379952 140045223440384 transformer_base.py:443] tbase: final FFN
I0123 12:32:01.380058 140045223440384 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:01.382435 140045223440384 nn_components.py:329] mlp: final activation = None
I0123 12:32:01.384326 140045223440384 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.384418 140045223440384 nn_components.py:261] mlp: residual
I0123 12:32:01.384705 140045223440384 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:01.384790 140045223440384 decoder_stack.py:344] dstack: Final layernorm.
I0123 12:32:01.387647 140045223440384 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:05.835699 140045223440384 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
I0123 12:32:06.396892 140045223440384 training_loop.py:409] No working directory specified.
I0123 12:32:06.397008 140045223440384 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0123 12:32:06.397776 140045223440384 checkpoints.py:1062] Restoring legacy Flax checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0123 12:32:09.332146 140045223440384 training_loop.py:447] Only restoring trainable parameters.
I0123 12:32:09.332875 140045223440384 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0123 12:32:09.332934 140045223440384 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0123 12:32:09.332981 140045223440384 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:32:09.333024 140045223440384 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:32:09.333065 140045223440384 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:09.333106 140045223440384 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:09.333147 140045223440384 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:09.333187 140045223440384 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:09.333226 140045223440384 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:32:09.333266 140045223440384 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:32:09.333304 140045223440384 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:09.333341 140045223440384 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:09.333379 140045223440384 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:32:09.333417 140045223440384 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:32:09.333454 140045223440384 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:09.333493 140045223440384 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:09.333531 140045223440384 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:09.333568 140045223440384 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:09.333605 140045223440384 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:32:09.333646 140045223440384 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:32:09.333700 140045223440384 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:09.333740 140045223440384 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:09.333778 140045223440384 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:32:09.333815 140045223440384 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:32:09.333852 140045223440384 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:09.333887 140045223440384 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:09.333923 140045223440384 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:09.333960 140045223440384 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:09.333998 140045223440384 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:32:09.334036 140045223440384 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:32:09.334073 140045223440384 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:09.334108 140045223440384 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:09.334145 140045223440384 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:32:09.334181 140045223440384 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:32:09.334218 140045223440384 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:09.334253 140045223440384 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:09.334290 140045223440384 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:09.334326 140045223440384 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:09.334362 140045223440384 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:32:09.334398 140045223440384 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:32:09.334433 140045223440384 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:09.334468 140045223440384 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:09.334504 140045223440384 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:32:09.334541 140045223440384 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:32:09.334577 140045223440384 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:09.334612 140045223440384 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:09.334654 140045223440384 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:09.334692 140045223440384 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:09.334729 140045223440384 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:32:09.334766 140045223440384 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:32:09.334801 140045223440384 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:09.334837 140045223440384 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:09.334873 140045223440384 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:32:09.334909 140045223440384 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:32:09.334945 140045223440384 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:09.334981 140045223440384 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:09.335016 140045223440384 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:09.335051 140045223440384 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:09.335088 140045223440384 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:32:09.335124 140045223440384 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:32:09.335159 140045223440384 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:09.335195 140045223440384 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:09.335230 140045223440384 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:32:09.335266 140045223440384 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:32:09.335302 140045223440384 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:09.335338 140045223440384 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:09.335374 140045223440384 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:09.335409 140045223440384 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:09.335445 140045223440384 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:32:09.335481 140045223440384 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:32:09.335517 140045223440384 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:09.335553 140045223440384 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:09.335589 140045223440384 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:32:09.335630 140045223440384 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:32:09.335668 140045223440384 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:09.335704 140045223440384 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:09.335739 140045223440384 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:09.335774 140045223440384 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:09.335809 140045223440384 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:32:09.335845 140045223440384 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:32:09.335882 140045223440384 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:09.335918 140045223440384 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:09.335953 140045223440384 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:32:09.335989 140045223440384 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:32:09.336025 140045223440384 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:09.336061 140045223440384 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:09.336096 140045223440384 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:09.336133 140045223440384 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:09.336169 140045223440384 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:32:09.336205 140045223440384 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:32:09.336241 140045223440384 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:09.336277 140045223440384 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:09.336312 140045223440384 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:32:09.336348 140045223440384 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:32:09.336384 140045223440384 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:09.336419 140045223440384 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:09.336455 140045223440384 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:09.336491 140045223440384 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:09.336527 140045223440384 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:32:09.336563 140045223440384 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:32:09.336604 140045223440384 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:09.336641 140045223440384 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:09.336676 140045223440384 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:32:09.336711 140045223440384 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:32:09.336746 140045223440384 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:09.336783 140045223440384 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:09.336819 140045223440384 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:09.336854 140045223440384 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:09.336889 140045223440384 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:32:09.336925 140045223440384 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:32:09.336960 140045223440384 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:09.336996 140045223440384 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:09.337031 140045223440384 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:32:09.337067 140045223440384 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:32:09.337102 140045223440384 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:09.337137 140045223440384 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:09.337173 140045223440384 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:09.337209 140045223440384 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:09.337246 140045223440384 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:32:09.337283 140045223440384 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:32:09.337319 140045223440384 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:09.337354 140045223440384 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:09.337383 140045223440384 training_loop.py:725] Total parameters: 152072288
I0123 12:32:09.337594 140045223440384 training_loop.py:739] Total state size: 0
I0123 12:32:09.360275 140045223440384 training_loop.py:492] Training loop: creating task for mode beam_search
I0123 12:32:09.360550 140045223440384 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0123 12:32:09.361253 140045223440384 training_loop.py:652] Compiling mode beam_search with jit.
I0123 12:32:09.361578 140045223440384 training_loop.py:89] registering functions: dict_keys([])
I0123 12:32:09.378801 140045223440384 graph.py:499] a b c = triangle a b c; d = midpoint d c b; e = midpoint e c a; f = on_line f b a; g = on_pline g f a c, on_line g c b; h = on_circle h d c, on_line h f g; i = on_circle i d c, on_line i f g; j = on_circle j e c, on_line j i c ? para c b f j
I0123 12:32:10.503658 140045223440384 ddar.py:60] Depth 1/1000 time = 1.1018121242523193
I0123 12:32:12.118178 140045223440384 ddar.py:60] Depth 2/1000 time = 1.6143531799316406
I0123 12:32:13.899728 140045223440384 ddar.py:60] Depth 3/1000 time = 1.7813708782196045
I0123 12:32:15.820170 140045223440384 ddar.py:60] Depth 4/1000 time = 1.9202704429626465
I0123 12:32:15.827356 140045223440384 alphageometry.py:191] 
==========================
 * From theorem premises:
A B C D E F G I J : Points
D,C,B are collinear [00]
DC = DB [01]
A,E,C are collinear [02]
EC = EA [03]
A,F,B are collinear [04]
GF ∥ AC [05]
G,C,B are collinear [06]
DI = DC [07]
I,F,G are collinear [08]
EJ = EC [09]
I,C,J are collinear [10]

 * Auxiliary Constructions:
H : Points
DH = DC [11]
H,F,G are collinear [12]

 * Proof steps:
001. D,C,B are collinear [00] & DC = DB [01] ⇒  D is midpoint of CB [13]
002. A,E,C are collinear [02] & EC = EA [03] ⇒  E is midpoint of CA [14]
003. D is midpoint of CB [13] & E is midpoint of CA [14] ⇒  DE ∥ BA [15]
004. F,A,B are collinear [04] & A,E,C are collinear [02] & AB ∥ DE [15] & FG ∥ AC [05] ⇒  ∠BFG = ∠DEC [16]
005. G,B,C are collinear [06] & D,B,C are collinear [00] & A,E,C are collinear [02] & FG ∥ AC [05] ⇒  ∠BGF = ∠DCE [17]
006. ∠BFG = ∠DEC [16] & ∠BGF = ∠DCE [17] (Similar Triangles)⇒  GB:GF = CD:CE [18]
007. DC = DB [01] & DI = DC [07] & DH = DC [11] ⇒  I,H,C,B are concyclic [19]
008. I,H,C,B are concyclic [19] ⇒  ∠IHB = ∠ICB [20]
009. DH = DC [11] & DC = DB [01] ⇒  DB = DH [21]
010. DB = DH [21] ⇒  ∠DBH = ∠BHD [22]
011. A,E,C are collinear [02] & I,C,J are collinear [10] & ∠IHB = ∠ICB [20] & I,F,G are collinear [08] & H,F,G are collinear [12] & FG ∥ AC [05] & ∠DBH = ∠BHD [22] & D,C,B are collinear [00] ⇒  ∠DHB = ∠ECJ [23]
012. EJ = EC [09] ⇒  ∠ECJ = ∠CJE [24]
013. D,C,B are collinear [00] & I,C,J are collinear [10] & ∠ECJ = ∠CJE [24] & A,E,C are collinear [02] & ∠IHB = ∠ICB [20] & I,F,G are collinear [08] & H,F,G are collinear [12] & FG ∥ AC [05] ⇒  ∠DBH = ∠EJC [25]
014. ∠DHB = ∠ECJ [23] & ∠DBH = ∠EJC [25] (Similar Triangles)⇒  HD:CE = HB:CJ [26]
015. GB:GF = CD:CE [18] & DI = DC [07] & EJ = EC [09] & HD:CE = HB:CJ [26] & DH = DC [11] ⇒  CJ:HB = FG:GB [27]
016. H,G,F are collinear [12] & B,G,C are collinear [06] & ∠IHB = ∠ICB [20] & I,F,G are collinear [08] ⇒  ∠GHB = ∠ICG [28]
017. G,B,C are collinear [06] & I,F,G are collinear [08] & ∠IHB = ∠ICB [20] & H,F,G are collinear [12] ⇒  ∠GBH = ∠CIG [29]
018. ∠GHB = ∠ICG [28] & ∠GBH = ∠CIG [29] (Similar Triangles)⇒  GB:IG = HB:IC [30]
019. CJ:HB = FG:GB [27] & GB:IG = HB:IC [30] ⇒  CJ:IC = FG:IG [31]
020. CJ:IC = FG:IG [31] & I,C,J are collinear [10] & I,F,G are collinear [08] ⇒  CG ∥ JF [32]
021. CG ∥ JF [32] & G,C,B are collinear [06] ⇒  BC ∥ FJ
==========================

