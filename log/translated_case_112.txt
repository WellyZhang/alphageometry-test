I0123 13:04:50.300658 140086163390464 inference_utils.py:69] Parsing gin configuration.
I0123 13:04:50.300759 140086163390464 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0123 13:04:50.300957 140086163390464 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0123 13:04:50.300991 140086163390464 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0123 13:04:50.301020 140086163390464 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0123 13:04:50.301049 140086163390464 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0123 13:04:50.301076 140086163390464 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0123 13:04:50.301106 140086163390464 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0123 13:04:50.301135 140086163390464 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0123 13:04:50.301162 140086163390464 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=32
I0123 13:04:50.301188 140086163390464 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0123 13:04:50.301214 140086163390464 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0123 13:04:50.301260 140086163390464 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0123 13:04:50.301396 140086163390464 resource_reader.py:55] Path not found: base_htrans.gin
I0123 13:04:50.301597 140086163390464 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0123 13:04:50.301711 140086163390464 resource_reader.py:55] Path not found: trainer_configuration.gin
I0123 13:04:50.308020 140086163390464 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0123 13:04:50.308136 140086163390464 resource_reader.py:55] Path not found: size/medium_150M.gin
I0123 13:04:50.308461 140086163390464 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0123 13:04:50.308565 140086163390464 resource_reader.py:55] Path not found: options/positions_t5.gin
I0123 13:04:50.308844 140086163390464 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0123 13:04:50.308944 140086163390464 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0123 13:04:50.309353 140086163390464 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0123 13:04:50.309453 140086163390464 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0123 13:04:50.313145 140086163390464 training_loop.py:334] ==== Training loop: initializing model ====
I0123 13:04:50.416628 140086163390464 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0123 13:04:50.417377 140086163390464 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0123 13:04:50.424230 140086163390464 training_loop.py:335] Process 0 of 1
I0123 13:04:50.424285 140086163390464 training_loop.py:336] Local device count = 1
I0123 13:04:50.424325 140086163390464 training_loop.py:337] Number of replicas = 1
I0123 13:04:50.424357 140086163390464 training_loop.py:339] Using random number seed 42
I0123 13:04:50.896570 140086163390464 training_loop.py:359] Initializing the model.
I0123 13:04:51.316610 140086163390464 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.316872 140086163390464 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 13:04:51.316978 140086163390464 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 13:04:51.317059 140086163390464 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 13:04:51.317134 140086163390464 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 13:04:51.317215 140086163390464 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 13:04:51.317287 140086163390464 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 13:04:51.317357 140086163390464 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 13:04:51.317426 140086163390464 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 13:04:51.317495 140086163390464 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 13:04:51.317564 140086163390464 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 13:04:51.317632 140086163390464 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 13:04:51.317714 140086163390464 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 13:04:51.317784 140086163390464 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 13:04:51.317823 140086163390464 attention.py:418] Single window, no scan.
I0123 13:04:51.317870 140086163390464 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 13:04:51.317985 140086163390464 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:04:51.318024 140086163390464 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:04:51.318055 140086163390464 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:04:51.320045 140086163390464 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.325321 140086163390464 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:04:51.335898 140086163390464 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.336180 140086163390464 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:04:51.340520 140086163390464 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:04:51.351051 140086163390464 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:04:51.351108 140086163390464 attention.py:418] Single window, no scan.
I0123 13:04:51.351146 140086163390464 transformer_layer.py:389] tlayer: self-attention.
I0123 13:04:51.351178 140086163390464 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.351242 140086163390464 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.352409 140086163390464 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.352488 140086163390464 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.353206 140086163390464 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.355672 140086163390464 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.361514 140086163390464 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.363525 140086163390464 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.363611 140086163390464 transformer_layer.py:468] tlayer: End windows.
I0123 13:04:51.363648 140086163390464 transformer_layer.py:472] tlayer: final FFN.
I0123 13:04:51.363712 140086163390464 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.363841 140086163390464 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:04:51.364183 140086163390464 nn_components.py:325] mlp: activation = None
I0123 13:04:51.364231 140086163390464 nn_components.py:329] mlp: final activation = None
I0123 13:04:51.366169 140086163390464 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.366270 140086163390464 nn_components.py:261] mlp: residual
I0123 13:04:51.369170 140086163390464 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.369250 140086163390464 transformer_base.py:443] tbase: final FFN
I0123 13:04:51.370219 140086163390464 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:04:51.380720 140086163390464 nn_components.py:329] mlp: final activation = None
I0123 13:04:51.389591 140086163390464 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.389708 140086163390464 nn_components.py:261] mlp: residual
I0123 13:04:51.390014 140086163390464 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.390097 140086163390464 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 13:04:51.390210 140086163390464 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:04:51.390250 140086163390464 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:04:51.390283 140086163390464 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:04:51.392136 140086163390464 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.394646 140086163390464 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:04:51.400266 140086163390464 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.400530 140086163390464 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:04:51.403204 140086163390464 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:04:51.407085 140086163390464 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:04:51.407144 140086163390464 attention.py:418] Single window, no scan.
I0123 13:04:51.407181 140086163390464 transformer_layer.py:389] tlayer: self-attention.
I0123 13:04:51.407213 140086163390464 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.407275 140086163390464 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.407857 140086163390464 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.407936 140086163390464 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.408304 140086163390464 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.409083 140086163390464 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.411583 140086163390464 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.412213 140086163390464 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.412291 140086163390464 transformer_layer.py:468] tlayer: End windows.
I0123 13:04:51.412327 140086163390464 transformer_layer.py:472] tlayer: final FFN.
I0123 13:04:51.412387 140086163390464 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.412514 140086163390464 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:04:51.412846 140086163390464 nn_components.py:325] mlp: activation = None
I0123 13:04:51.412890 140086163390464 nn_components.py:329] mlp: final activation = None
I0123 13:04:51.414856 140086163390464 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.414955 140086163390464 nn_components.py:261] mlp: residual
I0123 13:04:51.417509 140086163390464 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.417593 140086163390464 transformer_base.py:443] tbase: final FFN
I0123 13:04:51.418035 140086163390464 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:04:51.420373 140086163390464 nn_components.py:329] mlp: final activation = None
I0123 13:04:51.422295 140086163390464 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.422393 140086163390464 nn_components.py:261] mlp: residual
I0123 13:04:51.422696 140086163390464 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.422778 140086163390464 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 13:04:51.422887 140086163390464 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:04:51.422927 140086163390464 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:04:51.422958 140086163390464 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:04:51.424866 140086163390464 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.427294 140086163390464 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:04:51.433343 140086163390464 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.433608 140086163390464 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:04:51.436291 140086163390464 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:04:51.440189 140086163390464 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:04:51.440245 140086163390464 attention.py:418] Single window, no scan.
I0123 13:04:51.440282 140086163390464 transformer_layer.py:389] tlayer: self-attention.
I0123 13:04:51.440313 140086163390464 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.440379 140086163390464 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.440946 140086163390464 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.441022 140086163390464 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.441392 140086163390464 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.442173 140086163390464 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.444702 140086163390464 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.445376 140086163390464 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.445455 140086163390464 transformer_layer.py:468] tlayer: End windows.
I0123 13:04:51.445491 140086163390464 transformer_layer.py:472] tlayer: final FFN.
I0123 13:04:51.445551 140086163390464 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.445688 140086163390464 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:04:51.446010 140086163390464 nn_components.py:325] mlp: activation = None
I0123 13:04:51.446053 140086163390464 nn_components.py:329] mlp: final activation = None
I0123 13:04:51.447973 140086163390464 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.448069 140086163390464 nn_components.py:261] mlp: residual
I0123 13:04:51.450617 140086163390464 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.450703 140086163390464 transformer_base.py:443] tbase: final FFN
I0123 13:04:51.451199 140086163390464 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:04:51.453485 140086163390464 nn_components.py:329] mlp: final activation = None
I0123 13:04:51.455430 140086163390464 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.455524 140086163390464 nn_components.py:261] mlp: residual
I0123 13:04:51.455822 140086163390464 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.455903 140086163390464 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 13:04:51.456013 140086163390464 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:04:51.456053 140086163390464 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:04:51.456084 140086163390464 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:04:51.458025 140086163390464 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.460465 140086163390464 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:04:51.466418 140086163390464 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.466686 140086163390464 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:04:51.469370 140086163390464 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:04:51.473156 140086163390464 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:04:51.473211 140086163390464 attention.py:418] Single window, no scan.
I0123 13:04:51.473246 140086163390464 transformer_layer.py:389] tlayer: self-attention.
I0123 13:04:51.473276 140086163390464 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.473337 140086163390464 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.473913 140086163390464 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.473994 140086163390464 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.474364 140086163390464 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.475154 140086163390464 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.477761 140086163390464 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.478394 140086163390464 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.478474 140086163390464 transformer_layer.py:468] tlayer: End windows.
I0123 13:04:51.478513 140086163390464 transformer_layer.py:472] tlayer: final FFN.
I0123 13:04:51.478574 140086163390464 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.478708 140086163390464 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:04:51.479042 140086163390464 nn_components.py:325] mlp: activation = None
I0123 13:04:51.479085 140086163390464 nn_components.py:329] mlp: final activation = None
I0123 13:04:51.481003 140086163390464 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.481097 140086163390464 nn_components.py:261] mlp: residual
I0123 13:04:51.483705 140086163390464 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.483791 140086163390464 transformer_base.py:443] tbase: final FFN
I0123 13:04:51.484228 140086163390464 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:04:51.486534 140086163390464 nn_components.py:329] mlp: final activation = None
I0123 13:04:51.488447 140086163390464 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.488547 140086163390464 nn_components.py:261] mlp: residual
I0123 13:04:51.488846 140086163390464 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.488931 140086163390464 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 13:04:51.489043 140086163390464 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:04:51.489082 140086163390464 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:04:51.489114 140086163390464 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:04:51.491021 140086163390464 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.493445 140086163390464 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:04:51.499134 140086163390464 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.499395 140086163390464 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:04:51.502081 140086163390464 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:04:51.505832 140086163390464 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:04:51.505887 140086163390464 attention.py:418] Single window, no scan.
I0123 13:04:51.505923 140086163390464 transformer_layer.py:389] tlayer: self-attention.
I0123 13:04:51.505954 140086163390464 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.506018 140086163390464 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.506593 140086163390464 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.506670 140086163390464 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.507033 140086163390464 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.507800 140086163390464 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.510684 140086163390464 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.511313 140086163390464 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.511396 140086163390464 transformer_layer.py:468] tlayer: End windows.
I0123 13:04:51.511431 140086163390464 transformer_layer.py:472] tlayer: final FFN.
I0123 13:04:51.511495 140086163390464 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.511629 140086163390464 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:04:51.511962 140086163390464 nn_components.py:325] mlp: activation = None
I0123 13:04:51.512007 140086163390464 nn_components.py:329] mlp: final activation = None
I0123 13:04:51.513927 140086163390464 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.514021 140086163390464 nn_components.py:261] mlp: residual
I0123 13:04:51.516591 140086163390464 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.516671 140086163390464 transformer_base.py:443] tbase: final FFN
I0123 13:04:51.517105 140086163390464 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:04:51.519398 140086163390464 nn_components.py:329] mlp: final activation = None
I0123 13:04:51.521354 140086163390464 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.521450 140086163390464 nn_components.py:261] mlp: residual
I0123 13:04:51.521754 140086163390464 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.521838 140086163390464 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 13:04:51.521948 140086163390464 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:04:51.521987 140086163390464 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:04:51.522017 140086163390464 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:04:51.523871 140086163390464 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.526273 140086163390464 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:04:51.531931 140086163390464 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.532194 140086163390464 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:04:51.534883 140086163390464 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:04:51.538624 140086163390464 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:04:51.538679 140086163390464 attention.py:418] Single window, no scan.
I0123 13:04:51.538715 140086163390464 transformer_layer.py:389] tlayer: self-attention.
I0123 13:04:51.538746 140086163390464 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.538809 140086163390464 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.539417 140086163390464 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.539495 140086163390464 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.539855 140086163390464 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.540624 140086163390464 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.543131 140086163390464 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.543753 140086163390464 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.543831 140086163390464 transformer_layer.py:468] tlayer: End windows.
I0123 13:04:51.543865 140086163390464 transformer_layer.py:472] tlayer: final FFN.
I0123 13:04:51.543923 140086163390464 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.544050 140086163390464 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:04:51.544378 140086163390464 nn_components.py:325] mlp: activation = None
I0123 13:04:51.544422 140086163390464 nn_components.py:329] mlp: final activation = None
I0123 13:04:51.546328 140086163390464 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.546422 140086163390464 nn_components.py:261] mlp: residual
I0123 13:04:51.548977 140086163390464 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.549057 140086163390464 transformer_base.py:443] tbase: final FFN
I0123 13:04:51.549494 140086163390464 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:04:51.551813 140086163390464 nn_components.py:329] mlp: final activation = None
I0123 13:04:51.553719 140086163390464 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.553815 140086163390464 nn_components.py:261] mlp: residual
I0123 13:04:51.554114 140086163390464 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.554197 140086163390464 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 13:04:51.554307 140086163390464 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:04:51.554346 140086163390464 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:04:51.554377 140086163390464 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:04:51.556215 140086163390464 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.558698 140086163390464 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:04:51.564384 140086163390464 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.564653 140086163390464 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:04:51.567316 140086163390464 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:04:51.571099 140086163390464 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:04:51.571154 140086163390464 attention.py:418] Single window, no scan.
I0123 13:04:51.571190 140086163390464 transformer_layer.py:389] tlayer: self-attention.
I0123 13:04:51.571220 140086163390464 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.571282 140086163390464 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.571845 140086163390464 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.571922 140086163390464 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.572286 140086163390464 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.573062 140086163390464 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.575561 140086163390464 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.576182 140086163390464 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.576262 140086163390464 transformer_layer.py:468] tlayer: End windows.
I0123 13:04:51.576298 140086163390464 transformer_layer.py:472] tlayer: final FFN.
I0123 13:04:51.576357 140086163390464 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.576485 140086163390464 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:04:51.576811 140086163390464 nn_components.py:325] mlp: activation = None
I0123 13:04:51.576855 140086163390464 nn_components.py:329] mlp: final activation = None
I0123 13:04:51.578833 140086163390464 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.578932 140086163390464 nn_components.py:261] mlp: residual
I0123 13:04:51.581431 140086163390464 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.581513 140086163390464 transformer_base.py:443] tbase: final FFN
I0123 13:04:51.581955 140086163390464 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:04:51.584586 140086163390464 nn_components.py:329] mlp: final activation = None
I0123 13:04:51.586501 140086163390464 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.586601 140086163390464 nn_components.py:261] mlp: residual
I0123 13:04:51.586895 140086163390464 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.586980 140086163390464 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 13:04:51.587090 140086163390464 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:04:51.587129 140086163390464 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:04:51.587160 140086163390464 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:04:51.723282 140086163390464 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.726440 140086163390464 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:04:51.732408 140086163390464 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.732714 140086163390464 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:04:51.735448 140086163390464 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:04:51.739442 140086163390464 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:04:51.739502 140086163390464 attention.py:418] Single window, no scan.
I0123 13:04:51.739542 140086163390464 transformer_layer.py:389] tlayer: self-attention.
I0123 13:04:51.739575 140086163390464 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.739645 140086163390464 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.740274 140086163390464 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.740352 140086163390464 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.740721 140086163390464 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.741508 140086163390464 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.744158 140086163390464 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.744824 140086163390464 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.744904 140086163390464 transformer_layer.py:468] tlayer: End windows.
I0123 13:04:51.744941 140086163390464 transformer_layer.py:472] tlayer: final FFN.
I0123 13:04:51.745005 140086163390464 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.745136 140086163390464 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:04:51.745483 140086163390464 nn_components.py:325] mlp: activation = None
I0123 13:04:51.745527 140086163390464 nn_components.py:329] mlp: final activation = None
I0123 13:04:51.747483 140086163390464 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.747581 140086163390464 nn_components.py:261] mlp: residual
I0123 13:04:51.750252 140086163390464 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.750338 140086163390464 transformer_base.py:443] tbase: final FFN
I0123 13:04:51.750784 140086163390464 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:04:51.753167 140086163390464 nn_components.py:329] mlp: final activation = None
I0123 13:04:51.755128 140086163390464 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.755236 140086163390464 nn_components.py:261] mlp: residual
I0123 13:04:51.755539 140086163390464 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.755625 140086163390464 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 13:04:51.755740 140086163390464 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:04:51.755781 140086163390464 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:04:51.755814 140086163390464 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:04:51.757796 140086163390464 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.760231 140086163390464 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:04:51.765997 140086163390464 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.766272 140086163390464 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:04:51.769010 140086163390464 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:04:51.772854 140086163390464 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:04:51.772912 140086163390464 attention.py:418] Single window, no scan.
I0123 13:04:51.772949 140086163390464 transformer_layer.py:389] tlayer: self-attention.
I0123 13:04:51.772982 140086163390464 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.773045 140086163390464 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.773621 140086163390464 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.773707 140086163390464 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.774080 140086163390464 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.774861 140086163390464 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.777475 140086163390464 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.778121 140086163390464 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.778201 140086163390464 transformer_layer.py:468] tlayer: End windows.
I0123 13:04:51.778238 140086163390464 transformer_layer.py:472] tlayer: final FFN.
I0123 13:04:51.778299 140086163390464 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.778429 140086163390464 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:04:51.778757 140086163390464 nn_components.py:325] mlp: activation = None
I0123 13:04:51.778801 140086163390464 nn_components.py:329] mlp: final activation = None
I0123 13:04:51.780725 140086163390464 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.780820 140086163390464 nn_components.py:261] mlp: residual
I0123 13:04:51.783446 140086163390464 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.783527 140086163390464 transformer_base.py:443] tbase: final FFN
I0123 13:04:51.783958 140086163390464 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:04:51.786303 140086163390464 nn_components.py:329] mlp: final activation = None
I0123 13:04:51.788318 140086163390464 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.788414 140086163390464 nn_components.py:261] mlp: residual
I0123 13:04:51.788713 140086163390464 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.788805 140086163390464 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 13:04:51.788920 140086163390464 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:04:51.788960 140086163390464 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:04:51.788993 140086163390464 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:04:51.790862 140086163390464 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.793360 140086163390464 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:04:51.799012 140086163390464 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.799285 140086163390464 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:04:51.802363 140086163390464 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:04:51.806173 140086163390464 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:04:51.806230 140086163390464 attention.py:418] Single window, no scan.
I0123 13:04:51.806267 140086163390464 transformer_layer.py:389] tlayer: self-attention.
I0123 13:04:51.806300 140086163390464 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.806366 140086163390464 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.806982 140086163390464 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.807063 140086163390464 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.807433 140086163390464 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.808396 140086163390464 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.811230 140086163390464 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.811867 140086163390464 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.811950 140086163390464 transformer_layer.py:468] tlayer: End windows.
I0123 13:04:51.811986 140086163390464 transformer_layer.py:472] tlayer: final FFN.
I0123 13:04:51.812047 140086163390464 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.812178 140086163390464 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:04:51.812504 140086163390464 nn_components.py:325] mlp: activation = None
I0123 13:04:51.812552 140086163390464 nn_components.py:329] mlp: final activation = None
I0123 13:04:51.814494 140086163390464 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.814590 140086163390464 nn_components.py:261] mlp: residual
I0123 13:04:51.817197 140086163390464 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.817281 140086163390464 transformer_base.py:443] tbase: final FFN
I0123 13:04:51.817725 140086163390464 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:04:51.820070 140086163390464 nn_components.py:329] mlp: final activation = None
I0123 13:04:51.822031 140086163390464 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.822132 140086163390464 nn_components.py:261] mlp: residual
I0123 13:04:51.822434 140086163390464 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.822522 140086163390464 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 13:04:51.822636 140086163390464 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:04:51.822676 140086163390464 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:04:51.822709 140086163390464 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:04:51.824568 140086163390464 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.827079 140086163390464 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:04:51.832733 140086163390464 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.832996 140086163390464 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:04:51.835657 140086163390464 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:04:51.839477 140086163390464 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:04:51.839534 140086163390464 attention.py:418] Single window, no scan.
I0123 13:04:51.839571 140086163390464 transformer_layer.py:389] tlayer: self-attention.
I0123 13:04:51.839604 140086163390464 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.839666 140086163390464 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.840246 140086163390464 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.840322 140086163390464 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.840692 140086163390464 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.841479 140086163390464 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.843992 140086163390464 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.844632 140086163390464 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.844710 140086163390464 transformer_layer.py:468] tlayer: End windows.
I0123 13:04:51.844747 140086163390464 transformer_layer.py:472] tlayer: final FFN.
I0123 13:04:51.844809 140086163390464 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.844939 140086163390464 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:04:51.845268 140086163390464 nn_components.py:325] mlp: activation = None
I0123 13:04:51.845312 140086163390464 nn_components.py:329] mlp: final activation = None
I0123 13:04:51.847303 140086163390464 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.847399 140086163390464 nn_components.py:261] mlp: residual
I0123 13:04:51.850178 140086163390464 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.850259 140086163390464 transformer_base.py:443] tbase: final FFN
I0123 13:04:51.850695 140086163390464 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:04:51.853034 140086163390464 nn_components.py:329] mlp: final activation = None
I0123 13:04:51.854942 140086163390464 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.855038 140086163390464 nn_components.py:261] mlp: residual
I0123 13:04:51.855333 140086163390464 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.855415 140086163390464 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 13:04:51.855532 140086163390464 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:04:51.855572 140086163390464 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:04:51.855605 140086163390464 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:04:51.857543 140086163390464 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.859980 140086163390464 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:04:51.865586 140086163390464 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.865859 140086163390464 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:04:51.868496 140086163390464 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:04:51.872256 140086163390464 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:04:51.872312 140086163390464 attention.py:418] Single window, no scan.
I0123 13:04:51.872349 140086163390464 transformer_layer.py:389] tlayer: self-attention.
I0123 13:04:51.872380 140086163390464 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.872442 140086163390464 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.873002 140086163390464 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.873079 140086163390464 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.873439 140086163390464 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.874206 140086163390464 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.876672 140086163390464 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.877665 140086163390464 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.877745 140086163390464 transformer_layer.py:468] tlayer: End windows.
I0123 13:04:51.877782 140086163390464 transformer_layer.py:472] tlayer: final FFN.
I0123 13:04:51.877841 140086163390464 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.877969 140086163390464 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:04:51.878287 140086163390464 nn_components.py:325] mlp: activation = None
I0123 13:04:51.878333 140086163390464 nn_components.py:329] mlp: final activation = None
I0123 13:04:51.880224 140086163390464 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.880319 140086163390464 nn_components.py:261] mlp: residual
I0123 13:04:51.882812 140086163390464 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.882894 140086163390464 transformer_base.py:443] tbase: final FFN
I0123 13:04:51.883375 140086163390464 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:04:51.885615 140086163390464 nn_components.py:329] mlp: final activation = None
I0123 13:04:51.887505 140086163390464 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.887599 140086163390464 nn_components.py:261] mlp: residual
I0123 13:04:51.887886 140086163390464 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.888170 140086163390464 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 13:04:51.888240 140086163390464 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 13:04:51.888305 140086163390464 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 13:04:51.888364 140086163390464 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 13:04:51.888419 140086163390464 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 13:04:51.888474 140086163390464 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 13:04:51.888527 140086163390464 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 13:04:51.888582 140086163390464 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 13:04:51.888635 140086163390464 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 13:04:51.888688 140086163390464 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 13:04:51.888740 140086163390464 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 13:04:51.888792 140086163390464 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 13:04:51.888829 140086163390464 decoder_stack.py:344] dstack: Final layernorm.
I0123 13:04:51.892384 140086163390464 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:04:51.939920 140086163390464 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:51.940006 140086163390464 decoder_stack.py:333] dstack: autoregressive generator.
I0123 13:04:51.940061 140086163390464 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 13:04:51.940164 140086163390464 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:04:51.940202 140086163390464 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:04:51.940232 140086163390464 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:04:51.940300 140086163390464 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:51.942708 140086163390464 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:04:51.948135 140086163390464 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:51.948395 140086163390464 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:04:51.951025 140086163390464 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:04:51.967296 140086163390464 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:04:51.967353 140086163390464 attention.py:418] Single window, no scan.
I0123 13:04:51.967388 140086163390464 transformer_layer.py:389] tlayer: self-attention.
I0123 13:04:51.967419 140086163390464 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:51.967480 140086163390464 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:51.968607 140086163390464 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:51.968686 140086163390464 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:51.969391 140086163390464 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:51.971395 140086163390464 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:51.976104 140086163390464 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:51.977398 140086163390464 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:51.977485 140086163390464 transformer_layer.py:468] tlayer: End windows.
I0123 13:04:51.977522 140086163390464 transformer_layer.py:472] tlayer: final FFN.
I0123 13:04:51.977583 140086163390464 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:51.977724 140086163390464 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:04:51.977838 140086163390464 nn_components.py:325] mlp: activation = None
I0123 13:04:51.977876 140086163390464 nn_components.py:329] mlp: final activation = None
I0123 13:04:51.979764 140086163390464 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:51.979859 140086163390464 nn_components.py:261] mlp: residual
I0123 13:04:51.982276 140086163390464 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:51.982362 140086163390464 transformer_base.py:443] tbase: final FFN
I0123 13:04:51.982472 140086163390464 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:04:51.984680 140086163390464 nn_components.py:329] mlp: final activation = None
I0123 13:04:51.986611 140086163390464 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:51.986707 140086163390464 nn_components.py:261] mlp: residual
I0123 13:04:51.986997 140086163390464 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:51.987078 140086163390464 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 13:04:51.987186 140086163390464 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:04:51.987225 140086163390464 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:04:51.987255 140086163390464 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:04:51.987318 140086163390464 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:51.989544 140086163390464 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:04:51.994946 140086163390464 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:51.995201 140086163390464 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:04:51.997838 140086163390464 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:04:52.010675 140086163390464 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:04:52.010730 140086163390464 attention.py:418] Single window, no scan.
I0123 13:04:52.010765 140086163390464 transformer_layer.py:389] tlayer: self-attention.
I0123 13:04:52.010795 140086163390464 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.010859 140086163390464 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.011418 140086163390464 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.011494 140086163390464 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.011853 140086163390464 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.012535 140086163390464 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.015031 140086163390464 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.015652 140086163390464 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.015729 140086163390464 transformer_layer.py:468] tlayer: End windows.
I0123 13:04:52.015770 140086163390464 transformer_layer.py:472] tlayer: final FFN.
I0123 13:04:52.015830 140086163390464 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.015962 140086163390464 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:04:52.016072 140086163390464 nn_components.py:325] mlp: activation = None
I0123 13:04:52.016111 140086163390464 nn_components.py:329] mlp: final activation = None
I0123 13:04:52.018035 140086163390464 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.018130 140086163390464 nn_components.py:261] mlp: residual
I0123 13:04:52.020524 140086163390464 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.020604 140086163390464 transformer_base.py:443] tbase: final FFN
I0123 13:04:52.020714 140086163390464 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:04:52.022925 140086163390464 nn_components.py:329] mlp: final activation = None
I0123 13:04:52.024825 140086163390464 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.024920 140086163390464 nn_components.py:261] mlp: residual
I0123 13:04:52.025205 140086163390464 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.025286 140086163390464 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 13:04:52.025394 140086163390464 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:04:52.025433 140086163390464 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:04:52.025464 140086163390464 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:04:52.025524 140086163390464 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.027742 140086163390464 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:04:52.038592 140086163390464 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.038888 140086163390464 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:04:52.041630 140086163390464 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:04:52.054404 140086163390464 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:04:52.054461 140086163390464 attention.py:418] Single window, no scan.
I0123 13:04:52.054498 140086163390464 transformer_layer.py:389] tlayer: self-attention.
I0123 13:04:52.054530 140086163390464 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.054591 140086163390464 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.055175 140086163390464 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.055255 140086163390464 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.055620 140086163390464 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.056318 140086163390464 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.058867 140086163390464 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.059489 140086163390464 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.059567 140086163390464 transformer_layer.py:468] tlayer: End windows.
I0123 13:04:52.059601 140086163390464 transformer_layer.py:472] tlayer: final FFN.
I0123 13:04:52.059665 140086163390464 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.059797 140086163390464 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:04:52.059911 140086163390464 nn_components.py:325] mlp: activation = None
I0123 13:04:52.059951 140086163390464 nn_components.py:329] mlp: final activation = None
I0123 13:04:52.061877 140086163390464 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.061973 140086163390464 nn_components.py:261] mlp: residual
I0123 13:04:52.064450 140086163390464 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.064529 140086163390464 transformer_base.py:443] tbase: final FFN
I0123 13:04:52.064640 140086163390464 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:04:52.066900 140086163390464 nn_components.py:329] mlp: final activation = None
I0123 13:04:52.068812 140086163390464 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.068907 140086163390464 nn_components.py:261] mlp: residual
I0123 13:04:52.069193 140086163390464 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.069274 140086163390464 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 13:04:52.069385 140086163390464 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:04:52.069426 140086163390464 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:04:52.069457 140086163390464 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:04:52.069519 140086163390464 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.071774 140086163390464 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:04:52.077221 140086163390464 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.077479 140086163390464 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:04:52.080201 140086163390464 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:04:52.092753 140086163390464 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:04:52.092808 140086163390464 attention.py:418] Single window, no scan.
I0123 13:04:52.092844 140086163390464 transformer_layer.py:389] tlayer: self-attention.
I0123 13:04:52.092875 140086163390464 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.092937 140086163390464 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.093492 140086163390464 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.093568 140086163390464 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.093926 140086163390464 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.094616 140086163390464 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.097082 140086163390464 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.097712 140086163390464 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.097791 140086163390464 transformer_layer.py:468] tlayer: End windows.
I0123 13:04:52.097826 140086163390464 transformer_layer.py:472] tlayer: final FFN.
I0123 13:04:52.097887 140086163390464 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.098026 140086163390464 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:04:52.098138 140086163390464 nn_components.py:325] mlp: activation = None
I0123 13:04:52.098178 140086163390464 nn_components.py:329] mlp: final activation = None
I0123 13:04:52.100147 140086163390464 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.100242 140086163390464 nn_components.py:261] mlp: residual
I0123 13:04:52.102677 140086163390464 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.102758 140086163390464 transformer_base.py:443] tbase: final FFN
I0123 13:04:52.102868 140086163390464 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:04:52.105105 140086163390464 nn_components.py:329] mlp: final activation = None
I0123 13:04:52.106993 140086163390464 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.107091 140086163390464 nn_components.py:261] mlp: residual
I0123 13:04:52.107382 140086163390464 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.107463 140086163390464 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 13:04:52.107573 140086163390464 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:04:52.107614 140086163390464 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:04:52.107645 140086163390464 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:04:52.107711 140086163390464 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.110333 140086163390464 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:04:52.115864 140086163390464 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.116132 140086163390464 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:04:52.118788 140086163390464 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:04:52.131538 140086163390464 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:04:52.131596 140086163390464 attention.py:418] Single window, no scan.
I0123 13:04:52.131633 140086163390464 transformer_layer.py:389] tlayer: self-attention.
I0123 13:04:52.131665 140086163390464 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.131729 140086163390464 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.132285 140086163390464 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.132368 140086163390464 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.132732 140086163390464 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.133433 140086163390464 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.136556 140086163390464 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.137188 140086163390464 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.137266 140086163390464 transformer_layer.py:468] tlayer: End windows.
I0123 13:04:52.137303 140086163390464 transformer_layer.py:472] tlayer: final FFN.
I0123 13:04:52.137363 140086163390464 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.137503 140086163390464 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:04:52.137619 140086163390464 nn_components.py:325] mlp: activation = None
I0123 13:04:52.137667 140086163390464 nn_components.py:329] mlp: final activation = None
I0123 13:04:52.139710 140086163390464 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.139806 140086163390464 nn_components.py:261] mlp: residual
I0123 13:04:52.142262 140086163390464 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.142343 140086163390464 transformer_base.py:443] tbase: final FFN
I0123 13:04:52.142453 140086163390464 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:04:52.144752 140086163390464 nn_components.py:329] mlp: final activation = None
I0123 13:04:52.146638 140086163390464 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.146735 140086163390464 nn_components.py:261] mlp: residual
I0123 13:04:52.147025 140086163390464 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.147107 140086163390464 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 13:04:52.147217 140086163390464 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:04:52.147256 140086163390464 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:04:52.147288 140086163390464 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:04:52.147354 140086163390464 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.149638 140086163390464 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:04:52.155147 140086163390464 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.155407 140086163390464 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:04:52.158125 140086163390464 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:04:52.170849 140086163390464 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:04:52.170906 140086163390464 attention.py:418] Single window, no scan.
I0123 13:04:52.170942 140086163390464 transformer_layer.py:389] tlayer: self-attention.
I0123 13:04:52.170973 140086163390464 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.171035 140086163390464 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.171595 140086163390464 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.171673 140086163390464 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.172043 140086163390464 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.172753 140086163390464 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.175262 140086163390464 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.175888 140086163390464 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.175966 140086163390464 transformer_layer.py:468] tlayer: End windows.
I0123 13:04:52.176002 140086163390464 transformer_layer.py:472] tlayer: final FFN.
I0123 13:04:52.176067 140086163390464 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.176201 140086163390464 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:04:52.176317 140086163390464 nn_components.py:325] mlp: activation = None
I0123 13:04:52.176356 140086163390464 nn_components.py:329] mlp: final activation = None
I0123 13:04:52.178323 140086163390464 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.178422 140086163390464 nn_components.py:261] mlp: residual
I0123 13:04:52.180855 140086163390464 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.180936 140086163390464 transformer_base.py:443] tbase: final FFN
I0123 13:04:52.181045 140086163390464 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:04:52.183300 140086163390464 nn_components.py:329] mlp: final activation = None
I0123 13:04:52.185175 140086163390464 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.185270 140086163390464 nn_components.py:261] mlp: residual
I0123 13:04:52.185561 140086163390464 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.185649 140086163390464 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 13:04:52.185762 140086163390464 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:04:52.185801 140086163390464 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:04:52.185833 140086163390464 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:04:52.185898 140086163390464 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.188155 140086163390464 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:04:52.193775 140086163390464 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.194039 140086163390464 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:04:52.196662 140086163390464 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:04:52.209429 140086163390464 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:04:52.209489 140086163390464 attention.py:418] Single window, no scan.
I0123 13:04:52.209526 140086163390464 transformer_layer.py:389] tlayer: self-attention.
I0123 13:04:52.209558 140086163390464 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.209621 140086163390464 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.210188 140086163390464 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.210266 140086163390464 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.210629 140086163390464 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.211321 140086163390464 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.213832 140086163390464 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.214834 140086163390464 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.214914 140086163390464 transformer_layer.py:468] tlayer: End windows.
I0123 13:04:52.214950 140086163390464 transformer_layer.py:472] tlayer: final FFN.
I0123 13:04:52.215011 140086163390464 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.215146 140086163390464 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:04:52.215257 140086163390464 nn_components.py:325] mlp: activation = None
I0123 13:04:52.215300 140086163390464 nn_components.py:329] mlp: final activation = None
I0123 13:04:52.217219 140086163390464 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.217317 140086163390464 nn_components.py:261] mlp: residual
I0123 13:04:52.219757 140086163390464 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.219839 140086163390464 transformer_base.py:443] tbase: final FFN
I0123 13:04:52.219948 140086163390464 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:04:52.222208 140086163390464 nn_components.py:329] mlp: final activation = None
I0123 13:04:52.224164 140086163390464 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.224262 140086163390464 nn_components.py:261] mlp: residual
I0123 13:04:52.224554 140086163390464 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.224637 140086163390464 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 13:04:52.224749 140086163390464 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:04:52.224789 140086163390464 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:04:52.224821 140086163390464 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:04:52.224885 140086163390464 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.227155 140086163390464 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:04:52.232682 140086163390464 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.232957 140086163390464 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:04:52.235668 140086163390464 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:04:52.248646 140086163390464 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:04:52.248703 140086163390464 attention.py:418] Single window, no scan.
I0123 13:04:52.248739 140086163390464 transformer_layer.py:389] tlayer: self-attention.
I0123 13:04:52.248771 140086163390464 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.248834 140086163390464 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.249453 140086163390464 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.249532 140086163390464 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.249907 140086163390464 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.250610 140086163390464 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.253164 140086163390464 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.253809 140086163390464 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.253890 140086163390464 transformer_layer.py:468] tlayer: End windows.
I0123 13:04:52.253925 140086163390464 transformer_layer.py:472] tlayer: final FFN.
I0123 13:04:52.253987 140086163390464 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.254122 140086163390464 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:04:52.254232 140086163390464 nn_components.py:325] mlp: activation = None
I0123 13:04:52.254276 140086163390464 nn_components.py:329] mlp: final activation = None
I0123 13:04:52.256192 140086163390464 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.256289 140086163390464 nn_components.py:261] mlp: residual
I0123 13:04:52.258801 140086163390464 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.258888 140086163390464 transformer_base.py:443] tbase: final FFN
I0123 13:04:52.258999 140086163390464 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:04:52.261241 140086163390464 nn_components.py:329] mlp: final activation = None
I0123 13:04:52.263123 140086163390464 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.263220 140086163390464 nn_components.py:261] mlp: residual
I0123 13:04:52.263510 140086163390464 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.263594 140086163390464 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 13:04:52.263705 140086163390464 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:04:52.263744 140086163390464 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:04:52.263777 140086163390464 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:04:52.263841 140086163390464 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.266137 140086163390464 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:04:52.271724 140086163390464 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.271988 140086163390464 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:04:52.274645 140086163390464 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:04:52.287378 140086163390464 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:04:52.287436 140086163390464 attention.py:418] Single window, no scan.
I0123 13:04:52.287473 140086163390464 transformer_layer.py:389] tlayer: self-attention.
I0123 13:04:52.287505 140086163390464 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.287568 140086163390464 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.288140 140086163390464 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.288217 140086163390464 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.288572 140086163390464 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.289257 140086163390464 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.291782 140086163390464 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.292456 140086163390464 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.292535 140086163390464 transformer_layer.py:468] tlayer: End windows.
I0123 13:04:52.292571 140086163390464 transformer_layer.py:472] tlayer: final FFN.
I0123 13:04:52.292633 140086163390464 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.292759 140086163390464 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:04:52.292869 140086163390464 nn_components.py:325] mlp: activation = None
I0123 13:04:52.292908 140086163390464 nn_components.py:329] mlp: final activation = None
I0123 13:04:52.294798 140086163390464 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.294894 140086163390464 nn_components.py:261] mlp: residual
I0123 13:04:52.297319 140086163390464 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.297399 140086163390464 transformer_base.py:443] tbase: final FFN
I0123 13:04:52.297508 140086163390464 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:04:52.299760 140086163390464 nn_components.py:329] mlp: final activation = None
I0123 13:04:52.301711 140086163390464 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.301809 140086163390464 nn_components.py:261] mlp: residual
I0123 13:04:52.302103 140086163390464 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.302185 140086163390464 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 13:04:52.302296 140086163390464 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:04:52.302335 140086163390464 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:04:52.302367 140086163390464 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:04:52.302432 140086163390464 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.304713 140086163390464 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:04:52.310239 140086163390464 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.310500 140086163390464 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:04:52.313202 140086163390464 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:04:52.326072 140086163390464 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:04:52.326128 140086163390464 attention.py:418] Single window, no scan.
I0123 13:04:52.326168 140086163390464 transformer_layer.py:389] tlayer: self-attention.
I0123 13:04:52.326200 140086163390464 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.326264 140086163390464 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.326877 140086163390464 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.326956 140086163390464 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.327311 140086163390464 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.327997 140086163390464 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.330449 140086163390464 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.331070 140086163390464 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.331148 140086163390464 transformer_layer.py:468] tlayer: End windows.
I0123 13:04:52.331183 140086163390464 transformer_layer.py:472] tlayer: final FFN.
I0123 13:04:52.331241 140086163390464 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.331372 140086163390464 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:04:52.331481 140086163390464 nn_components.py:325] mlp: activation = None
I0123 13:04:52.331519 140086163390464 nn_components.py:329] mlp: final activation = None
I0123 13:04:52.333381 140086163390464 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.333482 140086163390464 nn_components.py:261] mlp: residual
I0123 13:04:52.335962 140086163390464 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.336047 140086163390464 transformer_base.py:443] tbase: final FFN
I0123 13:04:52.336156 140086163390464 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:04:52.338373 140086163390464 nn_components.py:329] mlp: final activation = None
I0123 13:04:52.340199 140086163390464 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.340294 140086163390464 nn_components.py:261] mlp: residual
I0123 13:04:52.340580 140086163390464 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.340662 140086163390464 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 13:04:52.340770 140086163390464 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:04:52.340809 140086163390464 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:04:52.340840 140086163390464 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:04:52.340902 140086163390464 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.343281 140086163390464 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:04:52.348820 140086163390464 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.349078 140086163390464 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:04:52.351700 140086163390464 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:04:52.364152 140086163390464 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:04:52.364209 140086163390464 attention.py:418] Single window, no scan.
I0123 13:04:52.364244 140086163390464 transformer_layer.py:389] tlayer: self-attention.
I0123 13:04:52.364275 140086163390464 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.364338 140086163390464 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.364892 140086163390464 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.364968 140086163390464 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.365324 140086163390464 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.366021 140086163390464 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.368481 140086163390464 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.369142 140086163390464 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.369220 140086163390464 transformer_layer.py:468] tlayer: End windows.
I0123 13:04:52.369254 140086163390464 transformer_layer.py:472] tlayer: final FFN.
I0123 13:04:52.369312 140086163390464 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.369438 140086163390464 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:04:52.369549 140086163390464 nn_components.py:325] mlp: activation = None
I0123 13:04:52.369589 140086163390464 nn_components.py:329] mlp: final activation = None
I0123 13:04:52.371466 140086163390464 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.371567 140086163390464 nn_components.py:261] mlp: residual
I0123 13:04:52.374004 140086163390464 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.374083 140086163390464 transformer_base.py:443] tbase: final FFN
I0123 13:04:52.374191 140086163390464 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:04:52.376374 140086163390464 nn_components.py:329] mlp: final activation = None
I0123 13:04:52.378289 140086163390464 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.378386 140086163390464 nn_components.py:261] mlp: residual
I0123 13:04:52.378675 140086163390464 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.378759 140086163390464 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 13:04:52.378867 140086163390464 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:04:52.378906 140086163390464 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:04:52.378936 140086163390464 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:04:52.378998 140086163390464 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.381221 140086163390464 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:04:52.386658 140086163390464 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.386919 140086163390464 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:04:52.389608 140086163390464 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:04:52.402067 140086163390464 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:04:52.402122 140086163390464 attention.py:418] Single window, no scan.
I0123 13:04:52.402158 140086163390464 transformer_layer.py:389] tlayer: self-attention.
I0123 13:04:52.402189 140086163390464 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.402251 140086163390464 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.402808 140086163390464 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.402885 140086163390464 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.403239 140086163390464 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.403966 140086163390464 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.406443 140086163390464 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.407060 140086163390464 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.407137 140086163390464 transformer_layer.py:468] tlayer: End windows.
I0123 13:04:52.407171 140086163390464 transformer_layer.py:472] tlayer: final FFN.
I0123 13:04:52.407229 140086163390464 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.407359 140086163390464 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:04:52.407467 140086163390464 nn_components.py:325] mlp: activation = None
I0123 13:04:52.407505 140086163390464 nn_components.py:329] mlp: final activation = None
I0123 13:04:52.409371 140086163390464 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.409465 140086163390464 nn_components.py:261] mlp: residual
I0123 13:04:52.411864 140086163390464 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.411944 140086163390464 transformer_base.py:443] tbase: final FFN
I0123 13:04:52.412050 140086163390464 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:04:52.414311 140086163390464 nn_components.py:329] mlp: final activation = None
I0123 13:04:52.416161 140086163390464 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.416257 140086163390464 nn_components.py:261] mlp: residual
I0123 13:04:52.416542 140086163390464 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.416632 140086163390464 decoder_stack.py:344] dstack: Final layernorm.
I0123 13:04:52.419513 140086163390464 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:04:52.474236 140086163390464 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.474323 140086163390464 decoder_stack.py:333] dstack: autoregressive generator.
I0123 13:04:52.474378 140086163390464 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 13:04:52.474483 140086163390464 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:04:52.474524 140086163390464 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:04:52.474555 140086163390464 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:04:52.474620 140086163390464 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.477299 140086163390464 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:04:52.482717 140086163390464 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.482983 140086163390464 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:04:52.485569 140086163390464 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:04:52.497942 140086163390464 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:04:52.497999 140086163390464 attention.py:418] Single window, no scan.
I0123 13:04:52.498036 140086163390464 transformer_layer.py:389] tlayer: self-attention.
I0123 13:04:52.498068 140086163390464 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.498131 140086163390464 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.498691 140086163390464 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.498768 140086163390464 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.499134 140086163390464 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.499822 140086163390464 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.502407 140086163390464 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.503029 140086163390464 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.503108 140086163390464 transformer_layer.py:468] tlayer: End windows.
I0123 13:04:52.503145 140086163390464 transformer_layer.py:472] tlayer: final FFN.
I0123 13:04:52.503205 140086163390464 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.503337 140086163390464 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:04:52.503454 140086163390464 nn_components.py:325] mlp: activation = None
I0123 13:04:52.503493 140086163390464 nn_components.py:329] mlp: final activation = None
I0123 13:04:52.505345 140086163390464 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.505442 140086163390464 nn_components.py:261] mlp: residual
I0123 13:04:52.507855 140086163390464 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.507937 140086163390464 transformer_base.py:443] tbase: final FFN
I0123 13:04:52.508047 140086163390464 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:04:52.510315 140086163390464 nn_components.py:329] mlp: final activation = None
I0123 13:04:52.512171 140086163390464 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.512268 140086163390464 nn_components.py:261] mlp: residual
I0123 13:04:52.512559 140086163390464 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.512643 140086163390464 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 13:04:52.512753 140086163390464 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:04:52.512792 140086163390464 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:04:52.512825 140086163390464 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:04:52.512890 140086163390464 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.515166 140086163390464 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:04:52.520581 140086163390464 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.520842 140086163390464 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:04:52.523508 140086163390464 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:04:52.535771 140086163390464 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:04:52.535828 140086163390464 attention.py:418] Single window, no scan.
I0123 13:04:52.535864 140086163390464 transformer_layer.py:389] tlayer: self-attention.
I0123 13:04:52.535895 140086163390464 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.535959 140086163390464 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.536517 140086163390464 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.536595 140086163390464 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.536953 140086163390464 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.537632 140086163390464 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.540169 140086163390464 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.540791 140086163390464 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.540870 140086163390464 transformer_layer.py:468] tlayer: End windows.
I0123 13:04:52.540907 140086163390464 transformer_layer.py:472] tlayer: final FFN.
I0123 13:04:52.540970 140086163390464 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.541100 140086163390464 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:04:52.541208 140086163390464 nn_components.py:325] mlp: activation = None
I0123 13:04:52.541255 140086163390464 nn_components.py:329] mlp: final activation = None
I0123 13:04:52.543125 140086163390464 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.543222 140086163390464 nn_components.py:261] mlp: residual
I0123 13:04:52.545635 140086163390464 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.545723 140086163390464 transformer_base.py:443] tbase: final FFN
I0123 13:04:52.545835 140086163390464 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:04:52.548112 140086163390464 nn_components.py:329] mlp: final activation = None
I0123 13:04:52.549977 140086163390464 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.550075 140086163390464 nn_components.py:261] mlp: residual
I0123 13:04:52.550367 140086163390464 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.550451 140086163390464 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 13:04:52.550560 140086163390464 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:04:52.550601 140086163390464 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:04:52.550633 140086163390464 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:04:52.550699 140086163390464 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.553101 140086163390464 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:04:52.558544 140086163390464 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.558805 140086163390464 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:04:52.561471 140086163390464 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:04:52.573879 140086163390464 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:04:52.573936 140086163390464 attention.py:418] Single window, no scan.
I0123 13:04:52.573972 140086163390464 transformer_layer.py:389] tlayer: self-attention.
I0123 13:04:52.574005 140086163390464 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.574069 140086163390464 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.574624 140086163390464 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.574703 140086163390464 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.575061 140086163390464 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.575739 140086163390464 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.578259 140086163390464 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.578874 140086163390464 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.578953 140086163390464 transformer_layer.py:468] tlayer: End windows.
I0123 13:04:52.578989 140086163390464 transformer_layer.py:472] tlayer: final FFN.
I0123 13:04:52.579051 140086163390464 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.579179 140086163390464 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:04:52.579289 140086163390464 nn_components.py:325] mlp: activation = None
I0123 13:04:52.579330 140086163390464 nn_components.py:329] mlp: final activation = None
I0123 13:04:52.581189 140086163390464 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.581284 140086163390464 nn_components.py:261] mlp: residual
I0123 13:04:52.583693 140086163390464 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.583774 140086163390464 transformer_base.py:443] tbase: final FFN
I0123 13:04:52.583886 140086163390464 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:04:52.586599 140086163390464 nn_components.py:329] mlp: final activation = None
I0123 13:04:52.588458 140086163390464 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.588556 140086163390464 nn_components.py:261] mlp: residual
I0123 13:04:52.588849 140086163390464 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.588933 140086163390464 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 13:04:52.589042 140086163390464 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:04:52.589082 140086163390464 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:04:52.589114 140086163390464 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:04:52.589180 140086163390464 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.591429 140086163390464 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:04:52.596828 140086163390464 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.597090 140086163390464 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:04:52.599779 140086163390464 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:04:52.612107 140086163390464 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:04:52.612165 140086163390464 attention.py:418] Single window, no scan.
I0123 13:04:52.612206 140086163390464 transformer_layer.py:389] tlayer: self-attention.
I0123 13:04:52.612246 140086163390464 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.612312 140086163390464 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.612884 140086163390464 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.612960 140086163390464 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.613316 140086163390464 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.614018 140086163390464 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.616889 140086163390464 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.617515 140086163390464 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.617592 140086163390464 transformer_layer.py:468] tlayer: End windows.
I0123 13:04:52.617627 140086163390464 transformer_layer.py:472] tlayer: final FFN.
I0123 13:04:52.617696 140086163390464 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.617827 140086163390464 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:04:52.617934 140086163390464 nn_components.py:325] mlp: activation = None
I0123 13:04:52.617973 140086163390464 nn_components.py:329] mlp: final activation = None
I0123 13:04:52.619854 140086163390464 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.619948 140086163390464 nn_components.py:261] mlp: residual
I0123 13:04:52.622336 140086163390464 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.622415 140086163390464 transformer_base.py:443] tbase: final FFN
I0123 13:04:52.622521 140086163390464 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:04:52.624834 140086163390464 nn_components.py:329] mlp: final activation = None
I0123 13:04:52.626732 140086163390464 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.626831 140086163390464 nn_components.py:261] mlp: residual
I0123 13:04:52.627128 140086163390464 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.627211 140086163390464 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 13:04:52.627321 140086163390464 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:04:52.627360 140086163390464 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:04:52.627392 140086163390464 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:04:52.627457 140086163390464 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.629701 140086163390464 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:04:52.635101 140086163390464 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.635357 140086163390464 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:04:52.638036 140086163390464 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:04:52.650520 140086163390464 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:04:52.650576 140086163390464 attention.py:418] Single window, no scan.
I0123 13:04:52.650610 140086163390464 transformer_layer.py:389] tlayer: self-attention.
I0123 13:04:52.650641 140086163390464 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.650703 140086163390464 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.651267 140086163390464 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.651343 140086163390464 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.651706 140086163390464 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.652390 140086163390464 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.654942 140086163390464 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.655565 140086163390464 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.655641 140086163390464 transformer_layer.py:468] tlayer: End windows.
I0123 13:04:52.655675 140086163390464 transformer_layer.py:472] tlayer: final FFN.
I0123 13:04:52.655734 140086163390464 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.655859 140086163390464 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:04:52.655972 140086163390464 nn_components.py:325] mlp: activation = None
I0123 13:04:52.656010 140086163390464 nn_components.py:329] mlp: final activation = None
I0123 13:04:52.657892 140086163390464 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.657991 140086163390464 nn_components.py:261] mlp: residual
I0123 13:04:52.660416 140086163390464 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.660494 140086163390464 transformer_base.py:443] tbase: final FFN
I0123 13:04:52.660601 140086163390464 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:04:52.662897 140086163390464 nn_components.py:329] mlp: final activation = None
I0123 13:04:52.664772 140086163390464 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.664866 140086163390464 nn_components.py:261] mlp: residual
I0123 13:04:52.665151 140086163390464 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.665230 140086163390464 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 13:04:52.665337 140086163390464 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:04:52.665374 140086163390464 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:04:52.665405 140086163390464 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:04:52.665468 140086163390464 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.667725 140086163390464 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:04:52.673156 140086163390464 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.673418 140086163390464 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:04:52.676138 140086163390464 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:04:52.688677 140086163390464 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:04:52.688732 140086163390464 attention.py:418] Single window, no scan.
I0123 13:04:52.688767 140086163390464 transformer_layer.py:389] tlayer: self-attention.
I0123 13:04:52.688798 140086163390464 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.688859 140086163390464 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.689422 140086163390464 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.689497 140086163390464 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.689860 140086163390464 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.690553 140086163390464 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.693105 140086163390464 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.693739 140086163390464 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.693816 140086163390464 transformer_layer.py:468] tlayer: End windows.
I0123 13:04:52.693851 140086163390464 transformer_layer.py:472] tlayer: final FFN.
I0123 13:04:52.693910 140086163390464 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.694037 140086163390464 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:04:52.694144 140086163390464 nn_components.py:325] mlp: activation = None
I0123 13:04:52.694181 140086163390464 nn_components.py:329] mlp: final activation = None
I0123 13:04:52.696056 140086163390464 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.696155 140086163390464 nn_components.py:261] mlp: residual
I0123 13:04:52.698574 140086163390464 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.698653 140086163390464 transformer_base.py:443] tbase: final FFN
I0123 13:04:52.698760 140086163390464 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:04:52.701454 140086163390464 nn_components.py:329] mlp: final activation = None
I0123 13:04:52.703335 140086163390464 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.703431 140086163390464 nn_components.py:261] mlp: residual
I0123 13:04:52.703721 140086163390464 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.703802 140086163390464 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 13:04:52.703909 140086163390464 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:04:52.703947 140086163390464 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:04:52.703978 140086163390464 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:04:52.704041 140086163390464 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.706295 140086163390464 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:04:52.711721 140086163390464 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.711982 140086163390464 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:04:52.714694 140086163390464 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:04:52.727672 140086163390464 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:04:52.727727 140086163390464 attention.py:418] Single window, no scan.
I0123 13:04:52.727762 140086163390464 transformer_layer.py:389] tlayer: self-attention.
I0123 13:04:52.727793 140086163390464 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.727855 140086163390464 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.728417 140086163390464 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.728492 140086163390464 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.728857 140086163390464 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.729547 140086163390464 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.732115 140086163390464 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.732747 140086163390464 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.732825 140086163390464 transformer_layer.py:468] tlayer: End windows.
I0123 13:04:52.732859 140086163390464 transformer_layer.py:472] tlayer: final FFN.
I0123 13:04:52.732918 140086163390464 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.733045 140086163390464 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:04:52.733151 140086163390464 nn_components.py:325] mlp: activation = None
I0123 13:04:52.733188 140086163390464 nn_components.py:329] mlp: final activation = None
I0123 13:04:52.735079 140086163390464 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.735174 140086163390464 nn_components.py:261] mlp: residual
I0123 13:04:52.737598 140086163390464 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.737689 140086163390464 transformer_base.py:443] tbase: final FFN
I0123 13:04:52.737799 140086163390464 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:04:52.740094 140086163390464 nn_components.py:329] mlp: final activation = None
I0123 13:04:52.741963 140086163390464 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.742058 140086163390464 nn_components.py:261] mlp: residual
I0123 13:04:52.742347 140086163390464 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.742427 140086163390464 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 13:04:52.742534 140086163390464 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:04:52.742572 140086163390464 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:04:52.742602 140086163390464 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:04:52.742665 140086163390464 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.744914 140086163390464 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:04:52.750373 140086163390464 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.750631 140086163390464 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:04:52.753327 140086163390464 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:04:52.765874 140086163390464 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:04:52.765928 140086163390464 attention.py:418] Single window, no scan.
I0123 13:04:52.765963 140086163390464 transformer_layer.py:389] tlayer: self-attention.
I0123 13:04:52.765994 140086163390464 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.766056 140086163390464 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.766611 140086163390464 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.766687 140086163390464 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.767050 140086163390464 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.767735 140086163390464 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.770301 140086163390464 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.770927 140086163390464 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.771005 140086163390464 transformer_layer.py:468] tlayer: End windows.
I0123 13:04:52.771040 140086163390464 transformer_layer.py:472] tlayer: final FFN.
I0123 13:04:52.771101 140086163390464 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.771227 140086163390464 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:04:52.771334 140086163390464 nn_components.py:325] mlp: activation = None
I0123 13:04:52.771372 140086163390464 nn_components.py:329] mlp: final activation = None
I0123 13:04:52.773257 140086163390464 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.773351 140086163390464 nn_components.py:261] mlp: residual
I0123 13:04:52.775771 140086163390464 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.775856 140086163390464 transformer_base.py:443] tbase: final FFN
I0123 13:04:52.775966 140086163390464 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:04:52.778318 140086163390464 nn_components.py:329] mlp: final activation = None
I0123 13:04:52.780198 140086163390464 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.780292 140086163390464 nn_components.py:261] mlp: residual
I0123 13:04:52.780581 140086163390464 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.780662 140086163390464 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 13:04:52.780771 140086163390464 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:04:52.780809 140086163390464 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:04:52.780839 140086163390464 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:04:52.780902 140086163390464 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.783182 140086163390464 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:04:52.788614 140086163390464 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.788876 140086163390464 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:04:52.791602 140086163390464 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:04:52.804128 140086163390464 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:04:52.804182 140086163390464 attention.py:418] Single window, no scan.
I0123 13:04:52.804216 140086163390464 transformer_layer.py:389] tlayer: self-attention.
I0123 13:04:52.804247 140086163390464 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.804309 140086163390464 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.804870 140086163390464 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.804948 140086163390464 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.805306 140086163390464 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.805994 140086163390464 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.808550 140086163390464 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.809177 140086163390464 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.809254 140086163390464 transformer_layer.py:468] tlayer: End windows.
I0123 13:04:52.809288 140086163390464 transformer_layer.py:472] tlayer: final FFN.
I0123 13:04:52.809347 140086163390464 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.809473 140086163390464 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:04:52.809581 140086163390464 nn_components.py:325] mlp: activation = None
I0123 13:04:52.809620 140086163390464 nn_components.py:329] mlp: final activation = None
I0123 13:04:52.811524 140086163390464 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.811619 140086163390464 nn_components.py:261] mlp: residual
I0123 13:04:52.814049 140086163390464 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.814134 140086163390464 transformer_base.py:443] tbase: final FFN
I0123 13:04:52.814248 140086163390464 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:04:52.816921 140086163390464 nn_components.py:329] mlp: final activation = None
I0123 13:04:52.818829 140086163390464 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.818924 140086163390464 nn_components.py:261] mlp: residual
I0123 13:04:52.819211 140086163390464 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.819292 140086163390464 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 13:04:52.819400 140086163390464 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:04:52.819437 140086163390464 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:04:52.819468 140086163390464 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:04:52.819532 140086163390464 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.821819 140086163390464 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:04:52.827460 140086163390464 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.827724 140086163390464 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:04:52.830605 140086163390464 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:04:52.843116 140086163390464 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:04:52.843172 140086163390464 attention.py:418] Single window, no scan.
I0123 13:04:52.843207 140086163390464 transformer_layer.py:389] tlayer: self-attention.
I0123 13:04:52.843237 140086163390464 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.843299 140086163390464 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.843864 140086163390464 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.844095 140086163390464 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.844449 140086163390464 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.845196 140086163390464 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.847782 140086163390464 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.848413 140086163390464 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.848489 140086163390464 transformer_layer.py:468] tlayer: End windows.
I0123 13:04:52.848524 140086163390464 transformer_layer.py:472] tlayer: final FFN.
I0123 13:04:52.848582 140086163390464 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.848709 140086163390464 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:04:52.848816 140086163390464 nn_components.py:325] mlp: activation = None
I0123 13:04:52.848854 140086163390464 nn_components.py:329] mlp: final activation = None
I0123 13:04:52.851188 140086163390464 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.851283 140086163390464 nn_components.py:261] mlp: residual
I0123 13:04:52.853709 140086163390464 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.853787 140086163390464 transformer_base.py:443] tbase: final FFN
I0123 13:04:52.853901 140086163390464 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:04:52.856161 140086163390464 nn_components.py:329] mlp: final activation = None
I0123 13:04:52.858024 140086163390464 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.858119 140086163390464 nn_components.py:261] mlp: residual
I0123 13:04:52.858407 140086163390464 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.858489 140086163390464 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 13:04:52.858597 140086163390464 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:04:52.858635 140086163390464 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:04:52.858666 140086163390464 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:04:52.858729 140086163390464 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.860991 140086163390464 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:04:52.866456 140086163390464 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.866715 140086163390464 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:04:52.869416 140086163390464 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:04:52.881976 140086163390464 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:04:52.882031 140086163390464 attention.py:418] Single window, no scan.
I0123 13:04:52.882067 140086163390464 transformer_layer.py:389] tlayer: self-attention.
I0123 13:04:52.882097 140086163390464 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.882159 140086163390464 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.882723 140086163390464 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.882799 140086163390464 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.883158 140086163390464 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.883853 140086163390464 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.886476 140086163390464 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.887108 140086163390464 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.887187 140086163390464 transformer_layer.py:468] tlayer: End windows.
I0123 13:04:52.887221 140086163390464 transformer_layer.py:472] tlayer: final FFN.
I0123 13:04:52.887281 140086163390464 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.887408 140086163390464 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:04:52.887516 140086163390464 nn_components.py:325] mlp: activation = None
I0123 13:04:52.887554 140086163390464 nn_components.py:329] mlp: final activation = None
I0123 13:04:52.889431 140086163390464 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.889524 140086163390464 nn_components.py:261] mlp: residual
I0123 13:04:52.891947 140086163390464 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.892026 140086163390464 transformer_base.py:443] tbase: final FFN
I0123 13:04:52.892133 140086163390464 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:04:52.894448 140086163390464 nn_components.py:329] mlp: final activation = None
I0123 13:04:52.896328 140086163390464 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.896422 140086163390464 nn_components.py:261] mlp: residual
I0123 13:04:52.896711 140086163390464 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.896792 140086163390464 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 13:04:52.896900 140086163390464 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:04:52.896938 140086163390464 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:04:52.896968 140086163390464 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:04:52.897031 140086163390464 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.899307 140086163390464 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:04:52.904758 140086163390464 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.905014 140086163390464 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:04:52.907751 140086163390464 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:04:52.920309 140086163390464 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:04:52.920363 140086163390464 attention.py:418] Single window, no scan.
I0123 13:04:52.920399 140086163390464 transformer_layer.py:389] tlayer: self-attention.
I0123 13:04:52.920430 140086163390464 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.920496 140086163390464 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.921052 140086163390464 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.921128 140086163390464 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.921490 140086163390464 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.922196 140086163390464 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.924732 140086163390464 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.925364 140086163390464 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.925441 140086163390464 transformer_layer.py:468] tlayer: End windows.
I0123 13:04:52.925475 140086163390464 transformer_layer.py:472] tlayer: final FFN.
I0123 13:04:52.925534 140086163390464 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.925670 140086163390464 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:04:52.925780 140086163390464 nn_components.py:325] mlp: activation = None
I0123 13:04:52.925818 140086163390464 nn_components.py:329] mlp: final activation = None
I0123 13:04:52.927695 140086163390464 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.927788 140086163390464 nn_components.py:261] mlp: residual
I0123 13:04:52.930219 140086163390464 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.930299 140086163390464 transformer_base.py:443] tbase: final FFN
I0123 13:04:52.930406 140086163390464 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:04:52.933081 140086163390464 nn_components.py:329] mlp: final activation = None
I0123 13:04:52.934978 140086163390464 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.935074 140086163390464 nn_components.py:261] mlp: residual
I0123 13:04:52.935364 140086163390464 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:52.935449 140086163390464 decoder_stack.py:344] dstack: Final layernorm.
I0123 13:04:52.938331 140086163390464 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:04:57.356726 140086163390464 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
I0123 13:04:57.884098 140086163390464 training_loop.py:409] No working directory specified.
I0123 13:04:57.884211 140086163390464 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0123 13:04:57.884972 140086163390464 checkpoints.py:1062] Restoring legacy Flax checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0123 13:05:00.797903 140086163390464 training_loop.py:447] Only restoring trainable parameters.
I0123 13:05:00.798599 140086163390464 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0123 13:05:00.798658 140086163390464 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0123 13:05:00.798704 140086163390464 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 13:05:00.798745 140086163390464 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 13:05:00.798786 140086163390464 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:00.798825 140086163390464 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 13:05:00.798864 140086163390464 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:00.798902 140086163390464 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:00.798939 140086163390464 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 13:05:00.798977 140086163390464 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 13:05:00.799015 140086163390464 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:00.799053 140086163390464 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 13:05:00.799093 140086163390464 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 13:05:00.799132 140086163390464 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 13:05:00.799170 140086163390464 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:00.799207 140086163390464 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 13:05:00.799245 140086163390464 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:00.799282 140086163390464 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:00.799319 140086163390464 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 13:05:00.799356 140086163390464 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 13:05:00.799406 140086163390464 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:00.799446 140086163390464 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 13:05:00.799482 140086163390464 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 13:05:00.799519 140086163390464 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 13:05:00.799556 140086163390464 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:00.799592 140086163390464 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 13:05:00.799629 140086163390464 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:00.799666 140086163390464 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:00.799702 140086163390464 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 13:05:00.799737 140086163390464 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 13:05:00.799774 140086163390464 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:00.799810 140086163390464 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 13:05:00.799847 140086163390464 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 13:05:00.799884 140086163390464 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 13:05:00.799920 140086163390464 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:00.799955 140086163390464 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 13:05:00.799991 140086163390464 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:00.800027 140086163390464 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:00.800063 140086163390464 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 13:05:00.800099 140086163390464 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 13:05:00.800135 140086163390464 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:00.800171 140086163390464 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 13:05:00.800207 140086163390464 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 13:05:00.800242 140086163390464 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 13:05:00.800277 140086163390464 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:00.800312 140086163390464 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 13:05:00.800354 140086163390464 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:00.800391 140086163390464 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:00.800427 140086163390464 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 13:05:00.800463 140086163390464 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 13:05:00.800499 140086163390464 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:00.800534 140086163390464 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 13:05:00.800570 140086163390464 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 13:05:00.800605 140086163390464 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 13:05:00.800641 140086163390464 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:00.800676 140086163390464 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 13:05:00.800712 140086163390464 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:00.800748 140086163390464 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:00.800784 140086163390464 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 13:05:00.800819 140086163390464 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 13:05:00.800855 140086163390464 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:00.800891 140086163390464 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 13:05:00.800927 140086163390464 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 13:05:00.800963 140086163390464 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 13:05:00.800999 140086163390464 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:00.801034 140086163390464 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 13:05:00.801070 140086163390464 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:00.801105 140086163390464 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:00.801141 140086163390464 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 13:05:00.801176 140086163390464 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 13:05:00.801368 140086163390464 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:00.801403 140086163390464 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 13:05:00.801439 140086163390464 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 13:05:00.801480 140086163390464 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 13:05:00.801517 140086163390464 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:00.801553 140086163390464 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 13:05:00.801589 140086163390464 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:00.801625 140086163390464 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:00.801680 140086163390464 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 13:05:00.801718 140086163390464 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 13:05:00.801756 140086163390464 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:00.801792 140086163390464 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 13:05:00.801829 140086163390464 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 13:05:00.801865 140086163390464 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 13:05:00.801900 140086163390464 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:00.801936 140086163390464 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 13:05:00.801972 140086163390464 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:00.802009 140086163390464 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:00.802044 140086163390464 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 13:05:00.802079 140086163390464 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 13:05:00.802116 140086163390464 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:00.802151 140086163390464 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 13:05:00.802187 140086163390464 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 13:05:00.802222 140086163390464 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 13:05:00.802258 140086163390464 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:00.802293 140086163390464 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 13:05:00.802328 140086163390464 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:00.802364 140086163390464 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:00.802400 140086163390464 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 13:05:00.802436 140086163390464 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 13:05:00.802477 140086163390464 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:00.802515 140086163390464 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 13:05:00.802552 140086163390464 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 13:05:00.802588 140086163390464 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 13:05:00.802623 140086163390464 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:00.802660 140086163390464 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 13:05:00.802695 140086163390464 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:00.802731 140086163390464 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:00.802766 140086163390464 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 13:05:00.802802 140086163390464 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 13:05:00.802838 140086163390464 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:00.802875 140086163390464 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 13:05:00.802911 140086163390464 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 13:05:00.802946 140086163390464 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 13:05:00.802982 140086163390464 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:00.803018 140086163390464 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 13:05:00.803054 140086163390464 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:00.803090 140086163390464 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:00.803127 140086163390464 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 13:05:00.803164 140086163390464 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 13:05:00.803199 140086163390464 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:00.803235 140086163390464 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 13:05:00.803263 140086163390464 training_loop.py:725] Total parameters: 152072288
I0123 13:05:00.803492 140086163390464 training_loop.py:739] Total state size: 0
I0123 13:05:00.824293 140086163390464 training_loop.py:492] Training loop: creating task for mode beam_search
I0123 13:05:00.824545 140086163390464 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0123 13:05:00.825068 140086163390464 training_loop.py:652] Compiling mode beam_search with jit.
I0123 13:05:00.825379 140086163390464 training_loop.py:89] registering functions: dict_keys([])
I0123 13:05:00.840826 140086163390464 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e c b a; f = on_line f b e; g = on_circle g c f, on_line g b f; h = on_circle h d c, on_circle h c f; i = on_circle i d c, on_circle i c f; j = circle j b f i; k = on_circle k j b, on_line k c b; l = circle l a h g; m = on_circle m l a, on_line m c a; n = on_line n i k, on_line n h m ? coll c d n
I0123 13:05:04.535803 140086163390464 ddar.py:60] Depth 1/1000 time = 3.6076502799987793
I0123 13:05:11.977291 140086163390464 ddar.py:60] Depth 2/1000 time = 7.441311836242676
I0123 13:05:22.034860 140086163390464 ddar.py:60] Depth 3/1000 time = 10.057363033294678
I0123 13:05:32.788921 140086163390464 ddar.py:60] Depth 4/1000 time = 10.753824710845947
I0123 13:05:32.798176 140086163390464 alphageometry.py:191] 
==========================
 * From theorem premises:
A B C D E F G H I J K L M N : Points
DC = DB [00]
DB = DA [01]
A,E,B are collinear [02]
B,E,F are collinear [03]
F,G,B are collinear [04]
CG = CF [05]
CH = CF [06]
DH = DC [07]
CI = CF [08]
DI = DC [09]
JB = JF [10]
JF = JI [11]
K,B,C are collinear [12]
JK = JB [13]
LH = LG [14]
LA = LH [15]
LM = LA [16]
A,M,C are collinear [17]
H,M,N are collinear [18]
N,I,K are collinear [19]

 * Auxiliary Constructions:
: Points


 * Proof steps:
001. CH = CF [06] & CI = CF [08]   CI = CH [20]
002. JB = JF [10] & JK = JB [13] & JF = JI [11]   F,I,K,B are concyclic [21]
003. F,I,K,B are concyclic [21]   FIK = FBK [22]
004. F,G,B are collinear [04] & B,E,F are collinear [03] & A,E,B are collinear [02] & K,B,C are collinear [12] & N,I,K are collinear [19] & FIK = FBK [22]   (GF-KB) = FIN [23]
005. CG = CF [05] & CI = CF [08] & CH = CF [06]   H,G,I,F are concyclic [24]
006. H,G,I,F are concyclic [24]   HGF = HIF [25]
007. (GF-KB) = FIN [23] & HGF = HIF [25]   NIH = (KB-HG) [26]
008. LH = LG [14] & LA = LH [15] & LM = LA [16]   H,A,M,G are concyclic [27]
009. H,A,M,G are concyclic [27]   HAM = HGM [28]
010. H,A,M,G are concyclic [27]   HAG = HMG [29]
011. DH = DC [07] & DI = DC [09] & DC = DB [00]   H,I,B,C are concyclic [30]
012. H,I,B,C are concyclic [30] & DB = DA [01] & DC = DB [00] & DH = DC [07]   H,I,B,A are concyclic [31]
013. H,I,B,C are concyclic [30] & H,I,B,A are concyclic [31]   C,H,I,A are concyclic [32]
014. C,H,I,A are concyclic [32]   CIH = CAH [33]
015. CI = CH [20]   CIH = IHC [34]
016. C,H,I,B are concyclic [30]   IHC = IBC [35]
017. K,B,C are collinear [12] & HAM = HGM [28] & A,M,C are collinear [17] & CIH = CAH [33] & CIH = IHC [34] & IHC = IBC [35]   IBK = MGH [36]
018. H,I,A,B are concyclic [31]   BIH = BAH [37]
019. HAG = HMG [29] & F,G,B are collinear [04] & B,E,F are collinear [03] & A,E,B are collinear [02] & BIH = BAH [37]   BIH = GMH [38]
020. IBK = MGH [36] & BIH = GMH [38]   (KB-HI) = GHM [39]
021. H,N,M are collinear [18] & N,I,K are collinear [19] & NIH = (KB-HG) [26] & K,B,C are collinear [12] & (KB-HI) = GHM [39]   NHI = HIN [40]
022. NHI = HIN [40]   NH = NI [41]
023. CI = CH [20] & NH = NI [41]   IH  CN [42]
024. DH = DC [07] & DI = DC [09]   DI = DH [43]
025. DI = DH [43] & CI = CH [20]   HI  CD [44]
026. IH  CN [42] & HI  CD [44]   N,D,C are collinear
==========================

