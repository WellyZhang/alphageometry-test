I0123 13:05:36.827971 140114565189632 inference_utils.py:69] Parsing gin configuration.
I0123 13:05:36.828070 140114565189632 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0123 13:05:36.828279 140114565189632 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0123 13:05:36.828315 140114565189632 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0123 13:05:36.828345 140114565189632 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0123 13:05:36.828373 140114565189632 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0123 13:05:36.828400 140114565189632 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0123 13:05:36.828427 140114565189632 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0123 13:05:36.828453 140114565189632 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0123 13:05:36.828480 140114565189632 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=32
I0123 13:05:36.828506 140114565189632 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0123 13:05:36.828532 140114565189632 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0123 13:05:36.828579 140114565189632 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0123 13:05:36.828710 140114565189632 resource_reader.py:55] Path not found: base_htrans.gin
I0123 13:05:36.828914 140114565189632 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0123 13:05:36.829018 140114565189632 resource_reader.py:55] Path not found: trainer_configuration.gin
I0123 13:05:36.835385 140114565189632 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0123 13:05:36.835510 140114565189632 resource_reader.py:55] Path not found: size/medium_150M.gin
I0123 13:05:36.835837 140114565189632 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0123 13:05:36.835942 140114565189632 resource_reader.py:55] Path not found: options/positions_t5.gin
I0123 13:05:36.836221 140114565189632 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0123 13:05:36.836322 140114565189632 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0123 13:05:36.836731 140114565189632 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0123 13:05:36.836832 140114565189632 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0123 13:05:36.840623 140114565189632 training_loop.py:334] ==== Training loop: initializing model ====
I0123 13:05:36.945665 140114565189632 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0123 13:05:36.946421 140114565189632 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0123 13:05:36.953334 140114565189632 training_loop.py:335] Process 0 of 1
I0123 13:05:36.953387 140114565189632 training_loop.py:336] Local device count = 1
I0123 13:05:36.953426 140114565189632 training_loop.py:337] Number of replicas = 1
I0123 13:05:36.953456 140114565189632 training_loop.py:339] Using random number seed 42
I0123 13:05:37.443366 140114565189632 training_loop.py:359] Initializing the model.
I0123 13:05:37.868858 140114565189632 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:37.869152 140114565189632 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 13:05:37.869261 140114565189632 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 13:05:37.869342 140114565189632 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 13:05:37.869428 140114565189632 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 13:05:37.869513 140114565189632 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 13:05:37.869589 140114565189632 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 13:05:37.869668 140114565189632 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 13:05:37.869742 140114565189632 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 13:05:37.869812 140114565189632 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 13:05:37.869882 140114565189632 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 13:05:37.869951 140114565189632 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 13:05:37.870021 140114565189632 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 13:05:37.870090 140114565189632 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 13:05:37.870130 140114565189632 attention.py:418] Single window, no scan.
I0123 13:05:37.870176 140114565189632 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 13:05:37.870294 140114565189632 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:05:37.870334 140114565189632 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:05:37.870366 140114565189632 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:05:37.872424 140114565189632 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:37.877755 140114565189632 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:05:37.888605 140114565189632 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:37.888895 140114565189632 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:05:37.893344 140114565189632 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:05:37.904136 140114565189632 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:05:37.904194 140114565189632 attention.py:418] Single window, no scan.
I0123 13:05:37.904232 140114565189632 transformer_layer.py:389] tlayer: self-attention.
I0123 13:05:37.904264 140114565189632 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:37.904328 140114565189632 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:37.905508 140114565189632 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:37.905585 140114565189632 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:37.906296 140114565189632 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:37.908779 140114565189632 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:37.914510 140114565189632 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:37.916225 140114565189632 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:37.916304 140114565189632 transformer_layer.py:468] tlayer: End windows.
I0123 13:05:37.916339 140114565189632 transformer_layer.py:472] tlayer: final FFN.
I0123 13:05:37.916398 140114565189632 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:37.916523 140114565189632 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:05:37.916856 140114565189632 nn_components.py:325] mlp: activation = None
I0123 13:05:37.916903 140114565189632 nn_components.py:329] mlp: final activation = None
I0123 13:05:37.918823 140114565189632 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:37.918926 140114565189632 nn_components.py:261] mlp: residual
I0123 13:05:37.921789 140114565189632 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:37.921867 140114565189632 transformer_base.py:443] tbase: final FFN
I0123 13:05:37.922357 140114565189632 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:05:37.932382 140114565189632 nn_components.py:329] mlp: final activation = None
I0123 13:05:37.941062 140114565189632 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:37.941161 140114565189632 nn_components.py:261] mlp: residual
I0123 13:05:37.941455 140114565189632 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:37.941536 140114565189632 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 13:05:37.941655 140114565189632 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:05:37.941696 140114565189632 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:05:37.941726 140114565189632 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:05:37.943560 140114565189632 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:37.946010 140114565189632 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:05:37.951547 140114565189632 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:37.951810 140114565189632 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:05:37.954427 140114565189632 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:05:37.958248 140114565189632 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:05:37.958305 140114565189632 attention.py:418] Single window, no scan.
I0123 13:05:37.958341 140114565189632 transformer_layer.py:389] tlayer: self-attention.
I0123 13:05:37.958371 140114565189632 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:37.958432 140114565189632 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:37.959008 140114565189632 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:37.959084 140114565189632 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:37.959439 140114565189632 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:37.960199 140114565189632 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:37.962649 140114565189632 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:37.963274 140114565189632 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:37.963351 140114565189632 transformer_layer.py:468] tlayer: End windows.
I0123 13:05:37.963385 140114565189632 transformer_layer.py:472] tlayer: final FFN.
I0123 13:05:37.963443 140114565189632 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:37.963568 140114565189632 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:05:37.963892 140114565189632 nn_components.py:325] mlp: activation = None
I0123 13:05:37.963935 140114565189632 nn_components.py:329] mlp: final activation = None
I0123 13:05:37.965853 140114565189632 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:37.965950 140114565189632 nn_components.py:261] mlp: residual
I0123 13:05:37.968464 140114565189632 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:37.968547 140114565189632 transformer_base.py:443] tbase: final FFN
I0123 13:05:37.968977 140114565189632 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:05:37.971306 140114565189632 nn_components.py:329] mlp: final activation = None
I0123 13:05:37.973206 140114565189632 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:37.973299 140114565189632 nn_components.py:261] mlp: residual
I0123 13:05:37.973594 140114565189632 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:37.973683 140114565189632 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 13:05:37.973793 140114565189632 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:05:37.973832 140114565189632 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:05:37.973863 140114565189632 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:05:37.975768 140114565189632 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:37.978137 140114565189632 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:05:37.984089 140114565189632 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:37.984354 140114565189632 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:05:37.987013 140114565189632 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:05:37.990888 140114565189632 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:05:37.990945 140114565189632 attention.py:418] Single window, no scan.
I0123 13:05:37.990981 140114565189632 transformer_layer.py:389] tlayer: self-attention.
I0123 13:05:37.991012 140114565189632 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:37.991073 140114565189632 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:37.991635 140114565189632 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:37.991712 140114565189632 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:37.992074 140114565189632 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:37.992837 140114565189632 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:37.995370 140114565189632 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:37.996042 140114565189632 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:37.996122 140114565189632 transformer_layer.py:468] tlayer: End windows.
I0123 13:05:37.996156 140114565189632 transformer_layer.py:472] tlayer: final FFN.
I0123 13:05:37.996214 140114565189632 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:37.996343 140114565189632 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:05:37.996665 140114565189632 nn_components.py:325] mlp: activation = None
I0123 13:05:37.996707 140114565189632 nn_components.py:329] mlp: final activation = None
I0123 13:05:37.998622 140114565189632 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:37.998717 140114565189632 nn_components.py:261] mlp: residual
I0123 13:05:38.001239 140114565189632 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.001322 140114565189632 transformer_base.py:443] tbase: final FFN
I0123 13:05:38.001816 140114565189632 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:05:38.004093 140114565189632 nn_components.py:329] mlp: final activation = None
I0123 13:05:38.006025 140114565189632 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.006121 140114565189632 nn_components.py:261] mlp: residual
I0123 13:05:38.006414 140114565189632 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.006494 140114565189632 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 13:05:38.006604 140114565189632 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:05:38.006643 140114565189632 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:05:38.006674 140114565189632 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:05:38.008579 140114565189632 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.010985 140114565189632 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:05:38.016607 140114565189632 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.016872 140114565189632 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:05:38.019516 140114565189632 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:05:38.023331 140114565189632 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:05:38.023385 140114565189632 attention.py:418] Single window, no scan.
I0123 13:05:38.023421 140114565189632 transformer_layer.py:389] tlayer: self-attention.
I0123 13:05:38.023452 140114565189632 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.023514 140114565189632 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.024072 140114565189632 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.024149 140114565189632 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.024509 140114565189632 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.025277 140114565189632 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.028443 140114565189632 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.029330 140114565189632 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.029411 140114565189632 transformer_layer.py:468] tlayer: End windows.
I0123 13:05:38.029449 140114565189632 transformer_layer.py:472] tlayer: final FFN.
I0123 13:05:38.029512 140114565189632 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.029657 140114565189632 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:05:38.030009 140114565189632 nn_components.py:325] mlp: activation = None
I0123 13:05:38.030055 140114565189632 nn_components.py:329] mlp: final activation = None
I0123 13:05:38.031998 140114565189632 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.032090 140114565189632 nn_components.py:261] mlp: residual
I0123 13:05:38.034754 140114565189632 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.034840 140114565189632 transformer_base.py:443] tbase: final FFN
I0123 13:05:38.035270 140114565189632 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:05:38.037537 140114565189632 nn_components.py:329] mlp: final activation = None
I0123 13:05:38.039466 140114565189632 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.039566 140114565189632 nn_components.py:261] mlp: residual
I0123 13:05:38.039857 140114565189632 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.039937 140114565189632 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 13:05:38.040050 140114565189632 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:05:38.040090 140114565189632 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:05:38.040120 140114565189632 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:05:38.042038 140114565189632 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.044425 140114565189632 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:05:38.050078 140114565189632 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.050344 140114565189632 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:05:38.053037 140114565189632 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:05:38.056866 140114565189632 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:05:38.056923 140114565189632 attention.py:418] Single window, no scan.
I0123 13:05:38.056960 140114565189632 transformer_layer.py:389] tlayer: self-attention.
I0123 13:05:38.056990 140114565189632 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.057051 140114565189632 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.057627 140114565189632 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.057710 140114565189632 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.058071 140114565189632 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.058846 140114565189632 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.061740 140114565189632 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.062357 140114565189632 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.062434 140114565189632 transformer_layer.py:468] tlayer: End windows.
I0123 13:05:38.062469 140114565189632 transformer_layer.py:472] tlayer: final FFN.
I0123 13:05:38.062526 140114565189632 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.062659 140114565189632 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:05:38.062984 140114565189632 nn_components.py:325] mlp: activation = None
I0123 13:05:38.063027 140114565189632 nn_components.py:329] mlp: final activation = None
I0123 13:05:38.064898 140114565189632 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.064991 140114565189632 nn_components.py:261] mlp: residual
I0123 13:05:38.067539 140114565189632 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.067619 140114565189632 transformer_base.py:443] tbase: final FFN
I0123 13:05:38.068051 140114565189632 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:05:38.070326 140114565189632 nn_components.py:329] mlp: final activation = None
I0123 13:05:38.072271 140114565189632 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.072364 140114565189632 nn_components.py:261] mlp: residual
I0123 13:05:38.072655 140114565189632 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.072736 140114565189632 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 13:05:38.072844 140114565189632 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:05:38.072882 140114565189632 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:05:38.072913 140114565189632 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:05:38.074770 140114565189632 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.077134 140114565189632 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:05:38.082751 140114565189632 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.083003 140114565189632 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:05:38.085711 140114565189632 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:05:38.089479 140114565189632 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:05:38.089534 140114565189632 attention.py:418] Single window, no scan.
I0123 13:05:38.089570 140114565189632 transformer_layer.py:389] tlayer: self-attention.
I0123 13:05:38.089601 140114565189632 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.089675 140114565189632 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.090282 140114565189632 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.090360 140114565189632 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.090725 140114565189632 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.091501 140114565189632 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.094016 140114565189632 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.094635 140114565189632 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.094712 140114565189632 transformer_layer.py:468] tlayer: End windows.
I0123 13:05:38.094746 140114565189632 transformer_layer.py:472] tlayer: final FFN.
I0123 13:05:38.094804 140114565189632 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.094934 140114565189632 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:05:38.095257 140114565189632 nn_components.py:325] mlp: activation = None
I0123 13:05:38.095300 140114565189632 nn_components.py:329] mlp: final activation = None
I0123 13:05:38.097194 140114565189632 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.097291 140114565189632 nn_components.py:261] mlp: residual
I0123 13:05:38.099846 140114565189632 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.099926 140114565189632 transformer_base.py:443] tbase: final FFN
I0123 13:05:38.100365 140114565189632 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:05:38.102692 140114565189632 nn_components.py:329] mlp: final activation = None
I0123 13:05:38.104596 140114565189632 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.104694 140114565189632 nn_components.py:261] mlp: residual
I0123 13:05:38.104984 140114565189632 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.105065 140114565189632 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 13:05:38.105174 140114565189632 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:05:38.105212 140114565189632 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:05:38.105243 140114565189632 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:05:38.107086 140114565189632 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.109521 140114565189632 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:05:38.115150 140114565189632 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.115413 140114565189632 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:05:38.118074 140114565189632 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:05:38.121896 140114565189632 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:05:38.121951 140114565189632 attention.py:418] Single window, no scan.
I0123 13:05:38.121987 140114565189632 transformer_layer.py:389] tlayer: self-attention.
I0123 13:05:38.122018 140114565189632 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.122082 140114565189632 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.122635 140114565189632 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.122715 140114565189632 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.123077 140114565189632 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.123858 140114565189632 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.126370 140114565189632 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.126993 140114565189632 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.127073 140114565189632 transformer_layer.py:468] tlayer: End windows.
I0123 13:05:38.127108 140114565189632 transformer_layer.py:472] tlayer: final FFN.
I0123 13:05:38.127166 140114565189632 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.127299 140114565189632 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:05:38.127622 140114565189632 nn_components.py:325] mlp: activation = None
I0123 13:05:38.127665 140114565189632 nn_components.py:329] mlp: final activation = None
I0123 13:05:38.129651 140114565189632 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.129750 140114565189632 nn_components.py:261] mlp: residual
I0123 13:05:38.132395 140114565189632 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.132473 140114565189632 transformer_base.py:443] tbase: final FFN
I0123 13:05:38.132924 140114565189632 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:05:38.135723 140114565189632 nn_components.py:329] mlp: final activation = None
I0123 13:05:38.137657 140114565189632 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.137763 140114565189632 nn_components.py:261] mlp: residual
I0123 13:05:38.138062 140114565189632 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.138144 140114565189632 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 13:05:38.138255 140114565189632 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:05:38.138294 140114565189632 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:05:38.138326 140114565189632 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:05:38.280710 140114565189632 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.283911 140114565189632 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:05:38.289916 140114565189632 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.290227 140114565189632 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:05:38.292987 140114565189632 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:05:38.297048 140114565189632 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:05:38.297108 140114565189632 attention.py:418] Single window, no scan.
I0123 13:05:38.297147 140114565189632 transformer_layer.py:389] tlayer: self-attention.
I0123 13:05:38.297180 140114565189632 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.297251 140114565189632 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.297884 140114565189632 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.297961 140114565189632 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.298331 140114565189632 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.299124 140114565189632 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.301798 140114565189632 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.302453 140114565189632 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.302532 140114565189632 transformer_layer.py:468] tlayer: End windows.
I0123 13:05:38.302568 140114565189632 transformer_layer.py:472] tlayer: final FFN.
I0123 13:05:38.302631 140114565189632 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.302763 140114565189632 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:05:38.303111 140114565189632 nn_components.py:325] mlp: activation = None
I0123 13:05:38.303155 140114565189632 nn_components.py:329] mlp: final activation = None
I0123 13:05:38.305099 140114565189632 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.305195 140114565189632 nn_components.py:261] mlp: residual
I0123 13:05:38.307829 140114565189632 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.307909 140114565189632 transformer_base.py:443] tbase: final FFN
I0123 13:05:38.308352 140114565189632 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:05:38.310697 140114565189632 nn_components.py:329] mlp: final activation = None
I0123 13:05:38.312834 140114565189632 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.312940 140114565189632 nn_components.py:261] mlp: residual
I0123 13:05:38.313234 140114565189632 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.313318 140114565189632 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 13:05:38.313427 140114565189632 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:05:38.313466 140114565189632 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:05:38.313497 140114565189632 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:05:38.315461 140114565189632 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.317852 140114565189632 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:05:38.323513 140114565189632 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.323772 140114565189632 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:05:38.326488 140114565189632 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:05:38.330326 140114565189632 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:05:38.330381 140114565189632 attention.py:418] Single window, no scan.
I0123 13:05:38.330417 140114565189632 transformer_layer.py:389] tlayer: self-attention.
I0123 13:05:38.330448 140114565189632 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.330510 140114565189632 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.331076 140114565189632 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.331153 140114565189632 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.331517 140114565189632 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.332293 140114565189632 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.334892 140114565189632 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.335515 140114565189632 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.335593 140114565189632 transformer_layer.py:468] tlayer: End windows.
I0123 13:05:38.335628 140114565189632 transformer_layer.py:472] tlayer: final FFN.
I0123 13:05:38.335687 140114565189632 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.335812 140114565189632 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:05:38.336140 140114565189632 nn_components.py:325] mlp: activation = None
I0123 13:05:38.336184 140114565189632 nn_components.py:329] mlp: final activation = None
I0123 13:05:38.338092 140114565189632 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.338187 140114565189632 nn_components.py:261] mlp: residual
I0123 13:05:38.340777 140114565189632 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.340857 140114565189632 transformer_base.py:443] tbase: final FFN
I0123 13:05:38.341292 140114565189632 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:05:38.343588 140114565189632 nn_components.py:329] mlp: final activation = None
I0123 13:05:38.345563 140114565189632 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.345665 140114565189632 nn_components.py:261] mlp: residual
I0123 13:05:38.345963 140114565189632 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.346054 140114565189632 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 13:05:38.346168 140114565189632 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:05:38.346209 140114565189632 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:05:38.346240 140114565189632 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:05:38.348069 140114565189632 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.350520 140114565189632 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:05:38.356059 140114565189632 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.356330 140114565189632 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:05:38.359380 140114565189632 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:05:38.363144 140114565189632 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:05:38.363200 140114565189632 attention.py:418] Single window, no scan.
I0123 13:05:38.363235 140114565189632 transformer_layer.py:389] tlayer: self-attention.
I0123 13:05:38.363265 140114565189632 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.363327 140114565189632 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.363932 140114565189632 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.364008 140114565189632 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.364369 140114565189632 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.365135 140114565189632 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.367619 140114565189632 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.368245 140114565189632 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.368325 140114565189632 transformer_layer.py:468] tlayer: End windows.
I0123 13:05:38.368360 140114565189632 transformer_layer.py:472] tlayer: final FFN.
I0123 13:05:38.368418 140114565189632 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.368544 140114565189632 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:05:38.368866 140114565189632 nn_components.py:325] mlp: activation = None
I0123 13:05:38.368910 140114565189632 nn_components.py:329] mlp: final activation = None
I0123 13:05:38.370815 140114565189632 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.370910 140114565189632 nn_components.py:261] mlp: residual
I0123 13:05:38.373451 140114565189632 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.373533 140114565189632 transformer_base.py:443] tbase: final FFN
I0123 13:05:38.373971 140114565189632 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:05:38.376260 140114565189632 nn_components.py:329] mlp: final activation = None
I0123 13:05:38.378190 140114565189632 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.378286 140114565189632 nn_components.py:261] mlp: residual
I0123 13:05:38.378577 140114565189632 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.378665 140114565189632 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 13:05:38.378777 140114565189632 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:05:38.378816 140114565189632 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:05:38.378847 140114565189632 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:05:38.380685 140114565189632 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.383128 140114565189632 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:05:38.388750 140114565189632 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.389016 140114565189632 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:05:38.391657 140114565189632 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:05:38.395450 140114565189632 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:05:38.395506 140114565189632 attention.py:418] Single window, no scan.
I0123 13:05:38.395543 140114565189632 transformer_layer.py:389] tlayer: self-attention.
I0123 13:05:38.395574 140114565189632 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.395635 140114565189632 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.396206 140114565189632 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.396283 140114565189632 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.396641 140114565189632 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.397416 140114565189632 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.399894 140114565189632 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.400521 140114565189632 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.400598 140114565189632 transformer_layer.py:468] tlayer: End windows.
I0123 13:05:38.400633 140114565189632 transformer_layer.py:472] tlayer: final FFN.
I0123 13:05:38.400693 140114565189632 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.400817 140114565189632 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:05:38.401141 140114565189632 nn_components.py:325] mlp: activation = None
I0123 13:05:38.401184 140114565189632 nn_components.py:329] mlp: final activation = None
I0123 13:05:38.403122 140114565189632 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.403218 140114565189632 nn_components.py:261] mlp: residual
I0123 13:05:38.405971 140114565189632 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.406052 140114565189632 transformer_base.py:443] tbase: final FFN
I0123 13:05:38.406482 140114565189632 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:05:38.408806 140114565189632 nn_components.py:329] mlp: final activation = None
I0123 13:05:38.410710 140114565189632 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.410806 140114565189632 nn_components.py:261] mlp: residual
I0123 13:05:38.411217 140114565189632 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.411298 140114565189632 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 13:05:38.411417 140114565189632 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:05:38.411457 140114565189632 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:05:38.411488 140114565189632 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:05:38.413383 140114565189632 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.415762 140114565189632 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:05:38.421615 140114565189632 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.421896 140114565189632 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:05:38.424605 140114565189632 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:05:38.428415 140114565189632 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:05:38.428471 140114565189632 attention.py:418] Single window, no scan.
I0123 13:05:38.428506 140114565189632 transformer_layer.py:389] tlayer: self-attention.
I0123 13:05:38.428538 140114565189632 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.428598 140114565189632 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.429165 140114565189632 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.429241 140114565189632 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.429600 140114565189632 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.430373 140114565189632 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.432857 140114565189632 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.433846 140114565189632 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.433927 140114565189632 transformer_layer.py:468] tlayer: End windows.
I0123 13:05:38.433962 140114565189632 transformer_layer.py:472] tlayer: final FFN.
I0123 13:05:38.434023 140114565189632 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.434150 140114565189632 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:05:38.434474 140114565189632 nn_components.py:325] mlp: activation = None
I0123 13:05:38.434518 140114565189632 nn_components.py:329] mlp: final activation = None
I0123 13:05:38.436411 140114565189632 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.436504 140114565189632 nn_components.py:261] mlp: residual
I0123 13:05:38.439013 140114565189632 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.439093 140114565189632 transformer_base.py:443] tbase: final FFN
I0123 13:05:38.439568 140114565189632 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:05:38.441830 140114565189632 nn_components.py:329] mlp: final activation = None
I0123 13:05:38.443732 140114565189632 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.443827 140114565189632 nn_components.py:261] mlp: residual
I0123 13:05:38.444118 140114565189632 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.444400 140114565189632 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 13:05:38.444469 140114565189632 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 13:05:38.444537 140114565189632 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 13:05:38.444594 140114565189632 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 13:05:38.444649 140114565189632 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 13:05:38.444701 140114565189632 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 13:05:38.444754 140114565189632 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 13:05:38.444806 140114565189632 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 13:05:38.444857 140114565189632 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 13:05:38.444909 140114565189632 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 13:05:38.444960 140114565189632 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 13:05:38.445013 140114565189632 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 13:05:38.445051 140114565189632 decoder_stack.py:344] dstack: Final layernorm.
I0123 13:05:38.448579 140114565189632 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:05:38.496296 140114565189632 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.496380 140114565189632 decoder_stack.py:333] dstack: autoregressive generator.
I0123 13:05:38.496434 140114565189632 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 13:05:38.496537 140114565189632 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:05:38.496576 140114565189632 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:05:38.496606 140114565189632 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:05:38.496673 140114565189632 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.499099 140114565189632 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:05:38.504641 140114565189632 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.504903 140114565189632 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:05:38.507579 140114565189632 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:05:38.524578 140114565189632 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:05:38.524635 140114565189632 attention.py:418] Single window, no scan.
I0123 13:05:38.524671 140114565189632 transformer_layer.py:389] tlayer: self-attention.
I0123 13:05:38.524702 140114565189632 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.524764 140114565189632 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.525901 140114565189632 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.525979 140114565189632 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.526678 140114565189632 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.528685 140114565189632 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.533436 140114565189632 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.534752 140114565189632 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.534842 140114565189632 transformer_layer.py:468] tlayer: End windows.
I0123 13:05:38.534878 140114565189632 transformer_layer.py:472] tlayer: final FFN.
I0123 13:05:38.534938 140114565189632 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.535070 140114565189632 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:05:38.535179 140114565189632 nn_components.py:325] mlp: activation = None
I0123 13:05:38.535218 140114565189632 nn_components.py:329] mlp: final activation = None
I0123 13:05:38.537123 140114565189632 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.537217 140114565189632 nn_components.py:261] mlp: residual
I0123 13:05:38.539679 140114565189632 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.539766 140114565189632 transformer_base.py:443] tbase: final FFN
I0123 13:05:38.539877 140114565189632 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:05:38.542138 140114565189632 nn_components.py:329] mlp: final activation = None
I0123 13:05:38.544104 140114565189632 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.544201 140114565189632 nn_components.py:261] mlp: residual
I0123 13:05:38.544496 140114565189632 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.544578 140114565189632 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 13:05:38.544688 140114565189632 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:05:38.544727 140114565189632 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:05:38.544757 140114565189632 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:05:38.544820 140114565189632 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.547138 140114565189632 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:05:38.552767 140114565189632 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.553027 140114565189632 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:05:38.555742 140114565189632 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:05:38.569096 140114565189632 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:05:38.569152 140114565189632 attention.py:418] Single window, no scan.
I0123 13:05:38.569188 140114565189632 transformer_layer.py:389] tlayer: self-attention.
I0123 13:05:38.569218 140114565189632 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.569279 140114565189632 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.569845 140114565189632 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.569923 140114565189632 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.570281 140114565189632 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.570985 140114565189632 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.573485 140114565189632 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.574116 140114565189632 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.574194 140114565189632 transformer_layer.py:468] tlayer: End windows.
I0123 13:05:38.574235 140114565189632 transformer_layer.py:472] tlayer: final FFN.
I0123 13:05:38.574295 140114565189632 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.574427 140114565189632 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:05:38.574540 140114565189632 nn_components.py:325] mlp: activation = None
I0123 13:05:38.574579 140114565189632 nn_components.py:329] mlp: final activation = None
I0123 13:05:38.576527 140114565189632 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.576623 140114565189632 nn_components.py:261] mlp: residual
I0123 13:05:38.579074 140114565189632 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.579154 140114565189632 transformer_base.py:443] tbase: final FFN
I0123 13:05:38.579261 140114565189632 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:05:38.581482 140114565189632 nn_components.py:329] mlp: final activation = None
I0123 13:05:38.583410 140114565189632 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.583508 140114565189632 nn_components.py:261] mlp: residual
I0123 13:05:38.583799 140114565189632 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.583881 140114565189632 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 13:05:38.583990 140114565189632 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:05:38.584029 140114565189632 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:05:38.584060 140114565189632 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:05:38.584123 140114565189632 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.586385 140114565189632 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:05:38.591883 140114565189632 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.592141 140114565189632 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:05:38.594843 140114565189632 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:05:38.607723 140114565189632 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:05:38.607779 140114565189632 attention.py:418] Single window, no scan.
I0123 13:05:38.607815 140114565189632 transformer_layer.py:389] tlayer: self-attention.
I0123 13:05:38.607846 140114565189632 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.607908 140114565189632 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.608467 140114565189632 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.608545 140114565189632 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.608906 140114565189632 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.609602 140114565189632 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.612193 140114565189632 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.612818 140114565189632 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.612894 140114565189632 transformer_layer.py:468] tlayer: End windows.
I0123 13:05:38.612929 140114565189632 transformer_layer.py:472] tlayer: final FFN.
I0123 13:05:38.612995 140114565189632 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.613124 140114565189632 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:05:38.613233 140114565189632 nn_components.py:325] mlp: activation = None
I0123 13:05:38.613271 140114565189632 nn_components.py:329] mlp: final activation = None
I0123 13:05:38.615252 140114565189632 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.615352 140114565189632 nn_components.py:261] mlp: residual
I0123 13:05:38.617817 140114565189632 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.617899 140114565189632 transformer_base.py:443] tbase: final FFN
I0123 13:05:38.618007 140114565189632 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:05:38.620240 140114565189632 nn_components.py:329] mlp: final activation = None
I0123 13:05:38.622193 140114565189632 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.622290 140114565189632 nn_components.py:261] mlp: residual
I0123 13:05:38.622578 140114565189632 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.622660 140114565189632 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 13:05:38.622769 140114565189632 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:05:38.622809 140114565189632 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:05:38.622840 140114565189632 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:05:38.622905 140114565189632 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.625475 140114565189632 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:05:38.630968 140114565189632 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.631226 140114565189632 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:05:38.633929 140114565189632 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:05:38.646790 140114565189632 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:05:38.646846 140114565189632 attention.py:418] Single window, no scan.
I0123 13:05:38.646882 140114565189632 transformer_layer.py:389] tlayer: self-attention.
I0123 13:05:38.646913 140114565189632 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.646974 140114565189632 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.647533 140114565189632 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.647608 140114565189632 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.647959 140114565189632 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.648637 140114565189632 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.651108 140114565189632 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.651728 140114565189632 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.651806 140114565189632 transformer_layer.py:468] tlayer: End windows.
I0123 13:05:38.651840 140114565189632 transformer_layer.py:472] tlayer: final FFN.
I0123 13:05:38.651897 140114565189632 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.652031 140114565189632 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:05:38.652140 140114565189632 nn_components.py:325] mlp: activation = None
I0123 13:05:38.652178 140114565189632 nn_components.py:329] mlp: final activation = None
I0123 13:05:38.654129 140114565189632 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.654224 140114565189632 nn_components.py:261] mlp: residual
I0123 13:05:38.656631 140114565189632 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.656711 140114565189632 transformer_base.py:443] tbase: final FFN
I0123 13:05:38.656822 140114565189632 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:05:38.659034 140114565189632 nn_components.py:329] mlp: final activation = None
I0123 13:05:38.660884 140114565189632 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.660979 140114565189632 nn_components.py:261] mlp: residual
I0123 13:05:38.661264 140114565189632 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.661345 140114565189632 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 13:05:38.661453 140114565189632 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:05:38.661490 140114565189632 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:05:38.661521 140114565189632 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:05:38.661582 140114565189632 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.664172 140114565189632 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:05:38.669656 140114565189632 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.669929 140114565189632 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:05:38.672560 140114565189632 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:05:38.685433 140114565189632 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:05:38.685490 140114565189632 attention.py:418] Single window, no scan.
I0123 13:05:38.685526 140114565189632 transformer_layer.py:389] tlayer: self-attention.
I0123 13:05:38.685558 140114565189632 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.685620 140114565189632 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.691637 140114565189632 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.691730 140114565189632 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.692126 140114565189632 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.692856 140114565189632 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.695515 140114565189632 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.696154 140114565189632 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.696234 140114565189632 transformer_layer.py:468] tlayer: End windows.
I0123 13:05:38.696269 140114565189632 transformer_layer.py:472] tlayer: final FFN.
I0123 13:05:38.696338 140114565189632 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.696472 140114565189632 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:05:38.696587 140114565189632 nn_components.py:325] mlp: activation = None
I0123 13:05:38.696626 140114565189632 nn_components.py:329] mlp: final activation = None
I0123 13:05:38.698667 140114565189632 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.698762 140114565189632 nn_components.py:261] mlp: residual
I0123 13:05:38.701268 140114565189632 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.701347 140114565189632 transformer_base.py:443] tbase: final FFN
I0123 13:05:38.701456 140114565189632 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:05:38.703817 140114565189632 nn_components.py:329] mlp: final activation = None
I0123 13:05:38.705720 140114565189632 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.705816 140114565189632 nn_components.py:261] mlp: residual
I0123 13:05:38.706109 140114565189632 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.706193 140114565189632 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 13:05:38.706308 140114565189632 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:05:38.706351 140114565189632 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:05:38.706382 140114565189632 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:05:38.706447 140114565189632 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.708735 140114565189632 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:05:38.714412 140114565189632 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.714674 140114565189632 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:05:38.717434 140114565189632 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:05:38.730510 140114565189632 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:05:38.730568 140114565189632 attention.py:418] Single window, no scan.
I0123 13:05:38.730605 140114565189632 transformer_layer.py:389] tlayer: self-attention.
I0123 13:05:38.730636 140114565189632 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.730697 140114565189632 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.731264 140114565189632 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.731341 140114565189632 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.731705 140114565189632 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.732411 140114565189632 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.734903 140114565189632 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.735523 140114565189632 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.735603 140114565189632 transformer_layer.py:468] tlayer: End windows.
I0123 13:05:38.735637 140114565189632 transformer_layer.py:472] tlayer: final FFN.
I0123 13:05:38.735696 140114565189632 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.735826 140114565189632 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:05:38.735946 140114565189632 nn_components.py:325] mlp: activation = None
I0123 13:05:38.735985 140114565189632 nn_components.py:329] mlp: final activation = None
I0123 13:05:38.737941 140114565189632 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.738037 140114565189632 nn_components.py:261] mlp: residual
I0123 13:05:38.740468 140114565189632 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.740546 140114565189632 transformer_base.py:443] tbase: final FFN
I0123 13:05:38.740655 140114565189632 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:05:38.742909 140114565189632 nn_components.py:329] mlp: final activation = None
I0123 13:05:38.744799 140114565189632 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.744894 140114565189632 nn_components.py:261] mlp: residual
I0123 13:05:38.745180 140114565189632 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.745262 140114565189632 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 13:05:38.745371 140114565189632 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:05:38.745410 140114565189632 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:05:38.745441 140114565189632 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:05:38.745503 140114565189632 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.747775 140114565189632 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:05:38.753364 140114565189632 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.753623 140114565189632 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:05:38.756233 140114565189632 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:05:38.769146 140114565189632 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:05:38.769206 140114565189632 attention.py:418] Single window, no scan.
I0123 13:05:38.769242 140114565189632 transformer_layer.py:389] tlayer: self-attention.
I0123 13:05:38.769273 140114565189632 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.769334 140114565189632 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.769898 140114565189632 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.769975 140114565189632 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.770337 140114565189632 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.771034 140114565189632 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.773524 140114565189632 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.774523 140114565189632 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.774602 140114565189632 transformer_layer.py:468] tlayer: End windows.
I0123 13:05:38.774638 140114565189632 transformer_layer.py:472] tlayer: final FFN.
I0123 13:05:38.774699 140114565189632 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.774831 140114565189632 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:05:38.774941 140114565189632 nn_components.py:325] mlp: activation = None
I0123 13:05:38.774984 140114565189632 nn_components.py:329] mlp: final activation = None
I0123 13:05:38.776907 140114565189632 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.777001 140114565189632 nn_components.py:261] mlp: residual
I0123 13:05:38.779452 140114565189632 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.779533 140114565189632 transformer_base.py:443] tbase: final FFN
I0123 13:05:38.779642 140114565189632 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:05:38.781904 140114565189632 nn_components.py:329] mlp: final activation = None
I0123 13:05:38.783872 140114565189632 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.783968 140114565189632 nn_components.py:261] mlp: residual
I0123 13:05:38.784258 140114565189632 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.784340 140114565189632 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 13:05:38.784451 140114565189632 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:05:38.784490 140114565189632 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:05:38.784522 140114565189632 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:05:38.784584 140114565189632 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.786861 140114565189632 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:05:38.792376 140114565189632 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.792647 140114565189632 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:05:38.795359 140114565189632 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:05:38.808215 140114565189632 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:05:38.808271 140114565189632 attention.py:418] Single window, no scan.
I0123 13:05:38.808307 140114565189632 transformer_layer.py:389] tlayer: self-attention.
I0123 13:05:38.808338 140114565189632 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.808399 140114565189632 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.809007 140114565189632 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.809085 140114565189632 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.809449 140114565189632 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.810140 140114565189632 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.812614 140114565189632 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.813340 140114565189632 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.813418 140114565189632 transformer_layer.py:468] tlayer: End windows.
I0123 13:05:38.813452 140114565189632 transformer_layer.py:472] tlayer: final FFN.
I0123 13:05:38.813510 140114565189632 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.813645 140114565189632 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:05:38.813756 140114565189632 nn_components.py:325] mlp: activation = None
I0123 13:05:38.813799 140114565189632 nn_components.py:329] mlp: final activation = None
I0123 13:05:38.815689 140114565189632 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.815784 140114565189632 nn_components.py:261] mlp: residual
I0123 13:05:38.818268 140114565189632 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.818349 140114565189632 transformer_base.py:443] tbase: final FFN
I0123 13:05:38.818458 140114565189632 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:05:38.820708 140114565189632 nn_components.py:329] mlp: final activation = None
I0123 13:05:38.822593 140114565189632 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.822689 140114565189632 nn_components.py:261] mlp: residual
I0123 13:05:38.822975 140114565189632 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.823056 140114565189632 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 13:05:38.823164 140114565189632 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:05:38.823203 140114565189632 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:05:38.823233 140114565189632 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:05:38.823296 140114565189632 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.825545 140114565189632 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:05:38.831081 140114565189632 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.831344 140114565189632 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:05:38.833993 140114565189632 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:05:38.846877 140114565189632 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:05:38.846935 140114565189632 attention.py:418] Single window, no scan.
I0123 13:05:38.846971 140114565189632 transformer_layer.py:389] tlayer: self-attention.
I0123 13:05:38.847002 140114565189632 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.847066 140114565189632 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.847628 140114565189632 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.847705 140114565189632 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.848068 140114565189632 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.848755 140114565189632 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.851266 140114565189632 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.851931 140114565189632 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.852008 140114565189632 transformer_layer.py:468] tlayer: End windows.
I0123 13:05:38.852044 140114565189632 transformer_layer.py:472] tlayer: final FFN.
I0123 13:05:38.852107 140114565189632 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.852236 140114565189632 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:05:38.852345 140114565189632 nn_components.py:325] mlp: activation = None
I0123 13:05:38.852384 140114565189632 nn_components.py:329] mlp: final activation = None
I0123 13:05:38.854276 140114565189632 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.854371 140114565189632 nn_components.py:261] mlp: residual
I0123 13:05:38.856795 140114565189632 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.856875 140114565189632 transformer_base.py:443] tbase: final FFN
I0123 13:05:38.856982 140114565189632 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:05:38.859244 140114565189632 nn_components.py:329] mlp: final activation = None
I0123 13:05:38.861271 140114565189632 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.861373 140114565189632 nn_components.py:261] mlp: residual
I0123 13:05:38.861679 140114565189632 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.861767 140114565189632 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 13:05:38.861883 140114565189632 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:05:38.861924 140114565189632 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:05:38.861956 140114565189632 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:05:38.862022 140114565189632 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.864352 140114565189632 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:05:38.869946 140114565189632 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.870213 140114565189632 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:05:38.872958 140114565189632 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:05:38.886142 140114565189632 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:05:38.886197 140114565189632 attention.py:418] Single window, no scan.
I0123 13:05:38.886233 140114565189632 transformer_layer.py:389] tlayer: self-attention.
I0123 13:05:38.886264 140114565189632 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.886327 140114565189632 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.886934 140114565189632 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.887011 140114565189632 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.887362 140114565189632 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.888052 140114565189632 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.890534 140114565189632 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.891161 140114565189632 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.891239 140114565189632 transformer_layer.py:468] tlayer: End windows.
I0123 13:05:38.891273 140114565189632 transformer_layer.py:472] tlayer: final FFN.
I0123 13:05:38.891331 140114565189632 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.891459 140114565189632 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:05:38.891567 140114565189632 nn_components.py:325] mlp: activation = None
I0123 13:05:38.891606 140114565189632 nn_components.py:329] mlp: final activation = None
I0123 13:05:38.893512 140114565189632 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.893615 140114565189632 nn_components.py:261] mlp: residual
I0123 13:05:38.896083 140114565189632 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.896162 140114565189632 transformer_base.py:443] tbase: final FFN
I0123 13:05:38.896272 140114565189632 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:05:38.898517 140114565189632 nn_components.py:329] mlp: final activation = None
I0123 13:05:38.900393 140114565189632 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.900487 140114565189632 nn_components.py:261] mlp: residual
I0123 13:05:38.900771 140114565189632 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.900851 140114565189632 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 13:05:38.900962 140114565189632 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:05:38.901000 140114565189632 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:05:38.901031 140114565189632 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:05:38.901093 140114565189632 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.903358 140114565189632 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:05:38.908931 140114565189632 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.909201 140114565189632 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:05:38.911914 140114565189632 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:05:38.925036 140114565189632 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:05:38.925093 140114565189632 attention.py:418] Single window, no scan.
I0123 13:05:38.925129 140114565189632 transformer_layer.py:389] tlayer: self-attention.
I0123 13:05:38.925160 140114565189632 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.925222 140114565189632 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.925799 140114565189632 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.925878 140114565189632 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.926236 140114565189632 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.926927 140114565189632 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.929429 140114565189632 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.930105 140114565189632 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.930184 140114565189632 transformer_layer.py:468] tlayer: End windows.
I0123 13:05:38.930218 140114565189632 transformer_layer.py:472] tlayer: final FFN.
I0123 13:05:38.930276 140114565189632 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.930408 140114565189632 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:05:38.930519 140114565189632 nn_components.py:325] mlp: activation = None
I0123 13:05:38.930557 140114565189632 nn_components.py:329] mlp: final activation = None
I0123 13:05:38.932447 140114565189632 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.932547 140114565189632 nn_components.py:261] mlp: residual
I0123 13:05:38.935178 140114565189632 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.935261 140114565189632 transformer_base.py:443] tbase: final FFN
I0123 13:05:38.935370 140114565189632 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:05:38.937593 140114565189632 nn_components.py:329] mlp: final activation = None
I0123 13:05:38.939538 140114565189632 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.939633 140114565189632 nn_components.py:261] mlp: residual
I0123 13:05:38.939922 140114565189632 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.940003 140114565189632 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 13:05:38.940113 140114565189632 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:05:38.940152 140114565189632 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:05:38.940182 140114565189632 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:05:38.940243 140114565189632 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.942504 140114565189632 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:05:38.947955 140114565189632 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.948215 140114565189632 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:05:38.950930 140114565189632 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:05:38.963737 140114565189632 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:05:38.963795 140114565189632 attention.py:418] Single window, no scan.
I0123 13:05:38.963830 140114565189632 transformer_layer.py:389] tlayer: self-attention.
I0123 13:05:38.963860 140114565189632 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.963922 140114565189632 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.964486 140114565189632 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.964563 140114565189632 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.964913 140114565189632 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.965656 140114565189632 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.968147 140114565189632 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.968777 140114565189632 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.968856 140114565189632 transformer_layer.py:468] tlayer: End windows.
I0123 13:05:38.968890 140114565189632 transformer_layer.py:472] tlayer: final FFN.
I0123 13:05:38.968947 140114565189632 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.969077 140114565189632 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:05:38.969185 140114565189632 nn_components.py:325] mlp: activation = None
I0123 13:05:38.969223 140114565189632 nn_components.py:329] mlp: final activation = None
I0123 13:05:38.971107 140114565189632 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.971203 140114565189632 nn_components.py:261] mlp: residual
I0123 13:05:38.973659 140114565189632 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.973739 140114565189632 transformer_base.py:443] tbase: final FFN
I0123 13:05:38.973849 140114565189632 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:05:38.976137 140114565189632 nn_components.py:329] mlp: final activation = None
I0123 13:05:38.978001 140114565189632 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.978097 140114565189632 nn_components.py:261] mlp: residual
I0123 13:05:38.978384 140114565189632 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:38.978473 140114565189632 decoder_stack.py:344] dstack: Final layernorm.
I0123 13:05:38.981343 140114565189632 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:05:39.037383 140114565189632 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.037471 140114565189632 decoder_stack.py:333] dstack: autoregressive generator.
I0123 13:05:39.037525 140114565189632 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 13:05:39.037629 140114565189632 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:05:39.037677 140114565189632 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:05:39.037708 140114565189632 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:05:39.037771 140114565189632 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.040452 140114565189632 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:05:39.045946 140114565189632 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.046207 140114565189632 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:05:39.048828 140114565189632 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:05:39.061372 140114565189632 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:05:39.061429 140114565189632 attention.py:418] Single window, no scan.
I0123 13:05:39.061465 140114565189632 transformer_layer.py:389] tlayer: self-attention.
I0123 13:05:39.061495 140114565189632 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.061557 140114565189632 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.062124 140114565189632 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.062201 140114565189632 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.062555 140114565189632 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.063235 140114565189632 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.065739 140114565189632 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.066358 140114565189632 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.066434 140114565189632 transformer_layer.py:468] tlayer: End windows.
I0123 13:05:39.066469 140114565189632 transformer_layer.py:472] tlayer: final FFN.
I0123 13:05:39.066529 140114565189632 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.066655 140114565189632 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:05:39.066770 140114565189632 nn_components.py:325] mlp: activation = None
I0123 13:05:39.066809 140114565189632 nn_components.py:329] mlp: final activation = None
I0123 13:05:39.068646 140114565189632 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.068740 140114565189632 nn_components.py:261] mlp: residual
I0123 13:05:39.071153 140114565189632 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.071233 140114565189632 transformer_base.py:443] tbase: final FFN
I0123 13:05:39.071341 140114565189632 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:05:39.073572 140114565189632 nn_components.py:329] mlp: final activation = None
I0123 13:05:39.075450 140114565189632 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.075546 140114565189632 nn_components.py:261] mlp: residual
I0123 13:05:39.075833 140114565189632 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.075914 140114565189632 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 13:05:39.076022 140114565189632 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:05:39.076061 140114565189632 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:05:39.076092 140114565189632 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:05:39.076155 140114565189632 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.078398 140114565189632 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:05:39.083788 140114565189632 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.084050 140114565189632 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:05:39.086739 140114565189632 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:05:39.099189 140114565189632 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:05:39.099245 140114565189632 attention.py:418] Single window, no scan.
I0123 13:05:39.099281 140114565189632 transformer_layer.py:389] tlayer: self-attention.
I0123 13:05:39.099311 140114565189632 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.099374 140114565189632 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.099923 140114565189632 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.099999 140114565189632 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.100347 140114565189632 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.101015 140114565189632 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.103506 140114565189632 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.104125 140114565189632 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.104203 140114565189632 transformer_layer.py:468] tlayer: End windows.
I0123 13:05:39.104238 140114565189632 transformer_layer.py:472] tlayer: final FFN.
I0123 13:05:39.104296 140114565189632 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.104424 140114565189632 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:05:39.104535 140114565189632 nn_components.py:325] mlp: activation = None
I0123 13:05:39.104582 140114565189632 nn_components.py:329] mlp: final activation = None
I0123 13:05:39.106428 140114565189632 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.106524 140114565189632 nn_components.py:261] mlp: residual
I0123 13:05:39.108904 140114565189632 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.108983 140114565189632 transformer_base.py:443] tbase: final FFN
I0123 13:05:39.109091 140114565189632 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:05:39.111330 140114565189632 nn_components.py:329] mlp: final activation = None
I0123 13:05:39.113175 140114565189632 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.113271 140114565189632 nn_components.py:261] mlp: residual
I0123 13:05:39.113557 140114565189632 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.113645 140114565189632 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 13:05:39.113757 140114565189632 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:05:39.113797 140114565189632 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:05:39.113828 140114565189632 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:05:39.113890 140114565189632 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.116105 140114565189632 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:05:39.121466 140114565189632 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.121730 140114565189632 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:05:39.124390 140114565189632 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:05:39.136763 140114565189632 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:05:39.136820 140114565189632 attention.py:418] Single window, no scan.
I0123 13:05:39.136856 140114565189632 transformer_layer.py:389] tlayer: self-attention.
I0123 13:05:39.136887 140114565189632 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.136948 140114565189632 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.137495 140114565189632 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.137571 140114565189632 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.137929 140114565189632 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.138606 140114565189632 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.141103 140114565189632 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.141727 140114565189632 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.141806 140114565189632 transformer_layer.py:468] tlayer: End windows.
I0123 13:05:39.141841 140114565189632 transformer_layer.py:472] tlayer: final FFN.
I0123 13:05:39.141900 140114565189632 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.142027 140114565189632 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:05:39.142137 140114565189632 nn_components.py:325] mlp: activation = None
I0123 13:05:39.142176 140114565189632 nn_components.py:329] mlp: final activation = None
I0123 13:05:39.144139 140114565189632 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.144233 140114565189632 nn_components.py:261] mlp: residual
I0123 13:05:39.146734 140114565189632 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.146813 140114565189632 transformer_base.py:443] tbase: final FFN
I0123 13:05:39.146923 140114565189632 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:05:39.149610 140114565189632 nn_components.py:329] mlp: final activation = None
I0123 13:05:39.151466 140114565189632 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.151562 140114565189632 nn_components.py:261] mlp: residual
I0123 13:05:39.151851 140114565189632 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.151934 140114565189632 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 13:05:39.152043 140114565189632 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:05:39.152082 140114565189632 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:05:39.152113 140114565189632 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:05:39.152177 140114565189632 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.154399 140114565189632 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:05:39.159760 140114565189632 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.160022 140114565189632 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:05:39.162690 140114565189632 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:05:39.175244 140114565189632 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:05:39.175301 140114565189632 attention.py:418] Single window, no scan.
I0123 13:05:39.175345 140114565189632 transformer_layer.py:389] tlayer: self-attention.
I0123 13:05:39.175386 140114565189632 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.175449 140114565189632 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.176007 140114565189632 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.176081 140114565189632 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.176439 140114565189632 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.177130 140114565189632 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.179688 140114565189632 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.180311 140114565189632 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.180386 140114565189632 transformer_layer.py:468] tlayer: End windows.
I0123 13:05:39.180420 140114565189632 transformer_layer.py:472] tlayer: final FFN.
I0123 13:05:39.180478 140114565189632 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.180602 140114565189632 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:05:39.180709 140114565189632 nn_components.py:325] mlp: activation = None
I0123 13:05:39.180748 140114565189632 nn_components.py:329] mlp: final activation = None
I0123 13:05:39.182623 140114565189632 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.182716 140114565189632 nn_components.py:261] mlp: residual
I0123 13:05:39.185104 140114565189632 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.185182 140114565189632 transformer_base.py:443] tbase: final FFN
I0123 13:05:39.185288 140114565189632 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:05:39.187566 140114565189632 nn_components.py:329] mlp: final activation = None
I0123 13:05:39.189431 140114565189632 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.189525 140114565189632 nn_components.py:261] mlp: residual
I0123 13:05:39.189822 140114565189632 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.189903 140114565189632 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 13:05:39.190012 140114565189632 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:05:39.190049 140114565189632 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:05:39.190078 140114565189632 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:05:39.190140 140114565189632 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.192394 140114565189632 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:05:39.197844 140114565189632 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.198104 140114565189632 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:05:39.200787 140114565189632 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:05:39.213526 140114565189632 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:05:39.213580 140114565189632 attention.py:418] Single window, no scan.
I0123 13:05:39.213614 140114565189632 transformer_layer.py:389] tlayer: self-attention.
I0123 13:05:39.213649 140114565189632 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.213713 140114565189632 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.214266 140114565189632 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.214341 140114565189632 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.214701 140114565189632 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.215396 140114565189632 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.217937 140114565189632 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.218562 140114565189632 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.218638 140114565189632 transformer_layer.py:468] tlayer: End windows.
I0123 13:05:39.218672 140114565189632 transformer_layer.py:472] tlayer: final FFN.
I0123 13:05:39.218729 140114565189632 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.218857 140114565189632 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:05:39.218965 140114565189632 nn_components.py:325] mlp: activation = None
I0123 13:05:39.219003 140114565189632 nn_components.py:329] mlp: final activation = None
I0123 13:05:39.220886 140114565189632 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.220986 140114565189632 nn_components.py:261] mlp: residual
I0123 13:05:39.223443 140114565189632 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.223521 140114565189632 transformer_base.py:443] tbase: final FFN
I0123 13:05:39.223630 140114565189632 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:05:39.225927 140114565189632 nn_components.py:329] mlp: final activation = None
I0123 13:05:39.227787 140114565189632 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.227881 140114565189632 nn_components.py:261] mlp: residual
I0123 13:05:39.228169 140114565189632 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.228249 140114565189632 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 13:05:39.228358 140114565189632 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:05:39.228395 140114565189632 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:05:39.228425 140114565189632 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:05:39.228487 140114565189632 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.230751 140114565189632 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:05:39.236165 140114565189632 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.236425 140114565189632 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:05:39.239114 140114565189632 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:05:39.252007 140114565189632 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:05:39.252062 140114565189632 attention.py:418] Single window, no scan.
I0123 13:05:39.252096 140114565189632 transformer_layer.py:389] tlayer: self-attention.
I0123 13:05:39.252125 140114565189632 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.252187 140114565189632 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.252748 140114565189632 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.252822 140114565189632 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.253180 140114565189632 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.253878 140114565189632 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.256461 140114565189632 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.257074 140114565189632 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.257150 140114565189632 transformer_layer.py:468] tlayer: End windows.
I0123 13:05:39.257184 140114565189632 transformer_layer.py:472] tlayer: final FFN.
I0123 13:05:39.257242 140114565189632 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.257366 140114565189632 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:05:39.257478 140114565189632 nn_components.py:325] mlp: activation = None
I0123 13:05:39.257516 140114565189632 nn_components.py:329] mlp: final activation = None
I0123 13:05:39.259393 140114565189632 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.259493 140114565189632 nn_components.py:261] mlp: residual
I0123 13:05:39.261914 140114565189632 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.261993 140114565189632 transformer_base.py:443] tbase: final FFN
I0123 13:05:39.262101 140114565189632 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:05:39.264780 140114565189632 nn_components.py:329] mlp: final activation = None
I0123 13:05:39.266659 140114565189632 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.266754 140114565189632 nn_components.py:261] mlp: residual
I0123 13:05:39.267040 140114565189632 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.267120 140114565189632 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 13:05:39.267228 140114565189632 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:05:39.267265 140114565189632 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:05:39.267295 140114565189632 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:05:39.267356 140114565189632 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.269578 140114565189632 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:05:39.275026 140114565189632 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.275284 140114565189632 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:05:39.277966 140114565189632 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:05:39.290674 140114565189632 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:05:39.290728 140114565189632 attention.py:418] Single window, no scan.
I0123 13:05:39.290762 140114565189632 transformer_layer.py:389] tlayer: self-attention.
I0123 13:05:39.290792 140114565189632 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.290857 140114565189632 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.291421 140114565189632 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.291496 140114565189632 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.291848 140114565189632 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.292534 140114565189632 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.295071 140114565189632 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.295695 140114565189632 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.295771 140114565189632 transformer_layer.py:468] tlayer: End windows.
I0123 13:05:39.295804 140114565189632 transformer_layer.py:472] tlayer: final FFN.
I0123 13:05:39.295864 140114565189632 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.295994 140114565189632 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:05:39.296103 140114565189632 nn_components.py:325] mlp: activation = None
I0123 13:05:39.296140 140114565189632 nn_components.py:329] mlp: final activation = None
I0123 13:05:39.298013 140114565189632 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.298105 140114565189632 nn_components.py:261] mlp: residual
I0123 13:05:39.300528 140114565189632 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.300605 140114565189632 transformer_base.py:443] tbase: final FFN
I0123 13:05:39.300711 140114565189632 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:05:39.302998 140114565189632 nn_components.py:329] mlp: final activation = None
I0123 13:05:39.304847 140114565189632 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.304941 140114565189632 nn_components.py:261] mlp: residual
I0123 13:05:39.305226 140114565189632 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.305306 140114565189632 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 13:05:39.305413 140114565189632 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:05:39.305450 140114565189632 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:05:39.305480 140114565189632 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:05:39.305541 140114565189632 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.307800 140114565189632 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:05:39.313257 140114565189632 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.313514 140114565189632 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:05:39.316234 140114565189632 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:05:39.328929 140114565189632 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:05:39.328983 140114565189632 attention.py:418] Single window, no scan.
I0123 13:05:39.329017 140114565189632 transformer_layer.py:389] tlayer: self-attention.
I0123 13:05:39.329047 140114565189632 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.329111 140114565189632 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.329674 140114565189632 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.329751 140114565189632 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.330109 140114565189632 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.330799 140114565189632 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.333315 140114565189632 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.333942 140114565189632 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.334019 140114565189632 transformer_layer.py:468] tlayer: End windows.
I0123 13:05:39.334053 140114565189632 transformer_layer.py:472] tlayer: final FFN.
I0123 13:05:39.334111 140114565189632 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.334239 140114565189632 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:05:39.334348 140114565189632 nn_components.py:325] mlp: activation = None
I0123 13:05:39.334386 140114565189632 nn_components.py:329] mlp: final activation = None
I0123 13:05:39.336250 140114565189632 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.336341 140114565189632 nn_components.py:261] mlp: residual
I0123 13:05:39.338764 140114565189632 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.338848 140114565189632 transformer_base.py:443] tbase: final FFN
I0123 13:05:39.338957 140114565189632 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:05:39.341234 140114565189632 nn_components.py:329] mlp: final activation = None
I0123 13:05:39.343104 140114565189632 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.343199 140114565189632 nn_components.py:261] mlp: residual
I0123 13:05:39.343482 140114565189632 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.343560 140114565189632 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 13:05:39.343667 140114565189632 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:05:39.343705 140114565189632 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:05:39.343735 140114565189632 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:05:39.343797 140114565189632 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.346037 140114565189632 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:05:39.351584 140114565189632 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.351843 140114565189632 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:05:39.354520 140114565189632 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:05:39.367177 140114565189632 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:05:39.367231 140114565189632 attention.py:418] Single window, no scan.
I0123 13:05:39.367265 140114565189632 transformer_layer.py:389] tlayer: self-attention.
I0123 13:05:39.367296 140114565189632 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.367357 140114565189632 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.367915 140114565189632 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.367990 140114565189632 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.368350 140114565189632 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.369036 140114565189632 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.371604 140114565189632 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.372230 140114565189632 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.372305 140114565189632 transformer_layer.py:468] tlayer: End windows.
I0123 13:05:39.372339 140114565189632 transformer_layer.py:472] tlayer: final FFN.
I0123 13:05:39.372396 140114565189632 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.372522 140114565189632 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:05:39.372628 140114565189632 nn_components.py:325] mlp: activation = None
I0123 13:05:39.372665 140114565189632 nn_components.py:329] mlp: final activation = None
I0123 13:05:39.374548 140114565189632 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.374641 140114565189632 nn_components.py:261] mlp: residual
I0123 13:05:39.377041 140114565189632 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.377125 140114565189632 transformer_base.py:443] tbase: final FFN
I0123 13:05:39.377234 140114565189632 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:05:39.379914 140114565189632 nn_components.py:329] mlp: final activation = None
I0123 13:05:39.381806 140114565189632 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.381901 140114565189632 nn_components.py:261] mlp: residual
I0123 13:05:39.382189 140114565189632 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.382270 140114565189632 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 13:05:39.382378 140114565189632 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:05:39.382416 140114565189632 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:05:39.382446 140114565189632 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:05:39.382507 140114565189632 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.384752 140114565189632 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:05:39.390224 140114565189632 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.390486 140114565189632 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:05:39.393176 140114565189632 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:05:39.405834 140114565189632 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:05:39.405889 140114565189632 attention.py:418] Single window, no scan.
I0123 13:05:39.405923 140114565189632 transformer_layer.py:389] tlayer: self-attention.
I0123 13:05:39.405951 140114565189632 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.406011 140114565189632 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.406565 140114565189632 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.406640 140114565189632 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.406996 140114565189632 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.407686 140114565189632 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.410232 140114565189632 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.410855 140114565189632 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.410929 140114565189632 transformer_layer.py:468] tlayer: End windows.
I0123 13:05:39.410962 140114565189632 transformer_layer.py:472] tlayer: final FFN.
I0123 13:05:39.411017 140114565189632 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.411141 140114565189632 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:05:39.411248 140114565189632 nn_components.py:325] mlp: activation = None
I0123 13:05:39.411285 140114565189632 nn_components.py:329] mlp: final activation = None
I0123 13:05:39.413690 140114565189632 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.413785 140114565189632 nn_components.py:261] mlp: residual
I0123 13:05:39.416173 140114565189632 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.416250 140114565189632 transformer_base.py:443] tbase: final FFN
I0123 13:05:39.416363 140114565189632 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:05:39.418616 140114565189632 nn_components.py:329] mlp: final activation = None
I0123 13:05:39.420466 140114565189632 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.420560 140114565189632 nn_components.py:261] mlp: residual
I0123 13:05:39.420845 140114565189632 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.420923 140114565189632 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 13:05:39.421030 140114565189632 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:05:39.421068 140114565189632 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:05:39.421097 140114565189632 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:05:39.421158 140114565189632 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.423393 140114565189632 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:05:39.428785 140114565189632 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.429042 140114565189632 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:05:39.431718 140114565189632 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:05:39.444363 140114565189632 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:05:39.444416 140114565189632 attention.py:418] Single window, no scan.
I0123 13:05:39.444450 140114565189632 transformer_layer.py:389] tlayer: self-attention.
I0123 13:05:39.444480 140114565189632 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.444540 140114565189632 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.445093 140114565189632 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.445172 140114565189632 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.445526 140114565189632 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.446215 140114565189632 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.448741 140114565189632 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.449363 140114565189632 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.449440 140114565189632 transformer_layer.py:468] tlayer: End windows.
I0123 13:05:39.449472 140114565189632 transformer_layer.py:472] tlayer: final FFN.
I0123 13:05:39.449528 140114565189632 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.449664 140114565189632 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:05:39.449772 140114565189632 nn_components.py:325] mlp: activation = None
I0123 13:05:39.449810 140114565189632 nn_components.py:329] mlp: final activation = None
I0123 13:05:39.451675 140114565189632 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.451767 140114565189632 nn_components.py:261] mlp: residual
I0123 13:05:39.454177 140114565189632 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.454254 140114565189632 transformer_base.py:443] tbase: final FFN
I0123 13:05:39.454365 140114565189632 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:05:39.456635 140114565189632 nn_components.py:329] mlp: final activation = None
I0123 13:05:39.458495 140114565189632 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.458590 140114565189632 nn_components.py:261] mlp: residual
I0123 13:05:39.458875 140114565189632 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.458956 140114565189632 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 13:05:39.459062 140114565189632 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:05:39.459099 140114565189632 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:05:39.459127 140114565189632 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:05:39.459188 140114565189632 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.461416 140114565189632 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:05:39.466854 140114565189632 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.467106 140114565189632 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:05:39.469866 140114565189632 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:05:39.482488 140114565189632 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:05:39.482542 140114565189632 attention.py:418] Single window, no scan.
I0123 13:05:39.482578 140114565189632 transformer_layer.py:389] tlayer: self-attention.
I0123 13:05:39.482608 140114565189632 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.482669 140114565189632 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.483233 140114565189632 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.483308 140114565189632 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.483657 140114565189632 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.484346 140114565189632 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.486881 140114565189632 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.487496 140114565189632 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.487574 140114565189632 transformer_layer.py:468] tlayer: End windows.
I0123 13:05:39.487607 140114565189632 transformer_layer.py:472] tlayer: final FFN.
I0123 13:05:39.487664 140114565189632 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.487787 140114565189632 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:05:39.487894 140114565189632 nn_components.py:325] mlp: activation = None
I0123 13:05:39.487932 140114565189632 nn_components.py:329] mlp: final activation = None
I0123 13:05:39.489801 140114565189632 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.489893 140114565189632 nn_components.py:261] mlp: residual
I0123 13:05:39.492285 140114565189632 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.492363 140114565189632 transformer_base.py:443] tbase: final FFN
I0123 13:05:39.492470 140114565189632 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:05:39.495141 140114565189632 nn_components.py:329] mlp: final activation = None
I0123 13:05:39.497023 140114565189632 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.497116 140114565189632 nn_components.py:261] mlp: residual
I0123 13:05:39.497401 140114565189632 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:39.497484 140114565189632 decoder_stack.py:344] dstack: Final layernorm.
I0123 13:05:39.500346 140114565189632 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:05:43.947450 140114565189632 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
I0123 13:05:44.473193 140114565189632 training_loop.py:409] No working directory specified.
I0123 13:05:44.473309 140114565189632 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0123 13:05:44.474091 140114565189632 checkpoints.py:1062] Restoring legacy Flax checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0123 13:05:47.432469 140114565189632 training_loop.py:447] Only restoring trainable parameters.
I0123 13:05:47.433175 140114565189632 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0123 13:05:47.433233 140114565189632 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0123 13:05:47.433278 140114565189632 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 13:05:47.433320 140114565189632 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 13:05:47.433361 140114565189632 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:47.433399 140114565189632 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 13:05:47.433437 140114565189632 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:47.433474 140114565189632 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:47.433511 140114565189632 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 13:05:47.433548 140114565189632 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 13:05:47.433584 140114565189632 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:47.433621 140114565189632 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 13:05:47.433669 140114565189632 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 13:05:47.433707 140114565189632 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 13:05:47.433744 140114565189632 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:47.433781 140114565189632 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 13:05:47.433818 140114565189632 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:47.433855 140114565189632 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:47.433892 140114565189632 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 13:05:47.433928 140114565189632 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 13:05:47.433976 140114565189632 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:47.434014 140114565189632 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 13:05:47.434051 140114565189632 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 13:05:47.434087 140114565189632 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 13:05:47.434124 140114565189632 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:47.434161 140114565189632 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 13:05:47.434198 140114565189632 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:47.434234 140114565189632 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:47.434271 140114565189632 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 13:05:47.434307 140114565189632 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 13:05:47.434343 140114565189632 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:47.434380 140114565189632 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 13:05:47.434417 140114565189632 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 13:05:47.434453 140114565189632 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 13:05:47.434490 140114565189632 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:47.434526 140114565189632 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 13:05:47.434562 140114565189632 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:47.434598 140114565189632 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:47.434634 140114565189632 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 13:05:47.434670 140114565189632 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 13:05:47.434705 140114565189632 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:47.434741 140114565189632 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 13:05:47.434777 140114565189632 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 13:05:47.434813 140114565189632 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 13:05:47.434848 140114565189632 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:47.434883 140114565189632 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 13:05:47.434924 140114565189632 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:47.434961 140114565189632 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:47.434997 140114565189632 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 13:05:47.435032 140114565189632 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 13:05:47.435067 140114565189632 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:47.435103 140114565189632 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 13:05:47.435139 140114565189632 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 13:05:47.435174 140114565189632 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 13:05:47.435210 140114565189632 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:47.435246 140114565189632 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 13:05:47.435281 140114565189632 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:47.435317 140114565189632 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:47.435353 140114565189632 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 13:05:47.435389 140114565189632 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 13:05:47.435425 140114565189632 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:47.435460 140114565189632 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 13:05:47.435495 140114565189632 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 13:05:47.435531 140114565189632 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 13:05:47.435567 140114565189632 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:47.435602 140114565189632 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 13:05:47.435638 140114565189632 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:47.435673 140114565189632 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:47.435708 140114565189632 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 13:05:47.435744 140114565189632 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 13:05:47.435779 140114565189632 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:47.435815 140114565189632 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 13:05:47.435851 140114565189632 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 13:05:47.435892 140114565189632 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 13:05:47.435930 140114565189632 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:47.435968 140114565189632 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 13:05:47.436004 140114565189632 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:47.436039 140114565189632 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:47.436075 140114565189632 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 13:05:47.436111 140114565189632 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 13:05:47.436146 140114565189632 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:47.436182 140114565189632 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 13:05:47.436217 140114565189632 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 13:05:47.436253 140114565189632 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 13:05:47.436289 140114565189632 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:47.436325 140114565189632 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 13:05:47.436360 140114565189632 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:47.436396 140114565189632 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:47.436431 140114565189632 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 13:05:47.436466 140114565189632 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 13:05:47.436502 140114565189632 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:47.436537 140114565189632 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 13:05:47.436572 140114565189632 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 13:05:47.436608 140114565189632 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 13:05:47.436644 140114565189632 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:47.436679 140114565189632 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 13:05:47.436714 140114565189632 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:47.436749 140114565189632 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:47.436784 140114565189632 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 13:05:47.436820 140114565189632 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 13:05:47.436859 140114565189632 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:47.436897 140114565189632 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 13:05:47.436932 140114565189632 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 13:05:47.436967 140114565189632 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 13:05:47.437003 140114565189632 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:47.437038 140114565189632 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 13:05:47.437073 140114565189632 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:47.437109 140114565189632 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:47.437144 140114565189632 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 13:05:47.437180 140114565189632 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 13:05:47.437216 140114565189632 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:47.437251 140114565189632 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 13:05:47.437286 140114565189632 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 13:05:47.437321 140114565189632 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 13:05:47.437356 140114565189632 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:47.437392 140114565189632 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 13:05:47.437427 140114565189632 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:47.437463 140114565189632 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:47.437498 140114565189632 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 13:05:47.437532 140114565189632 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 13:05:47.437568 140114565189632 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:05:47.437604 140114565189632 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 13:05:47.437632 140114565189632 training_loop.py:725] Total parameters: 152072288
I0123 13:05:47.437858 140114565189632 training_loop.py:739] Total state size: 0
I0123 13:05:47.458387 140114565189632 training_loop.py:492] Training loop: creating task for mode beam_search
I0123 13:05:47.458647 140114565189632 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0123 13:05:47.458997 140114565189632 training_loop.py:652] Compiling mode beam_search with jit.
I0123 13:05:47.459313 140114565189632 training_loop.py:89] registering functions: dict_keys([])
I0123 13:05:47.475679 140114565189632 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e c b a; f = mirror f c d; g = mirror g f b; h = mirror h f a; i = circle i f e g; j = circle j f e h; k = on_circle k i f, on_line k c b; l = on_circle l i f, on_line l c b; m = on_circle m j f, on_line m c a; n = on_circle n j f, on_line n c a; o = circle o l m e ? perp b e e o
I0123 13:05:52.330319 140114565189632 ddar.py:60] Depth 1/1000 time = 4.723781108856201
I0123 13:06:09.880763 140114565189632 ddar.py:60] Depth 2/1000 time = 17.550105810165405
I0123 13:06:38.079274 140114565189632 ddar.py:60] Depth 3/1000 time = 28.197861433029175
I0123 13:07:06.957636 140114565189632 ddar.py:60] Depth 4/1000 time = 28.87751817703247
I0123 13:07:35.498921 140114565189632 ddar.py:60] Depth 5/1000 time = 28.54051399230957
I0123 13:08:04.263200 140114565189632 ddar.py:60] Depth 6/1000 time = 28.76273536682129
I0123 13:08:32.531430 140114565189632 ddar.py:60] Depth 7/1000 time = 28.04221487045288
I0123 13:09:00.684805 140114565189632 ddar.py:60] Depth 8/1000 time = 28.152685165405273
I0123 13:09:30.483625 140114565189632 ddar.py:60] Depth 9/1000 time = 29.798125743865967
I0123 13:10:00.467397 140114565189632 ddar.py:60] Depth 10/1000 time = 29.983004093170166
I0123 13:10:30.693070 140114565189632 ddar.py:60] Depth 11/1000 time = 30.22488307952881
I0123 13:11:00.651524 140114565189632 ddar.py:60] Depth 12/1000 time = 29.83475089073181
I0123 13:11:31.329048 140114565189632 ddar.py:60] Depth 13/1000 time = 30.660339832305908
I0123 13:12:04.149721 140114565189632 ddar.py:60] Depth 14/1000 time = 32.81981325149536
I0123 13:12:37.406721 140114565189632 ddar.py:60] Depth 15/1000 time = 33.25615119934082
I0123 13:12:37.767748 140114565189632 alphageometry.py:191] 
==========================
 * From theorem premises:
A B C D E F G H I J L M O : Points
ACB = ACB [00]
DA = DB [01]
DB = DC [02]
E,A,B are collinear [03]
CE  AB [04]
D,F,C are collinear [05]
DC = DF [06]
G,F,B are collinear [07]
BF = BG [08]
F,A,H are collinear [09]
AF = AH [10]
IF = IE [11]
IE = IG [12]
JE = JH [13]
JF = JE [14]
IL = IF [15]
L,C,B are collinear [16]
JM = JF [17]
M,C,A are collinear [18]
OM = OE [19]
OL = OM [20]

 * Auxiliary Constructions:
K N : Points
IK = IF [21]
C,K,B are collinear [22]
JN = JF [23]
N,C,A are collinear [24]

 * Proof steps:
001. DA = DB [01]   DBA = BAD [25]
002. D,F,C are collinear [05] & DC = DF [06]   D is midpoint of FC [26]
003. G,F,B are collinear [07] & BF = BG [08]   B is midpoint of FG [27]
004. D is midpoint of FC [26] & B is midpoint of FG [27]   DB  CG [28]
005. F,A,H are collinear [09] & AF = AH [10]   A is midpoint of FH [29]
006. D is midpoint of FC [26] & A is midpoint of FH [29]   DA  CH [30]
007. B is midpoint of FG [27] & A is midpoint of FH [29]   BA  GH [31]
008. DBA = BAD [25] & BD  CG [28] & AD  CH [30] & AB  GH [31]   CGH = GHC [32]
009. CGH = GHC [32]   CG = CH [33]
010. DB = DC [02] & DC = DF [06]   D is the circumcenter of \Delta FBC [34]
011. DB = DC [02] & DC = DF [06]   DF = DB [35]
012. D is the circumcenter of \Delta FBC [34] & D,F,C are collinear [05]   BF  BC [36]
013. G,F,B are collinear [07] & E,A,B are collinear [03] & CE  AB [04] & BF  BC [36]   CBG = AEC [37]
014. DB = DC [02] & DA = DB [01] & DC = DF [06]   F,C,A,B are concyclic [38]
015. DB = DC [02] & DA = DB [01] & DC = DF [06]   D is the circumcenter of \Delta FAC [39]
016. DB = DC [02] & DA = DB [01] & DC = DF [06]   DF = DA [40]
017. F,C,A,B are concyclic [38]   FCA = FBA [41]
018. F,C,A,B are concyclic [38]   FCB = FAB [42]
019. F,C,A,B are concyclic [38]   FAC = FBC [43]
020. FCA = FBA [41] & D,F,C are collinear [05]   DCA = FBA [44]
021. DF = DB [35]   DFB = FBD [45]
022. DFB = FBD [45] & D,F,C are collinear [05] & BD  CG [28]   (BF-CD) = (CG-BF) [46]
023. G,F,B are collinear [07] & E,A,B are collinear [03] & FCA = FBA [41] & D,F,C are collinear [05] & (BF-CD) = (CG-BF) [46]   CGB = EAC [47]
024. CBG = AEC [37] & CGB = EAC [47] (Similar Triangles)  ACB = ECG [48]
025. D is the circumcenter of \Delta FAC [39] & D,F,C are collinear [05]   AF  AC [49]
026. F,A,H are collinear [09] & E,A,B are collinear [03] & CE  AB [04] & AF  AC [49]   CAH = BEC [50]
027. DF = DA [40]   DFA = FAD [51]
028. DFA = FAD [51] & D,F,C are collinear [05] & AD  CH [30]   (AF-CD) = (CH-AF) [52]
029. F,A,H are collinear [09] & E,A,B are collinear [03] & FCB = FAB [42] & D,F,C are collinear [05] & (AF-CD) = (CH-AF) [52]   CHA = EBC [53]
030. CAH = BEC [50] & CHA = EBC [53] (Similar Triangles)  ACB = HCE [54]
031. ACB = ECG [48] & ACB = HCE [54]   HCE = ECG [55]
032. CG = CH [33] & HCE = ECG [55] (SAS)  EH = EG [56]
033. CG = CH [33] & HCE = ECG [55] (SAS)  CHE = EGC [57]
034. EH = EG [56] & CG = CH [33]   GH  EC [58]
035. IE = IG [12] & IF = IE [11] & IK = IF [21]   G,F,E,K are concyclic [59]
036. IE = IG [12] & IF = IE [11] & IK = IF [21]   I is the circumcenter of \Delta KFG [60]
037. IK = IF [21] & IL = IF [15] & IF = IE [11] & G,F,E,K are concyclic [59]   G,K,L,F are concyclic [61]
038. IK = IF [21] & IL = IF [15] & IF = IE [11] & G,F,E,K are concyclic [59]   G,K,L,E are concyclic [62]
039. G,K,L,F are concyclic [61]   GLK = GFK [63]
040. G,K,L,F are concyclic [61]   GKL = GFL [64]
041. GLK = GFK [63] & L,C,B are collinear [16] & C,K,B are collinear [22] & G,F,B are collinear [07]   (GL-BC) = BFK [65]
042. G,K,L,E are concyclic [62]   GLK = GEK [66]
043. G,K,L,E are concyclic [62]   GKL = GEL [67]
044. GLK = GEK [66] & L,C,B are collinear [16] & C,K,B are collinear [22]   (GL-BC) = GEK [68]
045. GKL = GEL [67] & L,C,B are collinear [16] & C,K,B are collinear [22]   (GK-BC) = GEL [69]
046. G,F,E,K are concyclic [59]   GKF = GEF [70]
047. JE = JH [13] & JF = JE [14] & JN = JF [23] & JM = JF [17]   N,F,M,H are concyclic [71]
048. N,H,M,F are concyclic [71]   NMH = NFH [72]
049. N,H,M,F are concyclic [71]   NHM = NFM [73]
050. NMH = NFH [72] & N,C,A are collinear [24] & M,C,A are collinear [18] & F,A,H are collinear [09]   (AC-HM) = NFA [74]
051. JE = JH [13] & JF = JE [14] & JM = JF [17]   JM = JH [75]
052. N,F,M,H are concyclic [71] & JE = JH [13] & JF = JE [14] & JM = JF [17]   N,F,E,H are concyclic [76]
053. N,F,M,H are concyclic [71] & N,F,E,H are concyclic [76] & JE = JH [13] & JF = JE [14] & JM = JF [17]   N,H,M,E are concyclic [77]
054. N,H,M,E are concyclic [77]   NMH = NEH [78]
055. N,H,M,E are concyclic [77]   NHM = NEM [79]
056. NMH = NEH [78] & N,C,A are collinear [24] & M,C,A are collinear [18]   (AC-HM) = NEH [80]
057. N,F,E,H are concyclic [76]   NHF = NEF [81]
058. NHF = NEF [81] & F,A,H are collinear [09]   (HN-AF) = NEF [82]
059. IF = IE [11] & IK = IF [21]   IK = IE [83]
060. IK = IE [83]   IKE = KEI [84]
061. IK = IF [21]   IKF = KFI [85]
062. IF = IE [11]   IEF = EFI [86]
063. JM = JF [17]   JFM = FMJ [87]
064. JE = JH [13] & JF = JE [14]   JF = JH [88]
065. JF = JH [88]   JFH = FHJ [89]
066. JFH = FHJ [89] & F,A,H are collinear [09]   JFA = (AF-HJ) [90]
067. JM = JH [75]   JMH = MHJ [91]
068. OM = OE [19]   OME = MEO [92]
069. OL = OM [20]   OML = MLO [93]
070. OL = OM [20] & OM = OE [19]   OE = OL [94]
071. OE = OL [94]   OEL = ELO [95]
072. IF = IE [11] & JF = JE [14]   EF  IJ [96]
073. IE = IG [12] & IF = IE [11]   IF = IG [97]
074. BF = BG [08] & IF = IG [97]   GF  BI [98]
075. IKE = KEI [84] & GF  BI [98] & BF  BC [36] & G,F,B are collinear [07] & C,K,B are collinear [22]   (BC-EK) = KEI [99]
076. AF = AH [10] & JF = JH [88]   HF  AJ [100]
077. JFM = FMJ [87] & HF  AJ [100] & AF  AC [49] & F,A,H are collinear [09] & M,C,A are collinear [18]   JFM = (FM-AC) [101]
078. CE  AB [04] & FAC = FBC [43] & DCA = FBA [44] & (GL-BC) = BFK [65] & (GL-BC) = GEK [68] & (GK-BC) = GEL [69] & GKF = GEF [70] & (AC-HM) = NFA [74] & (AC-HM) = NEH [80] & NHM = NFM [73] & NHM = NEM [79] & (HN-AF) = NEF [82] & (CD-BF) = (BF-CG) [46] & (CD-AF) = (AF-CH) [52] & IKE = KEI [84] & IKF = KFI [85] & IEF = EFI [86] & JFM = FMJ [87] & JFA = (AF-HJ) [90] & JMH = MHJ [91] & OME = MEO [92] & OML = MLO [93] & OEL = ELO [95] & EF  IJ [96] & (BC-EK) = KEI [99] & JFM = (FM-AC) [101] & CHE = EGC [57] (Angle chase)  (CE-LM) = (EO-IJ) [102]
079. JF = JE [14] & JM = JF [17]   JM = JE [103]
080. JM = JE [103] & OM = OE [19]   EM  JO [104]
081. CE  AB [04] & EM  JO [104] (Angle chase)  (AB-CE) = (JO-EM) [105]
082. MJ = EJ [103] & JN = JF [23] & JM = JF [17]   J is the circumcenter of \Delta MEN [106]
083. HF  AJ [100] & AF  AC [49] & F,A,H are collinear [09] & N,C,A are collinear [24] & M,C,A are collinear [18]   J,M,N are collinear [107]
084. J is the circumcenter of \Delta MEN [106] & J,M,N are collinear [107]   ME  EN [108]
085. E,A,B are collinear [03] & (AB-CE) = (JO-EM) [105] & ME  EN [108] & EM  JO [104]   MEN = CEA [109]
086. OM = OL [20] & EH = EG [56]   OM:OL = EH:EG [110]
087. OM = OL [20] & EH = EG [56]   OM:OL = EG:EH [111]
088. FAC = FBC [43] & (GL-BC) = BFK [65] & (GL-BC) = GEK [68] & (GK-BC) = GEL [69] & GKF = GEF [70] & (AC-HM) = NFA [74] & (AC-HM) = NEH [80] & NHM = NFM [73] & NHM = NEM [79] & (HN-AF) = NEF [82] & IKE = KEI [84] & IKF = KFI [85] & IEF = EFI [86] & JFM = FMJ [87] & JFA = (AF-HJ) [90] & JMH = MHJ [91] & OME = MEO [92] & OEL = ELO [95] & (BC-EK) = KEI [99] & JFM = (FM-AC) [101] (Angle chase)  GEH = LOM [112]
089. OM:OL = EH:EG [110] & GEH = LOM [112] (Similar Triangles)  MLO = HGE [113]
090. (AC-HM) = NFA [74] & NHM = NFM [73] & NHM = NEM [79] & JFM = FMJ [87] & JFA = (AF-HJ) [90] & JMH = MHJ [91] & OME = MEO [92] & OML = MLO [93] & OEL = ELO [95] & JFM = (FM-AC) [101] (Angle chase)  OLM = LEN [114]
091. E,A,B are collinear [03] & MLO = HGE [113] & AB  GH [31] & OLM = LEN [114] & ME  EN [108] & EM  JO [104]   NEL = AEG [115]
092. MEN = CEA [109] & NEL = AEG [115]   CEM = GEL [116]
093. I is the circumcenter of \Delta KFG [60] & B is midpoint of FG [27]   IFK = (BI-GK) [117]
094. C,K,B are collinear [22] & IFK = (BI-GK) [117] & IKF = KFI [85] & GF  BI [98] & BF  BC [36] & G,F,B are collinear [07]   FKC = CKG [118]
095. C,K,B are collinear [22] & GKL = GEL [67] & L,C,B are collinear [16]   (CK-LE) = KGE [119]
096. FKC = CKG [118] & (CK-LE) = KGE [119]   (FK-LE) = (CK-GE) [120]
097. C,K,B are collinear [22] & CEM = GEL [116] & (FK-LE) = (CK-GE) [120]   CKF = CEM [121]
098. E,A,B are collinear [03] & C,K,B are collinear [22] & G,F,B are collinear [07] & CE  AB [04] & BF  BC [36]   CEA = (CK-GF) [122]
099. E,A,B are collinear [03] & N,C,A are collinear [24] & G,F,B are collinear [07] & D,F,C are collinear [05] & FCA = FBA [41]   (EA-NC) = GFD [123]
100. CEA = (CK-GF) [122] & (EA-NC) = GFD [123]   ECN = (CK-DF) [124]
101. D,F,C are collinear [05] & C,K,B are collinear [22] & M,C,A are collinear [18] & ECN = (CK-DF) [124] & N,C,A are collinear [24]   FCK = MCE [125]
102. CKF = CEM [121] & FCK = MCE [125] (Similar Triangles)  FK:FC = ME:MC [126]
103. CKF = CEM [121] & FCK = MCE [125] (Similar Triangles)  KF:KC = EM:EC [127]
104. IL = IF [15] (SSS)  ILF = LFI [128]
105. IL = IF [15] & IE = IG [12] & IF = IE [11]   I is the circumcenter of \Delta LFG [129]
106. I is the circumcenter of \Delta LFG [129] & B is midpoint of FG [27]   LFI = (GL-BI) [130]
107. L,C,B are collinear [16] & C,K,B are collinear [22] & G,F,B are collinear [07] & GKL = GFL [64] & ILF = LFI [128] & GF  BI [98] & BF  BC [36] & LFI = (GL-BI) [130]   GLK = KGF [131]
108. G,K,L,F are concyclic [61] & GLK = KGF [131]   GK = KF [132]
109. FK:FC = ME:MC [126] & GK = KF [132]   GK:FC = ME:MC [133]
110. KF:KC = EM:EC [127] & GK = KF [132]   GK:CK = ME:EC [134]
111. C,K,B are collinear [22] & ILF = LFI [128] & GF  BI [98] & BF  BC [36] & G,F,B are collinear [07] & L,C,B are collinear [16] & LFI = (GL-BI) [130]   (LG-CK) = (CK-LF) [135]
112. C,K,B are collinear [22] & GLK = GEK [66] & L,C,B are collinear [16]   LGE = CKE [136]
113. (LG-CK) = (CK-LF) [135] & LGE = CKE [136]   (CK-GE) = (LF-EK) [137]
114. CE  AB [04] & (AC-HM) = NFA [74] & NHM = NFM [73] & NHM = NEM [79] & JFM = FMJ [87] & JFA = (AF-HJ) [90] & JMH = MHJ [91] & JFM = (FM-AC) [101] (Angle chase)  (CE-AB) = NEM [138]
115. E,A,B are collinear [03] & NEM = (CE-AB) [138]   NEM = CEA [139]
116. OM:OL = EG:EH [111] & GEH = LOM [112] (Similar Triangles)  OML = HGE [140]
117. IL = IF [15] & IF = IE [11]   IE = IL [141]
118. IE = IL [141] & OE = OL [94]   EL  IO [142]
119. OME = MEO [92] & OML = MLO [93] & OEL = ELO [95] & EL  IO [142] (Angle chase)  (IO-EM) = LMO [143]
120. E,A,B are collinear [03] & OML = HGE [140] & AB  GH [31] & (EM-IO) = OML [143]   (ME-OI) = AEG [144]
121. NEM = CEA [139] & (ME-OI) = AEG [144]   NEC = (IO-EG) [145]
122. KI = EI [83] & IL = IF [15] & IK = IF [21]   I is the circumcenter of \Delta KEL [146]
123. L,C,B are collinear [16] & C,K,B are collinear [22] & GF  BI [98] & BF  BC [36] & G,F,B are collinear [07]   I,K,L are collinear [147]
124. I is the circumcenter of \Delta KEL [146] & I,K,L are collinear [147]   EL  KE [148]
125. L,C,B are collinear [16] & (CK-GE) = (LF-EK) [137] & C,K,B are collinear [22] & NEC = (IO-EG) [145] & ME  EN [108] & EM  JO [104] & EL  KE [148] & EL  IO [142]   NEC = FLC [149]
126. N,C,A are collinear [24] & D,F,C are collinear [05] & L,C,B are collinear [16] & ECN = (CK-DF) [124] & C,K,B are collinear [22]   NCE = FCL [150]
127. NEC = FLC [149] & NCE = FCL [150] (Similar Triangles)  NE:NC = FL:FC [151]
128. NEC = FLC [149] & NCE = FCL [150] (Similar Triangles)  EN:EC = LF:LC [152]
129. L,C,B are collinear [16] & C,K,B are collinear [22] & G,F,B are collinear [07] & GLK = GFK [63] & IKF = KFI [85] & GF  BI [98] & BF  BC [36] & KFI = (GK-BI) [117]   GKL = LGF [153]
130. G,K,L,F are concyclic [61] & GKL = LGF [153]   GL = LF [154]
131. NE:NC = FL:FC [151] & GL = LF [154]   NE:NC = LG:FC [155]
132. EN:EC = LF:LC [152] & GL = LF [154]   NE:EC = LG:LC [156]
133. GK:FC = ME:MC [133] & GK:CK = ME:EC [134] & NE:NC = LG:FC [155] & NE:EC = LG:LC [156] (Ratio chase)  MC:LC = NC:CK [157]
134. M,C,A are collinear [18] & L,C,B are collinear [16] & N,C,A are collinear [24] & C,K,B are collinear [22] & ACB = ACB [00]   MCL = NCK [158]
135. MC:LC = NC:CK [157] & MCL = NCK [158] (Similar Triangles)  CML = CNK [159]
136. CML = CNK [159] & M,C,A are collinear [18] & N,C,A are collinear [24]   LM  KN [160]
137. IL = IF [15] & IK = IF [21] & JN = JF [23] & JM = JF [17]   IK:IL = JN:JM [161]
138. LM  KN [160] & I,K,L are collinear [147] & J,M,N are collinear [107] & IK:IL = JN:JM [161]   IJ  KN [162]
139. (EO-IJ) = (CE-LM) [102] & LM  KN [160] & IJ  KN [162]   (OE-JI) = (EC-JI) [163]
140. (OE-JI) = (EC-JI) [163]   OE  EC [164]
141. E,A,B are collinear [03] & GH  EC [58] & CE  AB [04] & CE  EO [164] & AB  GH [31]   BE  EO
==========================

