I0123 15:01:35.462768 139864642547712 inference_utils.py:69] Parsing gin configuration.
I0123 15:01:35.462868 139864642547712 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0123 15:01:35.463069 139864642547712 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0123 15:01:35.463102 139864642547712 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0123 15:01:35.463132 139864642547712 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0123 15:01:35.463159 139864642547712 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0123 15:01:35.463186 139864642547712 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0123 15:01:35.463212 139864642547712 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0123 15:01:35.463239 139864642547712 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0123 15:01:35.463265 139864642547712 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=32
I0123 15:01:35.463291 139864642547712 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0123 15:01:35.463317 139864642547712 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0123 15:01:35.463361 139864642547712 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0123 15:01:35.463493 139864642547712 resource_reader.py:55] Path not found: base_htrans.gin
I0123 15:01:35.463697 139864642547712 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0123 15:01:35.463795 139864642547712 resource_reader.py:55] Path not found: trainer_configuration.gin
I0123 15:01:35.470155 139864642547712 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0123 15:01:35.470272 139864642547712 resource_reader.py:55] Path not found: size/medium_150M.gin
I0123 15:01:35.470597 139864642547712 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0123 15:01:35.470702 139864642547712 resource_reader.py:55] Path not found: options/positions_t5.gin
I0123 15:01:35.470983 139864642547712 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0123 15:01:35.471082 139864642547712 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0123 15:01:35.471491 139864642547712 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0123 15:01:35.471591 139864642547712 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0123 15:01:35.475204 139864642547712 training_loop.py:334] ==== Training loop: initializing model ====
I0123 15:01:35.578204 139864642547712 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0123 15:01:35.578942 139864642547712 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0123 15:01:35.585525 139864642547712 training_loop.py:335] Process 0 of 1
I0123 15:01:35.585580 139864642547712 training_loop.py:336] Local device count = 1
I0123 15:01:35.585622 139864642547712 training_loop.py:337] Number of replicas = 1
I0123 15:01:35.585666 139864642547712 training_loop.py:339] Using random number seed 42
I0123 15:01:36.076452 139864642547712 training_loop.py:359] Initializing the model.
I0123 15:01:36.450324 139864642547712 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.450567 139864642547712 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 15:01:36.450676 139864642547712 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 15:01:36.450760 139864642547712 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 15:01:36.450840 139864642547712 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 15:01:36.450927 139864642547712 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 15:01:36.451004 139864642547712 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 15:01:36.451257 139864642547712 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 15:01:36.451330 139864642547712 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 15:01:36.451403 139864642547712 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 15:01:36.451474 139864642547712 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 15:01:36.451546 139864642547712 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 15:01:36.451617 139864642547712 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 15:01:36.451689 139864642547712 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 15:01:36.451729 139864642547712 attention.py:418] Single window, no scan.
I0123 15:01:36.451776 139864642547712 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 15:01:36.451896 139864642547712 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:01:36.451937 139864642547712 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:01:36.451967 139864642547712 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:01:36.453994 139864642547712 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.459364 139864642547712 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:01:36.470143 139864642547712 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.470426 139864642547712 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:01:36.474849 139864642547712 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:01:36.485528 139864642547712 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:01:36.485587 139864642547712 attention.py:418] Single window, no scan.
I0123 15:01:36.485625 139864642547712 transformer_layer.py:389] tlayer: self-attention.
I0123 15:01:36.485669 139864642547712 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.485734 139864642547712 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.486933 139864642547712 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.487012 139864642547712 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.487740 139864642547712 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.490228 139864642547712 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.495995 139864642547712 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.497735 139864642547712 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.497816 139864642547712 transformer_layer.py:468] tlayer: End windows.
I0123 15:01:36.497851 139864642547712 transformer_layer.py:472] tlayer: final FFN.
I0123 15:01:36.497913 139864642547712 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.498046 139864642547712 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:01:36.498379 139864642547712 nn_components.py:325] mlp: activation = None
I0123 15:01:36.498428 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:01:36.500377 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.500478 139864642547712 nn_components.py:261] mlp: residual
I0123 15:01:36.503395 139864642547712 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.503479 139864642547712 transformer_base.py:443] tbase: final FFN
I0123 15:01:36.503983 139864642547712 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:01:36.514290 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:01:36.523226 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.523325 139864642547712 nn_components.py:261] mlp: residual
I0123 15:01:36.523625 139864642547712 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.523706 139864642547712 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 15:01:36.523817 139864642547712 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:01:36.523857 139864642547712 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:01:36.523888 139864642547712 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:01:36.525740 139864642547712 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.528240 139864642547712 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:01:36.533880 139864642547712 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.534140 139864642547712 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:01:36.536795 139864642547712 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:01:36.540618 139864642547712 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:01:36.540673 139864642547712 attention.py:418] Single window, no scan.
I0123 15:01:36.540710 139864642547712 transformer_layer.py:389] tlayer: self-attention.
I0123 15:01:36.540741 139864642547712 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.540802 139864642547712 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.541376 139864642547712 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.541456 139864642547712 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.541829 139864642547712 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.542615 139864642547712 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.545146 139864642547712 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.545779 139864642547712 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.545856 139864642547712 transformer_layer.py:468] tlayer: End windows.
I0123 15:01:36.545891 139864642547712 transformer_layer.py:472] tlayer: final FFN.
I0123 15:01:36.545950 139864642547712 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.546080 139864642547712 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:01:36.546405 139864642547712 nn_components.py:325] mlp: activation = None
I0123 15:01:36.546449 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:01:36.548397 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.548493 139864642547712 nn_components.py:261] mlp: residual
I0123 15:01:36.551020 139864642547712 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.551102 139864642547712 transformer_base.py:443] tbase: final FFN
I0123 15:01:36.551529 139864642547712 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:01:36.553841 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:01:36.555751 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.555845 139864642547712 nn_components.py:261] mlp: residual
I0123 15:01:36.556139 139864642547712 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.556220 139864642547712 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 15:01:36.556330 139864642547712 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:01:36.556370 139864642547712 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:01:36.556401 139864642547712 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:01:36.558300 139864642547712 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.560666 139864642547712 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:01:36.566757 139864642547712 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.567029 139864642547712 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:01:36.569746 139864642547712 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:01:36.573542 139864642547712 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:01:36.573597 139864642547712 attention.py:418] Single window, no scan.
I0123 15:01:36.573634 139864642547712 transformer_layer.py:389] tlayer: self-attention.
I0123 15:01:36.573673 139864642547712 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.573738 139864642547712 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.574297 139864642547712 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.574372 139864642547712 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.574735 139864642547712 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.575494 139864642547712 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.578001 139864642547712 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.578672 139864642547712 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.578748 139864642547712 transformer_layer.py:468] tlayer: End windows.
I0123 15:01:36.578783 139864642547712 transformer_layer.py:472] tlayer: final FFN.
I0123 15:01:36.578842 139864642547712 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.578969 139864642547712 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:01:36.579288 139864642547712 nn_components.py:325] mlp: activation = None
I0123 15:01:36.579333 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:01:36.581249 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.581342 139864642547712 nn_components.py:261] mlp: residual
I0123 15:01:36.583876 139864642547712 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.583963 139864642547712 transformer_base.py:443] tbase: final FFN
I0123 15:01:36.584442 139864642547712 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:01:36.586730 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:01:36.588656 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.588751 139864642547712 nn_components.py:261] mlp: residual
I0123 15:01:36.589045 139864642547712 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.589124 139864642547712 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 15:01:36.589234 139864642547712 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:01:36.589273 139864642547712 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:01:36.589303 139864642547712 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:01:36.591205 139864642547712 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.593604 139864642547712 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:01:36.599234 139864642547712 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.599499 139864642547712 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:01:36.602157 139864642547712 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:01:36.605948 139864642547712 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:01:36.606004 139864642547712 attention.py:418] Single window, no scan.
I0123 15:01:36.606041 139864642547712 transformer_layer.py:389] tlayer: self-attention.
I0123 15:01:36.606072 139864642547712 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.606132 139864642547712 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.606696 139864642547712 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.606771 139864642547712 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.607142 139864642547712 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.607910 139864642547712 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.610581 139864642547712 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.611202 139864642547712 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.611278 139864642547712 transformer_layer.py:468] tlayer: End windows.
I0123 15:01:36.611313 139864642547712 transformer_layer.py:472] tlayer: final FFN.
I0123 15:01:36.611371 139864642547712 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.611500 139864642547712 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:01:36.611826 139864642547712 nn_components.py:325] mlp: activation = None
I0123 15:01:36.611869 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:01:36.613804 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.613897 139864642547712 nn_components.py:261] mlp: residual
I0123 15:01:36.616501 139864642547712 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.616586 139864642547712 transformer_base.py:443] tbase: final FFN
I0123 15:01:36.617020 139864642547712 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:01:36.619326 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:01:36.621254 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.621348 139864642547712 nn_components.py:261] mlp: residual
I0123 15:01:36.621649 139864642547712 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.621730 139864642547712 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 15:01:36.621839 139864642547712 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:01:36.621878 139864642547712 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:01:36.621908 139864642547712 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:01:36.623812 139864642547712 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.626234 139864642547712 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:01:36.632012 139864642547712 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.632281 139864642547712 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:01:36.635113 139864642547712 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:01:36.638900 139864642547712 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:01:36.638956 139864642547712 attention.py:418] Single window, no scan.
I0123 15:01:36.638992 139864642547712 transformer_layer.py:389] tlayer: self-attention.
I0123 15:01:36.639023 139864642547712 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.639090 139864642547712 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.639667 139864642547712 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.639743 139864642547712 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.640110 139864642547712 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.640886 139864642547712 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.643803 139864642547712 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.644435 139864642547712 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.644514 139864642547712 transformer_layer.py:468] tlayer: End windows.
I0123 15:01:36.644548 139864642547712 transformer_layer.py:472] tlayer: final FFN.
I0123 15:01:36.644608 139864642547712 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.644746 139864642547712 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:01:36.645080 139864642547712 nn_components.py:325] mlp: activation = None
I0123 15:01:36.645123 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:01:36.647079 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.647179 139864642547712 nn_components.py:261] mlp: residual
I0123 15:01:36.649805 139864642547712 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.649885 139864642547712 transformer_base.py:443] tbase: final FFN
I0123 15:01:36.650322 139864642547712 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:01:36.652621 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:01:36.654605 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.654704 139864642547712 nn_components.py:261] mlp: residual
I0123 15:01:36.654998 139864642547712 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.655080 139864642547712 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 15:01:36.655193 139864642547712 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:01:36.655234 139864642547712 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:01:36.655266 139864642547712 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:01:36.657137 139864642547712 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.659569 139864642547712 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:01:36.665254 139864642547712 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.665512 139864642547712 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:01:36.668217 139864642547712 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:01:36.672739 139864642547712 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:01:36.672845 139864642547712 attention.py:418] Single window, no scan.
I0123 15:01:36.672882 139864642547712 transformer_layer.py:389] tlayer: self-attention.
I0123 15:01:36.672913 139864642547712 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.672984 139864642547712 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.673612 139864642547712 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.673704 139864642547712 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.674078 139864642547712 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.674878 139864642547712 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.677423 139864642547712 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.678056 139864642547712 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.678135 139864642547712 transformer_layer.py:468] tlayer: End windows.
I0123 15:01:36.678170 139864642547712 transformer_layer.py:472] tlayer: final FFN.
I0123 15:01:36.678231 139864642547712 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.678366 139864642547712 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:01:36.678709 139864642547712 nn_components.py:325] mlp: activation = None
I0123 15:01:36.678753 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:01:36.680686 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.680781 139864642547712 nn_components.py:261] mlp: residual
I0123 15:01:36.683388 139864642547712 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.683468 139864642547712 transformer_base.py:443] tbase: final FFN
I0123 15:01:36.683903 139864642547712 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:01:36.686245 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:01:36.688198 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.688298 139864642547712 nn_components.py:261] mlp: residual
I0123 15:01:36.688597 139864642547712 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.688679 139864642547712 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 15:01:36.688791 139864642547712 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:01:36.688830 139864642547712 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:01:36.688860 139864642547712 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:01:36.690731 139864642547712 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.693206 139864642547712 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:01:36.698894 139864642547712 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.699158 139864642547712 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:01:36.701834 139864642547712 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:01:36.705675 139864642547712 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:01:36.705730 139864642547712 attention.py:418] Single window, no scan.
I0123 15:01:36.705766 139864642547712 transformer_layer.py:389] tlayer: self-attention.
I0123 15:01:36.705797 139864642547712 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.705859 139864642547712 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.706423 139864642547712 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.706500 139864642547712 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.706864 139864642547712 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.707644 139864642547712 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.710427 139864642547712 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.711056 139864642547712 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.711134 139864642547712 transformer_layer.py:468] tlayer: End windows.
I0123 15:01:36.711170 139864642547712 transformer_layer.py:472] tlayer: final FFN.
I0123 15:01:36.711230 139864642547712 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.711361 139864642547712 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:01:36.711683 139864642547712 nn_components.py:325] mlp: activation = None
I0123 15:01:36.711727 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:01:36.713713 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.713807 139864642547712 nn_components.py:261] mlp: residual
I0123 15:01:36.716331 139864642547712 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.716410 139864642547712 transformer_base.py:443] tbase: final FFN
I0123 15:01:36.716841 139864642547712 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:01:36.719509 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:01:36.721421 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.721522 139864642547712 nn_components.py:261] mlp: residual
I0123 15:01:36.721825 139864642547712 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.721907 139864642547712 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 15:01:36.722018 139864642547712 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:01:36.722058 139864642547712 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:01:36.722089 139864642547712 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:01:36.863547 139864642547712 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.866709 139864642547712 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:01:36.872771 139864642547712 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.873079 139864642547712 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:01:36.875837 139864642547712 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:01:36.879885 139864642547712 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:01:36.879948 139864642547712 attention.py:418] Single window, no scan.
I0123 15:01:36.879987 139864642547712 transformer_layer.py:389] tlayer: self-attention.
I0123 15:01:36.880020 139864642547712 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.880086 139864642547712 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.880701 139864642547712 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.880778 139864642547712 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.881152 139864642547712 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.881955 139864642547712 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.884583 139864642547712 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.885231 139864642547712 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.885313 139864642547712 transformer_layer.py:468] tlayer: End windows.
I0123 15:01:36.885350 139864642547712 transformer_layer.py:472] tlayer: final FFN.
I0123 15:01:36.885417 139864642547712 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.885554 139864642547712 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:01:36.885915 139864642547712 nn_components.py:325] mlp: activation = None
I0123 15:01:36.885963 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:01:36.887960 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.888056 139864642547712 nn_components.py:261] mlp: residual
I0123 15:01:36.890660 139864642547712 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.890740 139864642547712 transformer_base.py:443] tbase: final FFN
I0123 15:01:36.891187 139864642547712 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:01:36.893540 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:01:36.895492 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.895599 139864642547712 nn_components.py:261] mlp: residual
I0123 15:01:36.895898 139864642547712 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.895984 139864642547712 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 15:01:36.896100 139864642547712 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:01:36.896140 139864642547712 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:01:36.896171 139864642547712 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:01:36.898153 139864642547712 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.900584 139864642547712 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:01:36.906300 139864642547712 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.906563 139864642547712 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:01:36.909291 139864642547712 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:01:36.913092 139864642547712 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:01:36.913148 139864642547712 attention.py:418] Single window, no scan.
I0123 15:01:36.913184 139864642547712 transformer_layer.py:389] tlayer: self-attention.
I0123 15:01:36.913214 139864642547712 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.913275 139864642547712 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.913849 139864642547712 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.913927 139864642547712 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.914297 139864642547712 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.915077 139864642547712 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.917678 139864642547712 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.918311 139864642547712 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.918390 139864642547712 transformer_layer.py:468] tlayer: End windows.
I0123 15:01:36.918426 139864642547712 transformer_layer.py:472] tlayer: final FFN.
I0123 15:01:36.918486 139864642547712 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.918614 139864642547712 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:01:36.918941 139864642547712 nn_components.py:325] mlp: activation = None
I0123 15:01:36.918984 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:01:36.920905 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.921000 139864642547712 nn_components.py:261] mlp: residual
I0123 15:01:36.923579 139864642547712 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.923662 139864642547712 transformer_base.py:443] tbase: final FFN
I0123 15:01:36.924090 139864642547712 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:01:36.926405 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:01:36.928396 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.928492 139864642547712 nn_components.py:261] mlp: residual
I0123 15:01:36.928786 139864642547712 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.928875 139864642547712 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 15:01:36.928988 139864642547712 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:01:36.929028 139864642547712 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:01:36.929059 139864642547712 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:01:36.930933 139864642547712 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.933395 139864642547712 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:01:36.939018 139864642547712 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.939281 139864642547712 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:01:36.942368 139864642547712 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:01:36.946192 139864642547712 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:01:36.946251 139864642547712 attention.py:418] Single window, no scan.
I0123 15:01:36.946290 139864642547712 transformer_layer.py:389] tlayer: self-attention.
I0123 15:01:36.946322 139864642547712 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.946387 139864642547712 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.947017 139864642547712 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.947098 139864642547712 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.947486 139864642547712 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.948289 139864642547712 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.950893 139864642547712 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.951533 139864642547712 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.951611 139864642547712 transformer_layer.py:468] tlayer: End windows.
I0123 15:01:36.951646 139864642547712 transformer_layer.py:472] tlayer: final FFN.
I0123 15:01:36.951705 139864642547712 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.951833 139864642547712 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:01:36.952157 139864642547712 nn_components.py:325] mlp: activation = None
I0123 15:01:36.952201 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:01:36.954338 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.954433 139864642547712 nn_components.py:261] mlp: residual
I0123 15:01:36.956997 139864642547712 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.957077 139864642547712 transformer_base.py:443] tbase: final FFN
I0123 15:01:36.957512 139864642547712 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:01:36.959862 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:01:36.961784 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.961880 139864642547712 nn_components.py:261] mlp: residual
I0123 15:01:36.962180 139864642547712 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.962269 139864642547712 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 15:01:36.962384 139864642547712 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:01:36.962424 139864642547712 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:01:36.962454 139864642547712 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:01:36.964315 139864642547712 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.966797 139864642547712 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:01:36.972471 139864642547712 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.972738 139864642547712 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:01:36.975424 139864642547712 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:01:36.979220 139864642547712 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:01:36.979276 139864642547712 attention.py:418] Single window, no scan.
I0123 15:01:36.979313 139864642547712 transformer_layer.py:389] tlayer: self-attention.
I0123 15:01:36.979346 139864642547712 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.979408 139864642547712 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.979982 139864642547712 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.980059 139864642547712 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.980425 139864642547712 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.981205 139864642547712 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.983710 139864642547712 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.984347 139864642547712 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.984424 139864642547712 transformer_layer.py:468] tlayer: End windows.
I0123 15:01:36.984459 139864642547712 transformer_layer.py:472] tlayer: final FFN.
I0123 15:01:36.984519 139864642547712 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.984644 139864642547712 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:01:36.984963 139864642547712 nn_components.py:325] mlp: activation = None
I0123 15:01:36.985006 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:01:36.986985 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.987081 139864642547712 nn_components.py:261] mlp: residual
I0123 15:01:36.989890 139864642547712 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.989970 139864642547712 transformer_base.py:443] tbase: final FFN
I0123 15:01:36.990398 139864642547712 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:01:36.992744 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:01:36.994676 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.994772 139864642547712 nn_components.py:261] mlp: residual
I0123 15:01:36.995069 139864642547712 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.995152 139864642547712 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 15:01:36.995269 139864642547712 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:01:36.995310 139864642547712 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:01:36.995342 139864642547712 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:01:36.997255 139864642547712 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:36.999651 139864642547712 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:01:37.005309 139864642547712 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:37.005567 139864642547712 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:01:37.008231 139864642547712 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:01:37.012050 139864642547712 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:01:37.012105 139864642547712 attention.py:418] Single window, no scan.
I0123 15:01:37.012141 139864642547712 transformer_layer.py:389] tlayer: self-attention.
I0123 15:01:37.012172 139864642547712 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:37.012234 139864642547712 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:37.012795 139864642547712 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:37.012872 139864642547712 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:37.013236 139864642547712 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:37.014011 139864642547712 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:37.016510 139864642547712 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:37.017496 139864642547712 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:37.017575 139864642547712 transformer_layer.py:468] tlayer: End windows.
I0123 15:01:37.017611 139864642547712 transformer_layer.py:472] tlayer: final FFN.
I0123 15:01:37.017684 139864642547712 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:37.017816 139864642547712 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:01:37.018145 139864642547712 nn_components.py:325] mlp: activation = None
I0123 15:01:37.018190 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:01:37.020091 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:37.020186 139864642547712 nn_components.py:261] mlp: residual
I0123 15:01:37.022708 139864642547712 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:37.022792 139864642547712 transformer_base.py:443] tbase: final FFN
I0123 15:01:37.023271 139864642547712 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:01:37.025541 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:01:37.027471 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:37.027568 139864642547712 nn_components.py:261] mlp: residual
I0123 15:01:37.027863 139864642547712 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:37.028144 139864642547712 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 15:01:37.028220 139864642547712 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 15:01:37.028288 139864642547712 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 15:01:37.028348 139864642547712 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 15:01:37.028404 139864642547712 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 15:01:37.028460 139864642547712 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 15:01:37.028514 139864642547712 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 15:01:37.028568 139864642547712 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 15:01:37.028620 139864642547712 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 15:01:37.028674 139864642547712 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 15:01:37.028726 139864642547712 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 15:01:37.028779 139864642547712 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 15:01:37.028817 139864642547712 decoder_stack.py:344] dstack: Final layernorm.
I0123 15:01:37.032379 139864642547712 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:01:37.080498 139864642547712 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.080585 139864642547712 decoder_stack.py:333] dstack: autoregressive generator.
I0123 15:01:37.080639 139864642547712 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 15:01:37.080744 139864642547712 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:01:37.080783 139864642547712 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:01:37.080813 139864642547712 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:01:37.080877 139864642547712 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.083344 139864642547712 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:01:37.088891 139864642547712 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.089154 139864642547712 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:01:37.091827 139864642547712 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:01:37.108404 139864642547712 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:01:37.108461 139864642547712 attention.py:418] Single window, no scan.
I0123 15:01:37.108497 139864642547712 transformer_layer.py:389] tlayer: self-attention.
I0123 15:01:37.108528 139864642547712 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.108590 139864642547712 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.109732 139864642547712 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.109811 139864642547712 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.110528 139864642547712 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.112545 139864642547712 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.117346 139864642547712 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.118678 139864642547712 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.118767 139864642547712 transformer_layer.py:468] tlayer: End windows.
I0123 15:01:37.118803 139864642547712 transformer_layer.py:472] tlayer: final FFN.
I0123 15:01:37.118866 139864642547712 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.118998 139864642547712 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:01:37.119114 139864642547712 nn_components.py:325] mlp: activation = None
I0123 15:01:37.119154 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:01:37.121067 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.121162 139864642547712 nn_components.py:261] mlp: residual
I0123 15:01:37.123633 139864642547712 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.123714 139864642547712 transformer_base.py:443] tbase: final FFN
I0123 15:01:37.123824 139864642547712 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:01:37.126077 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:01:37.128034 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.128131 139864642547712 nn_components.py:261] mlp: residual
I0123 15:01:37.128425 139864642547712 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.128508 139864642547712 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 15:01:37.128618 139864642547712 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:01:37.128658 139864642547712 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:01:37.128689 139864642547712 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:01:37.128755 139864642547712 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.131052 139864642547712 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:01:37.136595 139864642547712 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.136856 139864642547712 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:01:37.139572 139864642547712 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:01:37.152714 139864642547712 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:01:37.152772 139864642547712 attention.py:418] Single window, no scan.
I0123 15:01:37.152808 139864642547712 transformer_layer.py:389] tlayer: self-attention.
I0123 15:01:37.152839 139864642547712 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.152901 139864642547712 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.153472 139864642547712 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.153548 139864642547712 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.153916 139864642547712 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.154622 139864642547712 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.157138 139864642547712 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.157766 139864642547712 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.157845 139864642547712 transformer_layer.py:468] tlayer: End windows.
I0123 15:01:37.157885 139864642547712 transformer_layer.py:472] tlayer: final FFN.
I0123 15:01:37.157946 139864642547712 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.158078 139864642547712 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:01:37.158191 139864642547712 nn_components.py:325] mlp: activation = None
I0123 15:01:37.158230 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:01:37.160165 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.160259 139864642547712 nn_components.py:261] mlp: residual
I0123 15:01:37.162703 139864642547712 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.162785 139864642547712 transformer_base.py:443] tbase: final FFN
I0123 15:01:37.162893 139864642547712 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:01:37.165129 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:01:37.167070 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.167168 139864642547712 nn_components.py:261] mlp: residual
I0123 15:01:37.167461 139864642547712 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.167541 139864642547712 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 15:01:37.167652 139864642547712 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:01:37.167692 139864642547712 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:01:37.167723 139864642547712 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:01:37.167788 139864642547712 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.170063 139864642547712 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:01:37.175587 139864642547712 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.175852 139864642547712 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:01:37.178591 139864642547712 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:01:37.191334 139864642547712 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:01:37.191390 139864642547712 attention.py:418] Single window, no scan.
I0123 15:01:37.191426 139864642547712 transformer_layer.py:389] tlayer: self-attention.
I0123 15:01:37.191456 139864642547712 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.191517 139864642547712 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.192076 139864642547712 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.192153 139864642547712 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.192507 139864642547712 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.193208 139864642547712 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.195713 139864642547712 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.196347 139864642547712 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.196424 139864642547712 transformer_layer.py:468] tlayer: End windows.
I0123 15:01:37.196459 139864642547712 transformer_layer.py:472] tlayer: final FFN.
I0123 15:01:37.196523 139864642547712 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.196653 139864642547712 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:01:37.196763 139864642547712 nn_components.py:325] mlp: activation = None
I0123 15:01:37.196803 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:01:37.198760 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.198856 139864642547712 nn_components.py:261] mlp: residual
I0123 15:01:37.201316 139864642547712 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.201396 139864642547712 transformer_base.py:443] tbase: final FFN
I0123 15:01:37.201506 139864642547712 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:01:37.203779 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:01:37.205706 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.205802 139864642547712 nn_components.py:261] mlp: residual
I0123 15:01:37.206093 139864642547712 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.206172 139864642547712 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 15:01:37.206280 139864642547712 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:01:37.206320 139864642547712 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:01:37.206350 139864642547712 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:01:37.206413 139864642547712 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.208666 139864642547712 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:01:37.214221 139864642547712 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.214483 139864642547712 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:01:37.217191 139864642547712 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:01:37.230016 139864642547712 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:01:37.230073 139864642547712 attention.py:418] Single window, no scan.
I0123 15:01:37.230110 139864642547712 transformer_layer.py:389] tlayer: self-attention.
I0123 15:01:37.230141 139864642547712 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.230201 139864642547712 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.230763 139864642547712 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.230839 139864642547712 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.231200 139864642547712 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.231897 139864642547712 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.234424 139864642547712 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.235049 139864642547712 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.235126 139864642547712 transformer_layer.py:468] tlayer: End windows.
I0123 15:01:37.235162 139864642547712 transformer_layer.py:472] tlayer: final FFN.
I0123 15:01:37.235222 139864642547712 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.235359 139864642547712 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:01:37.235469 139864642547712 nn_components.py:325] mlp: activation = None
I0123 15:01:37.235507 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:01:37.237473 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.237568 139864642547712 nn_components.py:261] mlp: residual
I0123 15:01:37.240023 139864642547712 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.240103 139864642547712 transformer_base.py:443] tbase: final FFN
I0123 15:01:37.240218 139864642547712 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:01:37.242594 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:01:37.244491 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.244586 139864642547712 nn_components.py:261] mlp: residual
I0123 15:01:37.244879 139864642547712 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.244960 139864642547712 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 15:01:37.245069 139864642547712 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:01:37.245108 139864642547712 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:01:37.245140 139864642547712 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:01:37.245206 139864642547712 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.247832 139864642547712 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:01:37.253402 139864642547712 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.253678 139864642547712 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:01:37.256323 139864642547712 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:01:37.269156 139864642547712 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:01:37.269212 139864642547712 attention.py:418] Single window, no scan.
I0123 15:01:37.269249 139864642547712 transformer_layer.py:389] tlayer: self-attention.
I0123 15:01:37.269280 139864642547712 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.269345 139864642547712 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.269915 139864642547712 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.269992 139864642547712 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.270358 139864642547712 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.271066 139864642547712 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.273638 139864642547712 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.274280 139864642547712 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.274358 139864642547712 transformer_layer.py:468] tlayer: End windows.
I0123 15:01:37.274392 139864642547712 transformer_layer.py:472] tlayer: final FFN.
I0123 15:01:37.274452 139864642547712 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.274586 139864642547712 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:01:37.274696 139864642547712 nn_components.py:325] mlp: activation = None
I0123 15:01:37.274734 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:01:37.276634 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.276729 139864642547712 nn_components.py:261] mlp: residual
I0123 15:01:37.279188 139864642547712 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.279268 139864642547712 transformer_base.py:443] tbase: final FFN
I0123 15:01:37.279375 139864642547712 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:01:37.281684 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:01:37.283574 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.283671 139864642547712 nn_components.py:261] mlp: residual
I0123 15:01:37.283964 139864642547712 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.284044 139864642547712 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 15:01:37.284153 139864642547712 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:01:37.284192 139864642547712 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:01:37.284223 139864642547712 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:01:37.284287 139864642547712 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.286570 139864642547712 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:01:37.292083 139864642547712 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.292339 139864642547712 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:01:37.295045 139864642547712 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:01:37.307755 139864642547712 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:01:37.307812 139864642547712 attention.py:418] Single window, no scan.
I0123 15:01:37.307849 139864642547712 transformer_layer.py:389] tlayer: self-attention.
I0123 15:01:37.307880 139864642547712 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.307942 139864642547712 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.308502 139864642547712 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.308578 139864642547712 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.308945 139864642547712 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.309660 139864642547712 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.312170 139864642547712 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.312792 139864642547712 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.312868 139864642547712 transformer_layer.py:468] tlayer: End windows.
I0123 15:01:37.312903 139864642547712 transformer_layer.py:472] tlayer: final FFN.
I0123 15:01:37.312962 139864642547712 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.313092 139864642547712 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:01:37.313215 139864642547712 nn_components.py:325] mlp: activation = None
I0123 15:01:37.313255 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:01:37.315205 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.315300 139864642547712 nn_components.py:261] mlp: residual
I0123 15:01:37.317747 139864642547712 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.317826 139864642547712 transformer_base.py:443] tbase: final FFN
I0123 15:01:37.317935 139864642547712 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:01:37.320186 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:01:37.322070 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.322167 139864642547712 nn_components.py:261] mlp: residual
I0123 15:01:37.322459 139864642547712 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.322540 139864642547712 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 15:01:37.322648 139864642547712 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:01:37.322687 139864642547712 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:01:37.322717 139864642547712 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:01:37.322782 139864642547712 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.325118 139864642547712 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:01:37.335913 139864642547712 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.336227 139864642547712 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:01:37.338992 139864642547712 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:01:37.352155 139864642547712 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:01:37.352214 139864642547712 attention.py:418] Single window, no scan.
I0123 15:01:37.352254 139864642547712 transformer_layer.py:389] tlayer: self-attention.
I0123 15:01:37.352286 139864642547712 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.352348 139864642547712 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.352943 139864642547712 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.353024 139864642547712 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.353407 139864642547712 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.354131 139864642547712 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.356704 139864642547712 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.357721 139864642547712 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.357801 139864642547712 transformer_layer.py:468] tlayer: End windows.
I0123 15:01:37.357837 139864642547712 transformer_layer.py:472] tlayer: final FFN.
I0123 15:01:37.357900 139864642547712 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.358034 139864642547712 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:01:37.358148 139864642547712 nn_components.py:325] mlp: activation = None
I0123 15:01:37.358192 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:01:37.360140 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.360233 139864642547712 nn_components.py:261] mlp: residual
I0123 15:01:37.362723 139864642547712 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.362804 139864642547712 transformer_base.py:443] tbase: final FFN
I0123 15:01:37.362916 139864642547712 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:01:37.365215 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:01:37.367192 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.367289 139864642547712 nn_components.py:261] mlp: residual
I0123 15:01:37.367581 139864642547712 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.367663 139864642547712 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 15:01:37.367778 139864642547712 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:01:37.367820 139864642547712 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:01:37.367853 139864642547712 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:01:37.367918 139864642547712 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.370219 139864642547712 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:01:37.375735 139864642547712 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.376010 139864642547712 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:01:37.378733 139864642547712 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:01:37.391528 139864642547712 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:01:37.391584 139864642547712 attention.py:418] Single window, no scan.
I0123 15:01:37.391620 139864642547712 transformer_layer.py:389] tlayer: self-attention.
I0123 15:01:37.391651 139864642547712 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.391716 139864642547712 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.392322 139864642547712 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.392398 139864642547712 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.392761 139864642547712 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.393454 139864642547712 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.395963 139864642547712 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.396600 139864642547712 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.396678 139864642547712 transformer_layer.py:468] tlayer: End windows.
I0123 15:01:37.396713 139864642547712 transformer_layer.py:472] tlayer: final FFN.
I0123 15:01:37.396772 139864642547712 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.396904 139864642547712 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:01:37.397017 139864642547712 nn_components.py:325] mlp: activation = None
I0123 15:01:37.397063 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:01:37.398985 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.399081 139864642547712 nn_components.py:261] mlp: residual
I0123 15:01:37.401572 139864642547712 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.401657 139864642547712 transformer_base.py:443] tbase: final FFN
I0123 15:01:37.401772 139864642547712 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:01:37.404006 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:01:37.405911 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.406009 139864642547712 nn_components.py:261] mlp: residual
I0123 15:01:37.406299 139864642547712 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.406379 139864642547712 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 15:01:37.406489 139864642547712 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:01:37.406527 139864642547712 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:01:37.406558 139864642547712 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:01:37.406622 139864642547712 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.408876 139864642547712 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:01:37.414431 139864642547712 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.414692 139864642547712 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:01:37.417339 139864642547712 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:01:37.430162 139864642547712 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:01:37.430219 139864642547712 attention.py:418] Single window, no scan.
I0123 15:01:37.430255 139864642547712 transformer_layer.py:389] tlayer: self-attention.
I0123 15:01:37.430286 139864642547712 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.430348 139864642547712 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.430913 139864642547712 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.430991 139864642547712 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.431358 139864642547712 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.432055 139864642547712 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.434596 139864642547712 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.435273 139864642547712 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.435351 139864642547712 transformer_layer.py:468] tlayer: End windows.
I0123 15:01:37.435387 139864642547712 transformer_layer.py:472] tlayer: final FFN.
I0123 15:01:37.435451 139864642547712 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.435582 139864642547712 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:01:37.435692 139864642547712 nn_components.py:325] mlp: activation = None
I0123 15:01:37.435731 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:01:37.437654 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.437749 139864642547712 nn_components.py:261] mlp: residual
I0123 15:01:37.440180 139864642547712 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.440262 139864642547712 transformer_base.py:443] tbase: final FFN
I0123 15:01:37.440371 139864642547712 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:01:37.442623 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:01:37.444582 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.444677 139864642547712 nn_components.py:261] mlp: residual
I0123 15:01:37.444970 139864642547712 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.445051 139864642547712 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 15:01:37.445161 139864642547712 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:01:37.445201 139864642547712 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:01:37.445231 139864642547712 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:01:37.445294 139864642547712 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.447565 139864642547712 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:01:37.453055 139864642547712 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.453316 139864642547712 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:01:37.456013 139864642547712 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:01:37.469267 139864642547712 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:01:37.469327 139864642547712 attention.py:418] Single window, no scan.
I0123 15:01:37.469363 139864642547712 transformer_layer.py:389] tlayer: self-attention.
I0123 15:01:37.469394 139864642547712 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.469455 139864642547712 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.470077 139864642547712 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.470154 139864642547712 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.470516 139864642547712 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.471226 139864642547712 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.473741 139864642547712 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.474370 139864642547712 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.474446 139864642547712 transformer_layer.py:468] tlayer: End windows.
I0123 15:01:37.474481 139864642547712 transformer_layer.py:472] tlayer: final FFN.
I0123 15:01:37.474538 139864642547712 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.474664 139864642547712 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:01:37.474771 139864642547712 nn_components.py:325] mlp: activation = None
I0123 15:01:37.474809 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:01:37.476708 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.476808 139864642547712 nn_components.py:261] mlp: residual
I0123 15:01:37.479305 139864642547712 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.479385 139864642547712 transformer_base.py:443] tbase: final FFN
I0123 15:01:37.479492 139864642547712 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:01:37.481730 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:01:37.483593 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.483689 139864642547712 nn_components.py:261] mlp: residual
I0123 15:01:37.483978 139864642547712 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.484059 139864642547712 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 15:01:37.484166 139864642547712 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:01:37.484205 139864642547712 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:01:37.484235 139864642547712 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:01:37.484297 139864642547712 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.486551 139864642547712 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:01:37.492119 139864642547712 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.492377 139864642547712 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:01:37.495031 139864642547712 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:01:37.507710 139864642547712 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:01:37.507766 139864642547712 attention.py:418] Single window, no scan.
I0123 15:01:37.507801 139864642547712 transformer_layer.py:389] tlayer: self-attention.
I0123 15:01:37.507831 139864642547712 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.507895 139864642547712 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.508450 139864642547712 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.508525 139864642547712 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.508886 139864642547712 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.509583 139864642547712 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.512102 139864642547712 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.512767 139864642547712 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.512845 139864642547712 transformer_layer.py:468] tlayer: End windows.
I0123 15:01:37.512881 139864642547712 transformer_layer.py:472] tlayer: final FFN.
I0123 15:01:37.512939 139864642547712 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.513070 139864642547712 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:01:37.513180 139864642547712 nn_components.py:325] mlp: activation = None
I0123 15:01:37.513219 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:01:37.515120 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.515222 139864642547712 nn_components.py:261] mlp: residual
I0123 15:01:37.517702 139864642547712 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.517782 139864642547712 transformer_base.py:443] tbase: final FFN
I0123 15:01:37.517890 139864642547712 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:01:37.520130 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:01:37.522086 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.522183 139864642547712 nn_components.py:261] mlp: residual
I0123 15:01:37.522556 139864642547712 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.522637 139864642547712 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 15:01:37.522747 139864642547712 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:01:37.522787 139864642547712 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:01:37.522817 139864642547712 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:01:37.522881 139864642547712 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.525123 139864642547712 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:01:37.530619 139864642547712 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.530883 139864642547712 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:01:37.533599 139864642547712 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:01:37.546312 139864642547712 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:01:37.546368 139864642547712 attention.py:418] Single window, no scan.
I0123 15:01:37.546404 139864642547712 transformer_layer.py:389] tlayer: self-attention.
I0123 15:01:37.546435 139864642547712 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.546496 139864642547712 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.547058 139864642547712 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.547134 139864642547712 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.547496 139864642547712 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.548252 139864642547712 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.550807 139864642547712 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.551441 139864642547712 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.551520 139864642547712 transformer_layer.py:468] tlayer: End windows.
I0123 15:01:37.551556 139864642547712 transformer_layer.py:472] tlayer: final FFN.
I0123 15:01:37.551616 139864642547712 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.551749 139864642547712 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:01:37.551858 139864642547712 nn_components.py:325] mlp: activation = None
I0123 15:01:37.551897 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:01:37.553807 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.553903 139864642547712 nn_components.py:261] mlp: residual
I0123 15:01:37.556366 139864642547712 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.556446 139864642547712 transformer_base.py:443] tbase: final FFN
I0123 15:01:37.556559 139864642547712 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:01:37.558871 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:01:37.560758 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.560853 139864642547712 nn_components.py:261] mlp: residual
I0123 15:01:37.561147 139864642547712 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.561237 139864642547712 decoder_stack.py:344] dstack: Final layernorm.
I0123 15:01:37.564171 139864642547712 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:01:37.620223 139864642547712 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.620309 139864642547712 decoder_stack.py:333] dstack: autoregressive generator.
I0123 15:01:37.620363 139864642547712 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 15:01:37.620473 139864642547712 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:01:37.620512 139864642547712 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:01:37.620543 139864642547712 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:01:37.620606 139864642547712 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.623426 139864642547712 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:01:37.629026 139864642547712 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.629288 139864642547712 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:01:37.632040 139864642547712 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:01:37.644473 139864642547712 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:01:37.644530 139864642547712 attention.py:418] Single window, no scan.
I0123 15:01:37.644566 139864642547712 transformer_layer.py:389] tlayer: self-attention.
I0123 15:01:37.644596 139864642547712 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.644658 139864642547712 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.645223 139864642547712 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.645300 139864642547712 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.645670 139864642547712 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.646357 139864642547712 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.648879 139864642547712 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.649502 139864642547712 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.649579 139864642547712 transformer_layer.py:468] tlayer: End windows.
I0123 15:01:37.649614 139864642547712 transformer_layer.py:472] tlayer: final FFN.
I0123 15:01:37.649679 139864642547712 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.649812 139864642547712 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:01:37.649929 139864642547712 nn_components.py:325] mlp: activation = None
I0123 15:01:37.649969 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:01:37.651817 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.651912 139864642547712 nn_components.py:261] mlp: residual
I0123 15:01:37.654334 139864642547712 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.654414 139864642547712 transformer_base.py:443] tbase: final FFN
I0123 15:01:37.654524 139864642547712 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:01:37.656789 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:01:37.658658 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.658756 139864642547712 nn_components.py:261] mlp: residual
I0123 15:01:37.659048 139864642547712 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.659130 139864642547712 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 15:01:37.659239 139864642547712 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:01:37.659279 139864642547712 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:01:37.659310 139864642547712 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:01:37.659373 139864642547712 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.661624 139864642547712 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:01:37.667033 139864642547712 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.667290 139864642547712 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:01:37.669954 139864642547712 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:01:37.682277 139864642547712 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:01:37.682333 139864642547712 attention.py:418] Single window, no scan.
I0123 15:01:37.682369 139864642547712 transformer_layer.py:389] tlayer: self-attention.
I0123 15:01:37.682400 139864642547712 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.682460 139864642547712 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.683013 139864642547712 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.683088 139864642547712 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.683445 139864642547712 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.684134 139864642547712 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.686650 139864642547712 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.687267 139864642547712 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.687344 139864642547712 transformer_layer.py:468] tlayer: End windows.
I0123 15:01:37.687380 139864642547712 transformer_layer.py:472] tlayer: final FFN.
I0123 15:01:37.687439 139864642547712 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.687566 139864642547712 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:01:37.687673 139864642547712 nn_components.py:325] mlp: activation = None
I0123 15:01:37.687717 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:01:37.689581 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.689681 139864642547712 nn_components.py:261] mlp: residual
I0123 15:01:37.692093 139864642547712 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.692172 139864642547712 transformer_base.py:443] tbase: final FFN
I0123 15:01:37.692282 139864642547712 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:01:37.694544 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:01:37.696389 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.696485 139864642547712 nn_components.py:261] mlp: residual
I0123 15:01:37.696776 139864642547712 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.696857 139864642547712 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 15:01:37.696966 139864642547712 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:01:37.697006 139864642547712 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:01:37.697038 139864642547712 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:01:37.697103 139864642547712 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.699369 139864642547712 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:01:37.704807 139864642547712 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.705069 139864642547712 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:01:37.707753 139864642547712 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:01:37.720039 139864642547712 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:01:37.720094 139864642547712 attention.py:418] Single window, no scan.
I0123 15:01:37.720130 139864642547712 transformer_layer.py:389] tlayer: self-attention.
I0123 15:01:37.720161 139864642547712 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.720222 139864642547712 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.720776 139864642547712 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.720851 139864642547712 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.721209 139864642547712 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.721899 139864642547712 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.724418 139864642547712 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.725035 139864642547712 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.725113 139864642547712 transformer_layer.py:468] tlayer: End windows.
I0123 15:01:37.725149 139864642547712 transformer_layer.py:472] tlayer: final FFN.
I0123 15:01:37.725209 139864642547712 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.725339 139864642547712 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:01:37.725450 139864642547712 nn_components.py:325] mlp: activation = None
I0123 15:01:37.725488 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:01:37.727351 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.727447 139864642547712 nn_components.py:261] mlp: residual
I0123 15:01:37.729867 139864642547712 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.729948 139864642547712 transformer_base.py:443] tbase: final FFN
I0123 15:01:37.730057 139864642547712 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:01:37.732952 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:01:37.734833 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.734930 139864642547712 nn_components.py:261] mlp: residual
I0123 15:01:37.735223 139864642547712 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.735305 139864642547712 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 15:01:37.735611 139864642547712 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:01:37.735650 139864642547712 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:01:37.735680 139864642547712 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:01:37.735742 139864642547712 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.738298 139864642547712 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:01:37.743709 139864642547712 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.743970 139864642547712 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:01:37.746655 139864642547712 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:01:37.759022 139864642547712 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:01:37.759078 139864642547712 attention.py:418] Single window, no scan.
I0123 15:01:37.759117 139864642547712 transformer_layer.py:389] tlayer: self-attention.
I0123 15:01:37.759155 139864642547712 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.759219 139864642547712 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.759781 139864642547712 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.759856 139864642547712 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.760221 139864642547712 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.760902 139864642547712 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.763451 139864642547712 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.764075 139864642547712 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.764151 139864642547712 transformer_layer.py:468] tlayer: End windows.
I0123 15:01:37.764185 139864642547712 transformer_layer.py:472] tlayer: final FFN.
I0123 15:01:37.764245 139864642547712 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.764376 139864642547712 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:01:37.764484 139864642547712 nn_components.py:325] mlp: activation = None
I0123 15:01:37.764524 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:01:37.766416 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.766509 139864642547712 nn_components.py:261] mlp: residual
I0123 15:01:37.768924 139864642547712 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.769001 139864642547712 transformer_base.py:443] tbase: final FFN
I0123 15:01:37.769109 139864642547712 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:01:37.771404 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:01:37.773277 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.773372 139864642547712 nn_components.py:261] mlp: residual
I0123 15:01:37.773669 139864642547712 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.773751 139864642547712 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 15:01:37.773859 139864642547712 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:01:37.773897 139864642547712 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:01:37.773927 139864642547712 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:01:37.773991 139864642547712 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.776225 139864642547712 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:01:37.781653 139864642547712 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.781911 139864642547712 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:01:37.784590 139864642547712 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:01:37.797045 139864642547712 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:01:37.797100 139864642547712 attention.py:418] Single window, no scan.
I0123 15:01:37.797134 139864642547712 transformer_layer.py:389] tlayer: self-attention.
I0123 15:01:37.797163 139864642547712 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.797229 139864642547712 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.797798 139864642547712 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.797873 139864642547712 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.798233 139864642547712 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.798913 139864642547712 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.801443 139864642547712 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.802074 139864642547712 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.802151 139864642547712 transformer_layer.py:468] tlayer: End windows.
I0123 15:01:37.802185 139864642547712 transformer_layer.py:472] tlayer: final FFN.
I0123 15:01:37.802243 139864642547712 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.802370 139864642547712 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:01:37.802477 139864642547712 nn_components.py:325] mlp: activation = None
I0123 15:01:37.802514 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:01:37.804376 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.804474 139864642547712 nn_components.py:261] mlp: residual
I0123 15:01:37.806908 139864642547712 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.806987 139864642547712 transformer_base.py:443] tbase: final FFN
I0123 15:01:37.807095 139864642547712 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:01:37.809403 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:01:37.811298 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.811393 139864642547712 nn_components.py:261] mlp: residual
I0123 15:01:37.811681 139864642547712 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.811762 139864642547712 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 15:01:37.811872 139864642547712 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:01:37.811910 139864642547712 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:01:37.811939 139864642547712 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:01:37.812001 139864642547712 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.814252 139864642547712 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:01:37.819695 139864642547712 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.819952 139864642547712 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:01:37.822636 139864642547712 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:01:37.835183 139864642547712 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:01:37.835238 139864642547712 attention.py:418] Single window, no scan.
I0123 15:01:37.835273 139864642547712 transformer_layer.py:389] tlayer: self-attention.
I0123 15:01:37.835302 139864642547712 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.835361 139864642547712 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.835922 139864642547712 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.835995 139864642547712 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.836355 139864642547712 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.837036 139864642547712 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.839750 139864642547712 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.840370 139864642547712 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.840445 139864642547712 transformer_layer.py:468] tlayer: End windows.
I0123 15:01:37.840478 139864642547712 transformer_layer.py:472] tlayer: final FFN.
I0123 15:01:37.840534 139864642547712 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.840658 139864642547712 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:01:37.840764 139864642547712 nn_components.py:325] mlp: activation = None
I0123 15:01:37.840801 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:01:37.842682 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.842780 139864642547712 nn_components.py:261] mlp: residual
I0123 15:01:37.845185 139864642547712 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.845262 139864642547712 transformer_base.py:443] tbase: final FFN
I0123 15:01:37.845369 139864642547712 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:01:37.848055 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:01:37.849932 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.850030 139864642547712 nn_components.py:261] mlp: residual
I0123 15:01:37.850320 139864642547712 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.850401 139864642547712 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 15:01:37.850509 139864642547712 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:01:37.850546 139864642547712 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:01:37.850575 139864642547712 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:01:37.850637 139864642547712 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.852874 139864642547712 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:01:37.858636 139864642547712 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.858892 139864642547712 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:01:37.861599 139864642547712 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:01:37.874041 139864642547712 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:01:37.874094 139864642547712 attention.py:418] Single window, no scan.
I0123 15:01:37.874130 139864642547712 transformer_layer.py:389] tlayer: self-attention.
I0123 15:01:37.874159 139864642547712 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.874220 139864642547712 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.874781 139864642547712 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.874857 139864642547712 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.875217 139864642547712 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.875900 139864642547712 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.878463 139864642547712 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.879085 139864642547712 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.879162 139864642547712 transformer_layer.py:468] tlayer: End windows.
I0123 15:01:37.879195 139864642547712 transformer_layer.py:472] tlayer: final FFN.
I0123 15:01:37.879252 139864642547712 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.879378 139864642547712 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:01:37.879485 139864642547712 nn_components.py:325] mlp: activation = None
I0123 15:01:37.879523 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:01:37.881392 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.881484 139864642547712 nn_components.py:261] mlp: residual
I0123 15:01:37.883894 139864642547712 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.883973 139864642547712 transformer_base.py:443] tbase: final FFN
I0123 15:01:37.884080 139864642547712 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:01:37.886384 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:01:37.888233 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.888328 139864642547712 nn_components.py:261] mlp: residual
I0123 15:01:37.888613 139864642547712 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.888693 139864642547712 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 15:01:37.888801 139864642547712 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:01:37.888839 139864642547712 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:01:37.888868 139864642547712 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:01:37.888930 139864642547712 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.891180 139864642547712 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:01:37.896640 139864642547712 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.896897 139864642547712 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:01:37.899588 139864642547712 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:01:37.912059 139864642547712 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:01:37.912112 139864642547712 attention.py:418] Single window, no scan.
I0123 15:01:37.912147 139864642547712 transformer_layer.py:389] tlayer: self-attention.
I0123 15:01:37.912176 139864642547712 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.912237 139864642547712 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.912794 139864642547712 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.912873 139864642547712 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.913235 139864642547712 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.913932 139864642547712 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.916504 139864642547712 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.917133 139864642547712 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.917210 139864642547712 transformer_layer.py:468] tlayer: End windows.
I0123 15:01:37.917245 139864642547712 transformer_layer.py:472] tlayer: final FFN.
I0123 15:01:37.917306 139864642547712 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.917434 139864642547712 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:01:37.917542 139864642547712 nn_components.py:325] mlp: activation = None
I0123 15:01:37.917579 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:01:37.919461 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.919556 139864642547712 nn_components.py:261] mlp: residual
I0123 15:01:37.921977 139864642547712 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.922063 139864642547712 transformer_base.py:443] tbase: final FFN
I0123 15:01:37.922173 139864642547712 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:01:37.924470 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:01:37.926345 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.926440 139864642547712 nn_components.py:261] mlp: residual
I0123 15:01:37.926729 139864642547712 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.926809 139864642547712 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 15:01:37.926916 139864642547712 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:01:37.926955 139864642547712 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:01:37.926986 139864642547712 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:01:37.927049 139864642547712 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.929306 139864642547712 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:01:37.934746 139864642547712 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.935006 139864642547712 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:01:37.937692 139864642547712 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:01:37.950088 139864642547712 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:01:37.950144 139864642547712 attention.py:418] Single window, no scan.
I0123 15:01:37.950177 139864642547712 transformer_layer.py:389] tlayer: self-attention.
I0123 15:01:37.950206 139864642547712 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.950266 139864642547712 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.950825 139864642547712 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.950900 139864642547712 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.951254 139864642547712 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.951942 139864642547712 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.954532 139864642547712 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.955155 139864642547712 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.955231 139864642547712 transformer_layer.py:468] tlayer: End windows.
I0123 15:01:37.955265 139864642547712 transformer_layer.py:472] tlayer: final FFN.
I0123 15:01:37.955323 139864642547712 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.955450 139864642547712 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:01:37.955562 139864642547712 nn_components.py:325] mlp: activation = None
I0123 15:01:37.955599 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:01:37.957654 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.957749 139864642547712 nn_components.py:261] mlp: residual
I0123 15:01:37.960363 139864642547712 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.960446 139864642547712 transformer_base.py:443] tbase: final FFN
I0123 15:01:37.960556 139864642547712 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:01:37.963559 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:01:37.965427 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.965523 139864642547712 nn_components.py:261] mlp: residual
I0123 15:01:37.965821 139864642547712 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.965902 139864642547712 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 15:01:37.966010 139864642547712 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:01:37.966048 139864642547712 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:01:37.966078 139864642547712 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:01:37.966140 139864642547712 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.968401 139864642547712 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:01:37.973884 139864642547712 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.974145 139864642547712 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:01:37.977009 139864642547712 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:01:37.989659 139864642547712 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:01:37.989713 139864642547712 attention.py:418] Single window, no scan.
I0123 15:01:37.989747 139864642547712 transformer_layer.py:389] tlayer: self-attention.
I0123 15:01:37.989777 139864642547712 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.989838 139864642547712 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.990397 139864642547712 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.990472 139864642547712 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.990833 139864642547712 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.991526 139864642547712 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.994073 139864642547712 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.994699 139864642547712 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.994776 139864642547712 transformer_layer.py:468] tlayer: End windows.
I0123 15:01:37.994810 139864642547712 transformer_layer.py:472] tlayer: final FFN.
I0123 15:01:37.994867 139864642547712 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.994993 139864642547712 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:01:37.995100 139864642547712 nn_components.py:325] mlp: activation = None
I0123 15:01:37.995138 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:01:37.997501 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:37.997596 139864642547712 nn_components.py:261] mlp: residual
I0123 15:01:37.999985 139864642547712 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:38.000064 139864642547712 transformer_base.py:443] tbase: final FFN
I0123 15:01:38.000179 139864642547712 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:01:38.002433 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:01:38.004268 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:38.004361 139864642547712 nn_components.py:261] mlp: residual
I0123 15:01:38.004648 139864642547712 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:38.004727 139864642547712 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 15:01:38.004835 139864642547712 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:01:38.004873 139864642547712 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:01:38.004903 139864642547712 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:01:38.004964 139864642547712 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:38.007200 139864642547712 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:01:38.012601 139864642547712 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:38.012860 139864642547712 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:01:38.015548 139864642547712 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:01:38.027937 139864642547712 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:01:38.027993 139864642547712 attention.py:418] Single window, no scan.
I0123 15:01:38.028028 139864642547712 transformer_layer.py:389] tlayer: self-attention.
I0123 15:01:38.028059 139864642547712 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:38.028120 139864642547712 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:38.028678 139864642547712 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:38.028753 139864642547712 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:38.029108 139864642547712 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:38.029800 139864642547712 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:38.032350 139864642547712 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:38.032972 139864642547712 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:38.033048 139864642547712 transformer_layer.py:468] tlayer: End windows.
I0123 15:01:38.033082 139864642547712 transformer_layer.py:472] tlayer: final FFN.
I0123 15:01:38.033139 139864642547712 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:38.033264 139864642547712 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:01:38.033371 139864642547712 nn_components.py:325] mlp: activation = None
I0123 15:01:38.033408 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:01:38.035276 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:38.035369 139864642547712 nn_components.py:261] mlp: residual
I0123 15:01:38.037774 139864642547712 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:38.037853 139864642547712 transformer_base.py:443] tbase: final FFN
I0123 15:01:38.037961 139864642547712 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:01:38.040252 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:01:38.042112 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:38.042207 139864642547712 nn_components.py:261] mlp: residual
I0123 15:01:38.042496 139864642547712 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:38.042577 139864642547712 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 15:01:38.042683 139864642547712 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:01:38.042721 139864642547712 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:01:38.042751 139864642547712 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:01:38.042812 139864642547712 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:38.045031 139864642547712 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:01:38.050494 139864642547712 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:38.050755 139864642547712 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:01:38.053451 139864642547712 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:01:38.065964 139864642547712 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:01:38.066018 139864642547712 attention.py:418] Single window, no scan.
I0123 15:01:38.066053 139864642547712 transformer_layer.py:389] tlayer: self-attention.
I0123 15:01:38.066083 139864642547712 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:38.066142 139864642547712 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:38.066699 139864642547712 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:38.066777 139864642547712 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:38.067136 139864642547712 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:38.067847 139864642547712 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:38.070386 139864642547712 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:38.071009 139864642547712 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:38.071086 139864642547712 transformer_layer.py:468] tlayer: End windows.
I0123 15:01:38.071120 139864642547712 transformer_layer.py:472] tlayer: final FFN.
I0123 15:01:38.071177 139864642547712 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:38.071303 139864642547712 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:01:38.071415 139864642547712 nn_components.py:325] mlp: activation = None
I0123 15:01:38.071452 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:01:38.073321 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:38.073413 139864642547712 nn_components.py:261] mlp: residual
I0123 15:01:38.075828 139864642547712 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:38.075907 139864642547712 transformer_base.py:443] tbase: final FFN
I0123 15:01:38.076014 139864642547712 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:01:38.078680 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:01:38.080551 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:38.080646 139864642547712 nn_components.py:261] mlp: residual
I0123 15:01:38.080937 139864642547712 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:38.081021 139864642547712 decoder_stack.py:344] dstack: Final layernorm.
I0123 15:01:38.083867 139864642547712 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:01:42.513504 139864642547712 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
I0123 15:01:43.063717 139864642547712 training_loop.py:409] No working directory specified.
I0123 15:01:43.063832 139864642547712 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0123 15:01:43.064563 139864642547712 checkpoints.py:1062] Restoring legacy Flax checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0123 15:01:46.398461 139864642547712 training_loop.py:447] Only restoring trainable parameters.
I0123 15:01:46.399056 139864642547712 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0123 15:01:46.399133 139864642547712 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0123 15:01:46.399186 139864642547712 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 15:01:46.399232 139864642547712 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 15:01:46.399274 139864642547712 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:01:46.399315 139864642547712 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 15:01:46.399355 139864642547712 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:01:46.399394 139864642547712 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:01:46.399432 139864642547712 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 15:01:46.399470 139864642547712 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 15:01:46.399508 139864642547712 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:01:46.399545 139864642547712 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 15:01:46.399584 139864642547712 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 15:01:46.399623 139864642547712 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 15:01:46.399661 139864642547712 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:01:46.399698 139864642547712 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 15:01:46.399736 139864642547712 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:01:46.399772 139864642547712 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:01:46.399809 139864642547712 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 15:01:46.399847 139864642547712 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 15:01:46.399898 139864642547712 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:01:46.399937 139864642547712 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 15:01:46.399975 139864642547712 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 15:01:46.400012 139864642547712 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 15:01:46.400049 139864642547712 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:01:46.400086 139864642547712 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 15:01:46.400123 139864642547712 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:01:46.400160 139864642547712 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:01:46.400196 139864642547712 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 15:01:46.400232 139864642547712 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 15:01:46.400444 139864642547712 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:01:46.400480 139864642547712 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 15:01:46.400516 139864642547712 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 15:01:46.400552 139864642547712 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 15:01:46.400587 139864642547712 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:01:46.400624 139864642547712 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 15:01:46.400660 139864642547712 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:01:46.400697 139864642547712 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:01:46.400733 139864642547712 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 15:01:46.400769 139864642547712 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 15:01:46.400805 139864642547712 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:01:46.400841 139864642547712 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 15:01:46.400877 139864642547712 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 15:01:46.400913 139864642547712 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 15:01:46.400949 139864642547712 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:01:46.400985 139864642547712 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 15:01:46.401029 139864642547712 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:01:46.401068 139864642547712 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:01:46.401105 139864642547712 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 15:01:46.401141 139864642547712 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 15:01:46.401177 139864642547712 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:01:46.401213 139864642547712 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 15:01:46.401249 139864642547712 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 15:01:46.401283 139864642547712 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 15:01:46.401319 139864642547712 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:01:46.401354 139864642547712 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 15:01:46.401389 139864642547712 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:01:46.401425 139864642547712 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:01:46.401461 139864642547712 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 15:01:46.401496 139864642547712 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 15:01:46.401533 139864642547712 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:01:46.401569 139864642547712 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 15:01:46.401604 139864642547712 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 15:01:46.401651 139864642547712 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 15:01:46.401692 139864642547712 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:01:46.401730 139864642547712 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 15:01:46.401766 139864642547712 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:01:46.401802 139864642547712 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:01:46.401838 139864642547712 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 15:01:46.401873 139864642547712 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 15:01:46.401909 139864642547712 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:01:46.401945 139864642547712 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 15:01:46.401982 139864642547712 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 15:01:46.402024 139864642547712 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 15:01:46.402062 139864642547712 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:01:46.402100 139864642547712 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 15:01:46.402136 139864642547712 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:01:46.402173 139864642547712 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:01:46.402209 139864642547712 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 15:01:46.402245 139864642547712 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 15:01:46.402281 139864642547712 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:01:46.402318 139864642547712 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 15:01:46.402354 139864642547712 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 15:01:46.402390 139864642547712 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 15:01:46.402426 139864642547712 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:01:46.402462 139864642547712 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 15:01:46.402498 139864642547712 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:01:46.402534 139864642547712 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:01:46.402569 139864642547712 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 15:01:46.402605 139864642547712 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 15:01:46.402640 139864642547712 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:01:46.402676 139864642547712 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 15:01:46.402711 139864642547712 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 15:01:46.402747 139864642547712 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 15:01:46.402783 139864642547712 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:01:46.402819 139864642547712 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 15:01:46.402854 139864642547712 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:01:46.402891 139864642547712 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:01:46.402927 139864642547712 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 15:01:46.402963 139864642547712 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 15:01:46.403005 139864642547712 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:01:46.403043 139864642547712 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 15:01:46.403079 139864642547712 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 15:01:46.403115 139864642547712 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 15:01:46.403151 139864642547712 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:01:46.403186 139864642547712 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 15:01:46.403223 139864642547712 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:01:46.403259 139864642547712 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:01:46.403295 139864642547712 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 15:01:46.403332 139864642547712 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 15:01:46.403368 139864642547712 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:01:46.403404 139864642547712 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 15:01:46.403441 139864642547712 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 15:01:46.403477 139864642547712 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 15:01:46.403514 139864642547712 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:01:46.403550 139864642547712 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 15:01:46.403586 139864642547712 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:01:46.403622 139864642547712 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:01:46.403658 139864642547712 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 15:01:46.403694 139864642547712 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 15:01:46.403730 139864642547712 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:01:46.403766 139864642547712 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 15:01:46.403795 139864642547712 training_loop.py:725] Total parameters: 152072288
I0123 15:01:46.404025 139864642547712 training_loop.py:739] Total state size: 0
I0123 15:01:46.429409 139864642547712 training_loop.py:492] Training loop: creating task for mode beam_search
I0123 15:01:46.429661 139864642547712 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0123 15:01:46.430392 139864642547712 training_loop.py:652] Compiling mode beam_search with jit.
I0123 15:01:46.430727 139864642547712 training_loop.py:89] registering functions: dict_keys([])
I0123 15:01:46.447323 139864642547712 graph.py:499] a b c = triangle a b c; d = angle_bisector d b c a, on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g f b e; h = on_circle h g b, on_line h c b; i = circle i f a e; j = on_circle j i a, on_line j c a; k = midpoint k b a; l = lc_tangent l k a, on_line l c a; m = circle m c e a; n = circle n j l e; o = on_line o b a, on_line o j h ? coll o m n
I0123 15:01:49.429297 139864642547712 ddar.py:60] Depth 1/1000 time = 2.895693302154541
I0123 15:01:54.023392 139864642547712 ddar.py:60] Depth 2/1000 time = 4.5936667919158936
I0123 15:02:00.206926 139864642547712 ddar.py:60] Depth 3/1000 time = 6.183128356933594
I0123 15:02:06.032322 139864642547712 ddar.py:60] Depth 4/1000 time = 5.82506251335144
I0123 15:02:11.918229 139864642547712 ddar.py:60] Depth 5/1000 time = 5.885262727737427
I0123 15:02:19.844202 139864642547712 ddar.py:60] Depth 6/1000 time = 7.872190475463867
I0123 15:02:27.486208 139864642547712 ddar.py:60] Depth 7/1000 time = 7.641624689102173
I0123 15:02:35.050952 139864642547712 ddar.py:60] Depth 8/1000 time = 7.564445495605469
I0123 15:02:42.704570 139864642547712 ddar.py:60] Depth 9/1000 time = 7.6471357345581055
I0123 15:02:51.365731 139864642547712 ddar.py:60] Depth 10/1000 time = 8.660897731781006
I0123 15:03:00.858700 139864642547712 ddar.py:60] Depth 11/1000 time = 9.492666721343994
I0123 15:03:10.274671 139864642547712 ddar.py:60] Depth 12/1000 time = 9.4157075881958
I0123 15:03:21.334882 139864642547712 ddar.py:60] Depth 13/1000 time = 11.05989670753479
I0123 15:03:31.678217 139864642547712 ddar.py:60] Depth 14/1000 time = 10.34312129020691
I0123 15:03:43.659772 139864642547712 ddar.py:60] Depth 15/1000 time = 11.981327056884766
I0123 15:03:55.244884 139864642547712 ddar.py:60] Depth 16/1000 time = 11.584874868392944
I0123 15:04:06.898352 139864642547712 ddar.py:60] Depth 17/1000 time = 11.621401071548462
I0123 15:04:19.180863 139864642547712 ddar.py:60] Depth 18/1000 time = 12.240600109100342
I0123 15:04:31.720784 139864642547712 ddar.py:60] Depth 19/1000 time = 12.490452766418457
I0123 15:04:31.721011 139864642547712 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 15:04:31.721110 139864642547712 alphageometry.py:540] Depth 0. There are 1 nodes to expand:
I0123 15:04:31.721145 139864642547712 alphageometry.py:544] {S} a : ; b : ; c : ; d : C a b d 00 ^ c a c d c d c b 01 ; e : C c d e 02 ; f : ^ a b a e a f a c 03 ^ c a c f c e c b 04 ; g : D b g e g 05 D b g f g 06 ; h : C b c h 07 D b g g h 08 ; i : D a i e i 09 D a i f i 10 ; j : C a c j 11 D a i i j 12 ; k : C a b k 13 D a k b k 14 ; l : C a c l 15 T a k k l 16 ; m : D a m e m 17 D c m e m 18 ; n : D e n l n 19 D j n l n 20 ; o : C a b o 21 C h j o 22 ? C o m n {F1} x00
I0123 15:04:31.721176 139864642547712 alphageometry.py:549] Decoding from {S} a : ; b : ; c : ; d : C a b d 00 ^ c a c d c d c b 01 ; e : C c d e 02 ; f : ^ a b a e a f a c 03 ^ c a c f c e c b 04 ; g : D b g e g 05 D b g f g 06 ; h : C b c h 07 D b g g h 08 ; i : D a i e i 09 D a i f i 10 ; j : C a c j 11 D a i i j 12 ; k : C a b k 13 D a k b k 14 ; l : C a c l 15 T a k k l 16 ; m : D a m e m 17 D c m e m 18 ; n : D e n l n 19 D j n l n 20 ; o : C a b o 21 C h j o 22 ? C o m n {F1} x00
I0123 15:04:31.865865 139864642547712 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:31.866052 139864642547712 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 15:04:31.866153 139864642547712 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 15:04:31.866232 139864642547712 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 15:04:31.866305 139864642547712 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 15:04:31.866374 139864642547712 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 15:04:31.866446 139864642547712 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 15:04:31.866527 139864642547712 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 15:04:31.866600 139864642547712 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 15:04:31.866670 139864642547712 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 15:04:31.866739 139864642547712 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 15:04:31.866807 139864642547712 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 15:04:31.866875 139864642547712 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 15:04:31.866943 139864642547712 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 15:04:31.866984 139864642547712 attention.py:418] Single window, no scan.
I0123 15:04:31.867029 139864642547712 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 15:04:31.867139 139864642547712 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:04:31.867178 139864642547712 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:04:31.867208 139864642547712 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:04:31.869166 139864642547712 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:31.871943 139864642547712 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:04:31.877697 139864642547712 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:31.877964 139864642547712 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:04:31.880608 139864642547712 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:04:31.884430 139864642547712 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:04:31.884485 139864642547712 attention.py:418] Single window, no scan.
I0123 15:04:31.884522 139864642547712 transformer_layer.py:389] tlayer: self-attention.
I0123 15:04:31.884554 139864642547712 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:31.884614 139864642547712 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:31.885270 139864642547712 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:31.885346 139864642547712 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:31.885718 139864642547712 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:31.886490 139864642547712 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:31.889058 139864642547712 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:31.889693 139864642547712 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:31.889770 139864642547712 transformer_layer.py:468] tlayer: End windows.
I0123 15:04:31.889803 139864642547712 transformer_layer.py:472] tlayer: final FFN.
I0123 15:04:31.889861 139864642547712 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:31.889988 139864642547712 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:04:31.890318 139864642547712 nn_components.py:325] mlp: activation = None
I0123 15:04:31.890362 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:04:31.892370 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:31.892461 139864642547712 nn_components.py:261] mlp: residual
I0123 15:04:31.894991 139864642547712 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:31.895072 139864642547712 transformer_base.py:443] tbase: final FFN
I0123 15:04:31.895513 139864642547712 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:04:31.897856 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:04:31.899856 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:31.899960 139864642547712 nn_components.py:261] mlp: residual
I0123 15:04:31.900249 139864642547712 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:31.900328 139864642547712 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 15:04:31.900433 139864642547712 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:04:31.900470 139864642547712 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:04:31.900500 139864642547712 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:04:31.902391 139864642547712 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:31.904773 139864642547712 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:04:31.910425 139864642547712 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:31.910692 139864642547712 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:04:31.913797 139864642547712 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:04:31.917478 139864642547712 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:04:31.917532 139864642547712 attention.py:418] Single window, no scan.
I0123 15:04:31.917567 139864642547712 transformer_layer.py:389] tlayer: self-attention.
I0123 15:04:31.917598 139864642547712 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:31.917669 139864642547712 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:31.918227 139864642547712 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:31.918301 139864642547712 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:31.918657 139864642547712 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:31.919413 139864642547712 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:31.921892 139864642547712 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:31.922565 139864642547712 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:31.922642 139864642547712 transformer_layer.py:468] tlayer: End windows.
I0123 15:04:31.922676 139864642547712 transformer_layer.py:472] tlayer: final FFN.
I0123 15:04:31.922732 139864642547712 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:31.922862 139864642547712 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:04:31.923179 139864642547712 nn_components.py:325] mlp: activation = None
I0123 15:04:31.923221 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:04:31.925127 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:31.925219 139864642547712 nn_components.py:261] mlp: residual
I0123 15:04:31.927687 139864642547712 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:31.927772 139864642547712 transformer_base.py:443] tbase: final FFN
I0123 15:04:31.928198 139864642547712 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:04:31.930538 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:04:31.932459 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:31.932552 139864642547712 nn_components.py:261] mlp: residual
I0123 15:04:31.932841 139864642547712 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:31.932919 139864642547712 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 15:04:31.933025 139864642547712 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:04:31.933062 139864642547712 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:04:31.933091 139864642547712 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:04:31.934915 139864642547712 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:31.937212 139864642547712 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:04:31.942867 139864642547712 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:31.943121 139864642547712 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:04:31.945693 139864642547712 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:04:31.949315 139864642547712 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:04:31.949368 139864642547712 attention.py:418] Single window, no scan.
I0123 15:04:31.949403 139864642547712 transformer_layer.py:389] tlayer: self-attention.
I0123 15:04:31.949432 139864642547712 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:31.949492 139864642547712 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:31.950108 139864642547712 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:31.950184 139864642547712 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:31.950538 139864642547712 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:31.951285 139864642547712 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:31.953756 139864642547712 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:31.954379 139864642547712 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:31.954456 139864642547712 transformer_layer.py:468] tlayer: End windows.
I0123 15:04:31.954490 139864642547712 transformer_layer.py:472] tlayer: final FFN.
I0123 15:04:31.954546 139864642547712 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:31.954677 139864642547712 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:04:31.954993 139864642547712 nn_components.py:325] mlp: activation = None
I0123 15:04:31.955035 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:04:31.956995 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:31.957086 139864642547712 nn_components.py:261] mlp: residual
I0123 15:04:31.959543 139864642547712 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:31.959626 139864642547712 transformer_base.py:443] tbase: final FFN
I0123 15:04:31.960054 139864642547712 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:04:31.962313 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:04:31.964224 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:31.964316 139864642547712 nn_components.py:261] mlp: residual
I0123 15:04:31.964603 139864642547712 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:31.964682 139864642547712 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 15:04:31.964788 139864642547712 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:04:31.964826 139864642547712 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:04:31.964856 139864642547712 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:04:31.966736 139864642547712 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:31.969052 139864642547712 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:04:31.974862 139864642547712 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:31.975113 139864642547712 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:04:31.977701 139864642547712 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:04:31.981397 139864642547712 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:04:31.981451 139864642547712 attention.py:418] Single window, no scan.
I0123 15:04:31.981486 139864642547712 transformer_layer.py:389] tlayer: self-attention.
I0123 15:04:31.981516 139864642547712 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:31.981577 139864642547712 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:31.982142 139864642547712 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:31.982218 139864642547712 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:31.982575 139864642547712 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:31.983338 139864642547712 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:31.985836 139864642547712 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:31.986449 139864642547712 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:31.986525 139864642547712 transformer_layer.py:468] tlayer: End windows.
I0123 15:04:31.986558 139864642547712 transformer_layer.py:472] tlayer: final FFN.
I0123 15:04:31.986614 139864642547712 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:31.986742 139864642547712 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:04:31.987110 139864642547712 nn_components.py:325] mlp: activation = None
I0123 15:04:31.987152 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:04:31.989082 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:31.989174 139864642547712 nn_components.py:261] mlp: residual
I0123 15:04:31.991654 139864642547712 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:31.991733 139864642547712 transformer_base.py:443] tbase: final FFN
I0123 15:04:31.992170 139864642547712 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:04:31.994473 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:04:31.996449 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:31.996542 139864642547712 nn_components.py:261] mlp: residual
I0123 15:04:31.996837 139864642547712 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:31.996916 139864642547712 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 15:04:31.997022 139864642547712 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:04:31.997060 139864642547712 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:04:31.997088 139864642547712 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:04:31.998889 139864642547712 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.001227 139864642547712 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:04:32.006936 139864642547712 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.007194 139864642547712 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:04:32.009758 139864642547712 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:04:32.013362 139864642547712 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:04:32.013415 139864642547712 attention.py:418] Single window, no scan.
I0123 15:04:32.013450 139864642547712 transformer_layer.py:389] tlayer: self-attention.
I0123 15:04:32.013479 139864642547712 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.013538 139864642547712 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.014145 139864642547712 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.014220 139864642547712 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.014576 139864642547712 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.015332 139864642547712 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.017784 139864642547712 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.018399 139864642547712 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.018474 139864642547712 transformer_layer.py:468] tlayer: End windows.
I0123 15:04:32.018507 139864642547712 transformer_layer.py:472] tlayer: final FFN.
I0123 15:04:32.018563 139864642547712 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.018687 139864642547712 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:04:32.019004 139864642547712 nn_components.py:325] mlp: activation = None
I0123 15:04:32.019046 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:04:32.021380 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.021474 139864642547712 nn_components.py:261] mlp: residual
I0123 15:04:32.023974 139864642547712 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.024052 139864642547712 transformer_base.py:443] tbase: final FFN
I0123 15:04:32.024480 139864642547712 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:04:32.026737 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:04:32.028637 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.028731 139864642547712 nn_components.py:261] mlp: residual
I0123 15:04:32.029021 139864642547712 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.029099 139864642547712 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 15:04:32.029206 139864642547712 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:04:32.029244 139864642547712 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:04:32.029273 139864642547712 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:04:32.031144 139864642547712 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.033451 139864642547712 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:04:32.039034 139864642547712 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.039289 139864642547712 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:04:32.041841 139864642547712 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:04:32.045497 139864642547712 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:04:32.045549 139864642547712 attention.py:418] Single window, no scan.
I0123 15:04:32.045583 139864642547712 transformer_layer.py:389] tlayer: self-attention.
I0123 15:04:32.045612 139864642547712 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.045679 139864642547712 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.046230 139864642547712 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.046305 139864642547712 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.046654 139864642547712 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.047395 139864642547712 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.049815 139864642547712 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.050420 139864642547712 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.050495 139864642547712 transformer_layer.py:468] tlayer: End windows.
I0123 15:04:32.050527 139864642547712 transformer_layer.py:472] tlayer: final FFN.
I0123 15:04:32.050583 139864642547712 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.050734 139864642547712 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:04:32.051097 139864642547712 nn_components.py:325] mlp: activation = None
I0123 15:04:32.051139 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:04:32.053029 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.053119 139864642547712 nn_components.py:261] mlp: residual
I0123 15:04:32.055552 139864642547712 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.055631 139864642547712 transformer_base.py:443] tbase: final FFN
I0123 15:04:32.056053 139864642547712 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:04:32.058295 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:04:32.060248 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.060346 139864642547712 nn_components.py:261] mlp: residual
I0123 15:04:32.060634 139864642547712 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.060712 139864642547712 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 15:04:32.060818 139864642547712 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:04:32.060855 139864642547712 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:04:32.060884 139864642547712 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:04:32.062656 139864642547712 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.064946 139864642547712 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:04:32.070669 139864642547712 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.070921 139864642547712 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:04:32.073466 139864642547712 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:04:32.077075 139864642547712 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:04:32.077130 139864642547712 attention.py:418] Single window, no scan.
I0123 15:04:32.077163 139864642547712 transformer_layer.py:389] tlayer: self-attention.
I0123 15:04:32.077192 139864642547712 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.077254 139864642547712 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.077869 139864642547712 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.077944 139864642547712 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.078301 139864642547712 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.079061 139864642547712 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.081508 139864642547712 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.082125 139864642547712 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.082203 139864642547712 transformer_layer.py:468] tlayer: End windows.
I0123 15:04:32.082236 139864642547712 transformer_layer.py:472] tlayer: final FFN.
I0123 15:04:32.082293 139864642547712 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.082417 139864642547712 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:04:32.082726 139864642547712 nn_components.py:325] mlp: activation = None
I0123 15:04:32.082767 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:04:32.084711 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.084801 139864642547712 nn_components.py:261] mlp: residual
I0123 15:04:32.087230 139864642547712 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.087309 139864642547712 transformer_base.py:443] tbase: final FFN
I0123 15:04:32.087727 139864642547712 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:04:32.089979 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:04:32.091880 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.091981 139864642547712 nn_components.py:261] mlp: residual
I0123 15:04:32.092277 139864642547712 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.092356 139864642547712 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 15:04:32.092464 139864642547712 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:04:32.092503 139864642547712 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:04:32.092532 139864642547712 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:04:32.094398 139864642547712 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.096686 139864642547712 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:04:32.102178 139864642547712 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.102432 139864642547712 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:04:32.104965 139864642547712 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:04:32.108590 139864642547712 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:04:32.108645 139864642547712 attention.py:418] Single window, no scan.
I0123 15:04:32.108680 139864642547712 transformer_layer.py:389] tlayer: self-attention.
I0123 15:04:32.108708 139864642547712 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.108769 139864642547712 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.109317 139864642547712 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.109392 139864642547712 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.109746 139864642547712 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.110501 139864642547712 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.112937 139864642547712 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.113551 139864642547712 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.113627 139864642547712 transformer_layer.py:468] tlayer: End windows.
I0123 15:04:32.113669 139864642547712 transformer_layer.py:472] tlayer: final FFN.
I0123 15:04:32.113725 139864642547712 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.113851 139864642547712 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:04:32.114213 139864642547712 nn_components.py:325] mlp: activation = None
I0123 15:04:32.114256 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:04:32.116146 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.116237 139864642547712 nn_components.py:261] mlp: residual
I0123 15:04:32.118670 139864642547712 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.118748 139864642547712 transformer_base.py:443] tbase: final FFN
I0123 15:04:32.119168 139864642547712 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:04:32.121405 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:04:32.123390 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.123483 139864642547712 nn_components.py:261] mlp: residual
I0123 15:04:32.123779 139864642547712 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.123859 139864642547712 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 15:04:32.123967 139864642547712 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:04:32.124005 139864642547712 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:04:32.124035 139864642547712 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:04:32.125809 139864642547712 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.128119 139864642547712 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:04:32.134103 139864642547712 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.134356 139864642547712 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:04:32.136903 139864642547712 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:04:32.140517 139864642547712 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:04:32.140571 139864642547712 attention.py:418] Single window, no scan.
I0123 15:04:32.140605 139864642547712 transformer_layer.py:389] tlayer: self-attention.
I0123 15:04:32.140634 139864642547712 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.140695 139864642547712 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.141303 139864642547712 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.141378 139864642547712 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.141741 139864642547712 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.142503 139864642547712 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.144938 139864642547712 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.145548 139864642547712 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.145623 139864642547712 transformer_layer.py:468] tlayer: End windows.
I0123 15:04:32.145666 139864642547712 transformer_layer.py:472] tlayer: final FFN.
I0123 15:04:32.145723 139864642547712 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.145849 139864642547712 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:04:32.146165 139864642547712 nn_components.py:325] mlp: activation = None
I0123 15:04:32.146207 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:04:32.148083 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.148173 139864642547712 nn_components.py:261] mlp: residual
I0123 15:04:32.150669 139864642547712 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.150746 139864642547712 transformer_base.py:443] tbase: final FFN
I0123 15:04:32.151170 139864642547712 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:04:32.153409 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:04:32.155312 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.155406 139864642547712 nn_components.py:261] mlp: residual
I0123 15:04:32.155697 139864642547712 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.155786 139864642547712 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 15:04:32.155895 139864642547712 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:04:32.155934 139864642547712 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:04:32.155963 139864642547712 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:04:32.157751 139864642547712 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.160143 139864642547712 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:04:32.165715 139864642547712 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.165970 139864642547712 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:04:32.168720 139864642547712 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:04:32.172384 139864642547712 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:04:32.172440 139864642547712 attention.py:418] Single window, no scan.
I0123 15:04:32.172474 139864642547712 transformer_layer.py:389] tlayer: self-attention.
I0123 15:04:32.172504 139864642547712 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.172564 139864642547712 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.173173 139864642547712 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.173249 139864642547712 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.173605 139864642547712 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.174370 139864642547712 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.176818 139864642547712 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.177581 139864642547712 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.177670 139864642547712 transformer_layer.py:468] tlayer: End windows.
I0123 15:04:32.177705 139864642547712 transformer_layer.py:472] tlayer: final FFN.
I0123 15:04:32.177764 139864642547712 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.177896 139864642547712 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:04:32.178227 139864642547712 nn_components.py:325] mlp: activation = None
I0123 15:04:32.178270 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:04:32.180185 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.180277 139864642547712 nn_components.py:261] mlp: residual
I0123 15:04:32.182790 139864642547712 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.182869 139864642547712 transformer_base.py:443] tbase: final FFN
I0123 15:04:32.183291 139864642547712 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:04:32.185528 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:04:32.187428 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.187521 139864642547712 nn_components.py:261] mlp: residual
I0123 15:04:32.187811 139864642547712 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.187889 139864642547712 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 15:04:32.188000 139864642547712 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:04:32.188039 139864642547712 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:04:32.188067 139864642547712 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:04:32.189873 139864642547712 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.192261 139864642547712 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:04:32.197834 139864642547712 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.198088 139864642547712 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:04:32.200649 139864642547712 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:04:32.204265 139864642547712 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:04:32.204319 139864642547712 attention.py:418] Single window, no scan.
I0123 15:04:32.204353 139864642547712 transformer_layer.py:389] tlayer: self-attention.
I0123 15:04:32.204382 139864642547712 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.204442 139864642547712 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.205052 139864642547712 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.205127 139864642547712 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.205485 139864642547712 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.206244 139864642547712 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.208690 139864642547712 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.209302 139864642547712 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.209378 139864642547712 transformer_layer.py:468] tlayer: End windows.
I0123 15:04:32.209412 139864642547712 transformer_layer.py:472] tlayer: final FFN.
I0123 15:04:32.209468 139864642547712 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.209593 139864642547712 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:04:32.209914 139864642547712 nn_components.py:325] mlp: activation = None
I0123 15:04:32.209956 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:04:32.211866 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.211957 139864642547712 nn_components.py:261] mlp: residual
I0123 15:04:32.214465 139864642547712 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.214544 139864642547712 transformer_base.py:443] tbase: final FFN
I0123 15:04:32.214960 139864642547712 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:04:32.217184 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:04:32.219083 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.219177 139864642547712 nn_components.py:261] mlp: residual
I0123 15:04:32.219467 139864642547712 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.219547 139864642547712 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 15:04:32.219652 139864642547712 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:04:32.219696 139864642547712 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:04:32.219728 139864642547712 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:04:32.221504 139864642547712 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.223908 139864642547712 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:04:32.229487 139864642547712 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.229749 139864642547712 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:04:32.232314 139864642547712 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:04:32.236027 139864642547712 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:04:32.236080 139864642547712 attention.py:418] Single window, no scan.
I0123 15:04:32.236114 139864642547712 transformer_layer.py:389] tlayer: self-attention.
I0123 15:04:32.236143 139864642547712 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.236204 139864642547712 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.236809 139864642547712 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.236885 139864642547712 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.237244 139864642547712 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.238015 139864642547712 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.240472 139864642547712 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.241083 139864642547712 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.241160 139864642547712 transformer_layer.py:468] tlayer: End windows.
I0123 15:04:32.241194 139864642547712 transformer_layer.py:472] tlayer: final FFN.
I0123 15:04:32.241252 139864642547712 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.241379 139864642547712 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:04:32.241704 139864642547712 nn_components.py:325] mlp: activation = None
I0123 15:04:32.241748 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:04:32.243633 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.243723 139864642547712 nn_components.py:261] mlp: residual
I0123 15:04:32.246243 139864642547712 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.246321 139864642547712 transformer_base.py:443] tbase: final FFN
I0123 15:04:32.246737 139864642547712 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:04:32.248966 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:04:32.250867 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.250962 139864642547712 nn_components.py:261] mlp: residual
I0123 15:04:32.251250 139864642547712 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.251494 139864642547712 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 15:04:32.251561 139864642547712 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 15:04:32.251625 139864642547712 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 15:04:32.251681 139864642547712 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 15:04:32.251733 139864642547712 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 15:04:32.251784 139864642547712 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 15:04:32.251834 139864642547712 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 15:04:32.251884 139864642547712 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 15:04:32.251935 139864642547712 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 15:04:32.251984 139864642547712 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 15:04:32.252034 139864642547712 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 15:04:32.252084 139864642547712 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 15:04:32.252119 139864642547712 decoder_stack.py:344] dstack: Final layernorm.
I0123 15:04:32.255002 139864642547712 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:04:32.299674 139864642547712 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.299758 139864642547712 decoder_stack.py:333] dstack: autoregressive generator.
I0123 15:04:32.299810 139864642547712 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 15:04:32.299916 139864642547712 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:04:32.299953 139864642547712 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:04:32.299982 139864642547712 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:04:32.300043 139864642547712 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.302401 139864642547712 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:04:32.307853 139864642547712 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.308112 139864642547712 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:04:32.310667 139864642547712 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:04:32.323543 139864642547712 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:04:32.323600 139864642547712 attention.py:418] Single window, no scan.
I0123 15:04:32.323634 139864642547712 transformer_layer.py:389] tlayer: self-attention.
I0123 15:04:32.323664 139864642547712 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.323725 139864642547712 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.324288 139864642547712 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.324362 139864642547712 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.324717 139864642547712 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.325409 139864642547712 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.328008 139864642547712 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.328639 139864642547712 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.328726 139864642547712 transformer_layer.py:468] tlayer: End windows.
I0123 15:04:32.328760 139864642547712 transformer_layer.py:472] tlayer: final FFN.
I0123 15:04:32.328819 139864642547712 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.328950 139864642547712 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:04:32.329059 139864642547712 nn_components.py:325] mlp: activation = None
I0123 15:04:32.329096 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:04:32.330965 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.331059 139864642547712 nn_components.py:261] mlp: residual
I0123 15:04:32.333476 139864642547712 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.333554 139864642547712 transformer_base.py:443] tbase: final FFN
I0123 15:04:32.333670 139864642547712 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:04:32.335935 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:04:32.337804 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.337896 139864642547712 nn_components.py:261] mlp: residual
I0123 15:04:32.338188 139864642547712 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.338267 139864642547712 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 15:04:32.338374 139864642547712 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:04:32.338412 139864642547712 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:04:32.338441 139864642547712 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:04:32.338503 139864642547712 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.340719 139864642547712 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:04:32.346133 139864642547712 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.346392 139864642547712 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:04:32.349033 139864642547712 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:04:32.361331 139864642547712 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:04:32.361387 139864642547712 attention.py:418] Single window, no scan.
I0123 15:04:32.361420 139864642547712 transformer_layer.py:389] tlayer: self-attention.
I0123 15:04:32.361450 139864642547712 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.361510 139864642547712 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.362076 139864642547712 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.362151 139864642547712 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.362505 139864642547712 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.363237 139864642547712 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.365711 139864642547712 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.366333 139864642547712 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.366409 139864642547712 transformer_layer.py:468] tlayer: End windows.
I0123 15:04:32.366448 139864642547712 transformer_layer.py:472] tlayer: final FFN.
I0123 15:04:32.366508 139864642547712 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.366636 139864642547712 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:04:32.366744 139864642547712 nn_components.py:325] mlp: activation = None
I0123 15:04:32.366781 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:04:32.368635 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.368729 139864642547712 nn_components.py:261] mlp: residual
I0123 15:04:32.371116 139864642547712 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.371194 139864642547712 transformer_base.py:443] tbase: final FFN
I0123 15:04:32.371301 139864642547712 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:04:32.373557 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:04:32.375424 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.375518 139864642547712 nn_components.py:261] mlp: residual
I0123 15:04:32.375810 139864642547712 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.375890 139864642547712 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 15:04:32.375998 139864642547712 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:04:32.376036 139864642547712 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:04:32.376065 139864642547712 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:04:32.376126 139864642547712 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.378514 139864642547712 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:04:32.384074 139864642547712 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.384330 139864642547712 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:04:32.387002 139864642547712 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:04:32.399269 139864642547712 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:04:32.399324 139864642547712 attention.py:418] Single window, no scan.
I0123 15:04:32.399359 139864642547712 transformer_layer.py:389] tlayer: self-attention.
I0123 15:04:32.399388 139864642547712 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.399448 139864642547712 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.399998 139864642547712 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.400073 139864642547712 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.400428 139864642547712 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.401166 139864642547712 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.403625 139864642547712 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.404243 139864642547712 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.404318 139864642547712 transformer_layer.py:468] tlayer: End windows.
I0123 15:04:32.404351 139864642547712 transformer_layer.py:472] tlayer: final FFN.
I0123 15:04:32.404415 139864642547712 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.404545 139864642547712 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:04:32.404653 139864642547712 nn_components.py:325] mlp: activation = None
I0123 15:04:32.404691 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:04:32.406532 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.406625 139864642547712 nn_components.py:261] mlp: residual
I0123 15:04:32.409032 139864642547712 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.409110 139864642547712 transformer_base.py:443] tbase: final FFN
I0123 15:04:32.409216 139864642547712 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:04:32.411473 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:04:32.413321 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.413415 139864642547712 nn_components.py:261] mlp: residual
I0123 15:04:32.413712 139864642547712 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.413792 139864642547712 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 15:04:32.413900 139864642547712 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:04:32.413937 139864642547712 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:04:32.413966 139864642547712 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:04:32.414027 139864642547712 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.416249 139864642547712 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:04:32.421660 139864642547712 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.421920 139864642547712 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:04:32.424556 139864642547712 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:04:32.437199 139864642547712 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:04:32.437253 139864642547712 attention.py:418] Single window, no scan.
I0123 15:04:32.437288 139864642547712 transformer_layer.py:389] tlayer: self-attention.
I0123 15:04:32.437317 139864642547712 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.437377 139864642547712 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.437936 139864642547712 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.438012 139864642547712 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.438364 139864642547712 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.439090 139864642547712 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.441540 139864642547712 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.442160 139864642547712 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.442237 139864642547712 transformer_layer.py:468] tlayer: End windows.
I0123 15:04:32.442270 139864642547712 transformer_layer.py:472] tlayer: final FFN.
I0123 15:04:32.442327 139864642547712 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.442460 139864642547712 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:04:32.442568 139864642547712 nn_components.py:325] mlp: activation = None
I0123 15:04:32.442606 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:04:32.444449 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.444540 139864642547712 nn_components.py:261] mlp: residual
I0123 15:04:32.446965 139864642547712 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.447045 139864642547712 transformer_base.py:443] tbase: final FFN
I0123 15:04:32.447153 139864642547712 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:04:32.449387 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:04:32.451268 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.451363 139864642547712 nn_components.py:261] mlp: residual
I0123 15:04:32.451653 139864642547712 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.451734 139864642547712 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 15:04:32.451841 139864642547712 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:04:32.451879 139864642547712 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:04:32.451908 139864642547712 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:04:32.451970 139864642547712 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.454202 139864642547712 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:04:32.459625 139864642547712 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.459885 139864642547712 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:04:32.462537 139864642547712 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:04:32.474794 139864642547712 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:04:32.474849 139864642547712 attention.py:418] Single window, no scan.
I0123 15:04:32.474883 139864642547712 transformer_layer.py:389] tlayer: self-attention.
I0123 15:04:32.474912 139864642547712 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.474972 139864642547712 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.475523 139864642547712 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.475597 139864642547712 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.475951 139864642547712 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.476686 139864642547712 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.479146 139864642547712 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.479758 139864642547712 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.479833 139864642547712 transformer_layer.py:468] tlayer: End windows.
I0123 15:04:32.479867 139864642547712 transformer_layer.py:472] tlayer: final FFN.
I0123 15:04:32.479923 139864642547712 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.480240 139864642547712 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:04:32.480351 139864642547712 nn_components.py:325] mlp: activation = None
I0123 15:04:32.480389 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:04:32.482242 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.482335 139864642547712 nn_components.py:261] mlp: residual
I0123 15:04:32.484722 139864642547712 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.484799 139864642547712 transformer_base.py:443] tbase: final FFN
I0123 15:04:32.484906 139864642547712 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:04:32.487185 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:04:32.489047 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.489140 139864642547712 nn_components.py:261] mlp: residual
I0123 15:04:32.489429 139864642547712 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.489510 139864642547712 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 15:04:32.489617 139864642547712 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:04:32.489663 139864642547712 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:04:32.489694 139864642547712 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:04:32.489756 139864642547712 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.491982 139864642547712 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:04:32.497354 139864642547712 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.497610 139864642547712 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:04:32.500251 139864642547712 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:04:32.512479 139864642547712 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:04:32.512534 139864642547712 attention.py:418] Single window, no scan.
I0123 15:04:32.512568 139864642547712 transformer_layer.py:389] tlayer: self-attention.
I0123 15:04:32.512596 139864642547712 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.512657 139864642547712 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.513206 139864642547712 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.513280 139864642547712 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.513634 139864642547712 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.514375 139864642547712 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.516822 139864642547712 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.517432 139864642547712 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.517507 139864642547712 transformer_layer.py:468] tlayer: End windows.
I0123 15:04:32.517540 139864642547712 transformer_layer.py:472] tlayer: final FFN.
I0123 15:04:32.517596 139864642547712 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.517730 139864642547712 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:04:32.517846 139864642547712 nn_components.py:325] mlp: activation = None
I0123 15:04:32.517885 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:04:32.519734 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.519826 139864642547712 nn_components.py:261] mlp: residual
I0123 15:04:32.522238 139864642547712 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.522318 139864642547712 transformer_base.py:443] tbase: final FFN
I0123 15:04:32.522427 139864642547712 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:04:32.524674 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:04:32.526529 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.526622 139864642547712 nn_components.py:261] mlp: residual
I0123 15:04:32.526912 139864642547712 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.526991 139864642547712 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 15:04:32.527098 139864642547712 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:04:32.527137 139864642547712 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:04:32.527166 139864642547712 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:04:32.527226 139864642547712 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.529443 139864642547712 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:04:32.534887 139864642547712 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.535143 139864642547712 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:04:32.537798 139864642547712 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:04:32.550491 139864642547712 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:04:32.550545 139864642547712 attention.py:418] Single window, no scan.
I0123 15:04:32.550580 139864642547712 transformer_layer.py:389] tlayer: self-attention.
I0123 15:04:32.550609 139864642547712 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.550669 139864642547712 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.551223 139864642547712 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.551297 139864642547712 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.551651 139864642547712 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.552382 139864642547712 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.554857 139864642547712 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.555470 139864642547712 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.555546 139864642547712 transformer_layer.py:468] tlayer: End windows.
I0123 15:04:32.555580 139864642547712 transformer_layer.py:472] tlayer: final FFN.
I0123 15:04:32.555636 139864642547712 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.555760 139864642547712 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:04:32.555874 139864642547712 nn_components.py:325] mlp: activation = None
I0123 15:04:32.555912 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:04:32.557775 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.557866 139864642547712 nn_components.py:261] mlp: residual
I0123 15:04:32.560253 139864642547712 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.560331 139864642547712 transformer_base.py:443] tbase: final FFN
I0123 15:04:32.560437 139864642547712 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:04:32.562725 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:04:32.564617 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.564709 139864642547712 nn_components.py:261] mlp: residual
I0123 15:04:32.564999 139864642547712 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.565078 139864642547712 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 15:04:32.565186 139864642547712 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:04:32.565225 139864642547712 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:04:32.565254 139864642547712 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:04:32.565315 139864642547712 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.567611 139864642547712 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:04:32.573014 139864642547712 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.573278 139864642547712 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:04:32.575915 139864642547712 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:04:32.588073 139864642547712 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:04:32.588128 139864642547712 attention.py:418] Single window, no scan.
I0123 15:04:32.588162 139864642547712 transformer_layer.py:389] tlayer: self-attention.
I0123 15:04:32.588191 139864642547712 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.588251 139864642547712 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.588800 139864642547712 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.588875 139864642547712 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.589225 139864642547712 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.589951 139864642547712 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.592377 139864642547712 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.592996 139864642547712 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.593071 139864642547712 transformer_layer.py:468] tlayer: End windows.
I0123 15:04:32.593106 139864642547712 transformer_layer.py:472] tlayer: final FFN.
I0123 15:04:32.593162 139864642547712 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.593287 139864642547712 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:04:32.593394 139864642547712 nn_components.py:325] mlp: activation = None
I0123 15:04:32.593436 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:04:32.595279 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.595371 139864642547712 nn_components.py:261] mlp: residual
I0123 15:04:32.597748 139864642547712 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.597827 139864642547712 transformer_base.py:443] tbase: final FFN
I0123 15:04:32.597934 139864642547712 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:04:32.600169 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:04:32.601989 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.602082 139864642547712 nn_components.py:261] mlp: residual
I0123 15:04:32.602374 139864642547712 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.602452 139864642547712 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 15:04:32.602559 139864642547712 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:04:32.602596 139864642547712 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:04:32.602625 139864642547712 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:04:32.602687 139864642547712 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.604891 139864642547712 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:04:32.610354 139864642547712 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.610606 139864642547712 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:04:32.613214 139864642547712 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:04:32.625385 139864642547712 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:04:32.625440 139864642547712 attention.py:418] Single window, no scan.
I0123 15:04:32.625475 139864642547712 transformer_layer.py:389] tlayer: self-attention.
I0123 15:04:32.625505 139864642547712 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.625566 139864642547712 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.626142 139864642547712 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.626222 139864642547712 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.626594 139864642547712 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.627301 139864642547712 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.629829 139864642547712 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.630447 139864642547712 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.630524 139864642547712 transformer_layer.py:468] tlayer: End windows.
I0123 15:04:32.630558 139864642547712 transformer_layer.py:472] tlayer: final FFN.
I0123 15:04:32.630614 139864642547712 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.630740 139864642547712 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:04:32.630847 139864642547712 nn_components.py:325] mlp: activation = None
I0123 15:04:32.630885 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:04:32.632755 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.632847 139864642547712 nn_components.py:261] mlp: residual
I0123 15:04:32.635237 139864642547712 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.635316 139864642547712 transformer_base.py:443] tbase: final FFN
I0123 15:04:32.635423 139864642547712 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:04:32.637679 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:04:32.639510 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.639603 139864642547712 nn_components.py:261] mlp: residual
I0123 15:04:32.639896 139864642547712 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.639975 139864642547712 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 15:04:32.640082 139864642547712 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:04:32.640120 139864642547712 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:04:32.640150 139864642547712 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:04:32.640211 139864642547712 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.642435 139864642547712 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:04:32.647806 139864642547712 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.648056 139864642547712 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:04:32.650697 139864642547712 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:04:32.663355 139864642547712 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:04:32.663409 139864642547712 attention.py:418] Single window, no scan.
I0123 15:04:32.663443 139864642547712 transformer_layer.py:389] tlayer: self-attention.
I0123 15:04:32.663472 139864642547712 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.663537 139864642547712 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.664098 139864642547712 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.664173 139864642547712 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.664531 139864642547712 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.665209 139864642547712 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.667740 139864642547712 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.668355 139864642547712 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.668430 139864642547712 transformer_layer.py:468] tlayer: End windows.
I0123 15:04:32.668464 139864642547712 transformer_layer.py:472] tlayer: final FFN.
I0123 15:04:32.668521 139864642547712 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.668648 139864642547712 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:04:32.668755 139864642547712 nn_components.py:325] mlp: activation = None
I0123 15:04:32.668792 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:04:32.670655 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.670754 139864642547712 nn_components.py:261] mlp: residual
I0123 15:04:32.673161 139864642547712 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.673238 139864642547712 transformer_base.py:443] tbase: final FFN
I0123 15:04:32.673344 139864642547712 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:04:32.675608 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:04:32.677463 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.677557 139864642547712 nn_components.py:261] mlp: residual
I0123 15:04:32.677856 139864642547712 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.677937 139864642547712 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 15:04:32.678045 139864642547712 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:04:32.678082 139864642547712 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:04:32.678111 139864642547712 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:04:32.678171 139864642547712 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.680387 139864642547712 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:04:32.685789 139864642547712 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.686046 139864642547712 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:04:32.688686 139864642547712 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:04:32.701030 139864642547712 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:04:32.701084 139864642547712 attention.py:418] Single window, no scan.
I0123 15:04:32.701118 139864642547712 transformer_layer.py:389] tlayer: self-attention.
I0123 15:04:32.701147 139864642547712 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.701206 139864642547712 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.701768 139864642547712 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.701843 139864642547712 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.702199 139864642547712 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.702884 139864642547712 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.705394 139864642547712 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.706020 139864642547712 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.706097 139864642547712 transformer_layer.py:468] tlayer: End windows.
I0123 15:04:32.706130 139864642547712 transformer_layer.py:472] tlayer: final FFN.
I0123 15:04:32.706186 139864642547712 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.706314 139864642547712 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:04:32.706422 139864642547712 nn_components.py:325] mlp: activation = None
I0123 15:04:32.706458 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:04:32.708327 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.708424 139864642547712 nn_components.py:261] mlp: residual
I0123 15:04:32.710833 139864642547712 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.710911 139864642547712 transformer_base.py:443] tbase: final FFN
I0123 15:04:32.711018 139864642547712 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:04:32.713274 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:04:32.715146 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.715238 139864642547712 nn_components.py:261] mlp: residual
I0123 15:04:32.715525 139864642547712 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.715604 139864642547712 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 15:04:32.715711 139864642547712 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:04:32.715749 139864642547712 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:04:32.715777 139864642547712 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:04:32.715837 139864642547712 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.718075 139864642547712 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:04:32.723485 139864642547712 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.723738 139864642547712 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:04:32.726397 139864642547712 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:04:32.738631 139864642547712 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:04:32.738684 139864642547712 attention.py:418] Single window, no scan.
I0123 15:04:32.738718 139864642547712 transformer_layer.py:389] tlayer: self-attention.
I0123 15:04:32.738747 139864642547712 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.738806 139864642547712 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.739360 139864642547712 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.739435 139864642547712 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.739789 139864642547712 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.740462 139864642547712 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.742960 139864642547712 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.743573 139864642547712 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.743649 139864642547712 transformer_layer.py:468] tlayer: End windows.
I0123 15:04:32.743683 139864642547712 transformer_layer.py:472] tlayer: final FFN.
I0123 15:04:32.743739 139864642547712 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.743864 139864642547712 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:04:32.743972 139864642547712 nn_components.py:325] mlp: activation = None
I0123 15:04:32.744010 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:04:32.745867 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.745959 139864642547712 nn_components.py:261] mlp: residual
I0123 15:04:32.748358 139864642547712 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.748436 139864642547712 transformer_base.py:443] tbase: final FFN
I0123 15:04:32.748544 139864642547712 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:04:32.750822 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:04:32.752676 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.752768 139864642547712 nn_components.py:261] mlp: residual
I0123 15:04:32.753058 139864642547712 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.753143 139864642547712 decoder_stack.py:344] dstack: Final layernorm.
I0123 15:04:32.755993 139864642547712 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:04:32.805802 139864642547712 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.805885 139864642547712 decoder_stack.py:333] dstack: autoregressive generator.
I0123 15:04:32.805938 139864642547712 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 15:04:32.806040 139864642547712 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:04:32.806077 139864642547712 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:04:32.806107 139864642547712 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:04:32.806169 139864642547712 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.808453 139864642547712 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:04:32.813914 139864642547712 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.814167 139864642547712 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:04:32.816732 139864642547712 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:04:32.829052 139864642547712 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:04:32.829107 139864642547712 attention.py:418] Single window, no scan.
I0123 15:04:32.829142 139864642547712 transformer_layer.py:389] tlayer: self-attention.
I0123 15:04:32.829171 139864642547712 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.829232 139864642547712 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.829794 139864642547712 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.829870 139864642547712 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.830223 139864642547712 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.830895 139864642547712 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.833330 139864642547712 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.833948 139864642547712 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.834027 139864642547712 transformer_layer.py:468] tlayer: End windows.
I0123 15:04:32.834061 139864642547712 transformer_layer.py:472] tlayer: final FFN.
I0123 15:04:32.834117 139864642547712 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.834244 139864642547712 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:04:32.834358 139864642547712 nn_components.py:325] mlp: activation = None
I0123 15:04:32.834396 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:04:32.836684 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.836778 139864642547712 nn_components.py:261] mlp: residual
I0123 15:04:32.839191 139864642547712 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.839270 139864642547712 transformer_base.py:443] tbase: final FFN
I0123 15:04:32.839377 139864642547712 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:04:32.841556 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:04:32.843396 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.843491 139864642547712 nn_components.py:261] mlp: residual
I0123 15:04:32.843783 139864642547712 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.843863 139864642547712 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 15:04:32.843969 139864642547712 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:04:32.844006 139864642547712 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:04:32.844035 139864642547712 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:04:32.844096 139864642547712 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.846329 139864642547712 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:04:32.851774 139864642547712 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.852034 139864642547712 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:04:32.854588 139864642547712 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:04:32.867044 139864642547712 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:04:32.867100 139864642547712 attention.py:418] Single window, no scan.
I0123 15:04:32.867135 139864642547712 transformer_layer.py:389] tlayer: self-attention.
I0123 15:04:32.867165 139864642547712 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.867229 139864642547712 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.867795 139864642547712 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.867869 139864642547712 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.868222 139864642547712 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.868890 139864642547712 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.871361 139864642547712 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.871985 139864642547712 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.872060 139864642547712 transformer_layer.py:468] tlayer: End windows.
I0123 15:04:32.872093 139864642547712 transformer_layer.py:472] tlayer: final FFN.
I0123 15:04:32.872150 139864642547712 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.872275 139864642547712 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:04:32.872381 139864642547712 nn_components.py:325] mlp: activation = None
I0123 15:04:32.872424 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:04:32.874364 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.874460 139864642547712 nn_components.py:261] mlp: residual
I0123 15:04:32.876892 139864642547712 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.876969 139864642547712 transformer_base.py:443] tbase: final FFN
I0123 15:04:32.877076 139864642547712 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:04:32.879302 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:04:32.881150 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.881243 139864642547712 nn_components.py:261] mlp: residual
I0123 15:04:32.881533 139864642547712 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.881613 139864642547712 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 15:04:32.881726 139864642547712 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:04:32.881764 139864642547712 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:04:32.881793 139864642547712 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:04:32.881854 139864642547712 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.884129 139864642547712 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:04:32.889733 139864642547712 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.889991 139864642547712 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:04:32.892640 139864642547712 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:04:32.905184 139864642547712 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:04:32.905239 139864642547712 attention.py:418] Single window, no scan.
I0123 15:04:32.905273 139864642547712 transformer_layer.py:389] tlayer: self-attention.
I0123 15:04:32.905302 139864642547712 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.905361 139864642547712 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.905921 139864642547712 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.905997 139864642547712 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.906355 139864642547712 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.907049 139864642547712 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.909505 139864642547712 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.910126 139864642547712 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.910207 139864642547712 transformer_layer.py:468] tlayer: End windows.
I0123 15:04:32.910242 139864642547712 transformer_layer.py:472] tlayer: final FFN.
I0123 15:04:32.910300 139864642547712 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.910430 139864642547712 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:04:32.910541 139864642547712 nn_components.py:325] mlp: activation = None
I0123 15:04:32.910586 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:04:32.912572 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.912662 139864642547712 nn_components.py:261] mlp: residual
I0123 15:04:32.915094 139864642547712 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.915175 139864642547712 transformer_base.py:443] tbase: final FFN
I0123 15:04:32.915286 139864642547712 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:04:32.917491 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:04:32.919384 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.919481 139864642547712 nn_components.py:261] mlp: residual
I0123 15:04:32.919791 139864642547712 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.919886 139864642547712 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 15:04:32.919997 139864642547712 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:04:32.920034 139864642547712 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:04:32.920063 139864642547712 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:04:32.920123 139864642547712 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.922353 139864642547712 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:04:32.928072 139864642547712 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.928328 139864642547712 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:04:32.930934 139864642547712 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:04:32.943418 139864642547712 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:04:32.943475 139864642547712 attention.py:418] Single window, no scan.
I0123 15:04:32.943511 139864642547712 transformer_layer.py:389] tlayer: self-attention.
I0123 15:04:32.943543 139864642547712 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.943606 139864642547712 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.944168 139864642547712 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.944244 139864642547712 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.944604 139864642547712 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.945284 139864642547712 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.947782 139864642547712 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.948419 139864642547712 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.948494 139864642547712 transformer_layer.py:468] tlayer: End windows.
I0123 15:04:32.948528 139864642547712 transformer_layer.py:472] tlayer: final FFN.
I0123 15:04:32.948584 139864642547712 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.948709 139864642547712 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:04:32.948814 139864642547712 nn_components.py:325] mlp: activation = None
I0123 15:04:32.948851 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:04:32.951293 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.951391 139864642547712 nn_components.py:261] mlp: residual
I0123 15:04:32.953814 139864642547712 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.953894 139864642547712 transformer_base.py:443] tbase: final FFN
I0123 15:04:32.954004 139864642547712 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:04:32.956239 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:04:32.958083 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.958178 139864642547712 nn_components.py:261] mlp: residual
I0123 15:04:32.958480 139864642547712 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.958563 139864642547712 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 15:04:32.958673 139864642547712 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:04:32.958712 139864642547712 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:04:32.958742 139864642547712 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:04:32.958805 139864642547712 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.961056 139864642547712 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:04:32.966611 139864642547712 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.966883 139864642547712 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:04:32.969488 139864642547712 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:04:32.981797 139864642547712 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:04:32.981856 139864642547712 attention.py:418] Single window, no scan.
I0123 15:04:32.981890 139864642547712 transformer_layer.py:389] tlayer: self-attention.
I0123 15:04:32.981920 139864642547712 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.981980 139864642547712 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.982541 139864642547712 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.982618 139864642547712 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.982982 139864642547712 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.983677 139864642547712 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.986112 139864642547712 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.986726 139864642547712 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.986803 139864642547712 transformer_layer.py:468] tlayer: End windows.
I0123 15:04:32.986836 139864642547712 transformer_layer.py:472] tlayer: final FFN.
I0123 15:04:32.986894 139864642547712 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.987020 139864642547712 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:04:32.987128 139864642547712 nn_components.py:325] mlp: activation = None
I0123 15:04:32.987166 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:04:32.989081 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.989181 139864642547712 nn_components.py:261] mlp: residual
I0123 15:04:32.991567 139864642547712 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.991647 139864642547712 transformer_base.py:443] tbase: final FFN
I0123 15:04:32.991755 139864642547712 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:04:32.993928 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:04:32.995762 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.995856 139864642547712 nn_components.py:261] mlp: residual
I0123 15:04:32.996145 139864642547712 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.996225 139864642547712 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 15:04:32.996331 139864642547712 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:04:32.996368 139864642547712 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:04:32.996396 139864642547712 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:04:32.996456 139864642547712 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:32.998686 139864642547712 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:04:33.004130 139864642547712 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.004385 139864642547712 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:04:33.006965 139864642547712 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:04:33.019201 139864642547712 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:04:33.019256 139864642547712 attention.py:418] Single window, no scan.
I0123 15:04:33.019291 139864642547712 transformer_layer.py:389] tlayer: self-attention.
I0123 15:04:33.019320 139864642547712 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.019382 139864642547712 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.019935 139864642547712 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.020010 139864642547712 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.020365 139864642547712 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.021032 139864642547712 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.023484 139864642547712 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.024099 139864642547712 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.024177 139864642547712 transformer_layer.py:468] tlayer: End windows.
I0123 15:04:33.024210 139864642547712 transformer_layer.py:472] tlayer: final FFN.
I0123 15:04:33.024267 139864642547712 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.024394 139864642547712 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:04:33.024502 139864642547712 nn_components.py:325] mlp: activation = None
I0123 15:04:33.024540 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:04:33.026613 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.026712 139864642547712 nn_components.py:261] mlp: residual
I0123 15:04:33.029105 139864642547712 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.029183 139864642547712 transformer_base.py:443] tbase: final FFN
I0123 15:04:33.029290 139864642547712 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:04:33.031627 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:04:33.033456 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.033549 139864642547712 nn_components.py:261] mlp: residual
I0123 15:04:33.033846 139864642547712 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.033929 139864642547712 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 15:04:33.034037 139864642547712 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:04:33.034074 139864642547712 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:04:33.034103 139864642547712 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:04:33.034164 139864642547712 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.036381 139864642547712 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:04:33.041848 139864642547712 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.042109 139864642547712 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:04:33.044669 139864642547712 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:04:33.056864 139864642547712 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:04:33.056917 139864642547712 attention.py:418] Single window, no scan.
I0123 15:04:33.056951 139864642547712 transformer_layer.py:389] tlayer: self-attention.
I0123 15:04:33.056981 139864642547712 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.057041 139864642547712 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.057587 139864642547712 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.057669 139864642547712 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.058026 139864642547712 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.058702 139864642547712 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.061141 139864642547712 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.061764 139864642547712 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.061842 139864642547712 transformer_layer.py:468] tlayer: End windows.
I0123 15:04:33.061876 139864642547712 transformer_layer.py:472] tlayer: final FFN.
I0123 15:04:33.061932 139864642547712 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.062055 139864642547712 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:04:33.062161 139864642547712 nn_components.py:325] mlp: activation = None
I0123 15:04:33.062198 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:04:33.064491 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.064584 139864642547712 nn_components.py:261] mlp: residual
I0123 15:04:33.066963 139864642547712 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.067043 139864642547712 transformer_base.py:443] tbase: final FFN
I0123 15:04:33.067150 139864642547712 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:04:33.069326 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:04:33.071164 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.071257 139864642547712 nn_components.py:261] mlp: residual
I0123 15:04:33.071543 139864642547712 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.071622 139864642547712 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 15:04:33.071726 139864642547712 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:04:33.071763 139864642547712 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:04:33.071792 139864642547712 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:04:33.071853 139864642547712 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.074054 139864642547712 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:04:33.079428 139864642547712 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.079681 139864642547712 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:04:33.082250 139864642547712 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:04:33.094568 139864642547712 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:04:33.094623 139864642547712 attention.py:418] Single window, no scan.
I0123 15:04:33.094658 139864642547712 transformer_layer.py:389] tlayer: self-attention.
I0123 15:04:33.094686 139864642547712 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.094746 139864642547712 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.095297 139864642547712 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.095371 139864642547712 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.095722 139864642547712 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.096395 139864642547712 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.098825 139864642547712 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.099438 139864642547712 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.099511 139864642547712 transformer_layer.py:468] tlayer: End windows.
I0123 15:04:33.099544 139864642547712 transformer_layer.py:472] tlayer: final FFN.
I0123 15:04:33.099599 139864642547712 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.099721 139864642547712 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:04:33.099826 139864642547712 nn_components.py:325] mlp: activation = None
I0123 15:04:33.099863 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:04:33.101778 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.101871 139864642547712 nn_components.py:261] mlp: residual
I0123 15:04:33.104252 139864642547712 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.104333 139864642547712 transformer_base.py:443] tbase: final FFN
I0123 15:04:33.104442 139864642547712 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:04:33.106646 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:04:33.108486 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.108578 139864642547712 nn_components.py:261] mlp: residual
I0123 15:04:33.108871 139864642547712 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.108949 139864642547712 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 15:04:33.109056 139864642547712 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:04:33.109094 139864642547712 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:04:33.109123 139864642547712 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:04:33.109184 139864642547712 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.111403 139864642547712 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:04:33.116821 139864642547712 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.117076 139864642547712 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:04:33.119637 139864642547712 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:04:33.132062 139864642547712 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:04:33.132117 139864642547712 attention.py:418] Single window, no scan.
I0123 15:04:33.132152 139864642547712 transformer_layer.py:389] tlayer: self-attention.
I0123 15:04:33.132182 139864642547712 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.132242 139864642547712 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.132796 139864642547712 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.132870 139864642547712 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.133221 139864642547712 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.133893 139864642547712 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.136294 139864642547712 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.136898 139864642547712 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.136971 139864642547712 transformer_layer.py:468] tlayer: End windows.
I0123 15:04:33.137005 139864642547712 transformer_layer.py:472] tlayer: final FFN.
I0123 15:04:33.137060 139864642547712 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.137181 139864642547712 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:04:33.137287 139864642547712 nn_components.py:325] mlp: activation = None
I0123 15:04:33.137324 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:04:33.139220 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.139312 139864642547712 nn_components.py:261] mlp: residual
I0123 15:04:33.141694 139864642547712 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.141775 139864642547712 transformer_base.py:443] tbase: final FFN
I0123 15:04:33.141885 139864642547712 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:04:33.144085 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:04:33.145926 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.146020 139864642547712 nn_components.py:261] mlp: residual
I0123 15:04:33.146311 139864642547712 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.146389 139864642547712 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 15:04:33.146497 139864642547712 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:04:33.146534 139864642547712 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:04:33.146563 139864642547712 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:04:33.146622 139864642547712 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.148833 139864642547712 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:04:33.154253 139864642547712 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.154509 139864642547712 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:04:33.157065 139864642547712 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:04:33.169217 139864642547712 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:04:33.169272 139864642547712 attention.py:418] Single window, no scan.
I0123 15:04:33.169306 139864642547712 transformer_layer.py:389] tlayer: self-attention.
I0123 15:04:33.169336 139864642547712 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.169395 139864642547712 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.169950 139864642547712 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.170024 139864642547712 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.170372 139864642547712 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.171036 139864642547712 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.173441 139864642547712 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.174057 139864642547712 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.174132 139864642547712 transformer_layer.py:468] tlayer: End windows.
I0123 15:04:33.174164 139864642547712 transformer_layer.py:472] tlayer: final FFN.
I0123 15:04:33.174220 139864642547712 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.174343 139864642547712 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:04:33.174449 139864642547712 nn_components.py:325] mlp: activation = None
I0123 15:04:33.174485 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:04:33.176743 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.176834 139864642547712 nn_components.py:261] mlp: residual
I0123 15:04:33.179240 139864642547712 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.179317 139864642547712 transformer_base.py:443] tbase: final FFN
I0123 15:04:33.179432 139864642547712 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:04:33.181619 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:04:33.183468 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.183560 139864642547712 nn_components.py:261] mlp: residual
I0123 15:04:33.183852 139864642547712 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.183929 139864642547712 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 15:04:33.184034 139864642547712 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:04:33.184072 139864642547712 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:04:33.184100 139864642547712 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:04:33.184160 139864642547712 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.186376 139864642547712 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:04:33.191811 139864642547712 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.192065 139864642547712 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:04:33.194628 139864642547712 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:04:33.206760 139864642547712 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:04:33.206815 139864642547712 attention.py:418] Single window, no scan.
I0123 15:04:33.206849 139864642547712 transformer_layer.py:389] tlayer: self-attention.
I0123 15:04:33.206877 139864642547712 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.206934 139864642547712 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.207479 139864642547712 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.207553 139864642547712 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.207902 139864642547712 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.208576 139864642547712 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.211147 139864642547712 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.211758 139864642547712 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.211834 139864642547712 transformer_layer.py:468] tlayer: End windows.
I0123 15:04:33.211867 139864642547712 transformer_layer.py:472] tlayer: final FFN.
I0123 15:04:33.211924 139864642547712 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.212047 139864642547712 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:04:33.212154 139864642547712 nn_components.py:325] mlp: activation = None
I0123 15:04:33.212191 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:04:33.214075 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.214165 139864642547712 nn_components.py:261] mlp: residual
I0123 15:04:33.216518 139864642547712 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.216594 139864642547712 transformer_base.py:443] tbase: final FFN
I0123 15:04:33.216698 139864642547712 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:04:33.218867 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:04:33.220681 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.220774 139864642547712 nn_components.py:261] mlp: residual
I0123 15:04:33.221065 139864642547712 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.221143 139864642547712 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 15:04:33.221248 139864642547712 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:04:33.221284 139864642547712 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:04:33.221312 139864642547712 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:04:33.221372 139864642547712 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.223576 139864642547712 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:04:33.228967 139864642547712 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.229223 139864642547712 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:04:33.231790 139864642547712 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:04:33.243904 139864642547712 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:04:33.243958 139864642547712 attention.py:418] Single window, no scan.
I0123 15:04:33.243991 139864642547712 transformer_layer.py:389] tlayer: self-attention.
I0123 15:04:33.244020 139864642547712 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.244078 139864642547712 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.244618 139864642547712 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.244692 139864642547712 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.245037 139864642547712 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.245697 139864642547712 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.248093 139864642547712 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.248699 139864642547712 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.248774 139864642547712 transformer_layer.py:468] tlayer: End windows.
I0123 15:04:33.248807 139864642547712 transformer_layer.py:472] tlayer: final FFN.
I0123 15:04:33.248863 139864642547712 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.248988 139864642547712 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:04:33.249097 139864642547712 nn_components.py:325] mlp: activation = None
I0123 15:04:33.249135 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:04:33.251022 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.251114 139864642547712 nn_components.py:261] mlp: residual
I0123 15:04:33.253486 139864642547712 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.253563 139864642547712 transformer_base.py:443] tbase: final FFN
I0123 15:04:33.253678 139864642547712 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:04:33.255859 139864642547712 nn_components.py:329] mlp: final activation = None
I0123 15:04:33.257691 139864642547712 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.257783 139864642547712 nn_components.py:261] mlp: residual
I0123 15:04:33.258073 139864642547712 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:33.258155 139864642547712 decoder_stack.py:344] dstack: Final layernorm.
I0123 15:04:33.260924 139864642547712 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:04:49.201011 139864642547712 alphageometry.py:566] LM output (score=-1.204332): "p : C b e p 23 D b p e p 24 ;"
I0123 15:04:49.201256 139864642547712 alphageometry.py:567] Translation: "p = on_line p b e, on_bline p e b"

I0123 15:04:49.201312 139864642547712 alphageometry.py:576] Solving: "a b c = triangle a b c; d = angle_bisector d b c a, on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g f b e; h = on_circle h g b, on_line h c b; i = circle i f a e; j = on_circle j i a, on_line j c a; k = midpoint k b a; l = lc_tangent l k a, on_line l c a; m = circle m c e a; n = circle n j l e; o = on_line o b a, on_line o j h; p = on_line p b e, on_bline p e b ? coll o m n"
I0123 15:04:49.201501 139864642547712 graph.py:498] 
I0123 15:04:49.201560 139864642547712 graph.py:499] a b c = triangle a b c; d = angle_bisector d b c a, on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g f b e; h = on_circle h g b, on_line h c b; i = circle i f a e; j = on_circle j i a, on_line j c a; k = midpoint k b a; l = lc_tangent l k a, on_line l c a; m = circle m c e a; n = circle n j l e; o = on_line o b a, on_line o j h; p = on_line p b e, on_bline p e b ? coll o m n
I0123 15:04:52.632441 139864642547712 ddar.py:60] Depth 1/1000 time = 3.336735248565674
I0123 15:04:58.156306 139864642547712 ddar.py:60] Depth 2/1000 time = 5.523678302764893
I0123 15:05:05.490448 139864642547712 ddar.py:60] Depth 3/1000 time = 7.333957672119141
I0123 15:05:12.885947 139864642547712 ddar.py:60] Depth 4/1000 time = 7.3953166007995605
I0123 15:05:20.118415 139864642547712 ddar.py:60] Depth 5/1000 time = 7.23202919960022
I0123 15:05:29.517750 139864642547712 ddar.py:60] Depth 6/1000 time = 9.32703971862793
I0123 15:05:38.755512 139864642547712 ddar.py:60] Depth 7/1000 time = 9.23757791519165
I0123 15:05:48.304723 139864642547712 ddar.py:60] Depth 8/1000 time = 9.549006938934326
I0123 15:05:57.558970 139864642547712 ddar.py:60] Depth 9/1000 time = 9.245611429214478
I0123 15:06:07.983185 139864642547712 ddar.py:60] Depth 10/1000 time = 10.42401385307312
I0123 15:06:19.674289 139864642547712 ddar.py:60] Depth 11/1000 time = 11.690911531448364
I0123 15:06:31.306634 139864642547712 ddar.py:60] Depth 12/1000 time = 11.632119178771973
I0123 15:06:44.956985 139864642547712 ddar.py:60] Depth 13/1000 time = 13.65010690689087
I0123 15:06:57.587717 139864642547712 ddar.py:60] Depth 14/1000 time = 12.630494594573975
I0123 15:07:11.320200 139864642547712 ddar.py:60] Depth 15/1000 time = 13.732170820236206
I0123 15:07:25.656362 139864642547712 ddar.py:60] Depth 16/1000 time = 14.335752725601196
I0123 15:07:39.732002 139864642547712 ddar.py:60] Depth 17/1000 time = 14.039228439331055
I0123 15:07:53.960927 139864642547712 ddar.py:60] Depth 18/1000 time = 14.184130907058716
I0123 15:08:08.608443 139864642547712 ddar.py:60] Depth 19/1000 time = 14.592410564422607
I0123 15:08:08.608695 139864642547712 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 15:08:08.608796 139864642547712 alphageometry.py:566] LM output (score=-1.523195): "p : C b j p 23 D b p j p 24 ;"
I0123 15:08:08.608834 139864642547712 alphageometry.py:567] Translation: "p = on_line p b j, on_bline p j b"

I0123 15:08:08.608878 139864642547712 alphageometry.py:576] Solving: "a b c = triangle a b c; d = angle_bisector d b c a, on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g f b e; h = on_circle h g b, on_line h c b; i = circle i f a e; j = on_circle j i a, on_line j c a; k = midpoint k b a; l = lc_tangent l k a, on_line l c a; m = circle m c e a; n = circle n j l e; o = on_line o b a, on_line o j h; p = on_line p b j, on_bline p j b ? coll o m n"
I0123 15:08:08.609089 139864642547712 graph.py:498] 
I0123 15:08:08.609149 139864642547712 graph.py:499] a b c = triangle a b c; d = angle_bisector d b c a, on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g f b e; h = on_circle h g b, on_line h c b; i = circle i f a e; j = on_circle j i a, on_line j c a; k = midpoint k b a; l = lc_tangent l k a, on_line l c a; m = circle m c e a; n = circle n j l e; o = on_line o b a, on_line o j h; p = on_line p b j, on_bline p j b ? coll o m n
I0123 15:08:11.845162 139864642547712 ddar.py:60] Depth 1/1000 time = 3.1383426189422607
I0123 15:08:17.347012 139864642547712 ddar.py:60] Depth 2/1000 time = 5.501676559448242
I0123 15:08:24.543669 139864642547712 ddar.py:60] Depth 3/1000 time = 7.196466445922852
I0123 15:08:31.792074 139864642547712 ddar.py:60] Depth 4/1000 time = 7.248222827911377
I0123 15:08:38.827040 139864642547712 ddar.py:60] Depth 5/1000 time = 7.034553289413452
I0123 15:08:47.979834 139864642547712 ddar.py:60] Depth 6/1000 time = 9.089560747146606
I0123 15:08:56.892790 139864642547712 ddar.py:60] Depth 7/1000 time = 8.912745237350464
I0123 15:09:05.738317 139864642547712 ddar.py:60] Depth 8/1000 time = 8.845301151275635
I0123 15:09:14.889793 139864642547712 ddar.py:60] Depth 9/1000 time = 9.142656326293945
I0123 15:09:24.728947 139864642547712 ddar.py:60] Depth 10/1000 time = 9.83890151977539
I0123 15:09:35.542536 139864642547712 ddar.py:60] Depth 11/1000 time = 10.813257932662964
I0123 15:09:46.935993 139864642547712 ddar.py:60] Depth 12/1000 time = 11.393144845962524
I0123 15:09:59.289746 139864642547712 ddar.py:60] Depth 13/1000 time = 12.353368043899536
I0123 15:10:11.425406 139864642547712 ddar.py:60] Depth 14/1000 time = 12.135382175445557
I0123 15:10:24.667472 139864642547712 ddar.py:60] Depth 15/1000 time = 13.241786003112793
I0123 15:10:38.065272 139864642547712 ddar.py:60] Depth 16/1000 time = 13.397521495819092
I0123 15:10:51.088860 139864642547712 ddar.py:60] Depth 17/1000 time = 12.99091649055481
I0123 15:11:05.078588 139864642547712 ddar.py:60] Depth 18/1000 time = 13.945903062820435
I0123 15:11:18.838238 139864642547712 ddar.py:60] Depth 19/1000 time = 13.709270477294922
I0123 15:11:18.838485 139864642547712 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 15:11:18.838575 139864642547712 alphageometry.py:566] LM output (score=-1.787717): "p : C b c p 23 D b p c p 24 ;"
I0123 15:11:18.838611 139864642547712 alphageometry.py:567] Translation: "p = on_line p b c, on_bline p c b"

I0123 15:11:18.838653 139864642547712 alphageometry.py:576] Solving: "a b c = triangle a b c; d = angle_bisector d b c a, on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g f b e; h = on_circle h g b, on_line h c b; i = circle i f a e; j = on_circle j i a, on_line j c a; k = midpoint k b a; l = lc_tangent l k a, on_line l c a; m = circle m c e a; n = circle n j l e; o = on_line o b a, on_line o j h; p = on_line p b c, on_bline p c b ? coll o m n"
I0123 15:11:18.838847 139864642547712 graph.py:498] 
I0123 15:11:18.838906 139864642547712 graph.py:499] a b c = triangle a b c; d = angle_bisector d b c a, on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g f b e; h = on_circle h g b, on_line h c b; i = circle i f a e; j = on_circle j i a, on_line j c a; k = midpoint k b a; l = lc_tangent l k a, on_line l c a; m = circle m c e a; n = circle n j l e; o = on_line o b a, on_line o j h; p = on_line p b c, on_bline p c b ? coll o m n
I0123 15:11:22.622556 139864642547712 ddar.py:60] Depth 1/1000 time = 3.688943862915039
I0123 15:11:28.879725 139864642547712 ddar.py:60] Depth 2/1000 time = 6.256995439529419
I0123 15:11:37.006766 139864642547712 ddar.py:60] Depth 3/1000 time = 8.126834869384766
I0123 15:11:44.964595 139864642547712 ddar.py:60] Depth 4/1000 time = 7.957622051239014
I0123 15:11:52.661228 139864642547712 ddar.py:60] Depth 5/1000 time = 7.696141242980957
I0123 15:12:02.727685 139864642547712 ddar.py:60] Depth 6/1000 time = 10.003461837768555
I0123 15:12:12.740286 139864642547712 ddar.py:60] Depth 7/1000 time = 10.01240611076355
I0123 15:12:22.585866 139864642547712 ddar.py:60] Depth 8/1000 time = 9.845367431640625
I0123 15:12:32.207554 139864642547712 ddar.py:60] Depth 9/1000 time = 9.612876176834106
I0123 15:12:44.054412 139864642547712 ddar.py:60] Depth 10/1000 time = 11.846653699874878
I0123 15:12:56.357590 139864642547712 ddar.py:60] Depth 11/1000 time = 12.302889108657837
I0123 15:13:08.725368 139864642547712 ddar.py:60] Depth 12/1000 time = 12.36741042137146
I0123 15:13:22.824539 139864642547712 ddar.py:60] Depth 13/1000 time = 14.098843574523926
I0123 15:13:36.577100 139864642547712 ddar.py:60] Depth 14/1000 time = 13.752156734466553
I0123 15:13:51.047377 139864642547712 ddar.py:60] Depth 15/1000 time = 14.469997882843018
I0123 15:14:06.315492 139864642547712 ddar.py:60] Depth 16/1000 time = 15.267826795578003
I0123 15:14:21.470360 139864642547712 ddar.py:60] Depth 17/1000 time = 15.120216846466064
I0123 15:14:36.784187 139864642547712 ddar.py:60] Depth 18/1000 time = 15.26786756515503
I0123 15:14:52.071130 139864642547712 ddar.py:60] Depth 19/1000 time = 15.230895519256592
I0123 15:14:52.071561 139864642547712 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 15:14:52.071670 139864642547712 alphageometry.py:566] LM output (score=-2.033484): "p : C b l p 23 D b p l p 24 ;"
I0123 15:14:52.071709 139864642547712 alphageometry.py:567] Translation: "p = on_line p b l, on_bline p l b"

I0123 15:14:52.071756 139864642547712 alphageometry.py:576] Solving: "a b c = triangle a b c; d = angle_bisector d b c a, on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g f b e; h = on_circle h g b, on_line h c b; i = circle i f a e; j = on_circle j i a, on_line j c a; k = midpoint k b a; l = lc_tangent l k a, on_line l c a; m = circle m c e a; n = circle n j l e; o = on_line o b a, on_line o j h; p = on_line p b l, on_bline p l b ? coll o m n"
I0123 15:14:52.071954 139864642547712 graph.py:498] 
I0123 15:14:52.072017 139864642547712 graph.py:499] a b c = triangle a b c; d = angle_bisector d b c a, on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g f b e; h = on_circle h g b, on_line h c b; i = circle i f a e; j = on_circle j i a, on_line j c a; k = midpoint k b a; l = lc_tangent l k a, on_line l c a; m = circle m c e a; n = circle n j l e; o = on_line o b a, on_line o j h; p = on_line p b l, on_bline p l b ? coll o m n
I0123 15:14:55.206785 139864642547712 ddar.py:60] Depth 1/1000 time = 3.039825201034546
I0123 15:15:01.551199 139864642547712 ddar.py:60] Depth 2/1000 time = 6.344230890274048
I0123 15:15:09.392489 139864642547712 ddar.py:60] Depth 3/1000 time = 7.841079235076904
I0123 15:15:16.904722 139864642547712 ddar.py:60] Depth 4/1000 time = 7.511997222900391
I0123 15:15:24.409800 139864642547712 ddar.py:60] Depth 5/1000 time = 7.504638671875
I0123 15:15:34.102100 139864642547712 ddar.py:60] Depth 6/1000 time = 9.628514051437378
I0123 15:15:44.053021 139864642547712 ddar.py:60] Depth 7/1000 time = 9.950720071792603
I0123 15:15:53.554142 139864642547712 ddar.py:60] Depth 8/1000 time = 9.500883340835571
I0123 15:16:03.712128 139864642547712 ddar.py:60] Depth 9/1000 time = 10.149913787841797
I0123 15:16:14.449301 139864642547712 ddar.py:60] Depth 10/1000 time = 10.736928939819336
I0123 15:16:26.740224 139864642547712 ddar.py:60] Depth 11/1000 time = 12.290640354156494
I0123 15:16:39.190463 139864642547712 ddar.py:60] Depth 12/1000 time = 12.449978590011597
I0123 15:16:53.034363 139864642547712 ddar.py:60] Depth 13/1000 time = 13.843573093414307
I0123 15:17:06.148561 139864642547712 ddar.py:60] Depth 14/1000 time = 13.113800048828125
I0123 15:17:20.936823 139864642547712 ddar.py:60] Depth 15/1000 time = 14.787997961044312
I0123 15:17:35.462982 139864642547712 ddar.py:60] Depth 16/1000 time = 14.525862455368042
I0123 15:17:50.037539 139864642547712 ddar.py:60] Depth 17/1000 time = 14.543830394744873
I0123 15:18:05.225938 139864642547712 ddar.py:60] Depth 18/1000 time = 15.145153522491455
I0123 15:18:19.727270 139864642547712 ddar.py:60] Depth 19/1000 time = 14.446844577789307
I0123 15:18:19.727674 139864642547712 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 15:18:19.727801 139864642547712 alphageometry.py:566] LM output (score=-2.046108): "p : C e j p 23 D e p j p 24 ;"
I0123 15:18:19.727838 139864642547712 alphageometry.py:567] Translation: "p = on_line p e j, on_bline p j e"

I0123 15:18:19.727893 139864642547712 alphageometry.py:576] Solving: "a b c = triangle a b c; d = angle_bisector d b c a, on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g f b e; h = on_circle h g b, on_line h c b; i = circle i f a e; j = on_circle j i a, on_line j c a; k = midpoint k b a; l = lc_tangent l k a, on_line l c a; m = circle m c e a; n = circle n j l e; o = on_line o b a, on_line o j h; p = on_line p e j, on_bline p j e ? coll o m n"
I0123 15:18:19.728100 139864642547712 graph.py:498] 
I0123 15:18:19.728159 139864642547712 graph.py:499] a b c = triangle a b c; d = angle_bisector d b c a, on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g f b e; h = on_circle h g b, on_line h c b; i = circle i f a e; j = on_circle j i a, on_line j c a; k = midpoint k b a; l = lc_tangent l k a, on_line l c a; m = circle m c e a; n = circle n j l e; o = on_line o b a, on_line o j h; p = on_line p e j, on_bline p j e ? coll o m n
I0123 15:18:23.527637 139864642547712 ddar.py:60] Depth 1/1000 time = 3.7023653984069824
I0123 15:18:29.172103 139864642547712 ddar.py:60] Depth 2/1000 time = 5.64427638053894
I0123 15:18:37.398675 139864642547712 ddar.py:60] Depth 3/1000 time = 8.226390838623047
I0123 15:18:45.316753 139864642547712 ddar.py:60] Depth 4/1000 time = 7.917881011962891
I0123 15:18:53.399137 139864642547712 ddar.py:60] Depth 5/1000 time = 8.082174301147461
I0123 15:19:01.849935 139864642547712 ddar.py:60] Depth 6/1000 time = 8.45026183128357
I0123 15:19:10.016311 139864642547712 ddar.py:60] Depth 7/1000 time = 8.163683652877808
I0123 15:19:18.757553 139864642547712 ddar.py:60] Depth 8/1000 time = 8.74105978012085
I0123 15:19:28.007997 139864642547712 ddar.py:60] Depth 9/1000 time = 9.25023889541626
I0123 15:19:37.471837 139864642547712 ddar.py:60] Depth 10/1000 time = 9.463646650314331
I0123 15:19:46.914631 139864642547712 ddar.py:60] Depth 11/1000 time = 9.442533016204834
I0123 15:19:56.330115 139864642547712 ddar.py:60] Depth 12/1000 time = 9.35370135307312
I0123 15:20:05.860022 139864642547712 ddar.py:60] Depth 13/1000 time = 9.48411226272583
I0123 15:20:16.781073 139864642547712 ddar.py:60] Depth 14/1000 time = 10.920736312866211
I0123 15:20:28.580002 139864642547712 ddar.py:60] Depth 15/1000 time = 11.798653841018677
I0123 15:20:40.131970 139864642547712 ddar.py:60] Depth 16/1000 time = 11.551611185073853
I0123 15:20:53.559166 139864642547712 ddar.py:60] Depth 17/1000 time = 13.426880598068237
I0123 15:21:06.077451 139864642547712 ddar.py:60] Depth 18/1000 time = 12.517899990081787
I0123 15:21:20.519330 139864642547712 ddar.py:60] Depth 19/1000 time = 14.441629409790039
I0123 15:21:34.480244 139864642547712 ddar.py:60] Depth 20/1000 time = 13.960561990737915
I0123 15:21:49.268827 139864642547712 ddar.py:60] Depth 21/1000 time = 14.74256181716919
I0123 15:22:03.862420 139864642547712 ddar.py:60] Depth 22/1000 time = 14.541314363479614
I0123 15:22:03.863050 139864642547712 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 15:22:03.863151 139864642547712 alphageometry.py:566] LM output (score=-2.166508): "p : C c e p 23 D c p e p 24 ;"
I0123 15:22:03.863188 139864642547712 alphageometry.py:567] Translation: "p = on_line p c e, on_bline p e c"

I0123 15:22:03.863247 139864642547712 alphageometry.py:576] Solving: "a b c = triangle a b c; d = angle_bisector d b c a, on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g f b e; h = on_circle h g b, on_line h c b; i = circle i f a e; j = on_circle j i a, on_line j c a; k = midpoint k b a; l = lc_tangent l k a, on_line l c a; m = circle m c e a; n = circle n j l e; o = on_line o b a, on_line o j h; p = on_line p c e, on_bline p e c ? coll o m n"
I0123 15:22:03.863444 139864642547712 graph.py:498] 
I0123 15:22:03.863503 139864642547712 graph.py:499] a b c = triangle a b c; d = angle_bisector d b c a, on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g f b e; h = on_circle h g b, on_line h c b; i = circle i f a e; j = on_circle j i a, on_line j c a; k = midpoint k b a; l = lc_tangent l k a, on_line l c a; m = circle m c e a; n = circle n j l e; o = on_line o b a, on_line o j h; p = on_line p c e, on_bline p e c ? coll o m n
I0123 15:22:07.948503 139864642547712 ddar.py:60] Depth 1/1000 time = 3.988318681716919
I0123 15:22:14.454907 139864642547712 ddar.py:60] Depth 2/1000 time = 6.50615668296814
I0123 15:22:24.421042 139864642547712 ddar.py:60] Depth 3/1000 time = 9.96582818031311
I0123 15:22:34.926874 139864642547712 ddar.py:60] Depth 4/1000 time = 10.505584478378296
I0123 15:22:45.431451 139864642547712 ddar.py:60] Depth 5/1000 time = 10.504257678985596
I0123 15:22:55.927583 139864642547712 ddar.py:60] Depth 6/1000 time = 10.495602130889893
I0123 15:23:08.437011 139864642547712 ddar.py:60] Depth 7/1000 time = 12.4372878074646
I0123 15:23:20.625107 139864642547712 ddar.py:60] Depth 8/1000 time = 12.187798261642456
I0123 15:23:32.930433 139864642547712 ddar.py:60] Depth 9/1000 time = 12.304962635040283
I0123 15:23:45.534206 139864642547712 ddar.py:60] Depth 10/1000 time = 12.55606484413147
I0123 15:23:58.985696 139864642547712 ddar.py:60] Depth 11/1000 time = 13.451230525970459
I0123 15:24:14.399436 139864642547712 ddar.py:60] Depth 12/1000 time = 15.4134840965271
I0123 15:24:29.895154 139864642547712 ddar.py:60] Depth 13/1000 time = 15.495439767837524
I0123 15:24:46.704144 139864642547712 ddar.py:60] Depth 14/1000 time = 16.80867624282837
I0123 15:25:03.082256 139864642547712 ddar.py:60] Depth 15/1000 time = 16.37780451774597
I0123 15:25:20.865729 139864642547712 ddar.py:60] Depth 16/1000 time = 17.783154249191284
I0123 15:25:39.705571 139864642547712 ddar.py:60] Depth 17/1000 time = 18.83952283859253
I0123 15:25:57.963537 139864642547712 ddar.py:60] Depth 18/1000 time = 18.205882787704468
I0123 15:26:16.599067 139864642547712 ddar.py:60] Depth 19/1000 time = 18.58140254020691
I0123 15:26:16.599500 139864642547712 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 15:26:16.599642 139864642547712 alphageometry.py:566] LM output (score=-2.426578): "p : T e i e p 23 ;"
I0123 15:26:16.599679 139864642547712 alphageometry.py:567] Translation: "p = on_tline p e e i"

I0123 15:26:16.599736 139864642547712 alphageometry.py:576] Solving: "a b c = triangle a b c; d = angle_bisector d b c a, on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g f b e; h = on_circle h g b, on_line h c b; i = circle i f a e; j = on_circle j i a, on_line j c a; k = midpoint k b a; l = lc_tangent l k a, on_line l c a; m = circle m c e a; n = circle n j l e; o = on_line o b a, on_line o j h; p = on_tline p e e i ? coll o m n"
I0123 15:26:16.599951 139864642547712 graph.py:498] 
I0123 15:26:16.600010 139864642547712 graph.py:499] a b c = triangle a b c; d = angle_bisector d b c a, on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g f b e; h = on_circle h g b, on_line h c b; i = circle i f a e; j = on_circle j i a, on_line j c a; k = midpoint k b a; l = lc_tangent l k a, on_line l c a; m = circle m c e a; n = circle n j l e; o = on_line o b a, on_line o j h; p = on_tline p e e i ? coll o m n
I0123 15:26:19.615877 139864642547712 ddar.py:60] Depth 1/1000 time = 2.9359021186828613
I0123 15:26:25.161077 139864642547712 ddar.py:60] Depth 2/1000 time = 5.545004367828369
I0123 15:26:32.049416 139864642547712 ddar.py:60] Depth 3/1000 time = 6.88808798789978
I0123 15:26:39.250863 139864642547712 ddar.py:60] Depth 4/1000 time = 7.2011308670043945
I0123 15:26:46.538519 139864642547712 ddar.py:60] Depth 5/1000 time = 7.287456035614014
I0123 15:26:53.486825 139864642547712 ddar.py:60] Depth 6/1000 time = 6.947728872299194
I0123 15:27:01.054985 139864642547712 ddar.py:60] Depth 7/1000 time = 7.565564155578613
I0123 15:27:08.696326 139864642547712 ddar.py:60] Depth 8/1000 time = 7.641152381896973
I0123 15:27:16.687767 139864642547712 ddar.py:60] Depth 9/1000 time = 7.991184711456299
I0123 15:27:25.035405 139864642547712 ddar.py:60] Depth 10/1000 time = 8.347312688827515
I0123 15:27:33.487932 139864642547712 ddar.py:60] Depth 11/1000 time = 8.45232367515564
I0123 15:27:42.014820 139864642547712 ddar.py:60] Depth 12/1000 time = 8.457250833511353
I0123 15:27:50.657091 139864642547712 ddar.py:60] Depth 13/1000 time = 8.642077445983887
I0123 15:27:59.663972 139864642547712 ddar.py:60] Depth 14/1000 time = 8.962008714675903
I0123 15:28:09.457205 139864642547712 ddar.py:60] Depth 15/1000 time = 9.792902708053589
I0123 15:28:20.931466 139864642547712 ddar.py:60] Depth 16/1000 time = 11.473983764648438
I0123 15:28:31.859681 139864642547712 ddar.py:60] Depth 17/1000 time = 10.927857637405396
I0123 15:28:44.268231 139864642547712 ddar.py:60] Depth 18/1000 time = 12.408224821090698
I0123 15:28:56.878288 139864642547712 ddar.py:60] Depth 19/1000 time = 12.609676122665405
I0123 15:29:10.385970 139864642547712 ddar.py:60] Depth 20/1000 time = 13.507441282272339
I0123 15:29:23.621892 139864642547712 ddar.py:60] Depth 21/1000 time = 13.235577583312988
I0123 15:29:37.084668 139864642547712 ddar.py:60] Depth 22/1000 time = 13.412762880325317
I0123 15:29:50.784254 139864642547712 ddar.py:60] Depth 23/1000 time = 13.65096640586853
I0123 15:29:50.784579 139864642547712 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 15:29:50.784683 139864642547712 alphageometry.py:566] LM output (score=-2.727324): "p : T e g e p 23 ;"
I0123 15:29:50.784720 139864642547712 alphageometry.py:567] Translation: "p = on_tline p e e g"

I0123 15:29:50.784773 139864642547712 alphageometry.py:576] Solving: "a b c = triangle a b c; d = angle_bisector d b c a, on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g f b e; h = on_circle h g b, on_line h c b; i = circle i f a e; j = on_circle j i a, on_line j c a; k = midpoint k b a; l = lc_tangent l k a, on_line l c a; m = circle m c e a; n = circle n j l e; o = on_line o b a, on_line o j h; p = on_tline p e e g ? coll o m n"
I0123 15:29:50.784978 139864642547712 graph.py:498] 
I0123 15:29:50.785039 139864642547712 graph.py:499] a b c = triangle a b c; d = angle_bisector d b c a, on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g f b e; h = on_circle h g b, on_line h c b; i = circle i f a e; j = on_circle j i a, on_line j c a; k = midpoint k b a; l = lc_tangent l k a, on_line l c a; m = circle m c e a; n = circle n j l e; o = on_line o b a, on_line o j h; p = on_tline p e e g ? coll o m n
I0123 15:29:53.888700 139864642547712 ddar.py:60] Depth 1/1000 time = 3.023303747177124
I0123 15:29:58.850492 139864642547712 ddar.py:60] Depth 2/1000 time = 4.9616193771362305
I0123 15:30:05.915891 139864642547712 ddar.py:60] Depth 3/1000 time = 7.0651445388793945
I0123 15:30:12.668255 139864642547712 ddar.py:60] Depth 4/1000 time = 6.752047538757324
I0123 15:30:19.687900 139864642547712 ddar.py:60] Depth 5/1000 time = 7.019151449203491
I0123 15:30:28.340328 139864642547712 ddar.py:60] Depth 6/1000 time = 8.587347030639648
I0123 15:30:37.123926 139864642547712 ddar.py:60] Depth 7/1000 time = 8.783283233642578
I0123 15:30:45.530089 139864642547712 ddar.py:60] Depth 8/1000 time = 8.40595531463623
I0123 15:30:54.523985 139864642547712 ddar.py:60] Depth 9/1000 time = 8.987056493759155
I0123 15:31:04.998991 139864642547712 ddar.py:60] Depth 10/1000 time = 10.474811792373657
I0123 15:31:15.697924 139864642547712 ddar.py:60] Depth 11/1000 time = 10.698691368103027
I0123 15:31:26.472715 139864642547712 ddar.py:60] Depth 12/1000 time = 10.774554967880249
I0123 15:31:39.201871 139864642547712 ddar.py:60] Depth 13/1000 time = 12.728896379470825
I0123 15:31:51.235862 139864642547712 ddar.py:60] Depth 14/1000 time = 12.033750534057617
I0123 15:32:04.920387 139864642547712 ddar.py:60] Depth 15/1000 time = 13.684278726577759
I0123 15:32:18.579910 139864642547712 ddar.py:60] Depth 16/1000 time = 13.659175634384155
I0123 15:32:32.511524 139864642547712 ddar.py:60] Depth 17/1000 time = 13.896825551986694
I0123 15:32:46.010321 139864642547712 ddar.py:60] Depth 18/1000 time = 13.452971935272217
I0123 15:33:00.295936 139864642547712 ddar.py:60] Depth 19/1000 time = 14.2320556640625
I0123 15:33:00.296651 139864642547712 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 15:33:00.296792 139864642547712 alphageometry.py:566] LM output (score=-2.739131): "p : C c j p 23 D c p j p 24 ;"
I0123 15:33:00.296830 139864642547712 alphageometry.py:567] Translation: "p = on_line p c j, on_bline p j c"

I0123 15:33:00.296886 139864642547712 alphageometry.py:576] Solving: "a b c = triangle a b c; d = angle_bisector d b c a, on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g f b e; h = on_circle h g b, on_line h c b; i = circle i f a e; j = on_circle j i a, on_line j c a; k = midpoint k b a; l = lc_tangent l k a, on_line l c a; m = circle m c e a; n = circle n j l e; o = on_line o b a, on_line o j h; p = on_line p c j, on_bline p j c ? coll o m n"
I0123 15:33:00.297100 139864642547712 graph.py:498] 
I0123 15:33:00.297162 139864642547712 graph.py:499] a b c = triangle a b c; d = angle_bisector d b c a, on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g f b e; h = on_circle h g b, on_line h c b; i = circle i f a e; j = on_circle j i a, on_line j c a; k = midpoint k b a; l = lc_tangent l k a, on_line l c a; m = circle m c e a; n = circle n j l e; o = on_line o b a, on_line o j h; p = on_line p c j, on_bline p j c ? coll o m n
I0123 15:33:04.999353 139864642547712 ddar.py:60] Depth 1/1000 time = 4.602161169052124
I0123 15:33:11.933209 139864642547712 ddar.py:60] Depth 2/1000 time = 6.933669805526733
I0123 15:33:20.531618 139864642547712 ddar.py:60] Depth 3/1000 time = 8.598166227340698
I0123 15:33:29.499587 139864642547712 ddar.py:60] Depth 4/1000 time = 8.967651844024658
I0123 15:33:38.104239 139864642547712 ddar.py:60] Depth 5/1000 time = 8.604228973388672
I0123 15:33:49.238199 139864642547712 ddar.py:60] Depth 6/1000 time = 11.077941656112671
I0123 15:34:00.277875 139864642547712 ddar.py:60] Depth 7/1000 time = 11.039486169815063
I0123 15:34:10.982221 139864642547712 ddar.py:60] Depth 8/1000 time = 10.704124212265015
I0123 15:34:21.759159 139864642547712 ddar.py:60] Depth 9/1000 time = 10.770073890686035
I0123 15:34:34.188754 139864642547712 ddar.py:60] Depth 10/1000 time = 12.42938232421875
I0123 15:34:47.188264 139864642547712 ddar.py:60] Depth 11/1000 time = 12.999229669570923
I0123 15:34:59.842050 139864642547712 ddar.py:60] Depth 12/1000 time = 12.653425455093384
I0123 15:35:14.334807 139864642547712 ddar.py:60] Depth 13/1000 time = 14.492486238479614
I0123 15:35:28.870172 139864642547712 ddar.py:60] Depth 14/1000 time = 14.535088300704956
I0123 15:35:43.892527 139864642547712 ddar.py:60] Depth 15/1000 time = 15.022110223770142
I0123 15:35:59.680057 139864642547712 ddar.py:60] Depth 16/1000 time = 15.78727388381958
I0123 15:36:15.267601 139864642547712 ddar.py:60] Depth 17/1000 time = 15.553393363952637
I0123 15:36:31.579249 139864642547712 ddar.py:60] Depth 18/1000 time = 16.267710208892822
I0123 15:36:47.884777 139864642547712 ddar.py:60] Depth 19/1000 time = 16.255493879318237
I0123 15:36:47.885012 139864642547712 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 15:36:47.885110 139864642547712 alphageometry.py:566] LM output (score=-2.838944): "p : D c p e p 23 ^ e c e p c p c e 24 ;"
I0123 15:36:47.885161 139864642547712 alphageometry.py:567] Translation: "ERROR: Invalid predicate ^ e c e p c p c e"

I0123 15:36:47.885198 139864642547712 alphageometry.py:566] LM output (score=-2.914117): "p : D c p e p 23 ;"
I0123 15:36:47.885227 139864642547712 alphageometry.py:567] Translation: "p = on_bline p e c"

I0123 15:36:47.885268 139864642547712 alphageometry.py:576] Solving: "a b c = triangle a b c; d = angle_bisector d b c a, on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g f b e; h = on_circle h g b, on_line h c b; i = circle i f a e; j = on_circle j i a, on_line j c a; k = midpoint k b a; l = lc_tangent l k a, on_line l c a; m = circle m c e a; n = circle n j l e; o = on_line o b a, on_line o j h; p = on_bline p e c ? coll o m n"
I0123 15:36:47.885461 139864642547712 graph.py:498] 
I0123 15:36:47.885523 139864642547712 graph.py:499] a b c = triangle a b c; d = angle_bisector d b c a, on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g f b e; h = on_circle h g b, on_line h c b; i = circle i f a e; j = on_circle j i a, on_line j c a; k = midpoint k b a; l = lc_tangent l k a, on_line l c a; m = circle m c e a; n = circle n j l e; o = on_line o b a, on_line o j h; p = on_bline p e c ? coll o m n
I0123 15:36:50.495800 139864642547712 ddar.py:60] Depth 1/1000 time = 2.5095481872558594
I0123 15:36:56.248481 139864642547712 ddar.py:60] Depth 2/1000 time = 5.75247859954834
I0123 15:37:03.920289 139864642547712 ddar.py:60] Depth 3/1000 time = 7.671581745147705
I0123 15:37:10.843993 139864642547712 ddar.py:60] Depth 4/1000 time = 6.923502206802368
I0123 15:37:18.624189 139864642547712 ddar.py:60] Depth 5/1000 time = 7.779702663421631
I0123 15:37:26.192733 139864642547712 ddar.py:60] Depth 6/1000 time = 7.565796613693237
I0123 15:37:33.609059 139864642547712 ddar.py:60] Depth 7/1000 time = 7.416118621826172
I0123 15:37:41.977497 139864642547712 ddar.py:60] Depth 8/1000 time = 8.368200063705444
I0123 15:37:49.990210 139864642547712 ddar.py:60] Depth 9/1000 time = 8.012481927871704
I0123 15:37:58.013504 139864642547712 ddar.py:60] Depth 10/1000 time = 8.015710830688477
I0123 15:38:07.949081 139864642547712 ddar.py:60] Depth 11/1000 time = 9.865310430526733
I0123 15:38:17.763386 139864642547712 ddar.py:60] Depth 12/1000 time = 9.814040899276733
I0123 15:38:27.242410 139864642547712 ddar.py:60] Depth 13/1000 time = 9.478670835494995
I0123 15:38:36.978112 139864642547712 ddar.py:60] Depth 14/1000 time = 9.679190397262573
I0123 15:38:48.383107 139864642547712 ddar.py:60] Depth 15/1000 time = 11.40476393699646
I0123 15:39:00.310250 139864642547712 ddar.py:60] Depth 16/1000 time = 11.926858186721802
I0123 15:39:12.345704 139864642547712 ddar.py:60] Depth 17/1000 time = 12.035048007965088
I0123 15:39:26.135590 139864642547712 ddar.py:60] Depth 18/1000 time = 13.789544820785522
I0123 15:39:39.392665 139864642547712 ddar.py:60] Depth 19/1000 time = 13.25667691230774
I0123 15:39:54.454904 139864642547712 ddar.py:60] Depth 20/1000 time = 15.061974287033081
I0123 15:40:09.301187 139864642547712 ddar.py:60] Depth 21/1000 time = 14.845917224884033
I0123 15:40:24.581995 139864642547712 ddar.py:60] Depth 22/1000 time = 15.218828201293945
I0123 15:40:39.635244 139864642547712 ddar.py:60] Depth 23/1000 time = 14.990612745285034
I0123 15:40:39.635534 139864642547712 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 15:40:39.635633 139864642547712 alphageometry.py:566] LM output (score=-2.978062): "p : T h i h p 23 ;"
I0123 15:40:39.635674 139864642547712 alphageometry.py:567] Translation: "p = on_tline p h h i"

I0123 15:40:39.635722 139864642547712 alphageometry.py:576] Solving: "a b c = triangle a b c; d = angle_bisector d b c a, on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g f b e; h = on_circle h g b, on_line h c b; i = circle i f a e; j = on_circle j i a, on_line j c a; k = midpoint k b a; l = lc_tangent l k a, on_line l c a; m = circle m c e a; n = circle n j l e; o = on_line o b a, on_line o j h; p = on_tline p h h i ? coll o m n"
I0123 15:40:39.635932 139864642547712 graph.py:498] 
I0123 15:40:39.636000 139864642547712 graph.py:499] a b c = triangle a b c; d = angle_bisector d b c a, on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g f b e; h = on_circle h g b, on_line h c b; i = circle i f a e; j = on_circle j i a, on_line j c a; k = midpoint k b a; l = lc_tangent l k a, on_line l c a; m = circle m c e a; n = circle n j l e; o = on_line o b a, on_line o j h; p = on_tline p h h i ? coll o m n
I0123 15:40:42.875224 139864642547712 ddar.py:60] Depth 1/1000 time = 3.1587469577789307
I0123 15:40:48.060585 139864642547712 ddar.py:60] Depth 2/1000 time = 5.18512225151062
I0123 15:40:55.054800 139864642547712 ddar.py:60] Depth 3/1000 time = 6.993886470794678
I0123 15:41:02.321041 139864642547712 ddar.py:60] Depth 4/1000 time = 7.266047477722168
I0123 15:41:09.294355 139864642547712 ddar.py:60] Depth 5/1000 time = 6.9727044105529785
I0123 15:41:18.235768 139864642547712 ddar.py:60] Depth 6/1000 time = 8.879530906677246
I0123 15:41:26.620297 139864642547712 ddar.py:60] Depth 7/1000 time = 8.384333848953247
I0123 15:41:35.768696 139864642547712 ddar.py:60] Depth 8/1000 time = 9.148164510726929
I0123 15:41:44.261565 139864642547712 ddar.py:60] Depth 9/1000 time = 8.486194372177124
I0123 15:41:54.820486 139864642547712 ddar.py:60] Depth 10/1000 time = 10.558632135391235
I0123 15:42:05.540396 139864642547712 ddar.py:60] Depth 11/1000 time = 10.719568729400635
I0123 15:42:17.259786 139864642547712 ddar.py:60] Depth 12/1000 time = 11.719105005264282
I0123 15:42:29.414721 139864642547712 ddar.py:60] Depth 13/1000 time = 12.15458869934082
I0123 15:42:42.046429 139864642547712 ddar.py:60] Depth 14/1000 time = 12.631448984146118
I0123 15:42:55.765958 139864642547712 ddar.py:60] Depth 15/1000 time = 13.71920132637024
I0123 15:43:09.496586 139864642547712 ddar.py:60] Depth 16/1000 time = 13.730196714401245
I0123 15:43:23.344821 139864642547712 ddar.py:60] Depth 17/1000 time = 13.812573671340942
I0123 15:43:38.009265 139864642547712 ddar.py:60] Depth 18/1000 time = 14.621329307556152
I0123 15:43:52.012229 139864642547712 ddar.py:60] Depth 19/1000 time = 13.952881813049316
I0123 15:43:52.012652 139864642547712 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 15:43:52.012761 139864642547712 alphageometry.py:566] LM output (score=-2.999690): "p : C b g p 23 D b g g p 24 ;"
I0123 15:43:52.012801 139864642547712 alphageometry.py:567] Translation: "p = on_line p b g, on_circle p g b"

I0123 15:43:52.012848 139864642547712 alphageometry.py:576] Solving: "a b c = triangle a b c; d = angle_bisector d b c a, on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g f b e; h = on_circle h g b, on_line h c b; i = circle i f a e; j = on_circle j i a, on_line j c a; k = midpoint k b a; l = lc_tangent l k a, on_line l c a; m = circle m c e a; n = circle n j l e; o = on_line o b a, on_line o j h; p = on_line p b g, on_circle p g b ? coll o m n"
I0123 15:43:52.013048 139864642547712 graph.py:498] 
I0123 15:43:52.013116 139864642547712 graph.py:499] a b c = triangle a b c; d = angle_bisector d b c a, on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g f b e; h = on_circle h g b, on_line h c b; i = circle i f a e; j = on_circle j i a, on_line j c a; k = midpoint k b a; l = lc_tangent l k a, on_line l c a; m = circle m c e a; n = circle n j l e; o = on_line o b a, on_line o j h; p = on_line p b g, on_circle p g b ? coll o m n
I0123 15:43:55.335468 139864642547712 ddar.py:60] Depth 1/1000 time = 3.2340478897094727
I0123 15:44:02.771190 139864642547712 ddar.py:60] Depth 2/1000 time = 7.435448408126831
I0123 15:44:12.476193 139864642547712 ddar.py:60] Depth 3/1000 time = 9.704677104949951
I0123 15:44:22.225229 139864642547712 ddar.py:60] Depth 4/1000 time = 9.748777389526367
I0123 15:44:32.008326 139864642547712 ddar.py:60] Depth 5/1000 time = 9.782477378845215
I0123 15:44:41.989159 139864642547712 ddar.py:60] Depth 6/1000 time = 9.97757339477539
I0123 15:44:52.934200 139864642547712 ddar.py:60] Depth 7/1000 time = 10.944798231124878
I0123 15:45:03.648146 139864642547712 ddar.py:60] Depth 8/1000 time = 10.713721990585327
I0123 15:45:14.337221 139864642547712 ddar.py:60] Depth 9/1000 time = 10.688836097717285
I0123 15:45:26.917999 139864642547712 ddar.py:60] Depth 10/1000 time = 12.485094547271729
I0123 15:45:38.882845 139864642547712 ddar.py:60] Depth 11/1000 time = 11.964560985565186
I0123 15:45:51.253718 139864642547712 ddar.py:60] Depth 12/1000 time = 12.37059473991394
I0123 15:46:03.559019 139864642547712 ddar.py:60] Depth 13/1000 time = 12.244747638702393
I0123 15:46:18.038322 139864642547712 ddar.py:60] Depth 14/1000 time = 14.479083061218262
I0123 15:46:33.037415 139864642547712 ddar.py:60] Depth 15/1000 time = 14.998783588409424
I0123 15:46:47.984463 139864642547712 ddar.py:60] Depth 16/1000 time = 14.946675777435303
I0123 15:47:04.937778 139864642547712 ddar.py:60] Depth 17/1000 time = 16.953036069869995
I0123 15:47:21.287543 139864642547712 ddar.py:60] Depth 18/1000 time = 16.349435806274414
I0123 15:47:38.770320 139864642547712 ddar.py:60] Depth 19/1000 time = 17.482345819473267
I0123 15:47:57.307347 139864642547712 ddar.py:60] Depth 20/1000 time = 18.536569356918335
I0123 15:48:15.297876 139864642547712 ddar.py:60] Depth 21/1000 time = 17.93101477622986
I0123 15:48:34.447768 139864642547712 ddar.py:60] Depth 22/1000 time = 19.084675788879395
I0123 15:48:34.448017 139864642547712 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 15:48:34.448112 139864642547712 alphageometry.py:566] LM output (score=-3.078784): "p : T c p e f 23 ;"
I0123 15:48:34.448149 139864642547712 alphageometry.py:567] Translation: "p = on_tline p c e f"

I0123 15:48:34.448192 139864642547712 alphageometry.py:576] Solving: "a b c = triangle a b c; d = angle_bisector d b c a, on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g f b e; h = on_circle h g b, on_line h c b; i = circle i f a e; j = on_circle j i a, on_line j c a; k = midpoint k b a; l = lc_tangent l k a, on_line l c a; m = circle m c e a; n = circle n j l e; o = on_line o b a, on_line o j h; p = on_tline p c e f ? coll o m n"
I0123 15:48:34.448382 139864642547712 graph.py:498] 
I0123 15:48:34.448443 139864642547712 graph.py:499] a b c = triangle a b c; d = angle_bisector d b c a, on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g f b e; h = on_circle h g b, on_line h c b; i = circle i f a e; j = on_circle j i a, on_line j c a; k = midpoint k b a; l = lc_tangent l k a, on_line l c a; m = circle m c e a; n = circle n j l e; o = on_line o b a, on_line o j h; p = on_tline p c e f ? coll o m n
I0123 15:48:37.447101 139864642547712 ddar.py:60] Depth 1/1000 time = 2.918696641921997
I0123 15:48:42.698503 139864642547712 ddar.py:60] Depth 2/1000 time = 5.251104354858398
I0123 15:48:49.076935 139864642547712 ddar.py:60] Depth 3/1000 time = 6.378252267837524
I0123 15:48:55.869793 139864642547712 ddar.py:60] Depth 4/1000 time = 6.792674541473389
I0123 15:49:02.787386 139864642547712 ddar.py:60] Depth 5/1000 time = 6.917153835296631
I0123 15:49:11.230172 139864642547712 ddar.py:60] Depth 6/1000 time = 8.38167142868042
I0123 15:49:19.753046 139864642547712 ddar.py:60] Depth 7/1000 time = 8.52263069152832
I0123 15:49:28.338898 139864642547712 ddar.py:60] Depth 8/1000 time = 8.585525035858154
I0123 15:49:37.043631 139864642547712 ddar.py:60] Depth 9/1000 time = 8.698503494262695
I0123 15:49:46.821543 139864642547712 ddar.py:60] Depth 10/1000 time = 9.77770471572876
I0123 15:49:57.034204 139864642547712 ddar.py:60] Depth 11/1000 time = 10.212420463562012
I0123 15:50:08.262051 139864642547712 ddar.py:60] Depth 12/1000 time = 11.2276029586792
I0123 15:50:19.991382 139864642547712 ddar.py:60] Depth 13/1000 time = 11.729079961776733
I0123 15:50:31.295149 139864642547712 ddar.py:60] Depth 14/1000 time = 11.303515434265137
I0123 15:50:44.545540 139864642547712 ddar.py:60] Depth 15/1000 time = 13.250143051147461
I0123 15:50:57.948556 139864642547712 ddar.py:60] Depth 16/1000 time = 13.402731895446777
I0123 15:51:11.576139 139864642547712 ddar.py:60] Depth 17/1000 time = 13.594118595123291
I0123 15:51:24.893036 139864642547712 ddar.py:60] Depth 18/1000 time = 13.272279500961304
I0123 15:51:39.864075 139864642547712 ddar.py:60] Depth 19/1000 time = 14.918367862701416
I0123 15:51:39.865092 139864642547712 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 15:51:39.865415 139864642547712 alphageometry.py:566] LM output (score=-3.142088): "p : T c d c p 23 ;"
I0123 15:51:39.865463 139864642547712 alphageometry.py:567] Translation: "p = on_tline p c c d"

I0123 15:51:39.865559 139864642547712 alphageometry.py:576] Solving: "a b c = triangle a b c; d = angle_bisector d b c a, on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g f b e; h = on_circle h g b, on_line h c b; i = circle i f a e; j = on_circle j i a, on_line j c a; k = midpoint k b a; l = lc_tangent l k a, on_line l c a; m = circle m c e a; n = circle n j l e; o = on_line o b a, on_line o j h; p = on_tline p c c d ? coll o m n"
I0123 15:51:39.867293 139864642547712 graph.py:498] 
I0123 15:51:39.867486 139864642547712 graph.py:499] a b c = triangle a b c; d = angle_bisector d b c a, on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g f b e; h = on_circle h g b, on_line h c b; i = circle i f a e; j = on_circle j i a, on_line j c a; k = midpoint k b a; l = lc_tangent l k a, on_line l c a; m = circle m c e a; n = circle n j l e; o = on_line o b a, on_line o j h; p = on_tline p c c d ? coll o m n
I0123 15:51:42.716919 139864642547712 ddar.py:60] Depth 1/1000 time = 2.755659580230713
I0123 15:51:48.645039 139864642547712 ddar.py:60] Depth 2/1000 time = 5.92775297164917
I0123 15:51:55.527526 139864642547712 ddar.py:60] Depth 3/1000 time = 6.882078409194946
I0123 15:52:02.865585 139864642547712 ddar.py:60] Depth 4/1000 time = 7.33762788772583
I0123 15:52:09.693577 139864642547712 ddar.py:60] Depth 5/1000 time = 6.827173948287964
I0123 15:52:17.163948 139864642547712 ddar.py:60] Depth 6/1000 time = 7.468165159225464
I0123 15:52:24.882365 139864642547712 ddar.py:60] Depth 7/1000 time = 7.718077659606934
I0123 15:52:32.329822 139864642547712 ddar.py:60] Depth 8/1000 time = 7.447126865386963
I0123 15:52:40.224231 139864642547712 ddar.py:60] Depth 9/1000 time = 7.894087791442871
I0123 15:52:49.363907 139864642547712 ddar.py:60] Depth 10/1000 time = 9.070354700088501
I0123 15:52:58.984475 139864642547712 ddar.py:60] Depth 11/1000 time = 9.620166063308716
I0123 15:53:07.501186 139864642547712 ddar.py:60] Depth 12/1000 time = 8.516437768936157
I0123 15:53:16.793618 139864642547712 ddar.py:60] Depth 13/1000 time = 9.242615699768066
I0123 15:53:27.752659 139864642547712 ddar.py:60] Depth 14/1000 time = 10.958751440048218
I0123 15:53:38.887367 139864642547712 ddar.py:60] Depth 15/1000 time = 11.13440728187561
I0123 15:53:50.582042 139864642547712 ddar.py:60] Depth 16/1000 time = 11.694360971450806
I0123 15:54:03.494675 139864642547712 ddar.py:60] Depth 17/1000 time = 12.912087202072144
I0123 15:54:16.363459 139864642547712 ddar.py:60] Depth 18/1000 time = 12.868338346481323
I0123 15:54:30.679900 139864642547712 ddar.py:60] Depth 19/1000 time = 14.315979242324829
I0123 15:54:44.499203 139864642547712 ddar.py:60] Depth 20/1000 time = 13.818901300430298
I0123 15:54:58.520848 139864642547712 ddar.py:60] Depth 21/1000 time = 13.968997478485107
I0123 15:55:13.137237 139864642547712 ddar.py:60] Depth 22/1000 time = 14.567272424697876
I0123 15:55:13.137595 139864642547712 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 15:55:13.137762 139864642547712 alphageometry.py:566] LM output (score=-3.157784): "p : T d e d p 23 ;"
I0123 15:55:13.137804 139864642547712 alphageometry.py:567] Translation: "p = on_tline p d d e"

I0123 15:55:13.137858 139864642547712 alphageometry.py:576] Solving: "a b c = triangle a b c; d = angle_bisector d b c a, on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g f b e; h = on_circle h g b, on_line h c b; i = circle i f a e; j = on_circle j i a, on_line j c a; k = midpoint k b a; l = lc_tangent l k a, on_line l c a; m = circle m c e a; n = circle n j l e; o = on_line o b a, on_line o j h; p = on_tline p d d e ? coll o m n"
I0123 15:55:13.138103 139864642547712 graph.py:498] 
I0123 15:55:13.138174 139864642547712 graph.py:499] a b c = triangle a b c; d = angle_bisector d b c a, on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g f b e; h = on_circle h g b, on_line h c b; i = circle i f a e; j = on_circle j i a, on_line j c a; k = midpoint k b a; l = lc_tangent l k a, on_line l c a; m = circle m c e a; n = circle n j l e; o = on_line o b a, on_line o j h; p = on_tline p d d e ? coll o m n
I0123 15:55:16.640946 139864642547712 ddar.py:60] Depth 1/1000 time = 3.4202792644500732
I0123 15:55:22.112482 139864642547712 ddar.py:60] Depth 2/1000 time = 5.471274137496948
I0123 15:55:29.048233 139864642547712 ddar.py:60] Depth 3/1000 time = 6.935500144958496
I0123 15:55:35.997776 139864642547712 ddar.py:60] Depth 4/1000 time = 6.949295997619629
I0123 15:55:43.037992 139864642547712 ddar.py:60] Depth 5/1000 time = 7.039562463760376
I0123 15:55:50.134142 139864642547712 ddar.py:60] Depth 6/1000 time = 7.093923568725586
I0123 15:55:57.413288 139864642547712 ddar.py:60] Depth 7/1000 time = 7.278859376907349
I0123 15:56:04.888066 139864642547712 ddar.py:60] Depth 8/1000 time = 7.474422454833984
I0123 15:56:12.860830 139864642547712 ddar.py:60] Depth 9/1000 time = 7.972315549850464
I0123 15:56:22.372953 139864642547712 ddar.py:60] Depth 10/1000 time = 9.44281792640686
I0123 15:56:31.151867 139864642547712 ddar.py:60] Depth 11/1000 time = 8.778624057769775
I0123 15:56:40.254069 139864642547712 ddar.py:60] Depth 12/1000 time = 9.10189151763916
I0123 15:56:49.857462 139864642547712 ddar.py:60] Depth 13/1000 time = 9.552860021591187
I0123 15:57:00.527788 139864642547712 ddar.py:60] Depth 14/1000 time = 10.670006513595581
I0123 15:57:12.254454 139864642547712 ddar.py:60] Depth 15/1000 time = 11.726337671279907
I0123 15:57:24.110666 139864642547712 ddar.py:60] Depth 16/1000 time = 11.85585641860962
I0123 15:57:36.833156 139864642547712 ddar.py:60] Depth 17/1000 time = 12.722078561782837
I0123 15:57:49.696946 139864642547712 ddar.py:60] Depth 18/1000 time = 12.863134860992432
I0123 15:58:03.899735 139864642547712 ddar.py:60] Depth 19/1000 time = 14.202302694320679
I0123 15:58:18.587967 139864642547712 ddar.py:60] Depth 20/1000 time = 14.687806367874146
I0123 15:58:33.191883 139864642547712 ddar.py:60] Depth 21/1000 time = 14.551146507263184
I0123 15:58:48.434617 139864642547712 ddar.py:60] Depth 22/1000 time = 15.187888383865356
I0123 15:58:48.435094 139864642547712 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 15:58:48.435255 139864642547712 alphageometry.py:566] LM output (score=-3.207983): "p : T c f f p 23 ;"
I0123 15:58:48.435293 139864642547712 alphageometry.py:567] Translation: "p = on_tline p f c f"

I0123 15:58:48.435346 139864642547712 alphageometry.py:576] Solving: "a b c = triangle a b c; d = angle_bisector d b c a, on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g f b e; h = on_circle h g b, on_line h c b; i = circle i f a e; j = on_circle j i a, on_line j c a; k = midpoint k b a; l = lc_tangent l k a, on_line l c a; m = circle m c e a; n = circle n j l e; o = on_line o b a, on_line o j h; p = on_tline p f c f ? coll o m n"
I0123 15:58:48.435563 139864642547712 graph.py:498] 
I0123 15:58:48.435635 139864642547712 graph.py:499] a b c = triangle a b c; d = angle_bisector d b c a, on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g f b e; h = on_circle h g b, on_line h c b; i = circle i f a e; j = on_circle j i a, on_line j c a; k = midpoint k b a; l = lc_tangent l k a, on_line l c a; m = circle m c e a; n = circle n j l e; o = on_line o b a, on_line o j h; p = on_tline p f c f ? coll o m n
I0123 15:58:51.510863 139864642547712 ddar.py:60] Depth 1/1000 time = 2.990926742553711
I0123 15:58:56.970818 139864642547712 ddar.py:60] Depth 2/1000 time = 5.4596662521362305
I0123 15:59:03.915914 139864642547712 ddar.py:60] Depth 3/1000 time = 6.944775581359863
I0123 15:59:10.848711 139864642547712 ddar.py:60] Depth 4/1000 time = 6.932534694671631
I0123 15:59:17.827485 139864642547712 ddar.py:60] Depth 5/1000 time = 6.978052139282227
I0123 15:59:25.485026 139864642547712 ddar.py:60] Depth 6/1000 time = 7.655264854431152
I0123 15:59:32.989084 139864642547712 ddar.py:60] Depth 7/1000 time = 7.50376296043396
I0123 15:59:40.585515 139864642547712 ddar.py:60] Depth 8/1000 time = 7.5961151123046875
I0123 15:59:48.670723 139864642547712 ddar.py:60] Depth 9/1000 time = 8.084887504577637
I0123 15:59:57.829010 139864642547712 ddar.py:60] Depth 10/1000 time = 9.088269233703613
I0123 16:00:07.320182 139864642547712 ddar.py:60] Depth 11/1000 time = 9.49076223373413
I0123 16:00:16.241962 139864642547712 ddar.py:60] Depth 12/1000 time = 8.921425342559814
I0123 16:00:25.353354 139864642547712 ddar.py:60] Depth 13/1000 time = 9.060688018798828
I0123 16:00:36.070439 139864642547712 ddar.py:60] Depth 14/1000 time = 10.716818571090698
I0123 16:00:47.981424 139864642547712 ddar.py:60] Depth 15/1000 time = 11.910678625106812
I0123 16:00:59.443830 139864642547712 ddar.py:60] Depth 16/1000 time = 11.461983680725098
I0123 16:01:12.799816 139864642547712 ddar.py:60] Depth 17/1000 time = 13.355496168136597
I0123 16:01:25.591143 139864642547712 ddar.py:60] Depth 18/1000 time = 12.790871858596802
I0123 16:01:39.763503 139864642547712 ddar.py:60] Depth 19/1000 time = 14.171871662139893
I0123 16:01:54.252590 139864642547712 ddar.py:60] Depth 20/1000 time = 14.48865556716919
I0123 16:02:08.788716 139864642547712 ddar.py:60] Depth 21/1000 time = 14.481502771377563
I0123 16:02:23.605654 139864642547712 ddar.py:60] Depth 22/1000 time = 14.762598752975464
I0123 16:02:23.606164 139864642547712 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 16:02:23.606365 139864642547712 alphageometry.py:566] LM output (score=-3.266632): "p : T c d d p 23 ;"
I0123 16:02:23.606410 139864642547712 alphageometry.py:567] Translation: "p = on_tline p d c d"

I0123 16:02:23.606470 139864642547712 alphageometry.py:576] Solving: "a b c = triangle a b c; d = angle_bisector d b c a, on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g f b e; h = on_circle h g b, on_line h c b; i = circle i f a e; j = on_circle j i a, on_line j c a; k = midpoint k b a; l = lc_tangent l k a, on_line l c a; m = circle m c e a; n = circle n j l e; o = on_line o b a, on_line o j h; p = on_tline p d c d ? coll o m n"
I0123 16:02:23.606707 139864642547712 graph.py:498] 
I0123 16:02:23.606772 139864642547712 graph.py:499] a b c = triangle a b c; d = angle_bisector d b c a, on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g f b e; h = on_circle h g b, on_line h c b; i = circle i f a e; j = on_circle j i a, on_line j c a; k = midpoint k b a; l = lc_tangent l k a, on_line l c a; m = circle m c e a; n = circle n j l e; o = on_line o b a, on_line o j h; p = on_tline p d c d ? coll o m n
I0123 16:02:26.779419 139864642547712 ddar.py:60] Depth 1/1000 time = 3.0863540172576904
I0123 16:02:31.776980 139864642547712 ddar.py:60] Depth 2/1000 time = 4.997318267822266
I0123 16:02:39.255556 139864642547712 ddar.py:60] Depth 3/1000 time = 7.47829532623291
I0123 16:02:46.291453 139864642547712 ddar.py:60] Depth 4/1000 time = 7.035586833953857
I0123 16:02:53.280559 139864642547712 ddar.py:60] Depth 5/1000 time = 6.9883387088775635
I0123 16:03:00.474462 139864642547712 ddar.py:60] Depth 6/1000 time = 7.191730499267578
I0123 16:03:07.939997 139864642547712 ddar.py:60] Depth 7/1000 time = 7.465213298797607
I0123 16:03:15.964612 139864642547712 ddar.py:60] Depth 8/1000 time = 8.0242919921875
I0123 16:03:23.575783 139864642547712 ddar.py:60] Depth 9/1000 time = 7.610685348510742
I0123 16:03:32.549270 139864642547712 ddar.py:60] Depth 10/1000 time = 8.904224157333374
I0123 16:03:41.862044 139864642547712 ddar.py:60] Depth 11/1000 time = 9.31245756149292
I0123 16:03:51.391788 139864642547712 ddar.py:60] Depth 12/1000 time = 9.529414892196655
I0123 16:04:00.583751 139864642547712 ddar.py:60] Depth 13/1000 time = 9.141725063323975
I0123 16:04:10.928119 139864642547712 ddar.py:60] Depth 14/1000 time = 10.344014167785645
I0123 16:04:22.248229 139864642547712 ddar.py:60] Depth 15/1000 time = 11.319680452346802
I0123 16:04:34.359324 139864642547712 ddar.py:60] Depth 16/1000 time = 12.110723972320557
I0123 16:04:47.185878 139864642547712 ddar.py:60] Depth 17/1000 time = 12.826153039932251
I0123 16:04:59.609129 139864642547712 ddar.py:60] Depth 18/1000 time = 12.422858953475952
I0123 16:05:13.796730 139864642547712 ddar.py:60] Depth 19/1000 time = 14.187134981155396
I0123 16:05:28.055989 139864642547712 ddar.py:60] Depth 20/1000 time = 14.258683919906616
I0123 16:05:42.593734 139864642547712 ddar.py:60] Depth 21/1000 time = 14.48549771308899
I0123 16:05:57.285763 139864642547712 ddar.py:60] Depth 22/1000 time = 14.639127969741821
I0123 16:05:57.286143 139864642547712 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 16:05:57.286289 139864642547712 alphageometry.py:566] LM output (score=-3.273719): "p : T k l l p 23 ;"
I0123 16:05:57.286328 139864642547712 alphageometry.py:567] Translation: "p = on_tline p l k l"

I0123 16:05:57.286383 139864642547712 alphageometry.py:576] Solving: "a b c = triangle a b c; d = angle_bisector d b c a, on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g f b e; h = on_circle h g b, on_line h c b; i = circle i f a e; j = on_circle j i a, on_line j c a; k = midpoint k b a; l = lc_tangent l k a, on_line l c a; m = circle m c e a; n = circle n j l e; o = on_line o b a, on_line o j h; p = on_tline p l k l ? coll o m n"
I0123 16:05:57.286595 139864642547712 graph.py:498] 
I0123 16:05:57.286659 139864642547712 graph.py:499] a b c = triangle a b c; d = angle_bisector d b c a, on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g f b e; h = on_circle h g b, on_line h c b; i = circle i f a e; j = on_circle j i a, on_line j c a; k = midpoint k b a; l = lc_tangent l k a, on_line l c a; m = circle m c e a; n = circle n j l e; o = on_line o b a, on_line o j h; p = on_tline p l k l ? coll o m n
I0123 16:06:00.438863 139864642547712 ddar.py:60] Depth 1/1000 time = 3.0684802532196045
I0123 16:06:05.852279 139864642547712 ddar.py:60] Depth 2/1000 time = 5.413172006607056
I0123 16:06:12.787126 139864642547712 ddar.py:60] Depth 3/1000 time = 6.934513807296753
I0123 16:06:20.392332 139864642547712 ddar.py:60] Depth 4/1000 time = 7.604799032211304
I0123 16:06:26.793597 139864642547712 ddar.py:60] Depth 5/1000 time = 6.400644540786743
I0123 16:06:36.129676 139864642547712 ddar.py:60] Depth 6/1000 time = 9.278127670288086
I0123 16:06:44.849521 139864642547712 ddar.py:60] Depth 7/1000 time = 8.719435930252075
I0123 16:06:54.035496 139864642547712 ddar.py:60] Depth 8/1000 time = 9.185611486434937
I0123 16:07:03.479379 139864642547712 ddar.py:60] Depth 9/1000 time = 9.437052488327026
I0123 16:07:13.526541 139864642547712 ddar.py:60] Depth 10/1000 time = 10.046906232833862
I0123 16:07:25.039180 139864642547712 ddar.py:60] Depth 11/1000 time = 11.512340545654297
I0123 16:07:36.105152 139864642547712 ddar.py:60] Depth 12/1000 time = 11.065589904785156
I0123 16:07:48.312856 139864642547712 ddar.py:60] Depth 13/1000 time = 12.207171201705933
I0123 16:08:01.434853 139864642547712 ddar.py:60] Depth 14/1000 time = 13.121541500091553
I0123 16:08:14.508367 139864642547712 ddar.py:60] Depth 15/1000 time = 13.073090553283691
I0123 16:08:28.262753 139864642547712 ddar.py:60] Depth 16/1000 time = 13.753962516784668
I0123 16:08:41.352708 139864642547712 ddar.py:60] Depth 17/1000 time = 13.05740213394165
I0123 16:08:55.169813 139864642547712 ddar.py:60] Depth 18/1000 time = 13.77456021308899
I0123 16:09:09.845087 139864642547712 ddar.py:60] Depth 19/1000 time = 14.628346681594849
I0123 16:09:09.845520 139864642547712 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 16:09:09.845678 139864642547712 alphageometry.py:566] LM output (score=-3.300666): "p : T d e i p 23 ;"
I0123 16:09:09.845724 139864642547712 alphageometry.py:567] Translation: "p = on_tline p i d e"

I0123 16:09:09.845788 139864642547712 alphageometry.py:576] Solving: "a b c = triangle a b c; d = angle_bisector d b c a, on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g f b e; h = on_circle h g b, on_line h c b; i = circle i f a e; j = on_circle j i a, on_line j c a; k = midpoint k b a; l = lc_tangent l k a, on_line l c a; m = circle m c e a; n = circle n j l e; o = on_line o b a, on_line o j h; p = on_tline p i d e ? coll o m n"
I0123 16:09:09.846006 139864642547712 graph.py:498] 
I0123 16:09:09.846080 139864642547712 graph.py:499] a b c = triangle a b c; d = angle_bisector d b c a, on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g f b e; h = on_circle h g b, on_line h c b; i = circle i f a e; j = on_circle j i a, on_line j c a; k = midpoint k b a; l = lc_tangent l k a, on_line l c a; m = circle m c e a; n = circle n j l e; o = on_line o b a, on_line o j h; p = on_tline p i d e ? coll o m n
I0123 16:09:12.480026 139864642547712 ddar.py:60] Depth 1/1000 time = 2.5499722957611084
I0123 16:09:18.059674 139864642547712 ddar.py:60] Depth 2/1000 time = 5.579378604888916
I0123 16:09:25.270445 139864642547712 ddar.py:60] Depth 3/1000 time = 7.210442304611206
I0123 16:09:33.144171 139864642547712 ddar.py:60] Depth 4/1000 time = 7.873421669006348
I0123 16:09:41.083702 139864642547712 ddar.py:60] Depth 5/1000 time = 7.939256906509399
I0123 16:09:49.100614 139864642547712 ddar.py:60] Depth 6/1000 time = 8.016566514968872
I0123 16:09:57.072020 139864642547712 ddar.py:60] Depth 7/1000 time = 7.970648527145386
I0123 16:10:05.839461 139864642547712 ddar.py:60] Depth 8/1000 time = 8.765095710754395
I0123 16:10:14.421327 139864642547712 ddar.py:60] Depth 9/1000 time = 8.581382751464844
I0123 16:10:23.569767 139864642547712 ddar.py:60] Depth 10/1000 time = 9.148060321807861
I0123 16:10:32.315043 139864642547712 ddar.py:60] Depth 11/1000 time = 8.744797945022583
I0123 16:10:43.061582 139864642547712 ddar.py:60] Depth 12/1000 time = 10.672760486602783
I0123 16:10:53.616462 139864642547712 ddar.py:60] Depth 13/1000 time = 10.554536581039429
I0123 16:11:04.066602 139864642547712 ddar.py:60] Depth 14/1000 time = 10.44978380203247
I0123 16:11:14.884057 139864642547712 ddar.py:60] Depth 15/1000 time = 10.765206813812256
I0123 16:11:26.845961 139864642547712 ddar.py:60] Depth 16/1000 time = 11.961514234542847
I0123 16:11:39.715077 139864642547712 ddar.py:60] Depth 17/1000 time = 12.86863112449646
I0123 16:11:53.166913 139864642547712 ddar.py:60] Depth 18/1000 time = 13.451361179351807
I0123 16:12:07.507874 139864642547712 ddar.py:60] Depth 19/1000 time = 14.340405702590942
I0123 16:12:23.338851 139864642547712 ddar.py:60] Depth 20/1000 time = 15.830537796020508
I0123 16:12:41.001056 139864642547712 ddar.py:60] Depth 21/1000 time = 17.661756992340088
I0123 16:12:58.720192 139864642547712 ddar.py:60] Depth 22/1000 time = 17.71856951713562
I0123 16:13:17.280929 139864642547712 ddar.py:60] Depth 23/1000 time = 18.542161226272583
I0123 16:13:35.097790 139864642547712 ddar.py:60] Depth 24/1000 time = 17.754565238952637
I0123 16:13:53.046793 139864642547712 ddar.py:60] Depth 25/1000 time = 17.89312171936035
I0123 16:13:53.047357 139864642547712 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 16:13:53.047556 139864642547712 alphageometry.py:566] LM output (score=-3.301459): "p : T c e c p 23 ;"
I0123 16:13:53.047601 139864642547712 alphageometry.py:567] Translation: "p = on_tline p c c e"

I0123 16:13:53.047672 139864642547712 alphageometry.py:576] Solving: "a b c = triangle a b c; d = angle_bisector d b c a, on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g f b e; h = on_circle h g b, on_line h c b; i = circle i f a e; j = on_circle j i a, on_line j c a; k = midpoint k b a; l = lc_tangent l k a, on_line l c a; m = circle m c e a; n = circle n j l e; o = on_line o b a, on_line o j h; p = on_tline p c c e ? coll o m n"
I0123 16:13:53.047947 139864642547712 graph.py:498] 
I0123 16:13:53.048021 139864642547712 graph.py:499] a b c = triangle a b c; d = angle_bisector d b c a, on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g f b e; h = on_circle h g b, on_line h c b; i = circle i f a e; j = on_circle j i a, on_line j c a; k = midpoint k b a; l = lc_tangent l k a, on_line l c a; m = circle m c e a; n = circle n j l e; o = on_line o b a, on_line o j h; p = on_tline p c c e ? coll o m n
I0123 16:13:56.820884 139864642547712 ddar.py:60] Depth 1/1000 time = 3.6868088245391846
I0123 16:14:01.898989 139864642547712 ddar.py:60] Depth 2/1000 time = 5.077879190444946
I0123 16:14:09.611089 139864642547712 ddar.py:60] Depth 3/1000 time = 7.711843490600586
I0123 16:14:16.925664 139864642547712 ddar.py:60] Depth 4/1000 time = 7.314263105392456
I0123 16:14:23.618216 139864642547712 ddar.py:60] Depth 5/1000 time = 6.6918628215789795
I0123 16:14:31.129951 139864642547712 ddar.py:60] Depth 6/1000 time = 7.509593486785889
I0123 16:14:38.920358 139864642547712 ddar.py:60] Depth 7/1000 time = 7.790104866027832
I0123 16:14:46.807401 139864642547712 ddar.py:60] Depth 8/1000 time = 7.886708974838257
I0123 16:14:54.737726 139864642547712 ddar.py:60] Depth 9/1000 time = 7.930041313171387
I0123 16:15:04.064613 139864642547712 ddar.py:60] Depth 10/1000 time = 9.257333278656006
I0123 16:15:13.302449 139864642547712 ddar.py:60] Depth 11/1000 time = 9.23755407333374
I0123 16:15:22.571954 139864642547712 ddar.py:60] Depth 12/1000 time = 9.26921272277832
I0123 16:15:32.007650 139864642547712 ddar.py:60] Depth 13/1000 time = 9.38476824760437
I0123 16:15:42.549342 139864642547712 ddar.py:60] Depth 14/1000 time = 10.541317701339722
I0123 16:15:54.275695 139864642547712 ddar.py:60] Depth 15/1000 time = 11.725889205932617
I0123 16:16:06.883876 139864642547712 ddar.py:60] Depth 16/1000 time = 12.607806921005249
I0123 16:16:20.352070 139864642547712 ddar.py:60] Depth 17/1000 time = 13.467702627182007
I0123 16:16:33.276768 139864642547712 ddar.py:60] Depth 18/1000 time = 12.924153804779053
I0123 16:16:48.099450 139864642547712 ddar.py:60] Depth 19/1000 time = 14.822286128997803
I0123 16:17:03.030671 139864642547712 ddar.py:60] Depth 20/1000 time = 14.930771589279175
I0123 16:17:17.958725 139864642547712 ddar.py:60] Depth 21/1000 time = 14.876071691513062
I0123 16:17:33.278247 139864642547712 ddar.py:60] Depth 22/1000 time = 15.26535940170288
I0123 16:17:33.278637 139864642547712 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 16:17:33.278782 139864642547712 alphageometry.py:566] LM output (score=-3.302791): "p : T e g g p 23 ;"
I0123 16:17:33.278829 139864642547712 alphageometry.py:567] Translation: "p = on_tline p g e g"

I0123 16:17:33.278887 139864642547712 alphageometry.py:576] Solving: "a b c = triangle a b c; d = angle_bisector d b c a, on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g f b e; h = on_circle h g b, on_line h c b; i = circle i f a e; j = on_circle j i a, on_line j c a; k = midpoint k b a; l = lc_tangent l k a, on_line l c a; m = circle m c e a; n = circle n j l e; o = on_line o b a, on_line o j h; p = on_tline p g e g ? coll o m n"
I0123 16:17:33.279106 139864642547712 graph.py:498] 
I0123 16:17:33.279177 139864642547712 graph.py:499] a b c = triangle a b c; d = angle_bisector d b c a, on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g f b e; h = on_circle h g b, on_line h c b; i = circle i f a e; j = on_circle j i a, on_line j c a; k = midpoint k b a; l = lc_tangent l k a, on_line l c a; m = circle m c e a; n = circle n j l e; o = on_line o b a, on_line o j h; p = on_tline p g e g ? coll o m n
I0123 16:17:36.511517 139864642547712 ddar.py:60] Depth 1/1000 time = 3.1499054431915283
I0123 16:17:41.603990 139864642547712 ddar.py:60] Depth 2/1000 time = 5.09219217300415
I0123 16:17:48.907313 139864642547712 ddar.py:60] Depth 3/1000 time = 7.303042888641357
I0123 16:17:56.181667 139864642547712 ddar.py:60] Depth 4/1000 time = 7.274064540863037
I0123 16:18:03.517739 139864642547712 ddar.py:60] Depth 5/1000 time = 7.33549952507019
I0123 16:18:12.033369 139864642547712 ddar.py:60] Depth 6/1000 time = 8.451637029647827
I0123 16:18:21.219199 139864642547712 ddar.py:60] Depth 7/1000 time = 9.185537338256836
I0123 16:18:30.451147 139864642547712 ddar.py:60] Depth 8/1000 time = 9.231634140014648
I0123 16:18:39.757971 139864642547712 ddar.py:60] Depth 9/1000 time = 9.3001229763031
I0123 16:18:50.341893 139864642547712 ddar.py:60] Depth 10/1000 time = 10.58355712890625
I0123 16:19:01.738753 139864642547712 ddar.py:60] Depth 11/1000 time = 11.396432399749756
I0123 16:19:13.407452 139864642547712 ddar.py:60] Depth 12/1000 time = 11.66835069656372
I0123 16:19:26.102608 139864642547712 ddar.py:60] Depth 13/1000 time = 12.694774389266968
I0123 16:19:38.641476 139864642547712 ddar.py:60] Depth 14/1000 time = 12.538495063781738
I0123 16:19:52.849943 139864642547712 ddar.py:60] Depth 15/1000 time = 14.208017349243164
I0123 16:20:05.970469 139864642547712 ddar.py:60] Depth 16/1000 time = 13.119980096817017
I0123 16:20:20.350679 139864642547712 ddar.py:60] Depth 17/1000 time = 14.344348192214966
I0123 16:20:34.301974 139864642547712 ddar.py:60] Depth 18/1000 time = 13.906646013259888
I0123 16:20:48.933965 139864642547712 ddar.py:60] Depth 19/1000 time = 14.581271409988403
I0123 16:20:48.934327 139864642547712 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 16:20:48.934515 139864642547712 alphageometry.py:566] LM output (score=-3.306709): "p : T b h h p 23 ;"
I0123 16:20:48.934560 139864642547712 alphageometry.py:567] Translation: "p = on_tline p h b h"

I0123 16:20:48.934624 139864642547712 alphageometry.py:576] Solving: "a b c = triangle a b c; d = angle_bisector d b c a, on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g f b e; h = on_circle h g b, on_line h c b; i = circle i f a e; j = on_circle j i a, on_line j c a; k = midpoint k b a; l = lc_tangent l k a, on_line l c a; m = circle m c e a; n = circle n j l e; o = on_line o b a, on_line o j h; p = on_tline p h b h ? coll o m n"
I0123 16:20:48.934843 139864642547712 graph.py:498] 
I0123 16:20:48.934920 139864642547712 graph.py:499] a b c = triangle a b c; d = angle_bisector d b c a, on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g f b e; h = on_circle h g b, on_line h c b; i = circle i f a e; j = on_circle j i a, on_line j c a; k = midpoint k b a; l = lc_tangent l k a, on_line l c a; m = circle m c e a; n = circle n j l e; o = on_line o b a, on_line o j h; p = on_tline p h b h ? coll o m n
I0123 16:20:52.238730 139864642547712 ddar.py:60] Depth 1/1000 time = 3.221611499786377
I0123 16:20:57.460619 139864642547712 ddar.py:60] Depth 2/1000 time = 5.221487998962402
I0123 16:21:04.775776 139864642547712 ddar.py:60] Depth 3/1000 time = 7.314881801605225
I0123 16:21:12.120913 139864642547712 ddar.py:60] Depth 4/1000 time = 7.344890594482422
I0123 16:21:18.876244 139864642547712 ddar.py:60] Depth 5/1000 time = 6.754736423492432
I0123 16:21:26.299050 139864642547712 ddar.py:60] Depth 6/1000 time = 7.420443773269653
I0123 16:21:34.070104 139864642547712 ddar.py:60] Depth 7/1000 time = 7.77072811126709
I0123 16:21:42.142691 139864642547712 ddar.py:60] Depth 8/1000 time = 8.072154760360718
I0123 16:21:49.500521 139864642547712 ddar.py:60] Depth 9/1000 time = 7.357543468475342
I0123 16:21:59.198931 139864642547712 ddar.py:60] Depth 10/1000 time = 9.627516508102417
I0123 16:22:08.524753 139864642547712 ddar.py:60] Depth 11/1000 time = 9.325461626052856
I0123 16:22:17.912339 139864642547712 ddar.py:60] Depth 12/1000 time = 9.38722038269043
I0123 16:22:27.467421 139864642547712 ddar.py:60] Depth 13/1000 time = 9.509601354598999
I0123 16:22:38.282890 139864642547712 ddar.py:60] Depth 14/1000 time = 10.815099477767944
I0123 16:22:50.197492 139864642547712 ddar.py:60] Depth 15/1000 time = 11.914120435714722
I0123 16:23:01.566062 139864642547712 ddar.py:60] Depth 16/1000 time = 11.368138790130615
I0123 16:23:14.752442 139864642547712 ddar.py:60] Depth 17/1000 time = 13.185818195343018
I0123 16:23:27.764918 139864642547712 ddar.py:60] Depth 18/1000 time = 13.012055158615112
I0123 16:23:41.725438 139864642547712 ddar.py:60] Depth 19/1000 time = 13.96004056930542
I0123 16:23:56.264013 139864642547712 ddar.py:60] Depth 20/1000 time = 14.537980079650879
I0123 16:24:10.603586 139864642547712 ddar.py:60] Depth 21/1000 time = 14.287864685058594
I0123 16:24:25.074159 139864642547712 ddar.py:60] Depth 22/1000 time = 14.417500257492065
I0123 16:24:25.074757 139864642547712 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 16:24:25.074964 139864642547712 alphageometry.py:566] LM output (score=-3.313039): "p : C b c p 23 T b c p d 24 ;"
I0123 16:24:25.075012 139864642547712 alphageometry.py:567] Translation: "p = on_line p b c, on_tline p d b c"

I0123 16:24:25.075079 139864642547712 alphageometry.py:576] Solving: "a b c = triangle a b c; d = angle_bisector d b c a, on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g f b e; h = on_circle h g b, on_line h c b; i = circle i f a e; j = on_circle j i a, on_line j c a; k = midpoint k b a; l = lc_tangent l k a, on_line l c a; m = circle m c e a; n = circle n j l e; o = on_line o b a, on_line o j h; p = on_line p b c, on_tline p d b c ? coll o m n"
I0123 16:24:25.075329 139864642547712 graph.py:498] 
I0123 16:24:25.075400 139864642547712 graph.py:499] a b c = triangle a b c; d = angle_bisector d b c a, on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g f b e; h = on_circle h g b, on_line h c b; i = circle i f a e; j = on_circle j i a, on_line j c a; k = midpoint k b a; l = lc_tangent l k a, on_line l c a; m = circle m c e a; n = circle n j l e; o = on_line o b a, on_line o j h; p = on_line p b c, on_tline p d b c ? coll o m n
I0123 16:24:28.787688 139864642547712 ddar.py:60] Depth 1/1000 time = 3.6259093284606934
I0123 16:24:35.545187 139864642547712 ddar.py:60] Depth 2/1000 time = 6.757234334945679
I0123 16:24:44.182438 139864642547712 ddar.py:60] Depth 3/1000 time = 8.636885166168213
I0123 16:24:52.216656 139864642547712 ddar.py:60] Depth 4/1000 time = 8.033808946609497
I0123 16:25:00.812763 139864642547712 ddar.py:60] Depth 5/1000 time = 8.595443487167358
I0123 16:25:09.008461 139864642547712 ddar.py:60] Depth 6/1000 time = 8.193212270736694
I0123 16:25:17.950996 139864642547712 ddar.py:60] Depth 7/1000 time = 8.942194700241089
I0123 16:25:27.271703 139864642547712 ddar.py:60] Depth 8/1000 time = 9.320295572280884
I0123 16:25:36.479138 139864642547712 ddar.py:60] Depth 9/1000 time = 9.206958293914795
I0123 16:25:47.824161 139864642547712 ddar.py:60] Depth 10/1000 time = 11.27370309829712
I0123 16:25:57.914804 139864642547712 ddar.py:60] Depth 11/1000 time = 10.09021258354187
I0123 16:26:09.232743 139864642547712 ddar.py:60] Depth 12/1000 time = 11.3175208568573
I0123 16:26:19.486242 139864642547712 ddar.py:60] Depth 13/1000 time = 10.207931280136108
I0123 16:26:32.270447 139864642547712 ddar.py:60] Depth 14/1000 time = 12.78382682800293
I0123 16:26:46.182620 139864642547712 ddar.py:60] Depth 15/1000 time = 13.911723375320435
I0123 16:26:59.772097 139864642547712 ddar.py:60] Depth 16/1000 time = 13.589083194732666
I0123 16:27:14.820550 139864642547712 ddar.py:60] Depth 17/1000 time = 15.047925233840942
I0123 16:27:29.257878 139864642547712 ddar.py:60] Depth 18/1000 time = 14.436688661575317
I0123 16:27:45.578887 139864642547712 ddar.py:60] Depth 19/1000 time = 16.32044816017151
I0123 16:28:02.038502 139864642547712 ddar.py:60] Depth 20/1000 time = 16.45914912223816
I0123 16:28:18.734086 139864642547712 ddar.py:60] Depth 21/1000 time = 16.642356395721436
I0123 16:28:35.082356 139864642547712 ddar.py:60] Depth 22/1000 time = 16.29017949104309
I0123 16:28:35.082958 139864642547712 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 16:28:35.083193 139864642547712 alphageometry.py:566] LM output (score=-3.314264): "p : T e j e p 23 ;"
I0123 16:28:35.083241 139864642547712 alphageometry.py:567] Translation: "p = on_tline p e e j"

I0123 16:28:35.083318 139864642547712 alphageometry.py:576] Solving: "a b c = triangle a b c; d = angle_bisector d b c a, on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g f b e; h = on_circle h g b, on_line h c b; i = circle i f a e; j = on_circle j i a, on_line j c a; k = midpoint k b a; l = lc_tangent l k a, on_line l c a; m = circle m c e a; n = circle n j l e; o = on_line o b a, on_line o j h; p = on_tline p e e j ? coll o m n"
I0123 16:28:35.083567 139864642547712 graph.py:498] 
I0123 16:28:35.083637 139864642547712 graph.py:499] a b c = triangle a b c; d = angle_bisector d b c a, on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g f b e; h = on_circle h g b, on_line h c b; i = circle i f a e; j = on_circle j i a, on_line j c a; k = midpoint k b a; l = lc_tangent l k a, on_line l c a; m = circle m c e a; n = circle n j l e; o = on_line o b a, on_line o j h; p = on_tline p e e j ? coll o m n
I0123 16:28:38.461049 139864642547712 ddar.py:60] Depth 1/1000 time = 3.2908003330230713
I0123 16:28:44.046905 139864642547712 ddar.py:60] Depth 2/1000 time = 5.585585355758667
I0123 16:28:50.561921 139864642547712 ddar.py:60] Depth 3/1000 time = 6.51470947265625
I0123 16:28:57.726301 139864642547712 ddar.py:60] Depth 4/1000 time = 7.164071798324585
I0123 16:29:04.819756 139864642547712 ddar.py:60] Depth 5/1000 time = 7.092816591262817
I0123 16:29:13.328998 139864642547712 ddar.py:60] Depth 6/1000 time = 8.450303077697754
I0123 16:29:22.372970 139864642547712 ddar.py:60] Depth 7/1000 time = 9.043653726577759
I0123 16:29:31.284608 139864642547712 ddar.py:60] Depth 8/1000 time = 8.911290645599365
I0123 16:29:39.824979 139864642547712 ddar.py:60] Depth 9/1000 time = 8.533611059188843
I0123 16:29:49.957762 139864642547712 ddar.py:60] Depth 10/1000 time = 10.132426738739014
I0123 16:30:01.326153 139864642547712 ddar.py:60] Depth 11/1000 time = 11.368043184280396
I0123 16:30:12.168691 139864642547712 ddar.py:60] Depth 12/1000 time = 10.842122793197632
I0123 16:30:24.212848 139864642547712 ddar.py:60] Depth 13/1000 time = 12.043748378753662
I0123 16:30:36.380322 139864642547712 ddar.py:60] Depth 14/1000 time = 12.167077541351318
I0123 16:30:49.709911 139864642547712 ddar.py:60] Depth 15/1000 time = 13.32922101020813
I0123 16:31:02.410838 139864642547712 ddar.py:60] Depth 16/1000 time = 12.700417280197144
I0123 16:31:15.735307 139864642547712 ddar.py:60] Depth 17/1000 time = 13.291807651519775
I0123 16:31:29.956243 139864642547712 ddar.py:60] Depth 18/1000 time = 14.179106950759888
I0123 16:31:43.114999 139864642547712 ddar.py:60] Depth 19/1000 time = 13.109068393707275
I0123 16:31:43.115540 139864642547712 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 16:31:43.115743 139864642547712 alphageometry.py:566] LM output (score=-3.319055): "p : T d e e p 23 ;"
I0123 16:31:43.115784 139864642547712 alphageometry.py:567] Translation: "p = on_tline p e d e"

I0123 16:31:43.115845 139864642547712 alphageometry.py:576] Solving: "a b c = triangle a b c; d = angle_bisector d b c a, on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g f b e; h = on_circle h g b, on_line h c b; i = circle i f a e; j = on_circle j i a, on_line j c a; k = midpoint k b a; l = lc_tangent l k a, on_line l c a; m = circle m c e a; n = circle n j l e; o = on_line o b a, on_line o j h; p = on_tline p e d e ? coll o m n"
I0123 16:31:43.116085 139864642547712 graph.py:498] 
I0123 16:31:43.116148 139864642547712 graph.py:499] a b c = triangle a b c; d = angle_bisector d b c a, on_line d b a; e = on_line e c d; f = on_aline f c a b c e, on_aline f a c b a e; g = circle g f b e; h = on_circle h g b, on_line h c b; i = circle i f a e; j = on_circle j i a, on_line j c a; k = midpoint k b a; l = lc_tangent l k a, on_line l c a; m = circle m c e a; n = circle n j l e; o = on_line o b a, on_line o j h; p = on_tline p e d e ? coll o m n
I0123 16:31:46.470928 139864642547712 ddar.py:60] Depth 1/1000 time = 3.2716822624206543
I0123 16:31:52.307396 139864642547712 ddar.py:60] Depth 2/1000 time = 5.836213827133179
I0123 16:31:59.840721 139864642547712 ddar.py:60] Depth 3/1000 time = 7.533043622970581
I0123 16:32:06.805594 139864642547712 ddar.py:60] Depth 4/1000 time = 6.964571237564087
I0123 16:32:06.806419 139864642547712 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 16:32:06.806467 139864642547712 alphageometry.py:585] Timeout.
