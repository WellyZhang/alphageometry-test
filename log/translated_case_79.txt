I0123 13:26:37.151324 139683861139456 inference_utils.py:69] Parsing gin configuration.
I0123 13:26:37.151421 139683861139456 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0123 13:26:37.151620 139683861139456 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0123 13:26:37.151654 139683861139456 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0123 13:26:37.151686 139683861139456 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0123 13:26:37.151715 139683861139456 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0123 13:26:37.151743 139683861139456 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0123 13:26:37.151771 139683861139456 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0123 13:26:37.151798 139683861139456 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0123 13:26:37.151823 139683861139456 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=32
I0123 13:26:37.151849 139683861139456 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0123 13:26:37.151874 139683861139456 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0123 13:26:37.151918 139683861139456 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0123 13:26:37.152051 139683861139456 resource_reader.py:55] Path not found: base_htrans.gin
I0123 13:26:37.152254 139683861139456 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0123 13:26:37.152351 139683861139456 resource_reader.py:55] Path not found: trainer_configuration.gin
I0123 13:26:37.158822 139683861139456 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0123 13:26:37.158942 139683861139456 resource_reader.py:55] Path not found: size/medium_150M.gin
I0123 13:26:37.159265 139683861139456 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0123 13:26:37.159371 139683861139456 resource_reader.py:55] Path not found: options/positions_t5.gin
I0123 13:26:37.159645 139683861139456 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0123 13:26:37.159744 139683861139456 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0123 13:26:37.160148 139683861139456 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0123 13:26:37.160248 139683861139456 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0123 13:26:37.164078 139683861139456 training_loop.py:334] ==== Training loop: initializing model ====
I0123 13:26:37.263672 139683861139456 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0123 13:26:37.264409 139683861139456 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0123 13:26:37.271256 139683861139456 training_loop.py:335] Process 0 of 1
I0123 13:26:37.271311 139683861139456 training_loop.py:336] Local device count = 1
I0123 13:26:37.271350 139683861139456 training_loop.py:337] Number of replicas = 1
I0123 13:26:37.271382 139683861139456 training_loop.py:339] Using random number seed 42
I0123 13:26:37.747473 139683861139456 training_loop.py:359] Initializing the model.
I0123 13:26:38.132449 139683861139456 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.132696 139683861139456 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 13:26:38.132797 139683861139456 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 13:26:38.132873 139683861139456 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 13:26:38.132951 139683861139456 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 13:26:38.133034 139683861139456 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 13:26:38.133104 139683861139456 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 13:26:38.133174 139683861139456 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 13:26:38.133243 139683861139456 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 13:26:38.133312 139683861139456 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 13:26:38.133380 139683861139456 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 13:26:38.133447 139683861139456 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 13:26:38.133515 139683861139456 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 13:26:38.133583 139683861139456 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 13:26:38.133623 139683861139456 attention.py:418] Single window, no scan.
I0123 13:26:38.133677 139683861139456 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 13:26:38.133790 139683861139456 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:26:38.133829 139683861139456 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:26:38.133858 139683861139456 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:26:38.135838 139683861139456 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.141063 139683861139456 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:26:38.151669 139683861139456 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.151941 139683861139456 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:26:38.156250 139683861139456 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:26:38.166746 139683861139456 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:26:38.166801 139683861139456 attention.py:418] Single window, no scan.
I0123 13:26:38.166838 139683861139456 transformer_layer.py:389] tlayer: self-attention.
I0123 13:26:38.166869 139683861139456 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.166931 139683861139456 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.168100 139683861139456 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.168178 139683861139456 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.168879 139683861139456 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.171311 139683861139456 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.177007 139683861139456 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.178712 139683861139456 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.178797 139683861139456 transformer_layer.py:468] tlayer: End windows.
I0123 13:26:38.178833 139683861139456 transformer_layer.py:472] tlayer: final FFN.
I0123 13:26:38.178894 139683861139456 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.179022 139683861139456 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:26:38.179352 139683861139456 nn_components.py:325] mlp: activation = None
I0123 13:26:38.179400 139683861139456 nn_components.py:329] mlp: final activation = None
I0123 13:26:38.181284 139683861139456 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.181385 139683861139456 nn_components.py:261] mlp: residual
I0123 13:26:38.184396 139683861139456 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.184477 139683861139456 transformer_base.py:443] tbase: final FFN
I0123 13:26:38.185133 139683861139456 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:26:38.195102 139683861139456 nn_components.py:329] mlp: final activation = None
I0123 13:26:38.203835 139683861139456 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.203934 139683861139456 nn_components.py:261] mlp: residual
I0123 13:26:38.204232 139683861139456 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.204313 139683861139456 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 13:26:38.204421 139683861139456 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:26:38.204460 139683861139456 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:26:38.204491 139683861139456 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:26:38.206326 139683861139456 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.208774 139683861139456 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:26:38.214341 139683861139456 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.214601 139683861139456 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:26:38.217207 139683861139456 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:26:38.221013 139683861139456 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:26:38.221066 139683861139456 attention.py:418] Single window, no scan.
I0123 13:26:38.221101 139683861139456 transformer_layer.py:389] tlayer: self-attention.
I0123 13:26:38.221130 139683861139456 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.221190 139683861139456 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.221767 139683861139456 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.221842 139683861139456 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.222207 139683861139456 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.222986 139683861139456 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.225431 139683861139456 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.226063 139683861139456 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.226141 139683861139456 transformer_layer.py:468] tlayer: End windows.
I0123 13:26:38.226175 139683861139456 transformer_layer.py:472] tlayer: final FFN.
I0123 13:26:38.226233 139683861139456 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.226360 139683861139456 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:26:38.226679 139683861139456 nn_components.py:325] mlp: activation = None
I0123 13:26:38.226722 139683861139456 nn_components.py:329] mlp: final activation = None
I0123 13:26:38.228637 139683861139456 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.228730 139683861139456 nn_components.py:261] mlp: residual
I0123 13:26:38.231202 139683861139456 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.231284 139683861139456 transformer_base.py:443] tbase: final FFN
I0123 13:26:38.231707 139683861139456 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:26:38.233998 139683861139456 nn_components.py:329] mlp: final activation = None
I0123 13:26:38.235869 139683861139456 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.235961 139683861139456 nn_components.py:261] mlp: residual
I0123 13:26:38.236249 139683861139456 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.236328 139683861139456 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 13:26:38.236436 139683861139456 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:26:38.236474 139683861139456 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:26:38.236505 139683861139456 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:26:38.238390 139683861139456 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.240732 139683861139456 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:26:38.246571 139683861139456 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.246831 139683861139456 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:26:38.249450 139683861139456 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:26:38.253279 139683861139456 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:26:38.253334 139683861139456 attention.py:418] Single window, no scan.
I0123 13:26:38.253369 139683861139456 transformer_layer.py:389] tlayer: self-attention.
I0123 13:26:38.253399 139683861139456 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.253460 139683861139456 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.254031 139683861139456 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.254107 139683861139456 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.254462 139683861139456 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.255223 139683861139456 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.257709 139683861139456 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.258385 139683861139456 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.258464 139683861139456 transformer_layer.py:468] tlayer: End windows.
I0123 13:26:38.258498 139683861139456 transformer_layer.py:472] tlayer: final FFN.
I0123 13:26:38.258556 139683861139456 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.258682 139683861139456 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:26:38.259001 139683861139456 nn_components.py:325] mlp: activation = None
I0123 13:26:38.259044 139683861139456 nn_components.py:329] mlp: final activation = None
I0123 13:26:38.260955 139683861139456 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.261046 139683861139456 nn_components.py:261] mlp: residual
I0123 13:26:38.263546 139683861139456 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.263630 139683861139456 transformer_base.py:443] tbase: final FFN
I0123 13:26:38.264112 139683861139456 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:26:38.266364 139683861139456 nn_components.py:329] mlp: final activation = None
I0123 13:26:38.268238 139683861139456 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.268331 139683861139456 nn_components.py:261] mlp: residual
I0123 13:26:38.268619 139683861139456 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.268698 139683861139456 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 13:26:38.268806 139683861139456 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:26:38.268844 139683861139456 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:26:38.268874 139683861139456 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:26:38.270753 139683861139456 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.273121 139683861139456 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:26:38.278677 139683861139456 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.278931 139683861139456 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:26:38.281515 139683861139456 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:26:38.285526 139683861139456 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:26:38.285581 139683861139456 attention.py:418] Single window, no scan.
I0123 13:26:38.285616 139683861139456 transformer_layer.py:389] tlayer: self-attention.
I0123 13:26:38.285653 139683861139456 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.285717 139683861139456 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.286284 139683861139456 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.286360 139683861139456 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.286709 139683861139456 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.287466 139683861139456 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.290000 139683861139456 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.290619 139683861139456 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.290698 139683861139456 transformer_layer.py:468] tlayer: End windows.
I0123 13:26:38.290733 139683861139456 transformer_layer.py:472] tlayer: final FFN.
I0123 13:26:38.290792 139683861139456 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.290923 139683861139456 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:26:38.291243 139683861139456 nn_components.py:325] mlp: activation = None
I0123 13:26:38.291285 139683861139456 nn_components.py:329] mlp: final activation = None
I0123 13:26:38.293339 139683861139456 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.293430 139683861139456 nn_components.py:261] mlp: residual
I0123 13:26:38.295970 139683861139456 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.296055 139683861139456 transformer_base.py:443] tbase: final FFN
I0123 13:26:38.296486 139683861139456 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:26:38.298724 139683861139456 nn_components.py:329] mlp: final activation = None
I0123 13:26:38.300596 139683861139456 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.300690 139683861139456 nn_components.py:261] mlp: residual
I0123 13:26:38.300978 139683861139456 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.301058 139683861139456 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 13:26:38.301164 139683861139456 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:26:38.301203 139683861139456 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:26:38.301234 139683861139456 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:26:38.303109 139683861139456 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.305456 139683861139456 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:26:38.310965 139683861139456 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.311217 139683861139456 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:26:38.313857 139683861139456 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:26:38.317565 139683861139456 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:26:38.317619 139683861139456 attention.py:418] Single window, no scan.
I0123 13:26:38.317661 139683861139456 transformer_layer.py:389] tlayer: self-attention.
I0123 13:26:38.317693 139683861139456 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.317759 139683861139456 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.318326 139683861139456 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.318402 139683861139456 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.318752 139683861139456 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.319513 139683861139456 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.322328 139683861139456 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.322941 139683861139456 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.323021 139683861139456 transformer_layer.py:468] tlayer: End windows.
I0123 13:26:38.323056 139683861139456 transformer_layer.py:472] tlayer: final FFN.
I0123 13:26:38.323115 139683861139456 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.323249 139683861139456 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:26:38.323570 139683861139456 nn_components.py:325] mlp: activation = None
I0123 13:26:38.323613 139683861139456 nn_components.py:329] mlp: final activation = None
I0123 13:26:38.325476 139683861139456 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.325568 139683861139456 nn_components.py:261] mlp: residual
I0123 13:26:38.328077 139683861139456 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.328156 139683861139456 transformer_base.py:443] tbase: final FFN
I0123 13:26:38.328584 139683861139456 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:26:38.330820 139683861139456 nn_components.py:329] mlp: final activation = None
I0123 13:26:38.332732 139683861139456 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.332825 139683861139456 nn_components.py:261] mlp: residual
I0123 13:26:38.333113 139683861139456 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.333193 139683861139456 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 13:26:38.333302 139683861139456 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:26:38.333341 139683861139456 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:26:38.333371 139683861139456 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:26:38.335879 139683861139456 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.338361 139683861139456 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:26:38.343964 139683861139456 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.344229 139683861139456 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:26:38.346907 139683861139456 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:26:38.350684 139683861139456 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:26:38.350745 139683861139456 attention.py:418] Single window, no scan.
I0123 13:26:38.350781 139683861139456 transformer_layer.py:389] tlayer: self-attention.
I0123 13:26:38.350812 139683861139456 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.350874 139683861139456 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.351488 139683861139456 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.351567 139683861139456 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.351927 139683861139456 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.352707 139683861139456 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.355184 139683861139456 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.355793 139683861139456 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.355870 139683861139456 transformer_layer.py:468] tlayer: End windows.
I0123 13:26:38.355905 139683861139456 transformer_layer.py:472] tlayer: final FFN.
I0123 13:26:38.355964 139683861139456 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.356096 139683861139456 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:26:38.356416 139683861139456 nn_components.py:325] mlp: activation = None
I0123 13:26:38.356459 139683861139456 nn_components.py:329] mlp: final activation = None
I0123 13:26:38.358352 139683861139456 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.358450 139683861139456 nn_components.py:261] mlp: residual
I0123 13:26:38.360987 139683861139456 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.361066 139683861139456 transformer_base.py:443] tbase: final FFN
I0123 13:26:38.361503 139683861139456 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:26:38.363785 139683861139456 nn_components.py:329] mlp: final activation = None
I0123 13:26:38.365690 139683861139456 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.365790 139683861139456 nn_components.py:261] mlp: residual
I0123 13:26:38.366079 139683861139456 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.366160 139683861139456 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 13:26:38.366270 139683861139456 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:26:38.366309 139683861139456 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:26:38.366340 139683861139456 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:26:38.368184 139683861139456 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.370620 139683861139456 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:26:38.376176 139683861139456 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.376435 139683861139456 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:26:38.379061 139683861139456 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:26:38.382843 139683861139456 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:26:38.382897 139683861139456 attention.py:418] Single window, no scan.
I0123 13:26:38.382931 139683861139456 transformer_layer.py:389] tlayer: self-attention.
I0123 13:26:38.382962 139683861139456 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.383024 139683861139456 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.383588 139683861139456 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.383664 139683861139456 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.384022 139683861139456 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.384793 139683861139456 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.387262 139683861139456 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.387901 139683861139456 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.387979 139683861139456 transformer_layer.py:468] tlayer: End windows.
I0123 13:26:38.388014 139683861139456 transformer_layer.py:472] tlayer: final FFN.
I0123 13:26:38.388072 139683861139456 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.388205 139683861139456 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:26:38.388528 139683861139456 nn_components.py:325] mlp: activation = None
I0123 13:26:38.388572 139683861139456 nn_components.py:329] mlp: final activation = None
I0123 13:26:38.390520 139683861139456 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.390617 139683861139456 nn_components.py:261] mlp: residual
I0123 13:26:38.393140 139683861139456 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.393218 139683861139456 transformer_base.py:443] tbase: final FFN
I0123 13:26:38.393656 139683861139456 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:26:38.396274 139683861139456 nn_components.py:329] mlp: final activation = None
I0123 13:26:38.398200 139683861139456 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.398303 139683861139456 nn_components.py:261] mlp: residual
I0123 13:26:38.398601 139683861139456 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.398681 139683861139456 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 13:26:38.398791 139683861139456 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:26:38.398830 139683861139456 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:26:38.398860 139683861139456 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:26:38.542584 139683861139456 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.545782 139683861139456 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:26:38.551835 139683861139456 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.552141 139683861139456 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:26:38.554840 139683861139456 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:26:38.558809 139683861139456 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:26:38.558866 139683861139456 attention.py:418] Single window, no scan.
I0123 13:26:38.558903 139683861139456 transformer_layer.py:389] tlayer: self-attention.
I0123 13:26:38.558934 139683861139456 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.559002 139683861139456 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.559622 139683861139456 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.559699 139683861139456 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.560067 139683861139456 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.560844 139683861139456 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.563433 139683861139456 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.564082 139683861139456 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.564161 139683861139456 transformer_layer.py:468] tlayer: End windows.
I0123 13:26:38.564196 139683861139456 transformer_layer.py:472] tlayer: final FFN.
I0123 13:26:38.564258 139683861139456 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.564386 139683861139456 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:26:38.564732 139683861139456 nn_components.py:325] mlp: activation = None
I0123 13:26:38.564777 139683861139456 nn_components.py:329] mlp: final activation = None
I0123 13:26:38.566705 139683861139456 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.566799 139683861139456 nn_components.py:261] mlp: residual
I0123 13:26:38.569388 139683861139456 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.569467 139683861139456 transformer_base.py:443] tbase: final FFN
I0123 13:26:38.569920 139683861139456 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:26:38.572241 139683861139456 nn_components.py:329] mlp: final activation = None
I0123 13:26:38.574146 139683861139456 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.574254 139683861139456 nn_components.py:261] mlp: residual
I0123 13:26:38.574554 139683861139456 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.574639 139683861139456 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 13:26:38.574753 139683861139456 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:26:38.574792 139683861139456 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:26:38.574824 139683861139456 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:26:38.576753 139683861139456 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.579137 139683861139456 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:26:38.584748 139683861139456 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.585006 139683861139456 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:26:38.587685 139683861139456 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:26:38.591539 139683861139456 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:26:38.591594 139683861139456 attention.py:418] Single window, no scan.
I0123 13:26:38.591629 139683861139456 transformer_layer.py:389] tlayer: self-attention.
I0123 13:26:38.591660 139683861139456 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.591726 139683861139456 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.592295 139683861139456 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.592372 139683861139456 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.592731 139683861139456 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.593493 139683861139456 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.596046 139683861139456 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.596663 139683861139456 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.596741 139683861139456 transformer_layer.py:468] tlayer: End windows.
I0123 13:26:38.596776 139683861139456 transformer_layer.py:472] tlayer: final FFN.
I0123 13:26:38.596834 139683861139456 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.596961 139683861139456 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:26:38.597282 139683861139456 nn_components.py:325] mlp: activation = None
I0123 13:26:38.597326 139683861139456 nn_components.py:329] mlp: final activation = None
I0123 13:26:38.599234 139683861139456 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.599329 139683861139456 nn_components.py:261] mlp: residual
I0123 13:26:38.601887 139683861139456 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.601968 139683861139456 transformer_base.py:443] tbase: final FFN
I0123 13:26:38.602400 139683861139456 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:26:38.604657 139683861139456 nn_components.py:329] mlp: final activation = None
I0123 13:26:38.606623 139683861139456 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.606722 139683861139456 nn_components.py:261] mlp: residual
I0123 13:26:38.607009 139683861139456 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.607096 139683861139456 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 13:26:38.607208 139683861139456 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:26:38.607248 139683861139456 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:26:38.607279 139683861139456 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:26:38.609124 139683861139456 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.611619 139683861139456 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:26:38.617119 139683861139456 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.617379 139683861139456 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:26:38.620401 139683861139456 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:26:38.624192 139683861139456 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:26:38.624247 139683861139456 attention.py:418] Single window, no scan.
I0123 13:26:38.624282 139683861139456 transformer_layer.py:389] tlayer: self-attention.
I0123 13:26:38.624312 139683861139456 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.624378 139683861139456 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.624987 139683861139456 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.625062 139683861139456 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.625416 139683861139456 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.626191 139683861139456 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.628652 139683861139456 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.629283 139683861139456 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.629361 139683861139456 transformer_layer.py:468] tlayer: End windows.
I0123 13:26:38.629395 139683861139456 transformer_layer.py:472] tlayer: final FFN.
I0123 13:26:38.629454 139683861139456 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.629584 139683861139456 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:26:38.629921 139683861139456 nn_components.py:325] mlp: activation = None
I0123 13:26:38.629967 139683861139456 nn_components.py:329] mlp: final activation = None
I0123 13:26:38.631863 139683861139456 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.631957 139683861139456 nn_components.py:261] mlp: residual
I0123 13:26:38.634663 139683861139456 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.634742 139683861139456 transformer_base.py:443] tbase: final FFN
I0123 13:26:38.635178 139683861139456 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:26:38.637506 139683861139456 nn_components.py:329] mlp: final activation = None
I0123 13:26:38.639414 139683861139456 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.639509 139683861139456 nn_components.py:261] mlp: residual
I0123 13:26:38.639803 139683861139456 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.639889 139683861139456 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 13:26:38.639999 139683861139456 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:26:38.640037 139683861139456 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:26:38.640067 139683861139456 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:26:38.642091 139683861139456 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.644544 139683861139456 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:26:38.650075 139683861139456 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.650336 139683861139456 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:26:38.652949 139683861139456 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:26:38.656747 139683861139456 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:26:38.656802 139683861139456 attention.py:418] Single window, no scan.
I0123 13:26:38.656837 139683861139456 transformer_layer.py:389] tlayer: self-attention.
I0123 13:26:38.656867 139683861139456 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.656929 139683861139456 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.657503 139683861139456 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.657579 139683861139456 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.657936 139683861139456 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.658699 139683861139456 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.661160 139683861139456 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.661787 139683861139456 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.661865 139683861139456 transformer_layer.py:468] tlayer: End windows.
I0123 13:26:38.661899 139683861139456 transformer_layer.py:472] tlayer: final FFN.
I0123 13:26:38.661958 139683861139456 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.662083 139683861139456 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:26:38.662405 139683861139456 nn_components.py:325] mlp: activation = None
I0123 13:26:38.662447 139683861139456 nn_components.py:329] mlp: final activation = None
I0123 13:26:38.664389 139683861139456 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.664481 139683861139456 nn_components.py:261] mlp: residual
I0123 13:26:38.667201 139683861139456 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.667280 139683861139456 transformer_base.py:443] tbase: final FFN
I0123 13:26:38.667707 139683861139456 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:26:38.670003 139683861139456 nn_components.py:329] mlp: final activation = None
I0123 13:26:38.671876 139683861139456 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.671970 139683861139456 nn_components.py:261] mlp: residual
I0123 13:26:38.672262 139683861139456 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.672342 139683861139456 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 13:26:38.672456 139683861139456 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:26:38.672495 139683861139456 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:26:38.672525 139683861139456 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:26:38.674422 139683861139456 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.676759 139683861139456 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:26:38.682280 139683861139456 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.682536 139683861139456 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:26:38.685138 139683861139456 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:26:38.688955 139683861139456 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:26:38.689009 139683861139456 attention.py:418] Single window, no scan.
I0123 13:26:38.689044 139683861139456 transformer_layer.py:389] tlayer: self-attention.
I0123 13:26:38.689074 139683861139456 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.689136 139683861139456 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.689711 139683861139456 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.689788 139683861139456 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.690147 139683861139456 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.690929 139683861139456 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.693366 139683861139456 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.694356 139683861139456 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.694435 139683861139456 transformer_layer.py:468] tlayer: End windows.
I0123 13:26:38.694470 139683861139456 transformer_layer.py:472] tlayer: final FFN.
I0123 13:26:38.694530 139683861139456 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.694659 139683861139456 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:26:38.694982 139683861139456 nn_components.py:325] mlp: activation = None
I0123 13:26:38.695025 139683861139456 nn_components.py:329] mlp: final activation = None
I0123 13:26:38.696907 139683861139456 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.696999 139683861139456 nn_components.py:261] mlp: residual
I0123 13:26:38.699489 139683861139456 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.699574 139683861139456 transformer_base.py:443] tbase: final FFN
I0123 13:26:38.700051 139683861139456 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:26:38.702320 139683861139456 nn_components.py:329] mlp: final activation = None
I0123 13:26:38.704195 139683861139456 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.704289 139683861139456 nn_components.py:261] mlp: residual
I0123 13:26:38.704579 139683861139456 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.704866 139683861139456 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 13:26:38.704936 139683861139456 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 13:26:38.705000 139683861139456 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 13:26:38.705055 139683861139456 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 13:26:38.705109 139683861139456 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 13:26:38.705162 139683861139456 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 13:26:38.705215 139683861139456 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 13:26:38.705268 139683861139456 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 13:26:38.705319 139683861139456 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 13:26:38.705371 139683861139456 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 13:26:38.705423 139683861139456 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 13:26:38.705474 139683861139456 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 13:26:38.705511 139683861139456 decoder_stack.py:344] dstack: Final layernorm.
I0123 13:26:38.708992 139683861139456 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:38.757076 139683861139456 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.757162 139683861139456 decoder_stack.py:333] dstack: autoregressive generator.
I0123 13:26:38.757215 139683861139456 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 13:26:38.757319 139683861139456 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:26:38.757357 139683861139456 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:26:38.757386 139683861139456 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:26:38.757449 139683861139456 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.759878 139683861139456 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:26:38.765375 139683861139456 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.765634 139683861139456 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:26:38.768263 139683861139456 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:26:38.784902 139683861139456 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:26:38.784957 139683861139456 attention.py:418] Single window, no scan.
I0123 13:26:38.784992 139683861139456 transformer_layer.py:389] tlayer: self-attention.
I0123 13:26:38.785023 139683861139456 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.785085 139683861139456 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.786214 139683861139456 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.786293 139683861139456 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.786989 139683861139456 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.788962 139683861139456 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.793611 139683861139456 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.794919 139683861139456 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.795004 139683861139456 transformer_layer.py:468] tlayer: End windows.
I0123 13:26:38.795039 139683861139456 transformer_layer.py:472] tlayer: final FFN.
I0123 13:26:38.795099 139683861139456 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.795232 139683861139456 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:26:38.795340 139683861139456 nn_components.py:325] mlp: activation = None
I0123 13:26:38.795378 139683861139456 nn_components.py:329] mlp: final activation = None
I0123 13:26:38.797267 139683861139456 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.797360 139683861139456 nn_components.py:261] mlp: residual
I0123 13:26:38.799772 139683861139456 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.799852 139683861139456 transformer_base.py:443] tbase: final FFN
I0123 13:26:38.799958 139683861139456 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:26:38.802159 139683861139456 nn_components.py:329] mlp: final activation = None
I0123 13:26:38.804107 139683861139456 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.804203 139683861139456 nn_components.py:261] mlp: residual
I0123 13:26:38.804492 139683861139456 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.804575 139683861139456 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 13:26:38.804683 139683861139456 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:26:38.804722 139683861139456 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:26:38.804752 139683861139456 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:26:38.804816 139683861139456 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.807063 139683861139456 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:26:38.812625 139683861139456 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.812881 139683861139456 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:26:38.815591 139683861139456 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:26:38.828809 139683861139456 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:26:38.828866 139683861139456 attention.py:418] Single window, no scan.
I0123 13:26:38.828901 139683861139456 transformer_layer.py:389] tlayer: self-attention.
I0123 13:26:38.828931 139683861139456 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.828993 139683861139456 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.829556 139683861139456 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.829632 139683861139456 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.829994 139683861139456 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.830680 139683861139456 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.833151 139683861139456 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.833773 139683861139456 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.833852 139683861139456 transformer_layer.py:468] tlayer: End windows.
I0123 13:26:38.833892 139683861139456 transformer_layer.py:472] tlayer: final FFN.
I0123 13:26:38.833953 139683861139456 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.834081 139683861139456 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:26:38.834189 139683861139456 nn_components.py:325] mlp: activation = None
I0123 13:26:38.834232 139683861139456 nn_components.py:329] mlp: final activation = None
I0123 13:26:38.836148 139683861139456 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.836241 139683861139456 nn_components.py:261] mlp: residual
I0123 13:26:38.838629 139683861139456 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.838708 139683861139456 transformer_base.py:443] tbase: final FFN
I0123 13:26:38.838816 139683861139456 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:26:38.841039 139683861139456 nn_components.py:329] mlp: final activation = None
I0123 13:26:38.842948 139683861139456 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.843043 139683861139456 nn_components.py:261] mlp: residual
I0123 13:26:38.843327 139683861139456 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.843408 139683861139456 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 13:26:38.843517 139683861139456 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:26:38.843556 139683861139456 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:26:38.843587 139683861139456 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:26:38.843650 139683861139456 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.845896 139683861139456 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:26:38.851313 139683861139456 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.851569 139683861139456 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:26:38.854232 139683861139456 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:26:38.867069 139683861139456 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:26:38.867124 139683861139456 attention.py:418] Single window, no scan.
I0123 13:26:38.867159 139683861139456 transformer_layer.py:389] tlayer: self-attention.
I0123 13:26:38.867189 139683861139456 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.867250 139683861139456 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.867807 139683861139456 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.867884 139683861139456 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.868245 139683861139456 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.868923 139683861139456 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.871396 139683861139456 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.872026 139683861139456 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.872102 139683861139456 transformer_layer.py:468] tlayer: End windows.
I0123 13:26:38.872136 139683861139456 transformer_layer.py:472] tlayer: final FFN.
I0123 13:26:38.872198 139683861139456 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.872325 139683861139456 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:26:38.872433 139683861139456 nn_components.py:325] mlp: activation = None
I0123 13:26:38.872471 139683861139456 nn_components.py:329] mlp: final activation = None
I0123 13:26:38.874420 139683861139456 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.874513 139683861139456 nn_components.py:261] mlp: residual
I0123 13:26:38.876916 139683861139456 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.876994 139683861139456 transformer_base.py:443] tbase: final FFN
I0123 13:26:38.877099 139683861139456 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:26:38.879333 139683861139456 nn_components.py:329] mlp: final activation = None
I0123 13:26:38.881264 139683861139456 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.881359 139683861139456 nn_components.py:261] mlp: residual
I0123 13:26:38.881653 139683861139456 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.881736 139683861139456 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 13:26:38.881844 139683861139456 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:26:38.881883 139683861139456 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:26:38.881913 139683861139456 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:26:38.881976 139683861139456 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.884216 139683861139456 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:26:38.889695 139683861139456 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.889950 139683861139456 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:26:38.892630 139683861139456 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:26:38.905482 139683861139456 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:26:38.905538 139683861139456 attention.py:418] Single window, no scan.
I0123 13:26:38.905573 139683861139456 transformer_layer.py:389] tlayer: self-attention.
I0123 13:26:38.905603 139683861139456 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.905673 139683861139456 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.906236 139683861139456 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.906311 139683861139456 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.906658 139683861139456 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.907340 139683861139456 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.909796 139683861139456 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.910421 139683861139456 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.910498 139683861139456 transformer_layer.py:468] tlayer: End windows.
I0123 13:26:38.910531 139683861139456 transformer_layer.py:472] tlayer: final FFN.
I0123 13:26:38.910590 139683861139456 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.910722 139683861139456 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:26:38.910832 139683861139456 nn_components.py:325] mlp: activation = None
I0123 13:26:38.910871 139683861139456 nn_components.py:329] mlp: final activation = None
I0123 13:26:38.912808 139683861139456 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.912902 139683861139456 nn_components.py:261] mlp: residual
I0123 13:26:38.915374 139683861139456 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.915453 139683861139456 transformer_base.py:443] tbase: final FFN
I0123 13:26:38.915559 139683861139456 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:26:38.917765 139683861139456 nn_components.py:329] mlp: final activation = None
I0123 13:26:38.919616 139683861139456 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.919710 139683861139456 nn_components.py:261] mlp: residual
I0123 13:26:38.919991 139683861139456 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.920070 139683861139456 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 13:26:38.920178 139683861139456 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:26:38.920216 139683861139456 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:26:38.920247 139683861139456 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:26:38.920309 139683861139456 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.922892 139683861139456 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:26:38.928341 139683861139456 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.928602 139683861139456 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:26:38.931215 139683861139456 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:26:38.944050 139683861139456 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:26:38.944108 139683861139456 attention.py:418] Single window, no scan.
I0123 13:26:38.944144 139683861139456 transformer_layer.py:389] tlayer: self-attention.
I0123 13:26:38.944174 139683861139456 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.944236 139683861139456 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.944799 139683861139456 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.944879 139683861139456 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.945233 139683861139456 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.945927 139683861139456 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.948433 139683861139456 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.949051 139683861139456 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.949127 139683861139456 transformer_layer.py:468] tlayer: End windows.
I0123 13:26:38.949162 139683861139456 transformer_layer.py:472] tlayer: final FFN.
I0123 13:26:38.949220 139683861139456 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.949357 139683861139456 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:26:38.949467 139683861139456 nn_components.py:325] mlp: activation = None
I0123 13:26:38.949509 139683861139456 nn_components.py:329] mlp: final activation = None
I0123 13:26:38.951384 139683861139456 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.951477 139683861139456 nn_components.py:261] mlp: residual
I0123 13:26:38.953843 139683861139456 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.953922 139683861139456 transformer_base.py:443] tbase: final FFN
I0123 13:26:38.954028 139683861139456 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:26:38.956306 139683861139456 nn_components.py:329] mlp: final activation = None
I0123 13:26:38.958171 139683861139456 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.958266 139683861139456 nn_components.py:261] mlp: residual
I0123 13:26:38.958549 139683861139456 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.958630 139683861139456 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 13:26:38.958736 139683861139456 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:26:38.958775 139683861139456 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:26:38.958805 139683861139456 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:26:38.958867 139683861139456 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.961093 139683861139456 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:26:38.966533 139683861139456 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.966789 139683861139456 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:26:38.969435 139683861139456 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:26:38.982348 139683861139456 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:26:38.982403 139683861139456 attention.py:418] Single window, no scan.
I0123 13:26:38.982438 139683861139456 transformer_layer.py:389] tlayer: self-attention.
I0123 13:26:38.982468 139683861139456 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.982533 139683861139456 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.983094 139683861139456 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.983169 139683861139456 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.983524 139683861139456 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.984212 139683861139456 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.986675 139683861139456 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.987295 139683861139456 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.987372 139683861139456 transformer_layer.py:468] tlayer: End windows.
I0123 13:26:38.987406 139683861139456 transformer_layer.py:472] tlayer: final FFN.
I0123 13:26:38.987464 139683861139456 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.987593 139683861139456 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:26:38.987709 139683861139456 nn_components.py:325] mlp: activation = None
I0123 13:26:38.987747 139683861139456 nn_components.py:329] mlp: final activation = None
I0123 13:26:38.989704 139683861139456 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.989796 139683861139456 nn_components.py:261] mlp: residual
I0123 13:26:38.992176 139683861139456 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:38.992254 139683861139456 transformer_base.py:443] tbase: final FFN
I0123 13:26:38.992361 139683861139456 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:26:39.000232 139683861139456 nn_components.py:329] mlp: final activation = None
I0123 13:26:39.002200 139683861139456 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.002307 139683861139456 nn_components.py:261] mlp: residual
I0123 13:26:39.002602 139683861139456 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.002687 139683861139456 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 13:26:39.002801 139683861139456 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:26:39.002841 139683861139456 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:26:39.002871 139683861139456 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:26:39.002939 139683861139456 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.005244 139683861139456 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:26:39.010823 139683861139456 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.011084 139683861139456 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:26:39.013703 139683861139456 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:26:39.026702 139683861139456 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:26:39.026759 139683861139456 attention.py:418] Single window, no scan.
I0123 13:26:39.026797 139683861139456 transformer_layer.py:389] tlayer: self-attention.
I0123 13:26:39.026828 139683861139456 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.026890 139683861139456 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.027485 139683861139456 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.027561 139683861139456 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.027915 139683861139456 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.028603 139683861139456 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.031130 139683861139456 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.032130 139683861139456 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.032208 139683861139456 transformer_layer.py:468] tlayer: End windows.
I0123 13:26:39.032242 139683861139456 transformer_layer.py:472] tlayer: final FFN.
I0123 13:26:39.032300 139683861139456 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.032432 139683861139456 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:26:39.032540 139683861139456 nn_components.py:325] mlp: activation = None
I0123 13:26:39.032583 139683861139456 nn_components.py:329] mlp: final activation = None
I0123 13:26:39.034492 139683861139456 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.034586 139683861139456 nn_components.py:261] mlp: residual
I0123 13:26:39.036980 139683861139456 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.037062 139683861139456 transformer_base.py:443] tbase: final FFN
I0123 13:26:39.037172 139683861139456 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:26:39.039398 139683861139456 nn_components.py:329] mlp: final activation = None
I0123 13:26:39.041338 139683861139456 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.041434 139683861139456 nn_components.py:261] mlp: residual
I0123 13:26:39.041723 139683861139456 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.041805 139683861139456 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 13:26:39.041914 139683861139456 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:26:39.041953 139683861139456 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:26:39.041983 139683861139456 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:26:39.042044 139683861139456 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.044271 139683861139456 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:26:39.049736 139683861139456 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.050003 139683861139456 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:26:39.052659 139683861139456 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:26:39.065526 139683861139456 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:26:39.065581 139683861139456 attention.py:418] Single window, no scan.
I0123 13:26:39.065615 139683861139456 transformer_layer.py:389] tlayer: self-attention.
I0123 13:26:39.065651 139683861139456 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.065718 139683861139456 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.066329 139683861139456 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.066406 139683861139456 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.066756 139683861139456 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.067436 139683861139456 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.069908 139683861139456 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.070533 139683861139456 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.070610 139683861139456 transformer_layer.py:468] tlayer: End windows.
I0123 13:26:39.070643 139683861139456 transformer_layer.py:472] tlayer: final FFN.
I0123 13:26:39.070701 139683861139456 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.070831 139683861139456 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:26:39.070941 139683861139456 nn_components.py:325] mlp: activation = None
I0123 13:26:39.070985 139683861139456 nn_components.py:329] mlp: final activation = None
I0123 13:26:39.072870 139683861139456 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.072963 139683861139456 nn_components.py:261] mlp: residual
I0123 13:26:39.075405 139683861139456 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.075485 139683861139456 transformer_base.py:443] tbase: final FFN
I0123 13:26:39.075592 139683861139456 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:26:39.077826 139683861139456 nn_components.py:329] mlp: final activation = None
I0123 13:26:39.079686 139683861139456 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.079783 139683861139456 nn_components.py:261] mlp: residual
I0123 13:26:39.080065 139683861139456 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.080145 139683861139456 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 13:26:39.080253 139683861139456 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:26:39.080291 139683861139456 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:26:39.080322 139683861139456 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:26:39.080383 139683861139456 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.082626 139683861139456 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:26:39.088154 139683861139456 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.088414 139683861139456 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:26:39.091036 139683861139456 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:26:39.103900 139683861139456 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:26:39.103956 139683861139456 attention.py:418] Single window, no scan.
I0123 13:26:39.103995 139683861139456 transformer_layer.py:389] tlayer: self-attention.
I0123 13:26:39.104026 139683861139456 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.104087 139683861139456 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.104646 139683861139456 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.104723 139683861139456 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.105077 139683861139456 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.105766 139683861139456 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.108235 139683861139456 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.108906 139683861139456 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.108983 139683861139456 transformer_layer.py:468] tlayer: End windows.
I0123 13:26:39.109019 139683861139456 transformer_layer.py:472] tlayer: final FFN.
I0123 13:26:39.109078 139683861139456 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.109210 139683861139456 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:26:39.109319 139683861139456 nn_components.py:325] mlp: activation = None
I0123 13:26:39.109357 139683861139456 nn_components.py:329] mlp: final activation = None
I0123 13:26:39.111254 139683861139456 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.111350 139683861139456 nn_components.py:261] mlp: residual
I0123 13:26:39.113736 139683861139456 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.113815 139683861139456 transformer_base.py:443] tbase: final FFN
I0123 13:26:39.113922 139683861139456 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:26:39.116155 139683861139456 nn_components.py:329] mlp: final activation = None
I0123 13:26:39.118105 139683861139456 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.118200 139683861139456 nn_components.py:261] mlp: residual
I0123 13:26:39.118487 139683861139456 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.118570 139683861139456 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 13:26:39.118677 139683861139456 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:26:39.118716 139683861139456 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:26:39.118746 139683861139456 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:26:39.118808 139683861139456 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.121045 139683861139456 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:26:39.126483 139683861139456 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.126744 139683861139456 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:26:39.129427 139683861139456 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:26:39.142543 139683861139456 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:26:39.142598 139683861139456 attention.py:418] Single window, no scan.
I0123 13:26:39.142632 139683861139456 transformer_layer.py:389] tlayer: self-attention.
I0123 13:26:39.142661 139683861139456 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.142723 139683861139456 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.143326 139683861139456 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.143402 139683861139456 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.143754 139683861139456 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.144438 139683861139456 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.146891 139683861139456 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.147520 139683861139456 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.147596 139683861139456 transformer_layer.py:468] tlayer: End windows.
I0123 13:26:39.147629 139683861139456 transformer_layer.py:472] tlayer: final FFN.
I0123 13:26:39.147686 139683861139456 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.147815 139683861139456 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:26:39.147923 139683861139456 nn_components.py:325] mlp: activation = None
I0123 13:26:39.147961 139683861139456 nn_components.py:329] mlp: final activation = None
I0123 13:26:39.149830 139683861139456 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.149929 139683861139456 nn_components.py:261] mlp: residual
I0123 13:26:39.152356 139683861139456 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.152440 139683861139456 transformer_base.py:443] tbase: final FFN
I0123 13:26:39.152547 139683861139456 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:26:39.154758 139683861139456 nn_components.py:329] mlp: final activation = None
I0123 13:26:39.156624 139683861139456 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.156717 139683861139456 nn_components.py:261] mlp: residual
I0123 13:26:39.156999 139683861139456 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.157080 139683861139456 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 13:26:39.157187 139683861139456 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:26:39.157225 139683861139456 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:26:39.157255 139683861139456 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:26:39.157316 139683861139456 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.159544 139683861139456 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:26:39.164997 139683861139456 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.165253 139683861139456 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:26:39.167873 139683861139456 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:26:39.180717 139683861139456 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:26:39.180772 139683861139456 attention.py:418] Single window, no scan.
I0123 13:26:39.180806 139683861139456 transformer_layer.py:389] tlayer: self-attention.
I0123 13:26:39.180835 139683861139456 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.180898 139683861139456 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.181456 139683861139456 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.181532 139683861139456 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.181891 139683861139456 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.182578 139683861139456 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.185041 139683861139456 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.185710 139683861139456 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.185790 139683861139456 transformer_layer.py:468] tlayer: End windows.
I0123 13:26:39.185824 139683861139456 transformer_layer.py:472] tlayer: final FFN.
I0123 13:26:39.185881 139683861139456 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.186012 139683861139456 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:26:39.186119 139683861139456 nn_components.py:325] mlp: activation = None
I0123 13:26:39.186157 139683861139456 nn_components.py:329] mlp: final activation = None
I0123 13:26:39.188036 139683861139456 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.188135 139683861139456 nn_components.py:261] mlp: residual
I0123 13:26:39.190551 139683861139456 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.190628 139683861139456 transformer_base.py:443] tbase: final FFN
I0123 13:26:39.190733 139683861139456 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:26:39.192949 139683861139456 nn_components.py:329] mlp: final activation = None
I0123 13:26:39.194886 139683861139456 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.194982 139683861139456 nn_components.py:261] mlp: residual
I0123 13:26:39.195265 139683861139456 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.195347 139683861139456 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 13:26:39.195455 139683861139456 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:26:39.195493 139683861139456 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:26:39.195522 139683861139456 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:26:39.195584 139683861139456 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.197836 139683861139456 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:26:39.203263 139683861139456 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.203518 139683861139456 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:26:39.206211 139683861139456 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:26:39.218956 139683861139456 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:26:39.219012 139683861139456 attention.py:418] Single window, no scan.
I0123 13:26:39.219048 139683861139456 transformer_layer.py:389] tlayer: self-attention.
I0123 13:26:39.219078 139683861139456 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.219139 139683861139456 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.219687 139683861139456 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.219762 139683861139456 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.220114 139683861139456 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.220847 139683861139456 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.223333 139683861139456 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.223957 139683861139456 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.224033 139683861139456 transformer_layer.py:468] tlayer: End windows.
I0123 13:26:39.224067 139683861139456 transformer_layer.py:472] tlayer: final FFN.
I0123 13:26:39.224124 139683861139456 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.224254 139683861139456 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:26:39.224361 139683861139456 nn_components.py:325] mlp: activation = None
I0123 13:26:39.224399 139683861139456 nn_components.py:329] mlp: final activation = None
I0123 13:26:39.226269 139683861139456 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.226362 139683861139456 nn_components.py:261] mlp: residual
I0123 13:26:39.228753 139683861139456 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.228831 139683861139456 transformer_base.py:443] tbase: final FFN
I0123 13:26:39.228942 139683861139456 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:26:39.231246 139683861139456 nn_components.py:329] mlp: final activation = None
I0123 13:26:39.233108 139683861139456 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.233201 139683861139456 nn_components.py:261] mlp: residual
I0123 13:26:39.233483 139683861139456 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.233571 139683861139456 decoder_stack.py:344] dstack: Final layernorm.
I0123 13:26:39.236423 139683861139456 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:39.291800 139683861139456 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.291885 139683861139456 decoder_stack.py:333] dstack: autoregressive generator.
I0123 13:26:39.291938 139683861139456 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 13:26:39.292041 139683861139456 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:26:39.292078 139683861139456 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:26:39.292108 139683861139456 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:26:39.292169 139683861139456 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.294802 139683861139456 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:26:39.300086 139683861139456 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.300343 139683861139456 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:26:39.302876 139683861139456 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:26:39.315234 139683861139456 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:26:39.315289 139683861139456 attention.py:418] Single window, no scan.
I0123 13:26:39.315324 139683861139456 transformer_layer.py:389] tlayer: self-attention.
I0123 13:26:39.315354 139683861139456 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.315416 139683861139456 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.315973 139683861139456 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.316048 139683861139456 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.316391 139683861139456 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.317051 139683861139456 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.319528 139683861139456 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.320137 139683861139456 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.320214 139683861139456 transformer_layer.py:468] tlayer: End windows.
I0123 13:26:39.320248 139683861139456 transformer_layer.py:472] tlayer: final FFN.
I0123 13:26:39.320306 139683861139456 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.320436 139683861139456 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:26:39.320552 139683861139456 nn_components.py:325] mlp: activation = None
I0123 13:26:39.320592 139683861139456 nn_components.py:329] mlp: final activation = None
I0123 13:26:39.322419 139683861139456 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.322513 139683861139456 nn_components.py:261] mlp: residual
I0123 13:26:39.324856 139683861139456 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.324935 139683861139456 transformer_base.py:443] tbase: final FFN
I0123 13:26:39.325041 139683861139456 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:26:39.327262 139683861139456 nn_components.py:329] mlp: final activation = None
I0123 13:26:39.329073 139683861139456 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.329169 139683861139456 nn_components.py:261] mlp: residual
I0123 13:26:39.329448 139683861139456 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.329528 139683861139456 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 13:26:39.329634 139683861139456 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:26:39.329680 139683861139456 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:26:39.329711 139683861139456 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:26:39.329773 139683861139456 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.331982 139683861139456 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:26:39.337286 139683861139456 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.337538 139683861139456 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:26:39.340148 139683861139456 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:26:39.352434 139683861139456 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:26:39.352489 139683861139456 attention.py:418] Single window, no scan.
I0123 13:26:39.352524 139683861139456 transformer_layer.py:389] tlayer: self-attention.
I0123 13:26:39.352554 139683861139456 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.352616 139683861139456 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.353165 139683861139456 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.353240 139683861139456 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.353584 139683861139456 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.354255 139683861139456 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.356710 139683861139456 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.357316 139683861139456 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.357393 139683861139456 transformer_layer.py:468] tlayer: End windows.
I0123 13:26:39.357428 139683861139456 transformer_layer.py:472] tlayer: final FFN.
I0123 13:26:39.357486 139683861139456 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.357611 139683861139456 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:26:39.357725 139683861139456 nn_components.py:325] mlp: activation = None
I0123 13:26:39.357769 139683861139456 nn_components.py:329] mlp: final activation = None
I0123 13:26:39.359571 139683861139456 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.359662 139683861139456 nn_components.py:261] mlp: residual
I0123 13:26:39.362002 139683861139456 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.362081 139683861139456 transformer_base.py:443] tbase: final FFN
I0123 13:26:39.362189 139683861139456 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:26:39.364400 139683861139456 nn_components.py:329] mlp: final activation = None
I0123 13:26:39.366214 139683861139456 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.366309 139683861139456 nn_components.py:261] mlp: residual
I0123 13:26:39.366593 139683861139456 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.366673 139683861139456 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 13:26:39.366780 139683861139456 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:26:39.366817 139683861139456 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:26:39.366847 139683861139456 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:26:39.366907 139683861139456 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.369091 139683861139456 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:26:39.374347 139683861139456 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.374599 139683861139456 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:26:39.377195 139683861139456 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:26:39.389433 139683861139456 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:26:39.389488 139683861139456 attention.py:418] Single window, no scan.
I0123 13:26:39.389523 139683861139456 transformer_layer.py:389] tlayer: self-attention.
I0123 13:26:39.389553 139683861139456 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.389622 139683861139456 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.390182 139683861139456 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.390257 139683861139456 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.390602 139683861139456 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.391263 139683861139456 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.393718 139683861139456 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.394324 139683861139456 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.394400 139683861139456 transformer_layer.py:468] tlayer: End windows.
I0123 13:26:39.394435 139683861139456 transformer_layer.py:472] tlayer: final FFN.
I0123 13:26:39.394493 139683861139456 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.394619 139683861139456 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:26:39.394724 139683861139456 nn_components.py:325] mlp: activation = None
I0123 13:26:39.394762 139683861139456 nn_components.py:329] mlp: final activation = None
I0123 13:26:39.396568 139683861139456 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.396659 139683861139456 nn_components.py:261] mlp: residual
I0123 13:26:39.398998 139683861139456 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.399077 139683861139456 transformer_base.py:443] tbase: final FFN
I0123 13:26:39.399184 139683861139456 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:26:39.401867 139683861139456 nn_components.py:329] mlp: final activation = None
I0123 13:26:39.403690 139683861139456 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.403785 139683861139456 nn_components.py:261] mlp: residual
I0123 13:26:39.404064 139683861139456 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.404146 139683861139456 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 13:26:39.404253 139683861139456 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:26:39.404291 139683861139456 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:26:39.404320 139683861139456 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:26:39.404380 139683861139456 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.406553 139683861139456 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:26:39.411824 139683861139456 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.412080 139683861139456 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:26:39.414692 139683861139456 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:26:39.427058 139683861139456 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:26:39.427112 139683861139456 attention.py:418] Single window, no scan.
I0123 13:26:39.427155 139683861139456 transformer_layer.py:389] tlayer: self-attention.
I0123 13:26:39.427194 139683861139456 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.427257 139683861139456 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.427823 139683861139456 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.427898 139683861139456 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.428243 139683861139456 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.428911 139683861139456 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.431390 139683861139456 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.432009 139683861139456 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.432083 139683861139456 transformer_layer.py:468] tlayer: End windows.
I0123 13:26:39.432115 139683861139456 transformer_layer.py:472] tlayer: final FFN.
I0123 13:26:39.432172 139683861139456 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.432296 139683861139456 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:26:39.432401 139683861139456 nn_components.py:325] mlp: activation = None
I0123 13:26:39.432440 139683861139456 nn_components.py:329] mlp: final activation = None
I0123 13:26:39.434280 139683861139456 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.434370 139683861139456 nn_components.py:261] mlp: residual
I0123 13:26:39.436711 139683861139456 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.436786 139683861139456 transformer_base.py:443] tbase: final FFN
I0123 13:26:39.436892 139683861139456 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:26:39.439149 139683861139456 nn_components.py:329] mlp: final activation = None
I0123 13:26:39.440985 139683861139456 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.441077 139683861139456 nn_components.py:261] mlp: residual
I0123 13:26:39.441351 139683861139456 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.441430 139683861139456 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 13:26:39.441533 139683861139456 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:26:39.441569 139683861139456 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:26:39.441597 139683861139456 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:26:39.441662 139683861139456 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.443847 139683861139456 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:26:39.449178 139683861139456 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.449430 139683861139456 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:26:39.452088 139683861139456 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:26:39.464658 139683861139456 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:26:39.464711 139683861139456 attention.py:418] Single window, no scan.
I0123 13:26:39.464745 139683861139456 transformer_layer.py:389] tlayer: self-attention.
I0123 13:26:39.464773 139683861139456 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.464832 139683861139456 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.465384 139683861139456 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.465457 139683861139456 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.465814 139683861139456 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.466483 139683861139456 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.468971 139683861139456 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.469580 139683861139456 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.469661 139683861139456 transformer_layer.py:468] tlayer: End windows.
I0123 13:26:39.469696 139683861139456 transformer_layer.py:472] tlayer: final FFN.
I0123 13:26:39.469753 139683861139456 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.469877 139683861139456 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:26:39.469982 139683861139456 nn_components.py:325] mlp: activation = None
I0123 13:26:39.470019 139683861139456 nn_components.py:329] mlp: final activation = None
I0123 13:26:39.471870 139683861139456 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.471966 139683861139456 nn_components.py:261] mlp: residual
I0123 13:26:39.474324 139683861139456 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.474402 139683861139456 transformer_base.py:443] tbase: final FFN
I0123 13:26:39.474509 139683861139456 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:26:39.476754 139683861139456 nn_components.py:329] mlp: final activation = None
I0123 13:26:39.478577 139683861139456 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.478670 139683861139456 nn_components.py:261] mlp: residual
I0123 13:26:39.478946 139683861139456 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.479024 139683861139456 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 13:26:39.479129 139683861139456 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:26:39.479165 139683861139456 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:26:39.479193 139683861139456 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:26:39.479252 139683861139456 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.481433 139683861139456 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:26:39.486735 139683861139456 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.486984 139683861139456 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:26:39.489591 139683861139456 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:26:39.502075 139683861139456 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:26:39.502127 139683861139456 attention.py:418] Single window, no scan.
I0123 13:26:39.502160 139683861139456 transformer_layer.py:389] tlayer: self-attention.
I0123 13:26:39.502189 139683861139456 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.502249 139683861139456 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.502795 139683861139456 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.502869 139683861139456 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.503216 139683861139456 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.503957 139683861139456 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.506461 139683861139456 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.507073 139683861139456 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.507147 139683861139456 transformer_layer.py:468] tlayer: End windows.
I0123 13:26:39.507179 139683861139456 transformer_layer.py:472] tlayer: final FFN.
I0123 13:26:39.507235 139683861139456 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.507358 139683861139456 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:26:39.507462 139683861139456 nn_components.py:325] mlp: activation = None
I0123 13:26:39.507499 139683861139456 nn_components.py:329] mlp: final activation = None
I0123 13:26:39.509329 139683861139456 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.509424 139683861139456 nn_components.py:261] mlp: residual
I0123 13:26:39.511754 139683861139456 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.511831 139683861139456 transformer_base.py:443] tbase: final FFN
I0123 13:26:39.511935 139683861139456 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:26:39.514610 139683861139456 nn_components.py:329] mlp: final activation = None
I0123 13:26:39.516458 139683861139456 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.516551 139683861139456 nn_components.py:261] mlp: residual
I0123 13:26:39.516830 139683861139456 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.516908 139683861139456 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 13:26:39.517011 139683861139456 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:26:39.517047 139683861139456 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:26:39.517075 139683861139456 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:26:39.517133 139683861139456 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.519319 139683861139456 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:26:39.524606 139683861139456 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.524858 139683861139456 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:26:39.527500 139683861139456 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:26:39.540027 139683861139456 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:26:39.540081 139683861139456 attention.py:418] Single window, no scan.
I0123 13:26:39.540114 139683861139456 transformer_layer.py:389] tlayer: self-attention.
I0123 13:26:39.540142 139683861139456 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.540202 139683861139456 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.540751 139683861139456 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.540824 139683861139456 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.541170 139683861139456 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.541853 139683861139456 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.544342 139683861139456 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.544968 139683861139456 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.545043 139683861139456 transformer_layer.py:468] tlayer: End windows.
I0123 13:26:39.545075 139683861139456 transformer_layer.py:472] tlayer: final FFN.
I0123 13:26:39.545132 139683861139456 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.545257 139683861139456 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:26:39.545362 139683861139456 nn_components.py:325] mlp: activation = None
I0123 13:26:39.545399 139683861139456 nn_components.py:329] mlp: final activation = None
I0123 13:26:39.547252 139683861139456 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.547343 139683861139456 nn_components.py:261] mlp: residual
I0123 13:26:39.549693 139683861139456 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.549771 139683861139456 transformer_base.py:443] tbase: final FFN
I0123 13:26:39.549876 139683861139456 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:26:39.552117 139683861139456 nn_components.py:329] mlp: final activation = None
I0123 13:26:39.553963 139683861139456 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.554056 139683861139456 nn_components.py:261] mlp: residual
I0123 13:26:39.554336 139683861139456 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.554415 139683861139456 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 13:26:39.554518 139683861139456 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:26:39.554555 139683861139456 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:26:39.554583 139683861139456 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:26:39.554642 139683861139456 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.556826 139683861139456 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:26:39.562144 139683861139456 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.562397 139683861139456 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:26:39.565032 139683861139456 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:26:39.577486 139683861139456 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:26:39.577539 139683861139456 attention.py:418] Single window, no scan.
I0123 13:26:39.577572 139683861139456 transformer_layer.py:389] tlayer: self-attention.
I0123 13:26:39.577601 139683861139456 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.577668 139683861139456 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.578219 139683861139456 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.578294 139683861139456 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.578646 139683861139456 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.579312 139683861139456 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.581809 139683861139456 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.582434 139683861139456 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.582509 139683861139456 transformer_layer.py:468] tlayer: End windows.
I0123 13:26:39.582542 139683861139456 transformer_layer.py:472] tlayer: final FFN.
I0123 13:26:39.582598 139683861139456 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.582722 139683861139456 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:26:39.582828 139683861139456 nn_components.py:325] mlp: activation = None
I0123 13:26:39.582866 139683861139456 nn_components.py:329] mlp: final activation = None
I0123 13:26:39.584701 139683861139456 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.584792 139683861139456 nn_components.py:261] mlp: residual
I0123 13:26:39.587146 139683861139456 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.587228 139683861139456 transformer_base.py:443] tbase: final FFN
I0123 13:26:39.587334 139683861139456 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:26:39.589589 139683861139456 nn_components.py:329] mlp: final activation = None
I0123 13:26:39.591438 139683861139456 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.591530 139683861139456 nn_components.py:261] mlp: residual
I0123 13:26:39.591808 139683861139456 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.591886 139683861139456 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 13:26:39.591991 139683861139456 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:26:39.592028 139683861139456 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:26:39.592056 139683861139456 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:26:39.592115 139683861139456 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.594325 139683861139456 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:26:39.599612 139683861139456 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.599861 139683861139456 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:26:39.602519 139683861139456 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:26:39.615133 139683861139456 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:26:39.615186 139683861139456 attention.py:418] Single window, no scan.
I0123 13:26:39.615220 139683861139456 transformer_layer.py:389] tlayer: self-attention.
I0123 13:26:39.615249 139683861139456 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.615311 139683861139456 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.615864 139683861139456 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.615937 139683861139456 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.616279 139683861139456 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.616954 139683861139456 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.619442 139683861139456 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.620057 139683861139456 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.620132 139683861139456 transformer_layer.py:468] tlayer: End windows.
I0123 13:26:39.620163 139683861139456 transformer_layer.py:472] tlayer: final FFN.
I0123 13:26:39.620219 139683861139456 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.620341 139683861139456 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:26:39.620445 139683861139456 nn_components.py:325] mlp: activation = None
I0123 13:26:39.620481 139683861139456 nn_components.py:329] mlp: final activation = None
I0123 13:26:39.622340 139683861139456 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.622431 139683861139456 nn_components.py:261] mlp: residual
I0123 13:26:39.624768 139683861139456 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.624849 139683861139456 transformer_base.py:443] tbase: final FFN
I0123 13:26:39.624955 139683861139456 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:26:39.627592 139683861139456 nn_components.py:329] mlp: final activation = None
I0123 13:26:39.629413 139683861139456 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.629506 139683861139456 nn_components.py:261] mlp: residual
I0123 13:26:39.629791 139683861139456 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.629871 139683861139456 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 13:26:39.629976 139683861139456 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:26:39.630012 139683861139456 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:26:39.630041 139683861139456 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:26:39.630100 139683861139456 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.632277 139683861139456 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:26:39.637595 139683861139456 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.637858 139683861139456 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:26:39.640490 139683861139456 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:26:39.653053 139683861139456 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:26:39.653106 139683861139456 attention.py:418] Single window, no scan.
I0123 13:26:39.653139 139683861139456 transformer_layer.py:389] tlayer: self-attention.
I0123 13:26:39.653168 139683861139456 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.653228 139683861139456 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.653793 139683861139456 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.653867 139683861139456 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.654221 139683861139456 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.654899 139683861139456 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.657383 139683861139456 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.658009 139683861139456 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.658088 139683861139456 transformer_layer.py:468] tlayer: End windows.
I0123 13:26:39.658120 139683861139456 transformer_layer.py:472] tlayer: final FFN.
I0123 13:26:39.658175 139683861139456 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.658300 139683861139456 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:26:39.658406 139683861139456 nn_components.py:325] mlp: activation = None
I0123 13:26:39.658443 139683861139456 nn_components.py:329] mlp: final activation = None
I0123 13:26:39.660769 139683861139456 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.660862 139683861139456 nn_components.py:261] mlp: residual
I0123 13:26:39.663193 139683861139456 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.663270 139683861139456 transformer_base.py:443] tbase: final FFN
I0123 13:26:39.663383 139683861139456 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:26:39.665582 139683861139456 nn_components.py:329] mlp: final activation = None
I0123 13:26:39.667391 139683861139456 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.667483 139683861139456 nn_components.py:261] mlp: residual
I0123 13:26:39.667759 139683861139456 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.667838 139683861139456 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 13:26:39.667940 139683861139456 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:26:39.667976 139683861139456 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:26:39.668004 139683861139456 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:26:39.668062 139683861139456 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.670240 139683861139456 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:26:39.675545 139683861139456 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.675799 139683861139456 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:26:39.678434 139683861139456 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:26:39.690895 139683861139456 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:26:39.690948 139683861139456 attention.py:418] Single window, no scan.
I0123 13:26:39.690981 139683861139456 transformer_layer.py:389] tlayer: self-attention.
I0123 13:26:39.691010 139683861139456 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.691071 139683861139456 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.691629 139683861139456 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.691702 139683861139456 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.692043 139683861139456 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.692709 139683861139456 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.695196 139683861139456 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.695821 139683861139456 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.695897 139683861139456 transformer_layer.py:468] tlayer: End windows.
I0123 13:26:39.695930 139683861139456 transformer_layer.py:472] tlayer: final FFN.
I0123 13:26:39.695986 139683861139456 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.696114 139683861139456 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:26:39.696222 139683861139456 nn_components.py:325] mlp: activation = None
I0123 13:26:39.696259 139683861139456 nn_components.py:329] mlp: final activation = None
I0123 13:26:39.698099 139683861139456 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.698189 139683861139456 nn_components.py:261] mlp: residual
I0123 13:26:39.700530 139683861139456 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.700607 139683861139456 transformer_base.py:443] tbase: final FFN
I0123 13:26:39.700711 139683861139456 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:26:39.702955 139683861139456 nn_components.py:329] mlp: final activation = None
I0123 13:26:39.704770 139683861139456 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.704861 139683861139456 nn_components.py:261] mlp: residual
I0123 13:26:39.705141 139683861139456 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.705220 139683861139456 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 13:26:39.705323 139683861139456 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:26:39.705360 139683861139456 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:26:39.705388 139683861139456 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:26:39.705446 139683861139456 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.707634 139683861139456 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:26:39.712988 139683861139456 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.713239 139683861139456 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:26:39.715866 139683861139456 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:26:39.728606 139683861139456 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:26:39.728659 139683861139456 attention.py:418] Single window, no scan.
I0123 13:26:39.728692 139683861139456 transformer_layer.py:389] tlayer: self-attention.
I0123 13:26:39.728720 139683861139456 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.728780 139683861139456 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.729338 139683861139456 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.729412 139683861139456 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.729767 139683861139456 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.730450 139683861139456 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.732924 139683861139456 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.733533 139683861139456 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.733611 139683861139456 transformer_layer.py:468] tlayer: End windows.
I0123 13:26:39.733649 139683861139456 transformer_layer.py:472] tlayer: final FFN.
I0123 13:26:39.733708 139683861139456 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.733833 139683861139456 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:26:39.733943 139683861139456 nn_components.py:325] mlp: activation = None
I0123 13:26:39.733980 139683861139456 nn_components.py:329] mlp: final activation = None
I0123 13:26:39.735807 139683861139456 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.735897 139683861139456 nn_components.py:261] mlp: residual
I0123 13:26:39.738239 139683861139456 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.738316 139683861139456 transformer_base.py:443] tbase: final FFN
I0123 13:26:39.738590 139683861139456 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:26:39.741342 139683861139456 nn_components.py:329] mlp: final activation = None
I0123 13:26:39.743180 139683861139456 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.743273 139683861139456 nn_components.py:261] mlp: residual
I0123 13:26:39.743549 139683861139456 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:39.743632 139683861139456 decoder_stack.py:344] dstack: Final layernorm.
I0123 13:26:39.746428 139683861139456 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:44.188011 139683861139456 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
I0123 13:26:44.733011 139683861139456 training_loop.py:409] No working directory specified.
I0123 13:26:44.733120 139683861139456 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0123 13:26:44.733880 139683861139456 checkpoints.py:1062] Restoring legacy Flax checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0123 13:26:47.775667 139683861139456 training_loop.py:447] Only restoring trainable parameters.
I0123 13:26:47.776349 139683861139456 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0123 13:26:47.776405 139683861139456 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0123 13:26:47.776448 139683861139456 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 13:26:47.776488 139683861139456 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 13:26:47.776527 139683861139456 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:47.776566 139683861139456 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 13:26:47.776604 139683861139456 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:47.776641 139683861139456 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:47.776676 139683861139456 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 13:26:47.776713 139683861139456 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 13:26:47.776750 139683861139456 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:47.776786 139683861139456 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 13:26:47.776822 139683861139456 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 13:26:47.776859 139683861139456 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 13:26:47.776895 139683861139456 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:47.776930 139683861139456 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 13:26:47.776965 139683861139456 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:47.777001 139683861139456 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:47.777036 139683861139456 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 13:26:47.777071 139683861139456 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 13:26:47.777122 139683861139456 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:47.777160 139683861139456 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 13:26:47.777195 139683861139456 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 13:26:47.777232 139683861139456 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 13:26:47.777268 139683861139456 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:47.777305 139683861139456 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 13:26:47.777341 139683861139456 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:47.777376 139683861139456 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:47.777410 139683861139456 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 13:26:47.777445 139683861139456 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 13:26:47.777479 139683861139456 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:47.777514 139683861139456 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 13:26:47.777548 139683861139456 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 13:26:47.777582 139683861139456 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 13:26:47.777617 139683861139456 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:47.777669 139683861139456 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 13:26:47.777708 139683861139456 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:47.777744 139683861139456 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:47.777779 139683861139456 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 13:26:47.777815 139683861139456 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 13:26:47.777851 139683861139456 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:47.777887 139683861139456 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 13:26:47.777921 139683861139456 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 13:26:47.777955 139683861139456 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 13:26:47.777989 139683861139456 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:47.778025 139683861139456 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 13:26:47.778065 139683861139456 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:47.778101 139683861139456 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:47.778136 139683861139456 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 13:26:47.778170 139683861139456 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 13:26:47.778204 139683861139456 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:47.778239 139683861139456 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 13:26:47.778273 139683861139456 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 13:26:47.778308 139683861139456 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 13:26:47.778343 139683861139456 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:47.778378 139683861139456 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 13:26:47.778413 139683861139456 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:47.778447 139683861139456 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:47.778482 139683861139456 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 13:26:47.778516 139683861139456 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 13:26:47.778551 139683861139456 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:47.778585 139683861139456 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 13:26:47.778620 139683861139456 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 13:26:47.778656 139683861139456 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 13:26:47.778691 139683861139456 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:47.778726 139683861139456 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 13:26:47.778761 139683861139456 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:47.778796 139683861139456 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:47.778830 139683861139456 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 13:26:47.778864 139683861139456 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 13:26:47.778898 139683861139456 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:47.778933 139683861139456 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 13:26:47.778968 139683861139456 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 13:26:47.779007 139683861139456 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 13:26:47.779044 139683861139456 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:47.779079 139683861139456 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 13:26:47.779114 139683861139456 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:47.779148 139683861139456 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:47.779182 139683861139456 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 13:26:47.779217 139683861139456 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 13:26:47.779252 139683861139456 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:47.779287 139683861139456 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 13:26:47.779321 139683861139456 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 13:26:47.779356 139683861139456 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 13:26:47.779389 139683861139456 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:47.779423 139683861139456 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 13:26:47.779457 139683861139456 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:47.779491 139683861139456 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:47.779525 139683861139456 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 13:26:47.779558 139683861139456 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 13:26:47.779592 139683861139456 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:47.779625 139683861139456 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 13:26:47.779659 139683861139456 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 13:26:47.779693 139683861139456 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 13:26:47.779726 139683861139456 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:47.779758 139683861139456 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 13:26:47.779792 139683861139456 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:47.779824 139683861139456 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:47.779857 139683861139456 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 13:26:47.779891 139683861139456 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 13:26:47.779929 139683861139456 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:47.779964 139683861139456 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 13:26:47.779998 139683861139456 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 13:26:47.780031 139683861139456 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 13:26:47.780064 139683861139456 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:47.780097 139683861139456 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 13:26:47.780131 139683861139456 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:47.780164 139683861139456 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:47.780196 139683861139456 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 13:26:47.780229 139683861139456 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 13:26:47.780262 139683861139456 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:47.780296 139683861139456 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 13:26:47.780329 139683861139456 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 13:26:47.780362 139683861139456 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 13:26:47.780395 139683861139456 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:47.780429 139683861139456 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 13:26:47.780463 139683861139456 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:47.780496 139683861139456 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:47.780528 139683861139456 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 13:26:47.780562 139683861139456 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 13:26:47.780596 139683861139456 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:47.780630 139683861139456 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 13:26:47.780656 139683861139456 training_loop.py:725] Total parameters: 152072288
I0123 13:26:47.780863 139683861139456 training_loop.py:739] Total state size: 0
I0123 13:26:47.802496 139683861139456 training_loop.py:492] Training loop: creating task for mode beam_search
I0123 13:26:47.802732 139683861139456 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0123 13:26:47.803072 139683861139456 training_loop.py:652] Compiling mode beam_search with jit.
I0123 13:26:47.803386 139683861139456 training_loop.py:89] registering functions: dict_keys([])
I0123 13:26:47.819190 139683861139456 graph.py:499] a b c = triangle a b c; d = foot d b c a; e = foot e a c b; f = on_line f a e, on_line f b d; g = on_line g b a; h = circle h b g e; i = on_circle i h g, on_line i h g; j = circle j a g d; k = on_circle k j g, on_line k j g ? coll k f i
I0123 13:26:48.392134 139683861139456 ddar.py:60] Depth 1/1000 time = 0.5407886505126953
I0123 13:26:50.017034 139683861139456 ddar.py:60] Depth 2/1000 time = 1.6247320175170898
I0123 13:26:52.507187 139683861139456 ddar.py:60] Depth 3/1000 time = 2.4899959564208984
I0123 13:26:55.040478 139683861139456 ddar.py:60] Depth 4/1000 time = 2.5331296920776367
I0123 13:26:58.200111 139683861139456 ddar.py:60] Depth 5/1000 time = 3.1594269275665283
I0123 13:27:01.263221 139683861139456 ddar.py:60] Depth 6/1000 time = 3.0628669261932373
I0123 13:27:01.276733 139683861139456 alphageometry.py:191] 
==========================
 * From theorem premises:
A B C D E F G H I J K : Points
D,A,C are collinear [00]
BD  AC [01]
E,B,C are collinear [02]
AE  BC [03]
D,F,B are collinear [04]
F,E,A are collinear [05]
G,A,B are collinear [06]
HG = HE [07]
HB = HG [08]
HI = HG [09]
G,H,I are collinear [10]
JG = JD [11]
JA = JG [12]
JK = JG [13]
G,K,J are collinear [14]

 * Auxiliary Constructions:
: Points


 * Proof steps:
001. D,A,C are collinear [00] & D,F,B are collinear [04] & BD  AC [01]   ADF = BDC [15]
002. E,B,C are collinear [02] & F,E,A are collinear [05] & BC  AE [03]   EB  FE [16]
003. D,F,B are collinear [04] & D,A,C are collinear [00] & BD  AC [01]   DF  DA [17]
004. EB  FE [16] & DF  DA [17]   (EB-DF) = (FE-DA) [18]
005. EB  FE [16] & DF  DA [17]   (EB-DA) = EFD [19]
006. F,E,A are collinear [05] & D,A,C are collinear [00] & (EB-DF) = (FE-DA) [18] & E,B,C are collinear [02] & D,F,B are collinear [04]   FAD = CBD [20]
007. ADF = BDC [15] & FAD = CBD [20] (Similar Triangles)  DF:DA = DC:DB [21]
008. JG = JD [11] & JK = JG [13]   J is the circumcenter of \Delta GDK [22]
009. J is the circumcenter of \Delta GDK [22] & G,K,J are collinear [14]   DG  DK [23]
010. D,A,C are collinear [00] & BD  AC [01] & DG  DK [23]   GDK = BDA [24]
011. JA = JG [12] & JK = JG [13] & JG = JD [11]   D,G,K,A are concyclic [25]
012. D,G,K,A are concyclic [25]   DKG = DAG [26]
013. K,J,G are collinear [14] & D,A,C are collinear [00] & DKG = DAG [26] & G,A,B are collinear [06]   GKD = BAD [27]
014. GDK = BDA [24] & GKD = BAD [27] (Similar Triangles)  DG:DB = DK:DA [28]
015. DF:DA = DC:DB [21] & DG:DB = DK:DA [28]   DC:DG = DF:DK [29]
016. DG  DK [23] & DF  DA [17]   GDA = KDF [30]
017. D,F,B are collinear [04] & D,A,C are collinear [00] & GDA = KDF [30]   KDF = GDC [31]
018. DC:DG = DF:DK [29] & KDF = GDC [31] (Similar Triangles)  KFD = GCD [32]
019. E,B,C are collinear [02] & F,E,A are collinear [05] & D,F,B are collinear [04] & D,A,C are collinear [00] & BD  AC [01] & AE  BC [03]   BEF = FDA [33]
020. D,F,B are collinear [04] & F,E,A are collinear [05]   BFE = DFA [34]
021. BEF = FDA [33] & BFE = DFA [34] (Similar Triangles)  FE:EB = DF:DA [35]
022. E,B,C are collinear [02] & D,A,C are collinear [00] & D,F,B are collinear [04] & (EB-DA) = EFD [19] & F,E,A are collinear [05]   CEA = ADF [36]
023. D,A,C are collinear [00] & F,E,A are collinear [05]   CAE = DAF [37]
024. CEA = ADF [36] & CAE = DAF [37] (Similar Triangles)  EC:EA = DF:DA [38]
025. FE:EB = DF:DA [35] & EC:EA = DF:DA [38]   EC:EA = FE:EB [39]
026. HG = HE [07] & HI = HG [09]   H is the circumcenter of \Delta GEI [40]
027. H is the circumcenter of \Delta GEI [40] & G,H,I are collinear [10]   EG  EI [41]
028. E,B,C are collinear [02] & AE  BC [03] & EG  EI [41]   GEI = AEB [42]
029. HG = HE [07] & HI = HG [09] & HB = HG [08]   G,E,I,B are concyclic [43]
030. G,E,I,B are concyclic [43]   GIE = GBE [44]
031. G,H,I are collinear [10] & E,B,C are collinear [02] & GIE = GBE [44] & G,A,B are collinear [06]   GIE = ABE [45]
032. GEI = AEB [42] & GIE = ABE [45] (Similar Triangles)  GE:EA = EI:EB [46]
033. EC:EA = FE:EB [39] & GE:EA = EI:EB [46]   FE:EI = EC:GE [47]
034. EG  EI [41] & EB  FE [16]   IEF = GEB [48]
035. E,B,C are collinear [02] & F,E,A are collinear [05] & IEF = GEB [48]   CEG = FEI [49]
036. FE:EI = EC:GE [47] & CEG = FEI [49] (Similar Triangles)  ECG = EFI [50]
037. KFD = GCD [32] & D,F,B are collinear [04] & D,A,C are collinear [00] & (EB-DA) = EFD [19] & E,B,C are collinear [02] & F,E,A are collinear [05] & ECG = EFI [50]   (FI-GC) = (FK-GC) [51]
038. (FI-GC) = (FK-GC) [51]   FI  FK [52]
039. FI  FK [52]   F,K,I are collinear
==========================

