I0123 20:37:34.919941 140626031575040 inference_utils.py:69] Parsing gin configuration.
I0123 20:37:34.920042 140626031575040 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0123 20:37:34.920251 140626031575040 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0123 20:37:34.920285 140626031575040 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0123 20:37:34.920315 140626031575040 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0123 20:37:34.920344 140626031575040 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0123 20:37:34.920375 140626031575040 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0123 20:37:34.920404 140626031575040 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0123 20:37:34.920431 140626031575040 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0123 20:37:34.920458 140626031575040 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=32
I0123 20:37:34.920484 140626031575040 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0123 20:37:34.920510 140626031575040 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0123 20:37:34.920555 140626031575040 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0123 20:37:34.920688 140626031575040 resource_reader.py:55] Path not found: base_htrans.gin
I0123 20:37:34.920895 140626031575040 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0123 20:37:34.921000 140626031575040 resource_reader.py:55] Path not found: trainer_configuration.gin
I0123 20:37:34.927359 140626031575040 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0123 20:37:34.927483 140626031575040 resource_reader.py:55] Path not found: size/medium_150M.gin
I0123 20:37:34.927805 140626031575040 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0123 20:37:34.927909 140626031575040 resource_reader.py:55] Path not found: options/positions_t5.gin
I0123 20:37:34.928187 140626031575040 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0123 20:37:34.928286 140626031575040 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0123 20:37:34.928691 140626031575040 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0123 20:37:34.928791 140626031575040 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0123 20:37:34.932482 140626031575040 training_loop.py:334] ==== Training loop: initializing model ====
I0123 20:37:35.035120 140626031575040 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0123 20:37:35.035865 140626031575040 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0123 20:37:35.042687 140626031575040 training_loop.py:335] Process 0 of 1
I0123 20:37:35.042744 140626031575040 training_loop.py:336] Local device count = 1
I0123 20:37:35.042785 140626031575040 training_loop.py:337] Number of replicas = 1
I0123 20:37:35.042818 140626031575040 training_loop.py:339] Using random number seed 42
I0123 20:37:35.532078 140626031575040 training_loop.py:359] Initializing the model.
I0123 20:37:35.909877 140626031575040 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:35.910177 140626031575040 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 20:37:35.910286 140626031575040 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 20:37:35.910368 140626031575040 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 20:37:35.910448 140626031575040 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 20:37:35.911192 140626031575040 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 20:37:35.911272 140626031575040 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 20:37:35.911344 140626031575040 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 20:37:35.911417 140626031575040 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 20:37:35.911483 140626031575040 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 20:37:35.911550 140626031575040 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 20:37:35.911617 140626031575040 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 20:37:35.911685 140626031575040 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 20:37:35.911752 140626031575040 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 20:37:35.911791 140626031575040 attention.py:418] Single window, no scan.
I0123 20:37:35.911835 140626031575040 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 20:37:35.911951 140626031575040 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:37:35.911991 140626031575040 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:37:35.912022 140626031575040 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:37:35.914080 140626031575040 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:35.919426 140626031575040 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:37:35.930058 140626031575040 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:35.930343 140626031575040 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:37:35.934711 140626031575040 transformer_layer.py:213] tlayer: windowed attention.
I0123 20:37:35.945353 140626031575040 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:37:35.945413 140626031575040 attention.py:418] Single window, no scan.
I0123 20:37:35.945451 140626031575040 transformer_layer.py:389] tlayer: self-attention.
I0123 20:37:35.945482 140626031575040 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:35.945544 140626031575040 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:35.946773 140626031575040 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:35.946854 140626031575040 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:35.947571 140626031575040 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:35.950051 140626031575040 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:35.955840 140626031575040 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:35.957557 140626031575040 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:35.957638 140626031575040 transformer_layer.py:468] tlayer: End windows.
I0123 20:37:35.957684 140626031575040 transformer_layer.py:472] tlayer: final FFN.
I0123 20:37:35.957753 140626031575040 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:35.957887 140626031575040 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:37:35.958218 140626031575040 nn_components.py:325] mlp: activation = None
I0123 20:37:35.958267 140626031575040 nn_components.py:329] mlp: final activation = None
I0123 20:37:35.960167 140626031575040 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:35.960267 140626031575040 nn_components.py:261] mlp: residual
I0123 20:37:35.963110 140626031575040 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:35.963191 140626031575040 transformer_base.py:443] tbase: final FFN
I0123 20:37:35.963689 140626031575040 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:37:35.973739 140626031575040 nn_components.py:329] mlp: final activation = None
I0123 20:37:35.982438 140626031575040 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:35.982537 140626031575040 nn_components.py:261] mlp: residual
I0123 20:37:35.982835 140626031575040 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:35.982917 140626031575040 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 20:37:35.983028 140626031575040 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:37:35.983068 140626031575040 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:37:35.983100 140626031575040 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:37:35.984914 140626031575040 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:35.987369 140626031575040 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:37:35.992901 140626031575040 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:35.993162 140626031575040 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:37:35.995781 140626031575040 transformer_layer.py:213] tlayer: windowed attention.
I0123 20:37:35.999562 140626031575040 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:37:35.999619 140626031575040 attention.py:418] Single window, no scan.
I0123 20:37:35.999655 140626031575040 transformer_layer.py:389] tlayer: self-attention.
I0123 20:37:35.999687 140626031575040 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:35.999753 140626031575040 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.000317 140626031575040 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.000392 140626031575040 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.000745 140626031575040 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.001504 140626031575040 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.003939 140626031575040 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.004551 140626031575040 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.004628 140626031575040 transformer_layer.py:468] tlayer: End windows.
I0123 20:37:36.004663 140626031575040 transformer_layer.py:472] tlayer: final FFN.
I0123 20:37:36.004722 140626031575040 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.004847 140626031575040 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:37:36.005172 140626031575040 nn_components.py:325] mlp: activation = None
I0123 20:37:36.005215 140626031575040 nn_components.py:329] mlp: final activation = None
I0123 20:37:36.007134 140626031575040 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.007228 140626031575040 nn_components.py:261] mlp: residual
I0123 20:37:36.009703 140626031575040 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.009786 140626031575040 transformer_base.py:443] tbase: final FFN
I0123 20:37:36.010212 140626031575040 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:37:36.012498 140626031575040 nn_components.py:329] mlp: final activation = None
I0123 20:37:36.014381 140626031575040 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.014479 140626031575040 nn_components.py:261] mlp: residual
I0123 20:37:36.014767 140626031575040 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.014847 140626031575040 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 20:37:36.014954 140626031575040 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:37:36.014993 140626031575040 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:37:36.015024 140626031575040 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:37:36.016917 140626031575040 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.019238 140626031575040 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:37:36.025130 140626031575040 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.025391 140626031575040 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:37:36.028025 140626031575040 transformer_layer.py:213] tlayer: windowed attention.
I0123 20:37:36.031884 140626031575040 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:37:36.031940 140626031575040 attention.py:418] Single window, no scan.
I0123 20:37:36.031981 140626031575040 transformer_layer.py:389] tlayer: self-attention.
I0123 20:37:36.032014 140626031575040 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.032078 140626031575040 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.032635 140626031575040 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.032710 140626031575040 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.033064 140626031575040 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.033828 140626031575040 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.036267 140626031575040 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.036929 140626031575040 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.037005 140626031575040 transformer_layer.py:468] tlayer: End windows.
I0123 20:37:36.037041 140626031575040 transformer_layer.py:472] tlayer: final FFN.
I0123 20:37:36.037100 140626031575040 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.037229 140626031575040 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:37:36.037547 140626031575040 nn_components.py:325] mlp: activation = None
I0123 20:37:36.037590 140626031575040 nn_components.py:329] mlp: final activation = None
I0123 20:37:36.039482 140626031575040 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.039578 140626031575040 nn_components.py:261] mlp: residual
I0123 20:37:36.042053 140626031575040 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.042137 140626031575040 transformer_base.py:443] tbase: final FFN
I0123 20:37:36.042622 140626031575040 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:37:36.044869 140626031575040 nn_components.py:329] mlp: final activation = None
I0123 20:37:36.046773 140626031575040 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.046867 140626031575040 nn_components.py:261] mlp: residual
I0123 20:37:36.047155 140626031575040 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.047235 140626031575040 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 20:37:36.047344 140626031575040 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:37:36.047383 140626031575040 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:37:36.047414 140626031575040 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:37:36.049284 140626031575040 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.051656 140626031575040 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:37:36.057238 140626031575040 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.057499 140626031575040 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:37:36.060107 140626031575040 transformer_layer.py:213] tlayer: windowed attention.
I0123 20:37:36.064524 140626031575040 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:37:36.064630 140626031575040 attention.py:418] Single window, no scan.
I0123 20:37:36.064668 140626031575040 transformer_layer.py:389] tlayer: self-attention.
I0123 20:37:36.064699 140626031575040 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.064769 140626031575040 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.065350 140626031575040 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.065428 140626031575040 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.065790 140626031575040 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.066566 140626031575040 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.069105 140626031575040 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.069735 140626031575040 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.069812 140626031575040 transformer_layer.py:468] tlayer: End windows.
I0123 20:37:36.069847 140626031575040 transformer_layer.py:472] tlayer: final FFN.
I0123 20:37:36.069907 140626031575040 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.070036 140626031575040 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:37:36.070377 140626031575040 nn_components.py:325] mlp: activation = None
I0123 20:37:36.070421 140626031575040 nn_components.py:329] mlp: final activation = None
I0123 20:37:36.072333 140626031575040 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.072427 140626031575040 nn_components.py:261] mlp: residual
I0123 20:37:36.074964 140626031575040 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.075050 140626031575040 transformer_base.py:443] tbase: final FFN
I0123 20:37:36.075483 140626031575040 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:37:36.077777 140626031575040 nn_components.py:329] mlp: final activation = None
I0123 20:37:36.079669 140626031575040 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.079763 140626031575040 nn_components.py:261] mlp: residual
I0123 20:37:36.080050 140626031575040 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.080130 140626031575040 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 20:37:36.080238 140626031575040 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:37:36.080276 140626031575040 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:37:36.080307 140626031575040 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:37:36.082226 140626031575040 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.084579 140626031575040 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:37:36.090208 140626031575040 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.090478 140626031575040 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:37:36.093175 140626031575040 transformer_layer.py:213] tlayer: windowed attention.
I0123 20:37:36.096960 140626031575040 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:37:36.097017 140626031575040 attention.py:418] Single window, no scan.
I0123 20:37:36.097053 140626031575040 transformer_layer.py:389] tlayer: self-attention.
I0123 20:37:36.097084 140626031575040 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.097146 140626031575040 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.097721 140626031575040 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.097798 140626031575040 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.098153 140626031575040 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.098917 140626031575040 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.101754 140626031575040 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.102389 140626031575040 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.102466 140626031575040 transformer_layer.py:468] tlayer: End windows.
I0123 20:37:36.102502 140626031575040 transformer_layer.py:472] tlayer: final FFN.
I0123 20:37:36.102562 140626031575040 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.102699 140626031575040 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:37:36.103021 140626031575040 nn_components.py:325] mlp: activation = None
I0123 20:37:36.103065 140626031575040 nn_components.py:329] mlp: final activation = None
I0123 20:37:36.104953 140626031575040 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.105045 140626031575040 nn_components.py:261] mlp: residual
I0123 20:37:36.107595 140626031575040 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.107678 140626031575040 transformer_base.py:443] tbase: final FFN
I0123 20:37:36.108104 140626031575040 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:37:36.110380 140626031575040 nn_components.py:329] mlp: final activation = None
I0123 20:37:36.112338 140626031575040 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.112433 140626031575040 nn_components.py:261] mlp: residual
I0123 20:37:36.112724 140626031575040 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.112805 140626031575040 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 20:37:36.112914 140626031575040 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:37:36.112953 140626031575040 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:37:36.112985 140626031575040 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:37:36.114818 140626031575040 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.117177 140626031575040 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:37:36.122755 140626031575040 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.123013 140626031575040 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:37:36.125676 140626031575040 transformer_layer.py:213] tlayer: windowed attention.
I0123 20:37:36.129414 140626031575040 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:37:36.129469 140626031575040 attention.py:418] Single window, no scan.
I0123 20:37:36.129506 140626031575040 transformer_layer.py:389] tlayer: self-attention.
I0123 20:37:36.129538 140626031575040 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.129599 140626031575040 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.130219 140626031575040 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.130295 140626031575040 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.130657 140626031575040 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.131436 140626031575040 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.133911 140626031575040 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.134537 140626031575040 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.134615 140626031575040 transformer_layer.py:468] tlayer: End windows.
I0123 20:37:36.134650 140626031575040 transformer_layer.py:472] tlayer: final FFN.
I0123 20:37:36.134709 140626031575040 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.134838 140626031575040 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:37:36.135169 140626031575040 nn_components.py:325] mlp: activation = None
I0123 20:37:36.135213 140626031575040 nn_components.py:329] mlp: final activation = None
I0123 20:37:36.137088 140626031575040 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.137185 140626031575040 nn_components.py:261] mlp: residual
I0123 20:37:36.139737 140626031575040 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.139817 140626031575040 transformer_base.py:443] tbase: final FFN
I0123 20:37:36.140246 140626031575040 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:37:36.142571 140626031575040 nn_components.py:329] mlp: final activation = None
I0123 20:37:36.144474 140626031575040 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.144570 140626031575040 nn_components.py:261] mlp: residual
I0123 20:37:36.144863 140626031575040 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.144944 140626031575040 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 20:37:36.145055 140626031575040 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:37:36.145094 140626031575040 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:37:36.145126 140626031575040 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:37:36.146975 140626031575040 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.149408 140626031575040 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:37:36.154977 140626031575040 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.155244 140626031575040 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:37:36.157860 140626031575040 transformer_layer.py:213] tlayer: windowed attention.
I0123 20:37:36.161658 140626031575040 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:37:36.161715 140626031575040 attention.py:418] Single window, no scan.
I0123 20:37:36.161756 140626031575040 transformer_layer.py:389] tlayer: self-attention.
I0123 20:37:36.161788 140626031575040 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.161850 140626031575040 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.162420 140626031575040 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.162495 140626031575040 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.162845 140626031575040 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.163618 140626031575040 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.166071 140626031575040 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.166687 140626031575040 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.166764 140626031575040 transformer_layer.py:468] tlayer: End windows.
I0123 20:37:36.166799 140626031575040 transformer_layer.py:472] tlayer: final FFN.
I0123 20:37:36.166857 140626031575040 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.166986 140626031575040 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:37:36.167310 140626031575040 nn_components.py:325] mlp: activation = None
I0123 20:37:36.167354 140626031575040 nn_components.py:329] mlp: final activation = None
I0123 20:37:36.169294 140626031575040 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.169387 140626031575040 nn_components.py:261] mlp: residual
I0123 20:37:36.171880 140626031575040 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.171959 140626031575040 transformer_base.py:443] tbase: final FFN
I0123 20:37:36.172392 140626031575040 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:37:36.175029 140626031575040 nn_components.py:329] mlp: final activation = None
I0123 20:37:36.176938 140626031575040 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.177039 140626031575040 nn_components.py:261] mlp: residual
I0123 20:37:36.177337 140626031575040 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.177419 140626031575040 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 20:37:36.177529 140626031575040 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:37:36.177568 140626031575040 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:37:36.177599 140626031575040 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:37:36.317759 140626031575040 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.320899 140626031575040 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:37:36.326802 140626031575040 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.327101 140626031575040 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:37:36.329816 140626031575040 transformer_layer.py:213] tlayer: windowed attention.
I0123 20:37:36.333843 140626031575040 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:37:36.333903 140626031575040 attention.py:418] Single window, no scan.
I0123 20:37:36.333945 140626031575040 transformer_layer.py:389] tlayer: self-attention.
I0123 20:37:36.333978 140626031575040 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.334045 140626031575040 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.334681 140626031575040 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.334758 140626031575040 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.335128 140626031575040 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.335929 140626031575040 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.338548 140626031575040 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.339192 140626031575040 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.339274 140626031575040 transformer_layer.py:468] tlayer: End windows.
I0123 20:37:36.339311 140626031575040 transformer_layer.py:472] tlayer: final FFN.
I0123 20:37:36.339374 140626031575040 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.339504 140626031575040 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:37:36.339841 140626031575040 nn_components.py:325] mlp: activation = None
I0123 20:37:36.339885 140626031575040 nn_components.py:329] mlp: final activation = None
I0123 20:37:36.341821 140626031575040 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.341916 140626031575040 nn_components.py:261] mlp: residual
I0123 20:37:36.344481 140626031575040 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.344563 140626031575040 transformer_base.py:443] tbase: final FFN
I0123 20:37:36.345005 140626031575040 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:37:36.347351 140626031575040 nn_components.py:329] mlp: final activation = None
I0123 20:37:36.349280 140626031575040 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.349384 140626031575040 nn_components.py:261] mlp: residual
I0123 20:37:36.349681 140626031575040 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.349771 140626031575040 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 20:37:36.349882 140626031575040 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:37:36.349922 140626031575040 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:37:36.349954 140626031575040 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:37:36.351894 140626031575040 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.354277 140626031575040 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:37:36.359961 140626031575040 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.360227 140626031575040 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:37:36.362924 140626031575040 transformer_layer.py:213] tlayer: windowed attention.
I0123 20:37:36.366768 140626031575040 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:37:36.366825 140626031575040 attention.py:418] Single window, no scan.
I0123 20:37:36.366862 140626031575040 transformer_layer.py:389] tlayer: self-attention.
I0123 20:37:36.366893 140626031575040 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.366955 140626031575040 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.367519 140626031575040 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.367600 140626031575040 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.367960 140626031575040 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.368733 140626031575040 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.371275 140626031575040 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.371901 140626031575040 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.371978 140626031575040 transformer_layer.py:468] tlayer: End windows.
I0123 20:37:36.372013 140626031575040 transformer_layer.py:472] tlayer: final FFN.
I0123 20:37:36.372073 140626031575040 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.372201 140626031575040 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:37:36.372534 140626031575040 nn_components.py:325] mlp: activation = None
I0123 20:37:36.372577 140626031575040 nn_components.py:329] mlp: final activation = None
I0123 20:37:36.374493 140626031575040 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.374588 140626031575040 nn_components.py:261] mlp: residual
I0123 20:37:36.377147 140626031575040 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.377228 140626031575040 transformer_base.py:443] tbase: final FFN
I0123 20:37:36.377669 140626031575040 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:37:36.379957 140626031575040 nn_components.py:329] mlp: final activation = None
I0123 20:37:36.381960 140626031575040 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.382058 140626031575040 nn_components.py:261] mlp: residual
I0123 20:37:36.382353 140626031575040 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.382442 140626031575040 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 20:37:36.382555 140626031575040 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:37:36.382595 140626031575040 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:37:36.382627 140626031575040 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:37:36.384616 140626031575040 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.387086 140626031575040 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:37:36.392690 140626031575040 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.392962 140626031575040 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:37:36.396004 140626031575040 transformer_layer.py:213] tlayer: windowed attention.
I0123 20:37:36.399819 140626031575040 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:37:36.399876 140626031575040 attention.py:418] Single window, no scan.
I0123 20:37:36.399913 140626031575040 transformer_layer.py:389] tlayer: self-attention.
I0123 20:37:36.399945 140626031575040 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.400008 140626031575040 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.400609 140626031575040 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.400686 140626031575040 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.401054 140626031575040 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.401828 140626031575040 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.404294 140626031575040 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.404917 140626031575040 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.404995 140626031575040 transformer_layer.py:468] tlayer: End windows.
I0123 20:37:36.405031 140626031575040 transformer_layer.py:472] tlayer: final FFN.
I0123 20:37:36.405093 140626031575040 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.405220 140626031575040 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:37:36.405548 140626031575040 nn_components.py:325] mlp: activation = None
I0123 20:37:36.405592 140626031575040 nn_components.py:329] mlp: final activation = None
I0123 20:37:36.407521 140626031575040 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.407616 140626031575040 nn_components.py:261] mlp: residual
I0123 20:37:36.410201 140626031575040 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.410282 140626031575040 transformer_base.py:443] tbase: final FFN
I0123 20:37:36.410721 140626031575040 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:37:36.413056 140626031575040 nn_components.py:329] mlp: final activation = None
I0123 20:37:36.414989 140626031575040 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.415086 140626031575040 nn_components.py:261] mlp: residual
I0123 20:37:36.415382 140626031575040 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.415469 140626031575040 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 20:37:36.415583 140626031575040 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:37:36.415623 140626031575040 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:37:36.415656 140626031575040 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:37:36.417504 140626031575040 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.419957 140626031575040 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:37:36.425594 140626031575040 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.425866 140626031575040 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:37:36.428491 140626031575040 transformer_layer.py:213] tlayer: windowed attention.
I0123 20:37:36.432345 140626031575040 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:37:36.432401 140626031575040 attention.py:418] Single window, no scan.
I0123 20:37:36.432437 140626031575040 transformer_layer.py:389] tlayer: self-attention.
I0123 20:37:36.432469 140626031575040 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.432531 140626031575040 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.433091 140626031575040 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.433170 140626031575040 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.433527 140626031575040 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.434304 140626031575040 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.436780 140626031575040 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.437409 140626031575040 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.437487 140626031575040 transformer_layer.py:468] tlayer: End windows.
I0123 20:37:36.437522 140626031575040 transformer_layer.py:472] tlayer: final FFN.
I0123 20:37:36.437582 140626031575040 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.437717 140626031575040 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:37:36.438057 140626031575040 nn_components.py:325] mlp: activation = None
I0123 20:37:36.438101 140626031575040 nn_components.py:329] mlp: final activation = None
I0123 20:37:36.440058 140626031575040 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.440150 140626031575040 nn_components.py:261] mlp: residual
I0123 20:37:36.442946 140626031575040 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.443027 140626031575040 transformer_base.py:443] tbase: final FFN
I0123 20:37:36.443465 140626031575040 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:37:36.445822 140626031575040 nn_components.py:329] mlp: final activation = None
I0123 20:37:36.447746 140626031575040 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.447842 140626031575040 nn_components.py:261] mlp: residual
I0123 20:37:36.448134 140626031575040 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.448216 140626031575040 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 20:37:36.448333 140626031575040 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:37:36.448374 140626031575040 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:37:36.448406 140626031575040 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:37:36.450308 140626031575040 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.452689 140626031575040 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:37:36.458323 140626031575040 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.458587 140626031575040 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:37:36.461227 140626031575040 transformer_layer.py:213] tlayer: windowed attention.
I0123 20:37:36.465068 140626031575040 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:37:36.465125 140626031575040 attention.py:418] Single window, no scan.
I0123 20:37:36.465162 140626031575040 transformer_layer.py:389] tlayer: self-attention.
I0123 20:37:36.465194 140626031575040 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.465256 140626031575040 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.465821 140626031575040 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.465898 140626031575040 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.466262 140626031575040 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.467037 140626031575040 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.469500 140626031575040 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.470501 140626031575040 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.470584 140626031575040 transformer_layer.py:468] tlayer: End windows.
I0123 20:37:36.470619 140626031575040 transformer_layer.py:472] tlayer: final FFN.
I0123 20:37:36.470681 140626031575040 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.470808 140626031575040 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:37:36.471135 140626031575040 nn_components.py:325] mlp: activation = None
I0123 20:37:36.471179 140626031575040 nn_components.py:329] mlp: final activation = None
I0123 20:37:36.473082 140626031575040 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.473174 140626031575040 nn_components.py:261] mlp: residual
I0123 20:37:36.475688 140626031575040 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.475772 140626031575040 transformer_base.py:443] tbase: final FFN
I0123 20:37:36.476255 140626031575040 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:37:36.478533 140626031575040 nn_components.py:329] mlp: final activation = None
I0123 20:37:36.480462 140626031575040 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.480556 140626031575040 nn_components.py:261] mlp: residual
I0123 20:37:36.480849 140626031575040 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.481145 140626031575040 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 20:37:36.481216 140626031575040 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 20:37:36.481284 140626031575040 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 20:37:36.481343 140626031575040 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 20:37:36.481398 140626031575040 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 20:37:36.481453 140626031575040 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 20:37:36.481506 140626031575040 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 20:37:36.481562 140626031575040 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 20:37:36.481617 140626031575040 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 20:37:36.481678 140626031575040 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 20:37:36.481736 140626031575040 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 20:37:36.481790 140626031575040 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 20:37:36.481828 140626031575040 decoder_stack.py:344] dstack: Final layernorm.
I0123 20:37:36.485355 140626031575040 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 20:37:36.533526 140626031575040 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.533609 140626031575040 decoder_stack.py:333] dstack: autoregressive generator.
I0123 20:37:36.533671 140626031575040 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 20:37:36.533779 140626031575040 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:37:36.533818 140626031575040 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:37:36.533849 140626031575040 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:37:36.533912 140626031575040 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.536347 140626031575040 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:37:36.541955 140626031575040 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.542218 140626031575040 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:37:36.544895 140626031575040 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:37:36.561543 140626031575040 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:37:36.561602 140626031575040 attention.py:418] Single window, no scan.
I0123 20:37:36.561637 140626031575040 transformer_layer.py:389] tlayer: self-attention.
I0123 20:37:36.561679 140626031575040 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.561742 140626031575040 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.562915 140626031575040 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.562997 140626031575040 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.563709 140626031575040 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.565719 140626031575040 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.570449 140626031575040 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.571748 140626031575040 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.571836 140626031575040 transformer_layer.py:468] tlayer: End windows.
I0123 20:37:36.571872 140626031575040 transformer_layer.py:472] tlayer: final FFN.
I0123 20:37:36.571931 140626031575040 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.572064 140626031575040 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:37:36.572177 140626031575040 nn_components.py:325] mlp: activation = None
I0123 20:37:36.572216 140626031575040 nn_components.py:329] mlp: final activation = None
I0123 20:37:36.574114 140626031575040 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.574210 140626031575040 nn_components.py:261] mlp: residual
I0123 20:37:36.576582 140626031575040 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.576661 140626031575040 transformer_base.py:443] tbase: final FFN
I0123 20:37:36.576771 140626031575040 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:37:36.578972 140626031575040 nn_components.py:329] mlp: final activation = None
I0123 20:37:36.580903 140626031575040 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.580999 140626031575040 nn_components.py:261] mlp: residual
I0123 20:37:36.581285 140626031575040 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.581366 140626031575040 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 20:37:36.581474 140626031575040 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:37:36.581513 140626031575040 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:37:36.581543 140626031575040 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:37:36.581606 140626031575040 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.583827 140626031575040 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:37:36.589221 140626031575040 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.589477 140626031575040 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:37:36.592144 140626031575040 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:37:36.605243 140626031575040 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:37:36.605299 140626031575040 attention.py:418] Single window, no scan.
I0123 20:37:36.605335 140626031575040 transformer_layer.py:389] tlayer: self-attention.
I0123 20:37:36.605365 140626031575040 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.605426 140626031575040 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.605990 140626031575040 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.606066 140626031575040 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.606408 140626031575040 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.607086 140626031575040 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.609537 140626031575040 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.610158 140626031575040 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.610236 140626031575040 transformer_layer.py:468] tlayer: End windows.
I0123 20:37:36.610276 140626031575040 transformer_layer.py:472] tlayer: final FFN.
I0123 20:37:36.610336 140626031575040 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.610465 140626031575040 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:37:36.610574 140626031575040 nn_components.py:325] mlp: activation = None
I0123 20:37:36.610612 140626031575040 nn_components.py:329] mlp: final activation = None
I0123 20:37:36.612521 140626031575040 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.612614 140626031575040 nn_components.py:261] mlp: residual
I0123 20:37:36.614989 140626031575040 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.615072 140626031575040 transformer_base.py:443] tbase: final FFN
I0123 20:37:36.615182 140626031575040 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:37:36.617356 140626031575040 nn_components.py:329] mlp: final activation = None
I0123 20:37:36.619253 140626031575040 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.619349 140626031575040 nn_components.py:261] mlp: residual
I0123 20:37:36.619628 140626031575040 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.619707 140626031575040 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 20:37:36.619815 140626031575040 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:37:36.619853 140626031575040 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:37:36.619883 140626031575040 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:37:36.619946 140626031575040 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.622147 140626031575040 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:37:36.627577 140626031575040 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.627832 140626031575040 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:37:36.630478 140626031575040 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:37:36.643014 140626031575040 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:37:36.643070 140626031575040 attention.py:418] Single window, no scan.
I0123 20:37:36.643108 140626031575040 transformer_layer.py:389] tlayer: self-attention.
I0123 20:37:36.643139 140626031575040 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.643201 140626031575040 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.643753 140626031575040 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.643828 140626031575040 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.644178 140626031575040 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.644861 140626031575040 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.647297 140626031575040 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.647913 140626031575040 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.647988 140626031575040 transformer_layer.py:468] tlayer: End windows.
I0123 20:37:36.648021 140626031575040 transformer_layer.py:472] tlayer: final FFN.
I0123 20:37:36.648084 140626031575040 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.648210 140626031575040 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:37:36.648320 140626031575040 nn_components.py:325] mlp: activation = None
I0123 20:37:36.648359 140626031575040 nn_components.py:329] mlp: final activation = None
I0123 20:37:36.650253 140626031575040 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.650346 140626031575040 nn_components.py:261] mlp: residual
I0123 20:37:36.652722 140626031575040 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.652801 140626031575040 transformer_base.py:443] tbase: final FFN
I0123 20:37:36.652910 140626031575040 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:37:36.655104 140626031575040 nn_components.py:329] mlp: final activation = None
I0123 20:37:36.656985 140626031575040 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.657080 140626031575040 nn_components.py:261] mlp: residual
I0123 20:37:36.657363 140626031575040 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.657444 140626031575040 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 20:37:36.657553 140626031575040 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:37:36.657592 140626031575040 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:37:36.657623 140626031575040 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:37:36.657692 140626031575040 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.659885 140626031575040 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:37:36.665249 140626031575040 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.665505 140626031575040 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:37:36.668135 140626031575040 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:37:36.680804 140626031575040 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:37:36.680860 140626031575040 attention.py:418] Single window, no scan.
I0123 20:37:36.680895 140626031575040 transformer_layer.py:389] tlayer: self-attention.
I0123 20:37:36.680926 140626031575040 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.680986 140626031575040 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.681533 140626031575040 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.681608 140626031575040 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.681970 140626031575040 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.682655 140626031575040 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.685101 140626031575040 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.685730 140626031575040 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.685808 140626031575040 transformer_layer.py:468] tlayer: End windows.
I0123 20:37:36.685842 140626031575040 transformer_layer.py:472] tlayer: final FFN.
I0123 20:37:36.685899 140626031575040 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.686040 140626031575040 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:37:36.686150 140626031575040 nn_components.py:325] mlp: activation = None
I0123 20:37:36.686188 140626031575040 nn_components.py:329] mlp: final activation = None
I0123 20:37:36.688084 140626031575040 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.688176 140626031575040 nn_components.py:261] mlp: residual
I0123 20:37:36.690540 140626031575040 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.690620 140626031575040 transformer_base.py:443] tbase: final FFN
I0123 20:37:36.690727 140626031575040 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:37:36.692907 140626031575040 nn_components.py:329] mlp: final activation = None
I0123 20:37:36.694755 140626031575040 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.694849 140626031575040 nn_components.py:261] mlp: residual
I0123 20:37:36.695130 140626031575040 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.695211 140626031575040 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 20:37:36.695320 140626031575040 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:37:36.695358 140626031575040 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:37:36.695388 140626031575040 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:37:36.695451 140626031575040 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.698001 140626031575040 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:37:36.703376 140626031575040 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.703639 140626031575040 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:37:36.706207 140626031575040 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:37:36.718778 140626031575040 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:37:36.718833 140626031575040 attention.py:418] Single window, no scan.
I0123 20:37:36.718868 140626031575040 transformer_layer.py:389] tlayer: self-attention.
I0123 20:37:36.718899 140626031575040 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.718960 140626031575040 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.719518 140626031575040 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.719595 140626031575040 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.719953 140626031575040 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.720643 140626031575040 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.727575 140626031575040 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.728306 140626031575040 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.728389 140626031575040 transformer_layer.py:468] tlayer: End windows.
I0123 20:37:36.728424 140626031575040 transformer_layer.py:472] tlayer: final FFN.
I0123 20:37:36.728491 140626031575040 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.728633 140626031575040 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:37:36.728759 140626031575040 nn_components.py:325] mlp: activation = None
I0123 20:37:36.728798 140626031575040 nn_components.py:329] mlp: final activation = None
I0123 20:37:36.730783 140626031575040 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.730878 140626031575040 nn_components.py:261] mlp: residual
I0123 20:37:36.733293 140626031575040 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.733372 140626031575040 transformer_base.py:443] tbase: final FFN
I0123 20:37:36.733481 140626031575040 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:37:36.735773 140626031575040 nn_components.py:329] mlp: final activation = None
I0123 20:37:36.737638 140626031575040 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.737740 140626031575040 nn_components.py:261] mlp: residual
I0123 20:37:36.738021 140626031575040 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.738104 140626031575040 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 20:37:36.738213 140626031575040 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:37:36.738253 140626031575040 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:37:36.738284 140626031575040 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:37:36.738348 140626031575040 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.740555 140626031575040 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:37:36.745978 140626031575040 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.746239 140626031575040 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:37:36.748956 140626031575040 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:37:36.761833 140626031575040 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:37:36.761890 140626031575040 attention.py:418] Single window, no scan.
I0123 20:37:36.761926 140626031575040 transformer_layer.py:389] tlayer: self-attention.
I0123 20:37:36.761956 140626031575040 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.762024 140626031575040 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.762587 140626031575040 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.762662 140626031575040 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.763018 140626031575040 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.763705 140626031575040 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.766153 140626031575040 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.766775 140626031575040 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.766851 140626031575040 transformer_layer.py:468] tlayer: End windows.
I0123 20:37:36.766885 140626031575040 transformer_layer.py:472] tlayer: final FFN.
I0123 20:37:36.766943 140626031575040 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.767074 140626031575040 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:37:36.767189 140626031575040 nn_components.py:325] mlp: activation = None
I0123 20:37:36.767228 140626031575040 nn_components.py:329] mlp: final activation = None
I0123 20:37:36.769153 140626031575040 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.769246 140626031575040 nn_components.py:261] mlp: residual
I0123 20:37:36.771618 140626031575040 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.771698 140626031575040 transformer_base.py:443] tbase: final FFN
I0123 20:37:36.771806 140626031575040 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:37:36.774029 140626031575040 nn_components.py:329] mlp: final activation = None
I0123 20:37:36.775876 140626031575040 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.775971 140626031575040 nn_components.py:261] mlp: residual
I0123 20:37:36.776250 140626031575040 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.776330 140626031575040 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 20:37:36.776438 140626031575040 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:37:36.776477 140626031575040 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:37:36.776509 140626031575040 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:37:36.776573 140626031575040 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.778811 140626031575040 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:37:36.784379 140626031575040 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.784639 140626031575040 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:37:36.787262 140626031575040 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:37:36.800006 140626031575040 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:37:36.800062 140626031575040 attention.py:418] Single window, no scan.
I0123 20:37:36.800097 140626031575040 transformer_layer.py:389] tlayer: self-attention.
I0123 20:37:36.800128 140626031575040 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.800190 140626031575040 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.800750 140626031575040 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.800826 140626031575040 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.801180 140626031575040 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.801872 140626031575040 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.804305 140626031575040 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.805308 140626031575040 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.805388 140626031575040 transformer_layer.py:468] tlayer: End windows.
I0123 20:37:36.805424 140626031575040 transformer_layer.py:472] tlayer: final FFN.
I0123 20:37:36.805487 140626031575040 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.805618 140626031575040 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:37:36.805739 140626031575040 nn_components.py:325] mlp: activation = None
I0123 20:37:36.805783 140626031575040 nn_components.py:329] mlp: final activation = None
I0123 20:37:36.807681 140626031575040 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.807774 140626031575040 nn_components.py:261] mlp: residual
I0123 20:37:36.810168 140626031575040 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.810248 140626031575040 transformer_base.py:443] tbase: final FFN
I0123 20:37:36.810357 140626031575040 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:37:36.812591 140626031575040 nn_components.py:329] mlp: final activation = None
I0123 20:37:36.814530 140626031575040 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.814627 140626031575040 nn_components.py:261] mlp: residual
I0123 20:37:36.814915 140626031575040 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.814996 140626031575040 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 20:37:36.815107 140626031575040 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:37:36.815146 140626031575040 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:37:36.815178 140626031575040 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:37:36.815242 140626031575040 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.817472 140626031575040 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:37:36.822964 140626031575040 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.823232 140626031575040 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:37:36.825924 140626031575040 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:37:36.838821 140626031575040 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:37:36.838878 140626031575040 attention.py:418] Single window, no scan.
I0123 20:37:36.838915 140626031575040 transformer_layer.py:389] tlayer: self-attention.
I0123 20:37:36.838947 140626031575040 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.839010 140626031575040 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.839619 140626031575040 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.839696 140626031575040 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.840057 140626031575040 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.840753 140626031575040 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.843233 140626031575040 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.843866 140626031575040 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.843946 140626031575040 transformer_layer.py:468] tlayer: End windows.
I0123 20:37:36.843981 140626031575040 transformer_layer.py:472] tlayer: final FFN.
I0123 20:37:36.844041 140626031575040 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.844174 140626031575040 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:37:36.844286 140626031575040 nn_components.py:325] mlp: activation = None
I0123 20:37:36.844331 140626031575040 nn_components.py:329] mlp: final activation = None
I0123 20:37:36.846231 140626031575040 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.846326 140626031575040 nn_components.py:261] mlp: residual
I0123 20:37:36.848778 140626031575040 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.848858 140626031575040 transformer_base.py:443] tbase: final FFN
I0123 20:37:36.848968 140626031575040 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:37:36.851204 140626031575040 nn_components.py:329] mlp: final activation = None
I0123 20:37:36.853089 140626031575040 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.853184 140626031575040 nn_components.py:261] mlp: residual
I0123 20:37:36.853471 140626031575040 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.853553 140626031575040 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 20:37:36.853669 140626031575040 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:37:36.853711 140626031575040 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:37:36.853744 140626031575040 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:37:36.853809 140626031575040 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.856061 140626031575040 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:37:36.861621 140626031575040 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.861896 140626031575040 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:37:36.864522 140626031575040 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:37:36.877448 140626031575040 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:37:36.877506 140626031575040 attention.py:418] Single window, no scan.
I0123 20:37:36.877542 140626031575040 transformer_layer.py:389] tlayer: self-attention.
I0123 20:37:36.877573 140626031575040 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.877634 140626031575040 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.878207 140626031575040 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.878288 140626031575040 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.878650 140626031575040 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.879341 140626031575040 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.881849 140626031575040 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.882529 140626031575040 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.882607 140626031575040 transformer_layer.py:468] tlayer: End windows.
I0123 20:37:36.882644 140626031575040 transformer_layer.py:472] tlayer: final FFN.
I0123 20:37:36.882704 140626031575040 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.882837 140626031575040 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:37:36.882949 140626031575040 nn_components.py:325] mlp: activation = None
I0123 20:37:36.882989 140626031575040 nn_components.py:329] mlp: final activation = None
I0123 20:37:36.884916 140626031575040 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.885010 140626031575040 nn_components.py:261] mlp: residual
I0123 20:37:36.887434 140626031575040 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.887514 140626031575040 transformer_base.py:443] tbase: final FFN
I0123 20:37:36.887623 140626031575040 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:37:36.889887 140626031575040 nn_components.py:329] mlp: final activation = None
I0123 20:37:36.891907 140626031575040 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.892003 140626031575040 nn_components.py:261] mlp: residual
I0123 20:37:36.892295 140626031575040 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.892379 140626031575040 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 20:37:36.892489 140626031575040 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:37:36.892529 140626031575040 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:37:36.892560 140626031575040 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:37:36.892624 140626031575040 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.894940 140626031575040 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:37:36.900522 140626031575040 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.900783 140626031575040 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:37:36.903558 140626031575040 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:37:36.916748 140626031575040 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:37:36.916805 140626031575040 attention.py:418] Single window, no scan.
I0123 20:37:36.916842 140626031575040 transformer_layer.py:389] tlayer: self-attention.
I0123 20:37:36.916874 140626031575040 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.916938 140626031575040 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.917550 140626031575040 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.917631 140626031575040 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.918004 140626031575040 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.918699 140626031575040 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.921183 140626031575040 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.921826 140626031575040 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.921906 140626031575040 transformer_layer.py:468] tlayer: End windows.
I0123 20:37:36.921942 140626031575040 transformer_layer.py:472] tlayer: final FFN.
I0123 20:37:36.922002 140626031575040 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.922130 140626031575040 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:37:36.922241 140626031575040 nn_components.py:325] mlp: activation = None
I0123 20:37:36.922281 140626031575040 nn_components.py:329] mlp: final activation = None
I0123 20:37:36.924187 140626031575040 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.924288 140626031575040 nn_components.py:261] mlp: residual
I0123 20:37:36.926761 140626031575040 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.926842 140626031575040 transformer_base.py:443] tbase: final FFN
I0123 20:37:36.926954 140626031575040 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:37:36.929201 140626031575040 nn_components.py:329] mlp: final activation = None
I0123 20:37:36.931064 140626031575040 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.931160 140626031575040 nn_components.py:261] mlp: residual
I0123 20:37:36.931447 140626031575040 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.931530 140626031575040 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 20:37:36.931641 140626031575040 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:37:36.931680 140626031575040 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:37:36.931712 140626031575040 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:37:36.931778 140626031575040 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.934040 140626031575040 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:37:36.939643 140626031575040 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.939903 140626031575040 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:37:36.942574 140626031575040 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:37:36.955470 140626031575040 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:37:36.955526 140626031575040 attention.py:418] Single window, no scan.
I0123 20:37:36.955562 140626031575040 transformer_layer.py:389] tlayer: self-attention.
I0123 20:37:36.955593 140626031575040 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.955653 140626031575040 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.956213 140626031575040 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.956290 140626031575040 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.956647 140626031575040 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.957337 140626031575040 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.959834 140626031575040 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.960513 140626031575040 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.960592 140626031575040 transformer_layer.py:468] tlayer: End windows.
I0123 20:37:36.960628 140626031575040 transformer_layer.py:472] tlayer: final FFN.
I0123 20:37:36.960687 140626031575040 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.960819 140626031575040 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:37:36.960929 140626031575040 nn_components.py:325] mlp: activation = None
I0123 20:37:36.960968 140626031575040 nn_components.py:329] mlp: final activation = None
I0123 20:37:36.962889 140626031575040 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.962990 140626031575040 nn_components.py:261] mlp: residual
I0123 20:37:36.965443 140626031575040 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.965523 140626031575040 transformer_base.py:443] tbase: final FFN
I0123 20:37:36.965635 140626031575040 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:37:36.967879 140626031575040 nn_components.py:329] mlp: final activation = None
I0123 20:37:36.969857 140626031575040 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.969955 140626031575040 nn_components.py:261] mlp: residual
I0123 20:37:36.970248 140626031575040 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.970331 140626031575040 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 20:37:36.970443 140626031575040 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:37:36.970483 140626031575040 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:37:36.970515 140626031575040 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:37:36.970578 140626031575040 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.972837 140626031575040 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:37:36.978390 140626031575040 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.978652 140626031575040 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:37:36.981346 140626031575040 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:37:36.994236 140626031575040 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:37:36.994293 140626031575040 attention.py:418] Single window, no scan.
I0123 20:37:36.994331 140626031575040 transformer_layer.py:389] tlayer: self-attention.
I0123 20:37:36.994363 140626031575040 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.994425 140626031575040 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.994995 140626031575040 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.995071 140626031575040 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.995421 140626031575040 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.996162 140626031575040 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.998655 140626031575040 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.999294 140626031575040 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.999372 140626031575040 transformer_layer.py:468] tlayer: End windows.
I0123 20:37:36.999408 140626031575040 transformer_layer.py:472] tlayer: final FFN.
I0123 20:37:36.999467 140626031575040 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:36.999596 140626031575040 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:37:36.999707 140626031575040 nn_components.py:325] mlp: activation = None
I0123 20:37:36.999746 140626031575040 nn_components.py:329] mlp: final activation = None
I0123 20:37:37.001654 140626031575040 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:37.001750 140626031575040 nn_components.py:261] mlp: residual
I0123 20:37:37.004175 140626031575040 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:37.004255 140626031575040 transformer_base.py:443] tbase: final FFN
I0123 20:37:37.004366 140626031575040 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:37:37.006678 140626031575040 nn_components.py:329] mlp: final activation = None
I0123 20:37:37.008557 140626031575040 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:37.008653 140626031575040 nn_components.py:261] mlp: residual
I0123 20:37:37.008938 140626031575040 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:37.009027 140626031575040 decoder_stack.py:344] dstack: Final layernorm.
I0123 20:37:37.011913 140626031575040 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 20:37:37.067540 140626031575040 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.067629 140626031575040 decoder_stack.py:333] dstack: autoregressive generator.
I0123 20:37:37.067684 140626031575040 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 20:37:37.067791 140626031575040 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:37:37.067830 140626031575040 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:37:37.067862 140626031575040 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:37:37.067927 140626031575040 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.070597 140626031575040 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:37:37.076019 140626031575040 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.076283 140626031575040 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:37:37.078873 140626031575040 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:37:37.091376 140626031575040 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:37:37.091434 140626031575040 attention.py:418] Single window, no scan.
I0123 20:37:37.091472 140626031575040 transformer_layer.py:389] tlayer: self-attention.
I0123 20:37:37.091504 140626031575040 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.091567 140626031575040 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.092130 140626031575040 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.092208 140626031575040 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.092563 140626031575040 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.093238 140626031575040 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.095736 140626031575040 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.096362 140626031575040 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.096440 140626031575040 transformer_layer.py:468] tlayer: End windows.
I0123 20:37:37.096477 140626031575040 transformer_layer.py:472] tlayer: final FFN.
I0123 20:37:37.096537 140626031575040 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.096666 140626031575040 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:37:37.096783 140626031575040 nn_components.py:325] mlp: activation = None
I0123 20:37:37.096823 140626031575040 nn_components.py:329] mlp: final activation = None
I0123 20:37:37.098681 140626031575040 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.098776 140626031575040 nn_components.py:261] mlp: residual
I0123 20:37:37.101135 140626031575040 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.101215 140626031575040 transformer_base.py:443] tbase: final FFN
I0123 20:37:37.101325 140626031575040 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:37:37.103592 140626031575040 nn_components.py:329] mlp: final activation = None
I0123 20:37:37.105446 140626031575040 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.105543 140626031575040 nn_components.py:261] mlp: residual
I0123 20:37:37.105837 140626031575040 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.105920 140626031575040 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 20:37:37.106029 140626031575040 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:37:37.106069 140626031575040 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:37:37.106101 140626031575040 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:37:37.106165 140626031575040 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.108372 140626031575040 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:37:37.113785 140626031575040 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.114044 140626031575040 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:37:37.116698 140626031575040 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:37:37.129089 140626031575040 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:37:37.129146 140626031575040 attention.py:418] Single window, no scan.
I0123 20:37:37.129182 140626031575040 transformer_layer.py:389] tlayer: self-attention.
I0123 20:37:37.129214 140626031575040 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.129277 140626031575040 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.129842 140626031575040 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.129920 140626031575040 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.130274 140626031575040 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.130954 140626031575040 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.133432 140626031575040 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.134067 140626031575040 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.134146 140626031575040 transformer_layer.py:468] tlayer: End windows.
I0123 20:37:37.134182 140626031575040 transformer_layer.py:472] tlayer: final FFN.
I0123 20:37:37.134242 140626031575040 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.134371 140626031575040 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:37:37.134481 140626031575040 nn_components.py:325] mlp: activation = None
I0123 20:37:37.134527 140626031575040 nn_components.py:329] mlp: final activation = None
I0123 20:37:37.136383 140626031575040 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.136477 140626031575040 nn_components.py:261] mlp: residual
I0123 20:37:37.138870 140626031575040 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.138951 140626031575040 transformer_base.py:443] tbase: final FFN
I0123 20:37:37.139062 140626031575040 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:37:37.141330 140626031575040 nn_components.py:329] mlp: final activation = None
I0123 20:37:37.143181 140626031575040 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.143278 140626031575040 nn_components.py:261] mlp: residual
I0123 20:37:37.143564 140626031575040 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.143645 140626031575040 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 20:37:37.143754 140626031575040 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:37:37.143793 140626031575040 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:37:37.143825 140626031575040 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:37:37.143888 140626031575040 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.146097 140626031575040 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:37:37.151489 140626031575040 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.151746 140626031575040 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:37:37.154376 140626031575040 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:37:37.166771 140626031575040 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:37:37.166829 140626031575040 attention.py:418] Single window, no scan.
I0123 20:37:37.166866 140626031575040 transformer_layer.py:389] tlayer: self-attention.
I0123 20:37:37.166898 140626031575040 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.166960 140626031575040 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.167519 140626031575040 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.167596 140626031575040 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.167947 140626031575040 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.168625 140626031575040 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.171133 140626031575040 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.171754 140626031575040 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.171832 140626031575040 transformer_layer.py:468] tlayer: End windows.
I0123 20:37:37.171868 140626031575040 transformer_layer.py:472] tlayer: final FFN.
I0123 20:37:37.171928 140626031575040 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.172057 140626031575040 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:37:37.172169 140626031575040 nn_components.py:325] mlp: activation = None
I0123 20:37:37.172208 140626031575040 nn_components.py:329] mlp: final activation = None
I0123 20:37:37.174058 140626031575040 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.174153 140626031575040 nn_components.py:261] mlp: residual
I0123 20:37:37.176509 140626031575040 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.176589 140626031575040 transformer_base.py:443] tbase: final FFN
I0123 20:37:37.176699 140626031575040 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:37:37.179386 140626031575040 nn_components.py:329] mlp: final activation = None
I0123 20:37:37.181229 140626031575040 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.181326 140626031575040 nn_components.py:261] mlp: residual
I0123 20:37:37.181611 140626031575040 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.181704 140626031575040 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 20:37:37.181817 140626031575040 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:37:37.181857 140626031575040 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:37:37.181889 140626031575040 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:37:37.181955 140626031575040 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.184165 140626031575040 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:37:37.189553 140626031575040 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.189823 140626031575040 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:37:37.192498 140626031575040 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:37:37.205086 140626031575040 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:37:37.205142 140626031575040 attention.py:418] Single window, no scan.
I0123 20:37:37.205181 140626031575040 transformer_layer.py:389] tlayer: self-attention.
I0123 20:37:37.205225 140626031575040 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.205290 140626031575040 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.205853 140626031575040 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.205929 140626031575040 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.206284 140626031575040 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.206964 140626031575040 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.209484 140626031575040 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.210107 140626031575040 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.210185 140626031575040 transformer_layer.py:468] tlayer: End windows.
I0123 20:37:37.210219 140626031575040 transformer_layer.py:472] tlayer: final FFN.
I0123 20:37:37.210277 140626031575040 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.210402 140626031575040 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:37:37.210509 140626031575040 nn_components.py:325] mlp: activation = None
I0123 20:37:37.210549 140626031575040 nn_components.py:329] mlp: final activation = None
I0123 20:37:37.212437 140626031575040 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.212529 140626031575040 nn_components.py:261] mlp: residual
I0123 20:37:37.214913 140626031575040 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.214991 140626031575040 transformer_base.py:443] tbase: final FFN
I0123 20:37:37.215099 140626031575040 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:37:37.217375 140626031575040 nn_components.py:329] mlp: final activation = None
I0123 20:37:37.219243 140626031575040 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.219338 140626031575040 nn_components.py:261] mlp: residual
I0123 20:37:37.219619 140626031575040 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.219699 140626031575040 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 20:37:37.219806 140626031575040 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:37:37.219844 140626031575040 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:37:37.219874 140626031575040 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:37:37.219936 140626031575040 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.222166 140626031575040 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:37:37.227599 140626031575040 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.227859 140626031575040 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:37:37.230540 140626031575040 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:37:37.243257 140626031575040 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:37:37.243310 140626031575040 attention.py:418] Single window, no scan.
I0123 20:37:37.243344 140626031575040 transformer_layer.py:389] tlayer: self-attention.
I0123 20:37:37.243374 140626031575040 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.243434 140626031575040 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.243992 140626031575040 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.244067 140626031575040 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.244425 140626031575040 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.245110 140626031575040 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.247635 140626031575040 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.248255 140626031575040 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.248331 140626031575040 transformer_layer.py:468] tlayer: End windows.
I0123 20:37:37.248365 140626031575040 transformer_layer.py:472] tlayer: final FFN.
I0123 20:37:37.248423 140626031575040 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.248549 140626031575040 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:37:37.248658 140626031575040 nn_components.py:325] mlp: activation = None
I0123 20:37:37.248695 140626031575040 nn_components.py:329] mlp: final activation = None
I0123 20:37:37.250593 140626031575040 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.250692 140626031575040 nn_components.py:261] mlp: residual
I0123 20:37:37.253105 140626031575040 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.253182 140626031575040 transformer_base.py:443] tbase: final FFN
I0123 20:37:37.253291 140626031575040 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:37:37.255606 140626031575040 nn_components.py:329] mlp: final activation = None
I0123 20:37:37.257472 140626031575040 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.257567 140626031575040 nn_components.py:261] mlp: residual
I0123 20:37:37.257861 140626031575040 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.257941 140626031575040 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 20:37:37.258049 140626031575040 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:37:37.258088 140626031575040 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:37:37.258119 140626031575040 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:37:37.258182 140626031575040 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.260416 140626031575040 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:37:37.265832 140626031575040 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.266090 140626031575040 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:37:37.268775 140626031575040 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:37:37.281512 140626031575040 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:37:37.281566 140626031575040 attention.py:418] Single window, no scan.
I0123 20:37:37.281601 140626031575040 transformer_layer.py:389] tlayer: self-attention.
I0123 20:37:37.281631 140626031575040 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.281699 140626031575040 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.282251 140626031575040 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.282330 140626031575040 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.282688 140626031575040 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.283372 140626031575040 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.285920 140626031575040 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.286544 140626031575040 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.286619 140626031575040 transformer_layer.py:468] tlayer: End windows.
I0123 20:37:37.286654 140626031575040 transformer_layer.py:472] tlayer: final FFN.
I0123 20:37:37.286710 140626031575040 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.286839 140626031575040 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:37:37.286946 140626031575040 nn_components.py:325] mlp: activation = None
I0123 20:37:37.286984 140626031575040 nn_components.py:329] mlp: final activation = None
I0123 20:37:37.288852 140626031575040 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.288949 140626031575040 nn_components.py:261] mlp: residual
I0123 20:37:37.291324 140626031575040 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.291403 140626031575040 transformer_base.py:443] tbase: final FFN
I0123 20:37:37.291513 140626031575040 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:37:37.294208 140626031575040 nn_components.py:329] mlp: final activation = None
I0123 20:37:37.296072 140626031575040 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.296165 140626031575040 nn_components.py:261] mlp: residual
I0123 20:37:37.296452 140626031575040 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.296532 140626031575040 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 20:37:37.296640 140626031575040 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:37:37.296679 140626031575040 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:37:37.296709 140626031575040 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:37:37.296772 140626031575040 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.298998 140626031575040 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:37:37.304414 140626031575040 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.304668 140626031575040 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:37:37.307351 140626031575040 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:37:37.319963 140626031575040 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:37:37.320018 140626031575040 attention.py:418] Single window, no scan.
I0123 20:37:37.320053 140626031575040 transformer_layer.py:389] tlayer: self-attention.
I0123 20:37:37.320083 140626031575040 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.320145 140626031575040 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.320710 140626031575040 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.320785 140626031575040 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.321137 140626031575040 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.321830 140626031575040 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.324366 140626031575040 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.325001 140626031575040 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.325079 140626031575040 transformer_layer.py:468] tlayer: End windows.
I0123 20:37:37.325114 140626031575040 transformer_layer.py:472] tlayer: final FFN.
I0123 20:37:37.325172 140626031575040 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.325299 140626031575040 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:37:37.325413 140626031575040 nn_components.py:325] mlp: activation = None
I0123 20:37:37.325450 140626031575040 nn_components.py:329] mlp: final activation = None
I0123 20:37:37.327345 140626031575040 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.327439 140626031575040 nn_components.py:261] mlp: residual
I0123 20:37:37.329836 140626031575040 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.329914 140626031575040 transformer_base.py:443] tbase: final FFN
I0123 20:37:37.330024 140626031575040 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:37:37.332298 140626031575040 nn_components.py:329] mlp: final activation = None
I0123 20:37:37.334157 140626031575040 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.334253 140626031575040 nn_components.py:261] mlp: residual
I0123 20:37:37.334537 140626031575040 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.334616 140626031575040 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 20:37:37.334723 140626031575040 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:37:37.334761 140626031575040 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:37:37.334791 140626031575040 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:37:37.334853 140626031575040 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.337056 140626031575040 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:37:37.342538 140626031575040 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.342800 140626031575040 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:37:37.345457 140626031575040 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:37:37.358138 140626031575040 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:37:37.358193 140626031575040 attention.py:418] Single window, no scan.
I0123 20:37:37.358228 140626031575040 transformer_layer.py:389] tlayer: self-attention.
I0123 20:37:37.358259 140626031575040 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.358324 140626031575040 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.358883 140626031575040 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.358957 140626031575040 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.359310 140626031575040 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.359998 140626031575040 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.362544 140626031575040 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.363173 140626031575040 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.363249 140626031575040 transformer_layer.py:468] tlayer: End windows.
I0123 20:37:37.363283 140626031575040 transformer_layer.py:472] tlayer: final FFN.
I0123 20:37:37.363343 140626031575040 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.363468 140626031575040 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:37:37.363576 140626031575040 nn_components.py:325] mlp: activation = None
I0123 20:37:37.363613 140626031575040 nn_components.py:329] mlp: final activation = None
I0123 20:37:37.365474 140626031575040 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.365566 140626031575040 nn_components.py:261] mlp: residual
I0123 20:37:37.368005 140626031575040 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.368088 140626031575040 transformer_base.py:443] tbase: final FFN
I0123 20:37:37.368196 140626031575040 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:37:37.370485 140626031575040 nn_components.py:329] mlp: final activation = None
I0123 20:37:37.372344 140626031575040 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.372439 140626031575040 nn_components.py:261] mlp: residual
I0123 20:37:37.372722 140626031575040 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.372802 140626031575040 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 20:37:37.372910 140626031575040 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:37:37.372948 140626031575040 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:37:37.372978 140626031575040 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:37:37.373043 140626031575040 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.375271 140626031575040 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:37:37.380724 140626031575040 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.380984 140626031575040 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:37:37.383666 140626031575040 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:37:37.396395 140626031575040 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:37:37.396450 140626031575040 attention.py:418] Single window, no scan.
I0123 20:37:37.396486 140626031575040 transformer_layer.py:389] tlayer: self-attention.
I0123 20:37:37.396518 140626031575040 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.396579 140626031575040 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.397151 140626031575040 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.397227 140626031575040 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.397581 140626031575040 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.398280 140626031575040 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.400804 140626031575040 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.401425 140626031575040 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.401501 140626031575040 transformer_layer.py:468] tlayer: End windows.
I0123 20:37:37.401535 140626031575040 transformer_layer.py:472] tlayer: final FFN.
I0123 20:37:37.401592 140626031575040 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.401727 140626031575040 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:37:37.401835 140626031575040 nn_components.py:325] mlp: activation = None
I0123 20:37:37.401873 140626031575040 nn_components.py:329] mlp: final activation = None
I0123 20:37:37.403745 140626031575040 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.403837 140626031575040 nn_components.py:261] mlp: residual
I0123 20:37:37.406224 140626031575040 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.406309 140626031575040 transformer_base.py:443] tbase: final FFN
I0123 20:37:37.406419 140626031575040 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:37:37.409108 140626031575040 nn_components.py:329] mlp: final activation = None
I0123 20:37:37.410999 140626031575040 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.411095 140626031575040 nn_components.py:261] mlp: residual
I0123 20:37:37.411380 140626031575040 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.411461 140626031575040 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 20:37:37.411569 140626031575040 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:37:37.411607 140626031575040 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:37:37.411637 140626031575040 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:37:37.411700 140626031575040 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.413956 140626031575040 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:37:37.419411 140626031575040 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.419670 140626031575040 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:37:37.422372 140626031575040 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:37:37.435179 140626031575040 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:37:37.435233 140626031575040 attention.py:418] Single window, no scan.
I0123 20:37:37.435268 140626031575040 transformer_layer.py:389] tlayer: self-attention.
I0123 20:37:37.435299 140626031575040 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.435360 140626031575040 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.435927 140626031575040 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.436003 140626031575040 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.436363 140626031575040 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.437047 140626031575040 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.439599 140626031575040 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.440224 140626031575040 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.440301 140626031575040 transformer_layer.py:468] tlayer: End windows.
I0123 20:37:37.440335 140626031575040 transformer_layer.py:472] tlayer: final FFN.
I0123 20:37:37.440392 140626031575040 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.440518 140626031575040 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:37:37.440624 140626031575040 nn_components.py:325] mlp: activation = None
I0123 20:37:37.440662 140626031575040 nn_components.py:329] mlp: final activation = None
I0123 20:37:37.443061 140626031575040 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.443156 140626031575040 nn_components.py:261] mlp: residual
I0123 20:37:37.445524 140626031575040 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.445603 140626031575040 transformer_base.py:443] tbase: final FFN
I0123 20:37:37.445723 140626031575040 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:37:37.447976 140626031575040 nn_components.py:329] mlp: final activation = None
I0123 20:37:37.449818 140626031575040 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.449912 140626031575040 nn_components.py:261] mlp: residual
I0123 20:37:37.450196 140626031575040 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.450276 140626031575040 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 20:37:37.450383 140626031575040 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:37:37.450421 140626031575040 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:37:37.450452 140626031575040 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:37:37.450514 140626031575040 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.452733 140626031575040 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:37:37.458142 140626031575040 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.458404 140626031575040 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:37:37.461107 140626031575040 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:37:37.473788 140626031575040 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:37:37.473842 140626031575040 attention.py:418] Single window, no scan.
I0123 20:37:37.473877 140626031575040 transformer_layer.py:389] tlayer: self-attention.
I0123 20:37:37.473908 140626031575040 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.473969 140626031575040 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.474534 140626031575040 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.474609 140626031575040 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.474961 140626031575040 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.475650 140626031575040 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.478206 140626031575040 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.478829 140626031575040 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.478904 140626031575040 transformer_layer.py:468] tlayer: End windows.
I0123 20:37:37.478938 140626031575040 transformer_layer.py:472] tlayer: final FFN.
I0123 20:37:37.478996 140626031575040 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.479122 140626031575040 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:37:37.479233 140626031575040 nn_components.py:325] mlp: activation = None
I0123 20:37:37.479271 140626031575040 nn_components.py:329] mlp: final activation = None
I0123 20:37:37.481134 140626031575040 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.481226 140626031575040 nn_components.py:261] mlp: residual
I0123 20:37:37.483620 140626031575040 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.483705 140626031575040 transformer_base.py:443] tbase: final FFN
I0123 20:37:37.483813 140626031575040 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:37:37.486127 140626031575040 nn_components.py:329] mlp: final activation = None
I0123 20:37:37.487972 140626031575040 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.488068 140626031575040 nn_components.py:261] mlp: residual
I0123 20:37:37.488351 140626031575040 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.488431 140626031575040 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 20:37:37.488539 140626031575040 transformer_layer.py:154] tlayer: recurrent = False
I0123 20:37:37.488577 140626031575040 transformer_layer.py:155] tlayer: compute_importance = False
I0123 20:37:37.488608 140626031575040 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 20:37:37.488671 140626031575040 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.490912 140626031575040 transformer_base.py:161] kvq: pre_attn dropout.
I0123 20:37:37.496365 140626031575040 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.496623 140626031575040 transformer_base.py:194] kvq: normalize keys, queries.
I0123 20:37:37.499307 140626031575040 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 20:37:37.512006 140626031575040 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 20:37:37.512061 140626031575040 attention.py:418] Single window, no scan.
I0123 20:37:37.512096 140626031575040 transformer_layer.py:389] tlayer: self-attention.
I0123 20:37:37.512127 140626031575040 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.512188 140626031575040 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.512748 140626031575040 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.512824 140626031575040 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.513176 140626031575040 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.513882 140626031575040 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.516395 140626031575040 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.517016 140626031575040 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.517091 140626031575040 transformer_layer.py:468] tlayer: End windows.
I0123 20:37:37.517126 140626031575040 transformer_layer.py:472] tlayer: final FFN.
I0123 20:37:37.517183 140626031575040 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.517313 140626031575040 transformer_base.py:410] tbase: post-attention MLP.
I0123 20:37:37.517425 140626031575040 nn_components.py:325] mlp: activation = None
I0123 20:37:37.517463 140626031575040 nn_components.py:329] mlp: final activation = None
I0123 20:37:37.519338 140626031575040 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.519431 140626031575040 nn_components.py:261] mlp: residual
I0123 20:37:37.521815 140626031575040 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.521894 140626031575040 transformer_base.py:443] tbase: final FFN
I0123 20:37:37.522001 140626031575040 nn_components.py:320] mlp: hidden 4096, relu
I0123 20:37:37.524654 140626031575040 nn_components.py:329] mlp: final activation = None
I0123 20:37:37.526528 140626031575040 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.526623 140626031575040 nn_components.py:261] mlp: residual
I0123 20:37:37.526908 140626031575040 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:37.526993 140626031575040 decoder_stack.py:344] dstack: Final layernorm.
I0123 20:37:37.529823 140626031575040 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 20:37:41.975532 140626031575040 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
I0123 20:37:42.502693 140626031575040 training_loop.py:409] No working directory specified.
I0123 20:37:42.502814 140626031575040 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0123 20:37:42.503579 140626031575040 checkpoints.py:1062] Restoring legacy Flax checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0123 20:37:45.884545 140626031575040 training_loop.py:447] Only restoring trainable parameters.
I0123 20:37:45.885195 140626031575040 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0123 20:37:45.885278 140626031575040 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0123 20:37:45.885329 140626031575040 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 20:37:45.885375 140626031575040 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 20:37:45.885417 140626031575040 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:37:45.885457 140626031575040 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 20:37:45.885497 140626031575040 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:37:45.885535 140626031575040 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:37:45.885574 140626031575040 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 20:37:45.885611 140626031575040 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 20:37:45.885658 140626031575040 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:37:45.885700 140626031575040 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 20:37:45.885739 140626031575040 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 20:37:45.885782 140626031575040 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 20:37:45.885819 140626031575040 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:37:45.885856 140626031575040 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 20:37:45.885893 140626031575040 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:37:45.885930 140626031575040 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:37:45.885967 140626031575040 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 20:37:45.886003 140626031575040 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 20:37:45.886052 140626031575040 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:37:45.886090 140626031575040 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 20:37:45.886128 140626031575040 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 20:37:45.886164 140626031575040 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 20:37:45.886201 140626031575040 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:37:45.886237 140626031575040 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 20:37:45.886273 140626031575040 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:37:45.886309 140626031575040 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:37:45.886344 140626031575040 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 20:37:45.886381 140626031575040 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 20:37:45.886416 140626031575040 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:37:45.886452 140626031575040 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 20:37:45.886487 140626031575040 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 20:37:45.886523 140626031575040 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 20:37:45.886559 140626031575040 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:37:45.886595 140626031575040 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 20:37:45.886631 140626031575040 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:37:45.886666 140626031575040 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:37:45.886701 140626031575040 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 20:37:45.886736 140626031575040 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 20:37:45.886773 140626031575040 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:37:45.886808 140626031575040 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 20:37:45.886844 140626031575040 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 20:37:45.886880 140626031575040 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 20:37:45.886916 140626031575040 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:37:45.886952 140626031575040 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 20:37:45.886993 140626031575040 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:37:45.887031 140626031575040 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:37:45.887067 140626031575040 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 20:37:45.887103 140626031575040 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 20:37:45.887139 140626031575040 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:37:45.887175 140626031575040 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 20:37:45.887211 140626031575040 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 20:37:45.887247 140626031575040 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 20:37:45.887284 140626031575040 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:37:45.887320 140626031575040 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 20:37:45.887356 140626031575040 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:37:45.887392 140626031575040 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:37:45.887428 140626031575040 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 20:37:45.887465 140626031575040 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 20:37:45.887501 140626031575040 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:37:45.887538 140626031575040 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 20:37:45.887574 140626031575040 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 20:37:45.887611 140626031575040 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 20:37:45.887647 140626031575040 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:37:45.887683 140626031575040 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 20:37:45.887719 140626031575040 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:37:45.887755 140626031575040 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:37:45.887790 140626031575040 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 20:37:45.887826 140626031575040 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 20:37:45.887861 140626031575040 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:37:45.887897 140626031575040 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 20:37:45.887934 140626031575040 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 20:37:45.887976 140626031575040 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 20:37:45.888015 140626031575040 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:37:45.888051 140626031575040 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 20:37:45.888088 140626031575040 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:37:45.888125 140626031575040 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:37:45.888162 140626031575040 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 20:37:45.888199 140626031575040 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 20:37:45.888236 140626031575040 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:37:45.888272 140626031575040 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 20:37:45.888308 140626031575040 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 20:37:45.888345 140626031575040 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 20:37:45.888381 140626031575040 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:37:45.888417 140626031575040 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 20:37:45.888453 140626031575040 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:37:45.888488 140626031575040 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:37:45.888523 140626031575040 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 20:37:45.888559 140626031575040 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 20:37:45.888594 140626031575040 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:37:45.888629 140626031575040 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 20:37:45.888665 140626031575040 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 20:37:45.888700 140626031575040 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 20:37:45.888735 140626031575040 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:37:45.888772 140626031575040 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 20:37:45.888807 140626031575040 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:37:45.888843 140626031575040 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:37:45.888879 140626031575040 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 20:37:45.888914 140626031575040 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 20:37:45.888954 140626031575040 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:37:45.888991 140626031575040 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 20:37:45.889027 140626031575040 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 20:37:45.889063 140626031575040 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 20:37:45.889099 140626031575040 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:37:45.889133 140626031575040 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 20:37:45.889169 140626031575040 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:37:45.889205 140626031575040 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:37:45.889240 140626031575040 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 20:37:45.889277 140626031575040 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 20:37:45.889315 140626031575040 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:37:45.889353 140626031575040 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 20:37:45.889389 140626031575040 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 20:37:45.889426 140626031575040 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 20:37:45.889464 140626031575040 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:37:45.889500 140626031575040 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 20:37:45.889537 140626031575040 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:37:45.889573 140626031575040 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:37:45.889610 140626031575040 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 20:37:45.889655 140626031575040 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 20:37:45.889695 140626031575040 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 20:37:45.889734 140626031575040 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 20:37:45.889763 140626031575040 training_loop.py:725] Total parameters: 152072288
I0123 20:37:45.889987 140626031575040 training_loop.py:739] Total state size: 0
I0123 20:37:45.915717 140626031575040 training_loop.py:492] Training loop: creating task for mode beam_search
I0123 20:37:45.915944 140626031575040 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0123 20:37:45.916416 140626031575040 training_loop.py:652] Compiling mode beam_search with jit.
I0123 20:37:45.916759 140626031575040 training_loop.py:89] registering functions: dict_keys([])
I0123 20:37:45.933558 140626031575040 graph.py:499] a b c = triangle a b c; d = on_line d c a; e = on_line e b a; f = on_line f d e; g = foot g f b a; h = mirror h f g; i = foot i f c a; j = mirror j f i; k = foot k f c b; l = mirror l f k; m = circle m h j l; n = on_circle n m l; o = midpoint o n f; p = lc_tangent p o f, on_line p c f; q = on_line q o p, on_line q c b; r = on_line r o p, on_line r c a ? eqangle f q f b f r f a
I0123 20:37:52.435397 140626031575040 ddar.py:60] Depth 1/1000 time = 6.446441411972046
I0123 20:38:09.656404 140626031575040 ddar.py:60] Depth 2/1000 time = 17.220566511154175
I0123 20:38:31.920991 140626031575040 ddar.py:60] Depth 3/1000 time = 22.263745069503784
I0123 20:38:58.059761 140626031575040 ddar.py:60] Depth 4/1000 time = 26.137759685516357
I0123 20:39:23.643659 140626031575040 ddar.py:60] Depth 5/1000 time = 25.581904649734497
I0123 20:39:50.491174 140626031575040 ddar.py:60] Depth 6/1000 time = 26.599199056625366
I0123 20:39:50.498460 140626031575040 alphageometry.py:191] 
==========================
 * From theorem premises:
A B C F G H I J K L M N O P Q R : Points
B,A,G are collinear [00]
FG  AB [01]
F,H,G are collinear [02]
GF = GH [03]
C,I,A are collinear [04]
FI  AC [05]
F,J,I are collinear [06]
IF = IJ [07]
FK  BC [08]
B,K,C are collinear [09]
F,L,K are collinear [10]
KF = KL [11]
MJ = ML [12]
MH = MJ [13]
MN = ML [14]
F,O,N are collinear [15]
ON = OF [16]
OP  OF [17]
P,O,Q are collinear [18]
B,C,Q are collinear [19]
C,R,A are collinear [20]
P,R,O are collinear [21]

 * Auxiliary Constructions:
: Points


 * Proof steps:
001. OP  OF [17] & F,O,N are collinear [15]   OP  FN [22]
002. MJ = ML [12] & MN = ML [14] & MH = MJ [13]   J,L,H,N are concyclic [23]
003. J,L,H,N are concyclic [23]   JHL = JNL [24]
004. B,A,G are collinear [00] & I,A,C are collinear [04] & FI  AC [05] & FG  AB [01]   FGA = FIA [25]
005. FGA = FIA [25]   F,I,A,G are concyclic [26]
006. F,I,A,G are concyclic [26]   FAI = FGI [27]
007. F,H,G are collinear [02] & GF = GH [03]   G is midpoint of FH [28]
008. F,J,I are collinear [06] & IF = IJ [07]   I is midpoint of FJ [29]
009. G is midpoint of FH [28] & I is midpoint of FJ [29]   GI  HJ [30]
010. FAI = FGI [27] & C,I,A are collinear [04] & GI  HJ [30]   FAC = (FG-HJ) [31]
011. B,A,G are collinear [00] & B,K,C are collinear [09] & FK  BC [08] & FG  AB [01]   FGB = FKB [32]
012. FGB = FKB [32]   F,B,K,G are concyclic [33]
013. F,B,K,G are concyclic [33]   FBK = FGK [34]
014. F,L,K are collinear [10] & KF = KL [11]   K is midpoint of FL [35]
015. G is midpoint of FH [28] & K is midpoint of FL [35]   GK  HL [36]
016. FBK = FGK [34] & B,K,C are collinear [09] & GK  HL [36]   FBC = (FG-HL) [37]
017. C,R,A are collinear [20] & C,I,A are collinear [04] & F,O,N are collinear [15] & P,R,O are collinear [21] & OP  FN [22] & FI  AC [05]   FIR = FOR [38]
018. FIR = FOR [38]   F,I,R,O are concyclic [39]
019. F,I,R,O are concyclic [39]   FRI = FOI [40]
020. F,O,N are collinear [15] & ON = OF [16]   O is midpoint of FN [41]
021. I is midpoint of FJ [29] & O is midpoint of FN [41]   IO  JN [42]
022. FRI = FOI [40] & C,R,A are collinear [20] & C,I,A are collinear [04] & F,O,N are collinear [15] & IO  JN [42]   (FR-AC) = FNJ [43]
023. P,O,Q are collinear [18] & F,O,N are collinear [15] & B,C,Q are collinear [19] & B,K,C are collinear [09] & FK  BC [08] & OP  FN [22]   QOF = QKF [44]
024. QOF = QKF [44]   F,O,K,Q are concyclic [45]
025. F,O,K,Q are concyclic [45]   FKO = FQO [46]
026. K is midpoint of FL [35] & O is midpoint of FN [41]   KO  LN [47]
027. FKO = FQO [46] & P,O,Q are collinear [18] & KO  LN [47]   (FK-LN) = (FQ-OP) [48]
028. FK  BC [08] & OP  FN [22] & JHL = JNL [24] & FAC = (FG-HJ) [31] & FBC = (FG-HL) [37] & (FR-AC) = FNJ [43] & (FK-LN) = (FQ-OP) [48] (Angle chase)  AFR = BFQ
==========================

