I0123 12:41:11.383858 139856449679360 inference_utils.py:69] Parsing gin configuration.
I0123 12:41:11.383960 139856449679360 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0123 12:41:11.384169 139856449679360 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0123 12:41:11.384203 139856449679360 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0123 12:41:11.384233 139856449679360 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0123 12:41:11.384261 139856449679360 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0123 12:41:11.384288 139856449679360 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0123 12:41:11.384313 139856449679360 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0123 12:41:11.384339 139856449679360 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0123 12:41:11.384364 139856449679360 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=32
I0123 12:41:11.384389 139856449679360 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0123 12:41:11.384416 139856449679360 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0123 12:41:11.384461 139856449679360 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0123 12:41:11.384597 139856449679360 resource_reader.py:55] Path not found: base_htrans.gin
I0123 12:41:11.384806 139856449679360 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0123 12:41:11.384912 139856449679360 resource_reader.py:55] Path not found: trainer_configuration.gin
I0123 12:41:11.391264 139856449679360 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0123 12:41:11.391391 139856449679360 resource_reader.py:55] Path not found: size/medium_150M.gin
I0123 12:41:11.391716 139856449679360 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0123 12:41:11.391821 139856449679360 resource_reader.py:55] Path not found: options/positions_t5.gin
I0123 12:41:11.392103 139856449679360 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0123 12:41:11.392204 139856449679360 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0123 12:41:11.392612 139856449679360 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0123 12:41:11.392712 139856449679360 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0123 12:41:11.396439 139856449679360 training_loop.py:334] ==== Training loop: initializing model ====
I0123 12:41:11.489482 139856449679360 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0123 12:41:11.490226 139856449679360 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0123 12:41:11.497145 139856449679360 training_loop.py:335] Process 0 of 1
I0123 12:41:11.497202 139856449679360 training_loop.py:336] Local device count = 1
I0123 12:41:11.497242 139856449679360 training_loop.py:337] Number of replicas = 1
I0123 12:41:11.497275 139856449679360 training_loop.py:339] Using random number seed 42
I0123 12:41:11.975338 139856449679360 training_loop.py:359] Initializing the model.
I0123 12:41:12.374617 139856449679360 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.374847 139856449679360 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 12:41:12.374948 139856449679360 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:41:12.375079 139856449679360 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:41:12.375161 139856449679360 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:41:12.375243 139856449679360 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:41:12.375316 139856449679360 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:41:12.375387 139856449679360 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:41:12.375456 139856449679360 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:41:12.375525 139856449679360 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:41:12.375594 139856449679360 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:41:12.375662 139856449679360 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:41:12.375730 139856449679360 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:41:12.375798 139856449679360 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:41:12.375837 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:12.375882 139856449679360 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 12:41:12.375996 139856449679360 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:41:12.376036 139856449679360 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:41:12.376067 139856449679360 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:41:12.378043 139856449679360 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.383268 139856449679360 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:41:12.393915 139856449679360 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.394191 139856449679360 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:41:12.398524 139856449679360 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:41:12.409093 139856449679360 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:41:12.409152 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:12.409189 139856449679360 transformer_layer.py:389] tlayer: self-attention.
I0123 12:41:12.409222 139856449679360 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.409285 139856449679360 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.410465 139856449679360 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.410548 139856449679360 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.411254 139856449679360 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.413711 139856449679360 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.419931 139856449679360 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.421184 139856449679360 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.421266 139856449679360 transformer_layer.py:468] tlayer: End windows.
I0123 12:41:12.421301 139856449679360 transformer_layer.py:472] tlayer: final FFN.
I0123 12:41:12.421362 139856449679360 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.421491 139856449679360 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:41:12.421833 139856449679360 nn_components.py:325] mlp: activation = None
I0123 12:41:12.421881 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:12.423778 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.423882 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:12.427694 139856449679360 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.427839 139856449679360 transformer_base.py:443] tbase: final FFN
I0123 12:41:12.428500 139856449679360 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:41:12.438646 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:12.447416 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.447517 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:12.447816 139856449679360 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.447899 139856449679360 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 12:41:12.448013 139856449679360 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:41:12.448053 139856449679360 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:41:12.448084 139856449679360 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:41:12.450043 139856449679360 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.452770 139856449679360 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:41:12.458447 139856449679360 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.458713 139856449679360 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:41:12.461377 139856449679360 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:41:12.465322 139856449679360 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:41:12.465382 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:12.465419 139856449679360 transformer_layer.py:389] tlayer: self-attention.
I0123 12:41:12.465451 139856449679360 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.465516 139856449679360 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.466115 139856449679360 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.466196 139856449679360 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.466567 139856449679360 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.467350 139856449679360 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.469862 139856449679360 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.470542 139856449679360 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.470620 139856449679360 transformer_layer.py:468] tlayer: End windows.
I0123 12:41:12.470655 139856449679360 transformer_layer.py:472] tlayer: final FFN.
I0123 12:41:12.470714 139856449679360 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.470843 139856449679360 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:41:12.471174 139856449679360 nn_components.py:325] mlp: activation = None
I0123 12:41:12.471218 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:12.473128 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.473221 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:12.475751 139856449679360 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.475831 139856449679360 transformer_base.py:443] tbase: final FFN
I0123 12:41:12.476316 139856449679360 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:41:12.478579 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:12.480459 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.480552 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:12.480844 139856449679360 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.480923 139856449679360 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 12:41:12.481032 139856449679360 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:41:12.481072 139856449679360 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:41:12.481103 139856449679360 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:41:12.483019 139856449679360 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.485380 139856449679360 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:41:12.491347 139856449679360 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.491613 139856449679360 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:41:12.494283 139856449679360 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:41:12.498168 139856449679360 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:41:12.498223 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:12.498260 139856449679360 transformer_layer.py:389] tlayer: self-attention.
I0123 12:41:12.498292 139856449679360 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.498354 139856449679360 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.498915 139856449679360 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.498991 139856449679360 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.499348 139856449679360 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.500118 139856449679360 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.502759 139856449679360 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.503385 139856449679360 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.503463 139856449679360 transformer_layer.py:468] tlayer: End windows.
I0123 12:41:12.503498 139856449679360 transformer_layer.py:472] tlayer: final FFN.
I0123 12:41:12.503555 139856449679360 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.503685 139856449679360 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:41:12.504013 139856449679360 nn_components.py:325] mlp: activation = None
I0123 12:41:12.504057 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:12.505982 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.506077 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:12.508635 139856449679360 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.508719 139856449679360 transformer_base.py:443] tbase: final FFN
I0123 12:41:12.509157 139856449679360 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:41:12.511445 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:12.513357 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.513454 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:12.513755 139856449679360 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.513835 139856449679360 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 12:41:12.513944 139856449679360 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:41:12.513983 139856449679360 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:41:12.514014 139856449679360 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:41:12.515921 139856449679360 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.518322 139856449679360 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:41:12.523968 139856449679360 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.524234 139856449679360 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:41:12.526912 139856449679360 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:41:12.530701 139856449679360 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:41:12.530757 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:12.530792 139856449679360 transformer_layer.py:389] tlayer: self-attention.
I0123 12:41:12.530823 139856449679360 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.530885 139856449679360 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.531441 139856449679360 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.531517 139856449679360 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.531874 139856449679360 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.532660 139856449679360 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.535189 139856449679360 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.535804 139856449679360 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.535884 139856449679360 transformer_layer.py:468] tlayer: End windows.
I0123 12:41:12.535918 139856449679360 transformer_layer.py:472] tlayer: final FFN.
I0123 12:41:12.535976 139856449679360 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.536103 139856449679360 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:41:12.536427 139856449679360 nn_components.py:325] mlp: activation = None
I0123 12:41:12.536470 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:12.538364 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.538461 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:12.540979 139856449679360 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.541074 139856449679360 transformer_base.py:443] tbase: final FFN
I0123 12:41:12.541512 139856449679360 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:41:12.543784 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:12.545756 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.545855 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:12.546145 139856449679360 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.546226 139856449679360 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 12:41:12.546337 139856449679360 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:41:12.546377 139856449679360 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:41:12.546410 139856449679360 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:41:12.548234 139856449679360 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.550619 139856449679360 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:41:12.556399 139856449679360 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.556665 139856449679360 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:41:12.559402 139856449679360 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:41:12.563271 139856449679360 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:41:12.563330 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:12.563367 139856449679360 transformer_layer.py:389] tlayer: self-attention.
I0123 12:41:12.563400 139856449679360 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.563464 139856449679360 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.564399 139856449679360 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.564477 139856449679360 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.564850 139856449679360 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.565626 139856449679360 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.568230 139856449679360 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.568857 139856449679360 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.568934 139856449679360 transformer_layer.py:468] tlayer: End windows.
I0123 12:41:12.568969 139856449679360 transformer_layer.py:472] tlayer: final FFN.
I0123 12:41:12.569029 139856449679360 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.569163 139856449679360 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:41:12.569489 139856449679360 nn_components.py:325] mlp: activation = None
I0123 12:41:12.569532 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:12.571450 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.571545 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:12.574142 139856449679360 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.574223 139856449679360 transformer_base.py:443] tbase: final FFN
I0123 12:41:12.574658 139856449679360 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:41:12.576998 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:12.578910 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.579007 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:12.579296 139856449679360 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.579377 139856449679360 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 12:41:12.579488 139856449679360 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:41:12.579527 139856449679360 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:41:12.579558 139856449679360 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:41:12.581406 139856449679360 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.583874 139856449679360 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:41:12.589498 139856449679360 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.589762 139856449679360 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:41:12.592414 139856449679360 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:41:12.596257 139856449679360 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:41:12.596314 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:12.596350 139856449679360 transformer_layer.py:389] tlayer: self-attention.
I0123 12:41:12.596381 139856449679360 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.596444 139856449679360 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.597006 139856449679360 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.597083 139856449679360 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.597441 139856449679360 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.598228 139856449679360 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.600721 139856449679360 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.601344 139856449679360 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.601421 139856449679360 transformer_layer.py:468] tlayer: End windows.
I0123 12:41:12.601456 139856449679360 transformer_layer.py:472] tlayer: final FFN.
I0123 12:41:12.601515 139856449679360 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.601647 139856449679360 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:41:12.601976 139856449679360 nn_components.py:325] mlp: activation = None
I0123 12:41:12.602018 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:12.603983 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.604077 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:12.606610 139856449679360 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.606689 139856449679360 transformer_base.py:443] tbase: final FFN
I0123 12:41:12.607123 139856449679360 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:41:12.609474 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:12.611412 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.611508 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:12.611800 139856449679360 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.611885 139856449679360 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 12:41:12.611999 139856449679360 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:41:12.612039 139856449679360 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:41:12.612070 139856449679360 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:41:12.613992 139856449679360 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.616411 139856449679360 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:41:12.622229 139856449679360 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.622496 139856449679360 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:41:12.625159 139856449679360 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:41:12.629068 139856449679360 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:41:12.629124 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:12.629159 139856449679360 transformer_layer.py:389] tlayer: self-attention.
I0123 12:41:12.629191 139856449679360 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.629253 139856449679360 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.629829 139856449679360 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.629910 139856449679360 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.630291 139856449679360 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.631104 139856449679360 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.633611 139856449679360 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.634289 139856449679360 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.634371 139856449679360 transformer_layer.py:468] tlayer: End windows.
I0123 12:41:12.634407 139856449679360 transformer_layer.py:472] tlayer: final FFN.
I0123 12:41:12.634466 139856449679360 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.634593 139856449679360 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:41:12.634922 139856449679360 nn_components.py:325] mlp: activation = None
I0123 12:41:12.634965 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:12.636894 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.636988 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:12.639504 139856449679360 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.639585 139856449679360 transformer_base.py:443] tbase: final FFN
I0123 12:41:12.640375 139856449679360 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:41:12.642657 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:12.644578 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.644682 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:12.644978 139856449679360 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.645060 139856449679360 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 12:41:12.645171 139856449679360 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:41:12.645210 139856449679360 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:41:12.645242 139856449679360 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:41:12.784158 139856449679360 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.787071 139856449679360 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:41:12.793080 139856449679360 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.793382 139856449679360 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:41:12.796221 139856449679360 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:41:12.800157 139856449679360 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:41:12.800215 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:12.800252 139856449679360 transformer_layer.py:389] tlayer: self-attention.
I0123 12:41:12.800285 139856449679360 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.800352 139856449679360 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.800965 139856449679360 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.801041 139856449679360 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.801404 139856449679360 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.802191 139856449679360 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.804827 139856449679360 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.805476 139856449679360 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.805555 139856449679360 transformer_layer.py:468] tlayer: End windows.
I0123 12:41:12.805590 139856449679360 transformer_layer.py:472] tlayer: final FFN.
I0123 12:41:12.805661 139856449679360 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.805794 139856449679360 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:41:12.806129 139856449679360 nn_components.py:325] mlp: activation = None
I0123 12:41:12.806173 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:12.808094 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.808189 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:12.810808 139856449679360 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.810889 139856449679360 transformer_base.py:443] tbase: final FFN
I0123 12:41:12.811322 139856449679360 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:41:12.813676 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:12.815665 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.815770 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:12.816064 139856449679360 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.816147 139856449679360 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 12:41:12.816256 139856449679360 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:41:12.816296 139856449679360 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:41:12.816326 139856449679360 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:41:12.818202 139856449679360 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.820689 139856449679360 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:41:12.826350 139856449679360 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.826611 139856449679360 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:41:12.829323 139856449679360 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:41:12.833127 139856449679360 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:41:12.833183 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:12.833220 139856449679360 transformer_layer.py:389] tlayer: self-attention.
I0123 12:41:12.833251 139856449679360 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.833313 139856449679360 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.833940 139856449679360 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.834019 139856449679360 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.834372 139856449679360 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.835149 139856449679360 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.837654 139856449679360 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.838283 139856449679360 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.838360 139856449679360 transformer_layer.py:468] tlayer: End windows.
I0123 12:41:12.838395 139856449679360 transformer_layer.py:472] tlayer: final FFN.
I0123 12:41:12.838453 139856449679360 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.838578 139856449679360 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:41:12.838903 139856449679360 nn_components.py:325] mlp: activation = None
I0123 12:41:12.838946 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:12.840841 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.840934 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:12.843527 139856449679360 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.843608 139856449679360 transformer_base.py:443] tbase: final FFN
I0123 12:41:12.844040 139856449679360 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:41:12.846402 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:12.848318 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.848414 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:12.848710 139856449679360 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.848800 139856449679360 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 12:41:12.848912 139856449679360 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:41:12.848952 139856449679360 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:41:12.848983 139856449679360 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:41:12.850819 139856449679360 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.853425 139856449679360 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:41:12.859381 139856449679360 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.859642 139856449679360 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:41:12.862576 139856449679360 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:41:12.866482 139856449679360 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:41:12.866541 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:12.866578 139856449679360 transformer_layer.py:389] tlayer: self-attention.
I0123 12:41:12.866610 139856449679360 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.866675 139856449679360 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.867269 139856449679360 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.867349 139856449679360 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.867723 139856449679360 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.868504 139856449679360 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.871045 139856449679360 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.871700 139856449679360 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.871780 139856449679360 transformer_layer.py:468] tlayer: End windows.
I0123 12:41:12.871817 139856449679360 transformer_layer.py:472] tlayer: final FFN.
I0123 12:41:12.871876 139856449679360 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.872003 139856449679360 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:41:12.872323 139856449679360 nn_components.py:325] mlp: activation = None
I0123 12:41:12.872366 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:12.874348 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.874446 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:12.877011 139856449679360 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.877089 139856449679360 transformer_base.py:443] tbase: final FFN
I0123 12:41:12.877525 139856449679360 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:41:12.879970 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:12.881900 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.882001 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:12.882310 139856449679360 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.882401 139856449679360 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 12:41:12.882518 139856449679360 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:41:12.882559 139856449679360 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:41:12.882592 139856449679360 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:41:12.884580 139856449679360 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.887067 139856449679360 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:41:12.892802 139856449679360 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.893069 139856449679360 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:41:12.895768 139856449679360 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:41:12.899698 139856449679360 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:41:12.899757 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:12.899794 139856449679360 transformer_layer.py:389] tlayer: self-attention.
I0123 12:41:12.899827 139856449679360 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.899891 139856449679360 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.900471 139856449679360 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.900547 139856449679360 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.900901 139856449679360 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.901692 139856449679360 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.904246 139856449679360 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.904917 139856449679360 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.904995 139856449679360 transformer_layer.py:468] tlayer: End windows.
I0123 12:41:12.905030 139856449679360 transformer_layer.py:472] tlayer: final FFN.
I0123 12:41:12.905090 139856449679360 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.905222 139856449679360 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:41:12.905544 139856449679360 nn_components.py:325] mlp: activation = None
I0123 12:41:12.905588 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:12.907549 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.907647 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:12.910412 139856449679360 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.910495 139856449679360 transformer_base.py:443] tbase: final FFN
I0123 12:41:12.911003 139856449679360 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:41:12.913326 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:12.915286 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.915385 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:12.915692 139856449679360 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.915777 139856449679360 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 12:41:12.915898 139856449679360 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:41:12.915940 139856449679360 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:41:12.915972 139856449679360 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:41:12.917877 139856449679360 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.920382 139856449679360 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:41:12.926123 139856449679360 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.926395 139856449679360 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:41:12.929096 139856449679360 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:41:12.932963 139856449679360 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:41:12.933019 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:12.933054 139856449679360 transformer_layer.py:389] tlayer: self-attention.
I0123 12:41:12.933086 139856449679360 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.933148 139856449679360 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.933714 139856449679360 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.933791 139856449679360 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.934155 139856449679360 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.934958 139856449679360 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.937857 139856449679360 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.938513 139856449679360 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.938593 139856449679360 transformer_layer.py:468] tlayer: End windows.
I0123 12:41:12.938630 139856449679360 transformer_layer.py:472] tlayer: final FFN.
I0123 12:41:12.938692 139856449679360 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.938827 139856449679360 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:41:12.939168 139856449679360 nn_components.py:325] mlp: activation = None
I0123 12:41:12.939214 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:12.941123 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.941218 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:12.943855 139856449679360 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.943943 139856449679360 transformer_base.py:443] tbase: final FFN
I0123 12:41:12.944378 139856449679360 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:41:12.946648 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:12.948602 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.948697 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:12.948982 139856449679360 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:12.949267 139856449679360 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:41:12.949337 139856449679360 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:41:12.949404 139856449679360 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:41:12.949462 139856449679360 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:41:12.949518 139856449679360 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:41:12.949574 139856449679360 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:41:12.949628 139856449679360 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:41:12.949691 139856449679360 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:41:12.949746 139856449679360 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:41:12.949798 139856449679360 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:41:12.949851 139856449679360 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:41:12.949903 139856449679360 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:41:12.949941 139856449679360 decoder_stack.py:344] dstack: Final layernorm.
I0123 12:41:12.953554 139856449679360 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:13.001796 139856449679360 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.001882 139856449679360 decoder_stack.py:333] dstack: autoregressive generator.
I0123 12:41:13.001935 139856449679360 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 12:41:13.002039 139856449679360 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:41:13.002078 139856449679360 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:41:13.002108 139856449679360 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:41:13.002172 139856449679360 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.004592 139856449679360 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:41:13.010102 139856449679360 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.010363 139856449679360 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:41:13.012989 139856449679360 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:41:13.029607 139856449679360 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:41:13.029668 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:13.029705 139856449679360 transformer_layer.py:389] tlayer: self-attention.
I0123 12:41:13.029736 139856449679360 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.029798 139856449679360 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.030936 139856449679360 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.031015 139856449679360 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.031721 139856449679360 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.033722 139856449679360 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.038485 139856449679360 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.039784 139856449679360 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.039869 139856449679360 transformer_layer.py:468] tlayer: End windows.
I0123 12:41:13.039904 139856449679360 transformer_layer.py:472] tlayer: final FFN.
I0123 12:41:13.039963 139856449679360 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.040105 139856449679360 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:41:13.040216 139856449679360 nn_components.py:325] mlp: activation = None
I0123 12:41:13.040255 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:13.042177 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.042273 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:13.044707 139856449679360 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.044786 139856449679360 transformer_base.py:443] tbase: final FFN
I0123 12:41:13.044893 139856449679360 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:41:13.047147 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:13.049093 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.049190 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:13.049480 139856449679360 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.049561 139856449679360 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 12:41:13.049675 139856449679360 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:41:13.049716 139856449679360 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:41:13.049747 139856449679360 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:41:13.049813 139856449679360 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.052065 139856449679360 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:41:13.057532 139856449679360 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.057799 139856449679360 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:41:13.060496 139856449679360 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:41:13.073847 139856449679360 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:41:13.073903 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:13.073940 139856449679360 transformer_layer.py:389] tlayer: self-attention.
I0123 12:41:13.073970 139856449679360 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.074031 139856449679360 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.074584 139856449679360 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.074661 139856449679360 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.075024 139856449679360 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.075717 139856449679360 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.078253 139856449679360 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.078877 139856449679360 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.078956 139856449679360 transformer_layer.py:468] tlayer: End windows.
I0123 12:41:13.078997 139856449679360 transformer_layer.py:472] tlayer: final FFN.
I0123 12:41:13.079061 139856449679360 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.079192 139856449679360 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:41:13.079301 139856449679360 nn_components.py:325] mlp: activation = None
I0123 12:41:13.079339 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:13.081277 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.081371 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:13.088626 139856449679360 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.088745 139856449679360 transformer_base.py:443] tbase: final FFN
I0123 12:41:13.088863 139856449679360 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:41:13.091232 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:13.093192 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.093287 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:13.093582 139856449679360 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.093676 139856449679360 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 12:41:13.093792 139856449679360 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:41:13.093835 139856449679360 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:41:13.093865 139856449679360 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:41:13.093933 139856449679360 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.096228 139856449679360 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:41:13.101716 139856449679360 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.101978 139856449679360 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:41:13.104696 139856449679360 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:41:13.117684 139856449679360 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:41:13.117742 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:13.117779 139856449679360 transformer_layer.py:389] tlayer: self-attention.
I0123 12:41:13.117810 139856449679360 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.117873 139856449679360 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.118471 139856449679360 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.118548 139856449679360 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.118911 139856449679360 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.119602 139856449679360 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.122156 139856449679360 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.122783 139856449679360 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.122860 139856449679360 transformer_layer.py:468] tlayer: End windows.
I0123 12:41:13.122894 139856449679360 transformer_layer.py:472] tlayer: final FFN.
I0123 12:41:13.122957 139856449679360 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.123084 139856449679360 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:41:13.123193 139856449679360 nn_components.py:325] mlp: activation = None
I0123 12:41:13.123232 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:13.125175 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.125271 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:13.127719 139856449679360 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.127804 139856449679360 transformer_base.py:443] tbase: final FFN
I0123 12:41:13.127912 139856449679360 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:41:13.130135 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:13.132082 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.132178 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:13.132466 139856449679360 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.132547 139856449679360 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 12:41:13.132656 139856449679360 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:41:13.132695 139856449679360 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:41:13.132726 139856449679360 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:41:13.132790 139856449679360 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.135050 139856449679360 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:41:13.140547 139856449679360 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.140802 139856449679360 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:41:13.143531 139856449679360 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:41:13.156417 139856449679360 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:41:13.156473 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:13.156509 139856449679360 transformer_layer.py:389] tlayer: self-attention.
I0123 12:41:13.156539 139856449679360 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.156604 139856449679360 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.157161 139856449679360 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.157235 139856449679360 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.157588 139856449679360 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.158293 139856449679360 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.160788 139856449679360 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.161421 139856449679360 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.161498 139856449679360 transformer_layer.py:468] tlayer: End windows.
I0123 12:41:13.161533 139856449679360 transformer_layer.py:472] tlayer: final FFN.
I0123 12:41:13.161592 139856449679360 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.161735 139856449679360 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:41:13.161847 139856449679360 nn_components.py:325] mlp: activation = None
I0123 12:41:13.161886 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:13.163838 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.163933 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:13.166363 139856449679360 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.166442 139856449679360 transformer_base.py:443] tbase: final FFN
I0123 12:41:13.166549 139856449679360 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:41:13.168776 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:13.170654 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.170751 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:13.171035 139856449679360 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.171115 139856449679360 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 12:41:13.171223 139856449679360 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:41:13.171261 139856449679360 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:41:13.171292 139856449679360 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:41:13.171356 139856449679360 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.173961 139856449679360 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:41:13.179455 139856449679360 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.179715 139856449679360 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:41:13.182353 139856449679360 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:41:13.195105 139856449679360 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:41:13.195161 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:13.195198 139856449679360 transformer_layer.py:389] tlayer: self-attention.
I0123 12:41:13.195229 139856449679360 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.195290 139856449679360 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.195852 139856449679360 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.195928 139856449679360 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.196285 139856449679360 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.196986 139856449679360 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.199537 139856449679360 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.200169 139856449679360 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.200250 139856449679360 transformer_layer.py:468] tlayer: End windows.
I0123 12:41:13.200285 139856449679360 transformer_layer.py:472] tlayer: final FFN.
I0123 12:41:13.200344 139856449679360 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.200479 139856449679360 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:41:13.200590 139856449679360 nn_components.py:325] mlp: activation = None
I0123 12:41:13.200633 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:13.202523 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.202618 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:13.205021 139856449679360 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.205099 139856449679360 transformer_base.py:443] tbase: final FFN
I0123 12:41:13.205208 139856449679360 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:41:13.207493 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:13.209359 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.209454 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:13.209745 139856449679360 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.209827 139856449679360 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 12:41:13.209936 139856449679360 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:41:13.209976 139856449679360 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:41:13.210007 139856449679360 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:41:13.210072 139856449679360 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.212342 139856449679360 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:41:13.217840 139856449679360 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.218097 139856449679360 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:41:13.220815 139856449679360 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:41:13.233731 139856449679360 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:41:13.233787 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:13.233823 139856449679360 transformer_layer.py:389] tlayer: self-attention.
I0123 12:41:13.233853 139856449679360 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.233914 139856449679360 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.234475 139856449679360 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.234551 139856449679360 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.234908 139856449679360 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.235606 139856449679360 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.238095 139856449679360 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.238721 139856449679360 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.238801 139856449679360 transformer_layer.py:468] tlayer: End windows.
I0123 12:41:13.238835 139856449679360 transformer_layer.py:472] tlayer: final FFN.
I0123 12:41:13.238894 139856449679360 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.239023 139856449679360 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:41:13.239137 139856449679360 nn_components.py:325] mlp: activation = None
I0123 12:41:13.239176 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:13.241113 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.241206 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:13.243630 139856449679360 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.243710 139856449679360 transformer_base.py:443] tbase: final FFN
I0123 12:41:13.243818 139856449679360 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:41:13.246068 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:13.247935 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.248032 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:13.248319 139856449679360 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.248401 139856449679360 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 12:41:13.248510 139856449679360 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:41:13.248548 139856449679360 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:41:13.248579 139856449679360 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:41:13.248643 139856449679360 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.250893 139856449679360 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:41:13.256435 139856449679360 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.256693 139856449679360 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:41:13.259309 139856449679360 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:41:13.272013 139856449679360 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:41:13.272069 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:13.272105 139856449679360 transformer_layer.py:389] tlayer: self-attention.
I0123 12:41:13.272136 139856449679360 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.272196 139856449679360 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.272756 139856449679360 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.272836 139856449679360 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.273195 139856449679360 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.273893 139856449679360 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.276370 139856449679360 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.277354 139856449679360 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.277433 139856449679360 transformer_layer.py:468] tlayer: End windows.
I0123 12:41:13.277479 139856449679360 transformer_layer.py:472] tlayer: final FFN.
I0123 12:41:13.277538 139856449679360 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.277676 139856449679360 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:41:13.277787 139856449679360 nn_components.py:325] mlp: activation = None
I0123 12:41:13.277832 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:13.279736 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.279830 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:13.282257 139856449679360 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.282338 139856449679360 transformer_base.py:443] tbase: final FFN
I0123 12:41:13.282445 139856449679360 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:41:13.284657 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:13.286602 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.286699 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:13.286988 139856449679360 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.287068 139856449679360 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 12:41:13.287178 139856449679360 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:41:13.287217 139856449679360 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:41:13.287247 139856449679360 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:41:13.287310 139856449679360 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.289556 139856449679360 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:41:13.295025 139856449679360 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.295291 139856449679360 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:41:13.298003 139856449679360 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:41:13.310687 139856449679360 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:41:13.310742 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:13.310777 139856449679360 transformer_layer.py:389] tlayer: self-attention.
I0123 12:41:13.310808 139856449679360 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.310868 139856449679360 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.311475 139856449679360 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.311552 139856449679360 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.311906 139856449679360 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.312596 139856449679360 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.315098 139856449679360 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.315730 139856449679360 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.315809 139856449679360 transformer_layer.py:468] tlayer: End windows.
I0123 12:41:13.315844 139856449679360 transformer_layer.py:472] tlayer: final FFN.
I0123 12:41:13.315902 139856449679360 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.316032 139856449679360 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:41:13.316139 139856449679360 nn_components.py:325] mlp: activation = None
I0123 12:41:13.316184 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:13.318074 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.318169 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:13.320635 139856449679360 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.320715 139856449679360 transformer_base.py:443] tbase: final FFN
I0123 12:41:13.320823 139856449679360 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:41:13.323040 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:13.324918 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.325014 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:13.325299 139856449679360 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.325381 139856449679360 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 12:41:13.325489 139856449679360 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:41:13.325529 139856449679360 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:41:13.325560 139856449679360 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:41:13.325622 139856449679360 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.327927 139856449679360 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:41:13.333450 139856449679360 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.333720 139856449679360 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:41:13.336333 139856449679360 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:41:13.349246 139856449679360 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:41:13.349302 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:13.349337 139856449679360 transformer_layer.py:389] tlayer: self-attention.
I0123 12:41:13.349369 139856449679360 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.349431 139856449679360 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.349995 139856449679360 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.350073 139856449679360 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.350434 139856449679360 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.351126 139856449679360 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.353593 139856449679360 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.354265 139856449679360 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.354344 139856449679360 transformer_layer.py:468] tlayer: End windows.
I0123 12:41:13.354379 139856449679360 transformer_layer.py:472] tlayer: final FFN.
I0123 12:41:13.354437 139856449679360 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.354569 139856449679360 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:41:13.354681 139856449679360 nn_components.py:325] mlp: activation = None
I0123 12:41:13.354720 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:13.356595 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.356689 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:13.359097 139856449679360 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.359176 139856449679360 transformer_base.py:443] tbase: final FFN
I0123 12:41:13.359283 139856449679360 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:41:13.361498 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:13.363439 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.363535 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:13.363821 139856449679360 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.363903 139856449679360 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 12:41:13.364012 139856449679360 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:41:13.364051 139856449679360 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:41:13.364081 139856449679360 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:41:13.364144 139856449679360 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.366387 139856449679360 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:41:13.371824 139856449679360 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.372082 139856449679360 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:41:13.374766 139856449679360 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:41:13.387721 139856449679360 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:41:13.387776 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:13.387812 139856449679360 transformer_layer.py:389] tlayer: self-attention.
I0123 12:41:13.387843 139856449679360 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.387903 139856449679360 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.388505 139856449679360 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.388582 139856449679360 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.388940 139856449679360 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.389629 139856449679360 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.392132 139856449679360 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.392750 139856449679360 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.392827 139856449679360 transformer_layer.py:468] tlayer: End windows.
I0123 12:41:13.392861 139856449679360 transformer_layer.py:472] tlayer: final FFN.
I0123 12:41:13.392918 139856449679360 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.393049 139856449679360 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:41:13.393161 139856449679360 nn_components.py:325] mlp: activation = None
I0123 12:41:13.393199 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:13.395105 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.395206 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:13.397691 139856449679360 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.397776 139856449679360 transformer_base.py:443] tbase: final FFN
I0123 12:41:13.397891 139856449679360 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:41:13.400119 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:13.401996 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.402093 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:13.402387 139856449679360 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.402470 139856449679360 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 12:41:13.402581 139856449679360 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:41:13.402620 139856449679360 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:41:13.402651 139856449679360 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:41:13.402715 139856449679360 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.404958 139856449679360 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:41:13.410516 139856449679360 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.410777 139856449679360 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:41:13.413419 139856449679360 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:41:13.426129 139856449679360 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:41:13.426185 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:13.426219 139856449679360 transformer_layer.py:389] tlayer: self-attention.
I0123 12:41:13.426249 139856449679360 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.426310 139856449679360 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.426859 139856449679360 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.426935 139856449679360 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.427295 139856449679360 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.427987 139856449679360 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.430477 139856449679360 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.431137 139856449679360 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.431217 139856449679360 transformer_layer.py:468] tlayer: End windows.
I0123 12:41:13.431252 139856449679360 transformer_layer.py:472] tlayer: final FFN.
I0123 12:41:13.431310 139856449679360 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.431441 139856449679360 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:41:13.431550 139856449679360 nn_components.py:325] mlp: activation = None
I0123 12:41:13.431588 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:13.433481 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.433581 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:13.435999 139856449679360 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.436078 139856449679360 transformer_base.py:443] tbase: final FFN
I0123 12:41:13.436186 139856449679360 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:41:13.438400 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:13.440322 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.440417 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:13.440702 139856449679360 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.440783 139856449679360 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 12:41:13.440893 139856449679360 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:41:13.440931 139856449679360 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:41:13.440962 139856449679360 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:41:13.441024 139856449679360 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.443351 139856449679360 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:41:13.448983 139856449679360 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.449239 139856449679360 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:41:13.451919 139856449679360 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:41:13.464525 139856449679360 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:41:13.464581 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:13.464617 139856449679360 transformer_layer.py:389] tlayer: self-attention.
I0123 12:41:13.464648 139856449679360 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.464709 139856449679360 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.465265 139856449679360 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.465340 139856449679360 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.465708 139856449679360 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.466450 139856449679360 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.468935 139856449679360 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.469552 139856449679360 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.469630 139856449679360 transformer_layer.py:468] tlayer: End windows.
I0123 12:41:13.469671 139856449679360 transformer_layer.py:472] tlayer: final FFN.
I0123 12:41:13.469731 139856449679360 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.469864 139856449679360 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:41:13.469978 139856449679360 nn_components.py:325] mlp: activation = None
I0123 12:41:13.470016 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:13.471906 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.471998 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:13.474407 139856449679360 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.474487 139856449679360 transformer_base.py:443] tbase: final FFN
I0123 12:41:13.474595 139856449679360 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:41:13.476879 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:13.478767 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.478863 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:13.479149 139856449679360 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.479238 139856449679360 decoder_stack.py:344] dstack: Final layernorm.
I0123 12:41:13.482090 139856449679360 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:13.537775 139856449679360 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.537862 139856449679360 decoder_stack.py:333] dstack: autoregressive generator.
I0123 12:41:13.537915 139856449679360 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 12:41:13.538017 139856449679360 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:41:13.538056 139856449679360 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:41:13.538086 139856449679360 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:41:13.538148 139856449679360 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.540781 139856449679360 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:41:13.546288 139856449679360 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.546543 139856449679360 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:41:13.549101 139856449679360 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:41:13.561592 139856449679360 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:41:13.561656 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:13.561693 139856449679360 transformer_layer.py:389] tlayer: self-attention.
I0123 12:41:13.561724 139856449679360 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.561787 139856449679360 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.562355 139856449679360 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.562431 139856449679360 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.562789 139856449679360 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.563464 139856449679360 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.565954 139856449679360 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.566565 139856449679360 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.566642 139856449679360 transformer_layer.py:468] tlayer: End windows.
I0123 12:41:13.566677 139856449679360 transformer_layer.py:472] tlayer: final FFN.
I0123 12:41:13.566735 139856449679360 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.566863 139856449679360 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:41:13.566977 139856449679360 nn_components.py:325] mlp: activation = None
I0123 12:41:13.567016 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:13.568873 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.568968 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:13.571353 139856449679360 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.571434 139856449679360 transformer_base.py:443] tbase: final FFN
I0123 12:41:13.571542 139856449679360 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:41:13.573784 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:13.575620 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.575715 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:13.575999 139856449679360 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.576080 139856449679360 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 12:41:13.576186 139856449679360 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:41:13.576225 139856449679360 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:41:13.576256 139856449679360 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:41:13.576318 139856449679360 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.578550 139856449679360 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:41:13.583873 139856449679360 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.584130 139856449679360 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:41:13.586792 139856449679360 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:41:13.599017 139856449679360 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:41:13.599072 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:13.599108 139856449679360 transformer_layer.py:389] tlayer: self-attention.
I0123 12:41:13.599138 139856449679360 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.599200 139856449679360 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.599753 139856449679360 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.599829 139856449679360 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.600183 139856449679360 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.600852 139856449679360 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.603338 139856449679360 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.603949 139856449679360 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.604026 139856449679360 transformer_layer.py:468] tlayer: End windows.
I0123 12:41:13.604060 139856449679360 transformer_layer.py:472] tlayer: final FFN.
I0123 12:41:13.604120 139856449679360 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.604247 139856449679360 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:41:13.604353 139856449679360 nn_components.py:325] mlp: activation = None
I0123 12:41:13.604396 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:13.606254 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.606347 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:13.608716 139856449679360 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.608795 139856449679360 transformer_base.py:443] tbase: final FFN
I0123 12:41:13.608903 139856449679360 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:41:13.611148 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:13.612971 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.613067 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:13.613353 139856449679360 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.613436 139856449679360 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 12:41:13.613543 139856449679360 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:41:13.613582 139856449679360 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:41:13.613613 139856449679360 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:41:13.613681 139856449679360 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.615886 139856449679360 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:41:13.621233 139856449679360 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.621488 139856449679360 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:41:13.624126 139856449679360 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:41:13.636335 139856449679360 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:41:13.636391 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:13.636427 139856449679360 transformer_layer.py:389] tlayer: self-attention.
I0123 12:41:13.636459 139856449679360 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.636520 139856449679360 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.637073 139856449679360 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.637149 139856449679360 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.637503 139856449679360 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.638179 139856449679360 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.640661 139856449679360 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.641273 139856449679360 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.641350 139856449679360 transformer_layer.py:468] tlayer: End windows.
I0123 12:41:13.641385 139856449679360 transformer_layer.py:472] tlayer: final FFN.
I0123 12:41:13.641443 139856449679360 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.641571 139856449679360 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:41:13.641684 139856449679360 nn_components.py:325] mlp: activation = None
I0123 12:41:13.641723 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:13.643558 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.643653 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:13.646048 139856449679360 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.646128 139856449679360 transformer_base.py:443] tbase: final FFN
I0123 12:41:13.646236 139856449679360 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:41:13.648912 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:13.650873 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.650970 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:13.651257 139856449679360 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.651338 139856449679360 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 12:41:13.651445 139856449679360 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:41:13.651483 139856449679360 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:41:13.651514 139856449679360 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:41:13.651576 139856449679360 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.653824 139856449679360 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:41:13.659143 139856449679360 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.659398 139856449679360 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:41:13.662041 139856449679360 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:41:13.674352 139856449679360 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:41:13.674408 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:13.674446 139856449679360 transformer_layer.py:389] tlayer: self-attention.
I0123 12:41:13.674487 139856449679360 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.674550 139856449679360 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.675103 139856449679360 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.675179 139856449679360 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.675534 139856449679360 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.676208 139856449679360 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.678742 139856449679360 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.679363 139856449679360 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.679438 139856449679360 transformer_layer.py:468] tlayer: End windows.
I0123 12:41:13.679471 139856449679360 transformer_layer.py:472] tlayer: final FFN.
I0123 12:41:13.679529 139856449679360 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.679653 139856449679360 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:41:13.679758 139856449679360 nn_components.py:325] mlp: activation = None
I0123 12:41:13.679797 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:13.681671 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.681764 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:13.684117 139856449679360 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.684193 139856449679360 transformer_base.py:443] tbase: final FFN
I0123 12:41:13.684298 139856449679360 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:41:13.686571 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:13.688415 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.688508 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:13.688790 139856449679360 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.688869 139856449679360 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 12:41:13.688975 139856449679360 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:41:13.689011 139856449679360 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:41:13.689039 139856449679360 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:41:13.689100 139856449679360 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.691322 139856449679360 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:41:13.696684 139856449679360 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.696938 139856449679360 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:41:13.699580 139856449679360 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:41:13.711984 139856449679360 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:41:13.712037 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:13.712071 139856449679360 transformer_layer.py:389] tlayer: self-attention.
I0123 12:41:13.712099 139856449679360 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.712159 139856449679360 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.712712 139856449679360 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.712786 139856449679360 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.713141 139856449679360 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.713830 139856449679360 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.716356 139856449679360 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.716971 139856449679360 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.717046 139856449679360 transformer_layer.py:468] tlayer: End windows.
I0123 12:41:13.717080 139856449679360 transformer_layer.py:472] tlayer: final FFN.
I0123 12:41:13.717138 139856449679360 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.717261 139856449679360 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:41:13.717366 139856449679360 nn_components.py:325] mlp: activation = None
I0123 12:41:13.717403 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:13.719279 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.719377 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:13.721796 139856449679360 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.721874 139856449679360 transformer_base.py:443] tbase: final FFN
I0123 12:41:13.721979 139856449679360 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:41:13.724251 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:13.726099 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.726193 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:13.726488 139856449679360 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.726571 139856449679360 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 12:41:13.726681 139856449679360 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:41:13.726720 139856449679360 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:41:13.726750 139856449679360 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:41:13.726814 139856449679360 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.729070 139856449679360 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:41:13.734436 139856449679360 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.734690 139856449679360 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:41:13.737337 139856449679360 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:41:13.749798 139856449679360 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:41:13.749853 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:13.749887 139856449679360 transformer_layer.py:389] tlayer: self-attention.
I0123 12:41:13.749917 139856449679360 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.749978 139856449679360 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.750534 139856449679360 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.750608 139856449679360 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.750963 139856449679360 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.751650 139856449679360 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.754194 139856449679360 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.754808 139856449679360 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.754883 139856449679360 transformer_layer.py:468] tlayer: End windows.
I0123 12:41:13.754916 139856449679360 transformer_layer.py:472] tlayer: final FFN.
I0123 12:41:13.754972 139856449679360 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.755098 139856449679360 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:41:13.755208 139856449679360 nn_components.py:325] mlp: activation = None
I0123 12:41:13.755244 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:13.757104 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.757201 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:13.759571 139856449679360 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.759648 139856449679360 transformer_base.py:443] tbase: final FFN
I0123 12:41:13.759754 139856449679360 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:41:13.762437 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:13.764295 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.764389 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:13.764674 139856449679360 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.764754 139856449679360 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 12:41:13.764862 139856449679360 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:41:13.764898 139856449679360 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:41:13.764927 139856449679360 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:41:13.764988 139856449679360 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.767202 139856449679360 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:41:13.772594 139856449679360 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.772849 139856449679360 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:41:13.775537 139856449679360 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:41:13.788015 139856449679360 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:41:13.788068 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:13.788102 139856449679360 transformer_layer.py:389] tlayer: self-attention.
I0123 12:41:13.788132 139856449679360 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.788192 139856449679360 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.788754 139856449679360 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.788829 139856449679360 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.789182 139856449679360 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.789878 139856449679360 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.792394 139856449679360 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.793013 139856449679360 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.793089 139856449679360 transformer_layer.py:468] tlayer: End windows.
I0123 12:41:13.793122 139856449679360 transformer_layer.py:472] tlayer: final FFN.
I0123 12:41:13.793179 139856449679360 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.793306 139856449679360 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:41:13.793412 139856449679360 nn_components.py:325] mlp: activation = None
I0123 12:41:13.793449 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:13.795356 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.795457 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:13.797892 139856449679360 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.797971 139856449679360 transformer_base.py:443] tbase: final FFN
I0123 12:41:13.798077 139856449679360 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:41:13.800357 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:13.802213 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.802308 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:13.802589 139856449679360 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.802669 139856449679360 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 12:41:13.802775 139856449679360 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:41:13.802812 139856449679360 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:41:13.802841 139856449679360 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:41:13.802902 139856449679360 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.805101 139856449679360 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:41:13.810494 139856449679360 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.810750 139856449679360 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:41:13.813403 139856449679360 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:41:13.825819 139856449679360 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:41:13.825872 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:13.825906 139856449679360 transformer_layer.py:389] tlayer: self-attention.
I0123 12:41:13.825935 139856449679360 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.825994 139856449679360 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.826546 139856449679360 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.826620 139856449679360 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.826973 139856449679360 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.827657 139856449679360 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.830175 139856449679360 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.830799 139856449679360 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.830875 139856449679360 transformer_layer.py:468] tlayer: End windows.
I0123 12:41:13.830908 139856449679360 transformer_layer.py:472] tlayer: final FFN.
I0123 12:41:13.830967 139856449679360 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.831091 139856449679360 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:41:13.831197 139856449679360 nn_components.py:325] mlp: activation = None
I0123 12:41:13.831234 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:13.833077 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.833168 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:13.835549 139856449679360 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.835638 139856449679360 transformer_base.py:443] tbase: final FFN
I0123 12:41:13.835747 139856449679360 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:41:13.838035 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:13.839890 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.839983 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:13.840265 139856449679360 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.840345 139856449679360 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 12:41:13.840448 139856449679360 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:41:13.840486 139856449679360 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:41:13.840516 139856449679360 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:41:13.840577 139856449679360 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.842792 139856449679360 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:41:13.848111 139856449679360 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.848366 139856449679360 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:41:13.851035 139856449679360 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:41:13.863577 139856449679360 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:41:13.863633 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:13.863667 139856449679360 transformer_layer.py:389] tlayer: self-attention.
I0123 12:41:13.863697 139856449679360 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.863759 139856449679360 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.864331 139856449679360 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.864407 139856449679360 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.864761 139856449679360 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.865446 139856449679360 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.868065 139856449679360 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.868688 139856449679360 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.868763 139856449679360 transformer_layer.py:468] tlayer: End windows.
I0123 12:41:13.868797 139856449679360 transformer_layer.py:472] tlayer: final FFN.
I0123 12:41:13.868854 139856449679360 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.868977 139856449679360 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:41:13.869082 139856449679360 nn_components.py:325] mlp: activation = None
I0123 12:41:13.869119 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:13.871032 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.871127 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:13.873528 139856449679360 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.873611 139856449679360 transformer_base.py:443] tbase: final FFN
I0123 12:41:13.873726 139856449679360 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:41:13.876463 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:13.878319 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.878416 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:13.878702 139856449679360 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.878782 139856449679360 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 12:41:13.878888 139856449679360 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:41:13.878925 139856449679360 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:41:13.878954 139856449679360 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:41:13.879015 139856449679360 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.881239 139856449679360 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:41:13.886759 139856449679360 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.887025 139856449679360 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:41:13.889710 139856449679360 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:41:13.902388 139856449679360 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:41:13.902444 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:13.902480 139856449679360 transformer_layer.py:389] tlayer: self-attention.
I0123 12:41:13.902511 139856449679360 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.902573 139856449679360 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.903150 139856449679360 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.903227 139856449679360 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.903598 139856449679360 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.904294 139856449679360 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.906847 139856449679360 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.907500 139856449679360 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.907578 139856449679360 transformer_layer.py:468] tlayer: End windows.
I0123 12:41:13.907613 139856449679360 transformer_layer.py:472] tlayer: final FFN.
I0123 12:41:13.907672 139856449679360 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.907803 139856449679360 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:41:13.907914 139856449679360 nn_components.py:325] mlp: activation = None
I0123 12:41:13.907953 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:13.910297 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.910395 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:13.912807 139856449679360 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.912885 139856449679360 transformer_base.py:443] tbase: final FFN
I0123 12:41:13.912998 139856449679360 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:41:13.915304 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:13.917148 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.917241 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:13.917521 139856449679360 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.917602 139856449679360 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 12:41:13.917714 139856449679360 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:41:13.917753 139856449679360 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:41:13.917783 139856449679360 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:41:13.917843 139856449679360 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.920136 139856449679360 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:41:13.925816 139856449679360 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.926074 139856449679360 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:41:13.928801 139856449679360 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:41:13.941348 139856449679360 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:41:13.941402 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:13.941436 139856449679360 transformer_layer.py:389] tlayer: self-attention.
I0123 12:41:13.941465 139856449679360 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.941525 139856449679360 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.942094 139856449679360 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.942170 139856449679360 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.942522 139856449679360 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.943208 139856449679360 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.945743 139856449679360 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.946365 139856449679360 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.946441 139856449679360 transformer_layer.py:468] tlayer: End windows.
I0123 12:41:13.946475 139856449679360 transformer_layer.py:472] tlayer: final FFN.
I0123 12:41:13.946532 139856449679360 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.946662 139856449679360 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:41:13.946771 139856449679360 nn_components.py:325] mlp: activation = None
I0123 12:41:13.946808 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:13.948676 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.948767 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:13.951137 139856449679360 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.951219 139856449679360 transformer_base.py:443] tbase: final FFN
I0123 12:41:13.951325 139856449679360 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:41:13.953589 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:13.955434 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.955528 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:13.955810 139856449679360 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.955889 139856449679360 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 12:41:13.955995 139856449679360 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:41:13.956033 139856449679360 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:41:13.956063 139856449679360 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:41:13.956124 139856449679360 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.958347 139856449679360 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:41:13.963738 139856449679360 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.963992 139856449679360 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:41:13.966670 139856449679360 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:41:13.979168 139856449679360 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:41:13.979222 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:13.979255 139856449679360 transformer_layer.py:389] tlayer: self-attention.
I0123 12:41:13.979284 139856449679360 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.979344 139856449679360 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.979905 139856449679360 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.979981 139856449679360 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.980333 139856449679360 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.981022 139856449679360 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.983566 139856449679360 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.984186 139856449679360 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.984266 139856449679360 transformer_layer.py:468] tlayer: End windows.
I0123 12:41:13.984300 139856449679360 transformer_layer.py:472] tlayer: final FFN.
I0123 12:41:13.984356 139856449679360 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.984484 139856449679360 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:41:13.984595 139856449679360 nn_components.py:325] mlp: activation = None
I0123 12:41:13.984632 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:13.986500 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.986593 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:13.988970 139856449679360 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.989048 139856449679360 transformer_base.py:443] tbase: final FFN
I0123 12:41:13.989154 139856449679360 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:41:13.991797 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:13.993662 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.993756 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:13.994037 139856449679360 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:13.994120 139856449679360 decoder_stack.py:344] dstack: Final layernorm.
I0123 12:41:13.996901 139856449679360 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:18.434338 139856449679360 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
I0123 12:41:19.049245 139856449679360 training_loop.py:409] No working directory specified.
I0123 12:41:19.049371 139856449679360 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0123 12:41:19.050174 139856449679360 checkpoints.py:1062] Restoring legacy Flax checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0123 12:41:22.043558 139856449679360 training_loop.py:447] Only restoring trainable parameters.
I0123 12:41:22.044260 139856449679360 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0123 12:41:22.044318 139856449679360 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0123 12:41:22.044365 139856449679360 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:41:22.044408 139856449679360 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:41:22.044449 139856449679360 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:41:22.044490 139856449679360 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:41:22.044529 139856449679360 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:41:22.044568 139856449679360 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:41:22.044605 139856449679360 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:41:22.044643 139856449679360 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:41:22.044682 139856449679360 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:41:22.044720 139856449679360 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:41:22.044756 139856449679360 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:41:22.044793 139856449679360 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:41:22.044831 139856449679360 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:41:22.044868 139856449679360 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:41:22.044905 139856449679360 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:41:22.044941 139856449679360 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:41:22.044977 139856449679360 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:41:22.045013 139856449679360 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:41:22.045062 139856449679360 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:41:22.045100 139856449679360 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:41:22.045137 139856449679360 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:41:22.045173 139856449679360 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:41:22.045209 139856449679360 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:41:22.045246 139856449679360 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:41:22.045282 139856449679360 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:41:22.045317 139856449679360 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:41:22.045354 139856449679360 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:41:22.045389 139856449679360 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:41:22.045425 139856449679360 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:41:22.045463 139856449679360 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:41:22.045499 139856449679360 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:41:22.045535 139856449679360 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:41:22.045571 139856449679360 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:41:22.045607 139856449679360 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:41:22.045649 139856449679360 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:41:22.045690 139856449679360 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:41:22.045726 139856449679360 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:41:22.045762 139856449679360 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:41:22.045799 139856449679360 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:41:22.045835 139856449679360 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:41:22.045870 139856449679360 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:41:22.045905 139856449679360 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:41:22.045942 139856449679360 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:41:22.045979 139856449679360 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:41:22.046021 139856449679360 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:41:22.046059 139856449679360 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:41:22.046095 139856449679360 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:41:22.046131 139856449679360 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:41:22.046166 139856449679360 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:41:22.046202 139856449679360 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:41:22.046238 139856449679360 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:41:22.046272 139856449679360 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:41:22.046308 139856449679360 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:41:22.046343 139856449679360 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:41:22.046378 139856449679360 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:41:22.046413 139856449679360 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:41:22.046448 139856449679360 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:41:22.046484 139856449679360 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:41:22.046520 139856449679360 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:41:22.046555 139856449679360 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:41:22.046591 139856449679360 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:41:22.046628 139856449679360 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:41:22.046664 139856449679360 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:41:22.046701 139856449679360 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:41:22.046738 139856449679360 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:41:22.046774 139856449679360 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:41:22.046810 139856449679360 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:41:22.046847 139856449679360 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:41:22.046883 139856449679360 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:41:22.046919 139856449679360 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:41:22.046954 139856449679360 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:41:22.046995 139856449679360 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:41:22.047032 139856449679360 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:41:22.047068 139856449679360 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:41:22.047104 139856449679360 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:41:22.047139 139856449679360 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:41:22.047175 139856449679360 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:41:22.047210 139856449679360 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:41:22.047247 139856449679360 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:41:22.047284 139856449679360 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:41:22.047320 139856449679360 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:41:22.047356 139856449679360 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:41:22.047391 139856449679360 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:41:22.047427 139856449679360 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:41:22.047463 139856449679360 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:41:22.047498 139856449679360 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:41:22.047533 139856449679360 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:41:22.047570 139856449679360 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:41:22.047605 139856449679360 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:41:22.047641 139856449679360 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:41:22.047677 139856449679360 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:41:22.047713 139856449679360 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:41:22.047748 139856449679360 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:41:22.047784 139856449679360 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:41:22.047819 139856449679360 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:41:22.047855 139856449679360 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:41:22.047891 139856449679360 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:41:22.047927 139856449679360 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:41:22.047967 139856449679360 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:41:22.048005 139856449679360 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:41:22.048040 139856449679360 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:41:22.048076 139856449679360 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:41:22.048112 139856449679360 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:41:22.048149 139856449679360 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:41:22.048185 139856449679360 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:41:22.048221 139856449679360 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:41:22.048256 139856449679360 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:41:22.048292 139856449679360 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:41:22.048328 139856449679360 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:41:22.048364 139856449679360 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:41:22.048400 139856449679360 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:41:22.048436 139856449679360 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:41:22.048472 139856449679360 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:41:22.048507 139856449679360 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:41:22.048543 139856449679360 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:41:22.048580 139856449679360 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:41:22.048615 139856449679360 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:41:22.048650 139856449679360 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:41:22.048685 139856449679360 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:41:22.048720 139856449679360 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:41:22.048749 139856449679360 training_loop.py:725] Total parameters: 152072288
I0123 12:41:22.048959 139856449679360 training_loop.py:739] Total state size: 0
I0123 12:41:22.071136 139856449679360 training_loop.py:492] Training loop: creating task for mode beam_search
I0123 12:41:22.071407 139856449679360 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0123 12:41:22.071913 139856449679360 training_loop.py:652] Compiling mode beam_search with jit.
I0123 12:41:22.072230 139856449679360 training_loop.py:89] registering functions: dict_keys([])
I0123 12:41:22.088900 139856449679360 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = on_circle e d b; f = incenter f a c b; g = incenter g c b e; h = incenter h b e a; i = incenter i e a c ? perp f g g h
I0123 12:41:22.246654 139856449679360 ddar.py:60] Depth 1/1000 time = 0.1371443271636963
I0123 12:41:22.412978 139856449679360 ddar.py:60] Depth 2/1000 time = 0.16624164581298828
I0123 12:41:22.588440 139856449679360 ddar.py:60] Depth 3/1000 time = 0.17418289184570312
I0123 12:41:22.844236 139856449679360 ddar.py:60] Depth 4/1000 time = 0.24471282958984375
I0123 12:41:22.844319 139856449679360 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:41:22.844373 139856449679360 alphageometry.py:540] Depth 0. There are 1 nodes to expand:
I0123 12:41:22.844406 139856449679360 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : D b d d e 02 ; f : ^ a b a f a f a c 03 ^ b a b f b f b c 04 ; g : ^ c b c g c g c e 05 ^ e b e g e g e c 06 ; h : ^ a b a h a h a e 07 ^ b a b h b h b e 08 ; i : ^ c a c i c i c e 09 ^ e a e i e i e c 10 ? T f g g h {F1} x00
I0123 12:41:22.844435 139856449679360 alphageometry.py:549] Decoding from {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : D b d d e 02 ; f : ^ a b a f a f a c 03 ^ b a b f b f b c 04 ; g : ^ c b c g c g c e 05 ^ e b e g e g e c 06 ; h : ^ a b a h a h a e 07 ^ b a b h b h b e 08 ; i : ^ c a c i c i c e 09 ^ e a e i e i e c 10 ? T f g g h {F1} x00
I0123 12:41:22.970535 139856449679360 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:22.970702 139856449679360 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 12:41:22.970803 139856449679360 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 12:41:22.970879 139856449679360 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 12:41:22.970947 139856449679360 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 12:41:22.971016 139856449679360 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 12:41:22.971082 139856449679360 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 12:41:22.971151 139856449679360 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 12:41:22.971218 139856449679360 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 12:41:22.971285 139856449679360 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 12:41:22.971351 139856449679360 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 12:41:22.971416 139856449679360 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 12:41:22.971482 139856449679360 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 12:41:22.971547 139856449679360 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 12:41:22.971585 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:22.971628 139856449679360 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 12:41:22.971731 139856449679360 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:41:22.971769 139856449679360 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:41:22.971797 139856449679360 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:41:22.973637 139856449679360 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:22.976080 139856449679360 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:41:22.981742 139856449679360 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:22.982008 139856449679360 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:41:22.984628 139856449679360 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:41:22.988427 139856449679360 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:41:22.988483 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:22.988518 139856449679360 transformer_layer.py:389] tlayer: self-attention.
I0123 12:41:22.988549 139856449679360 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:22.988613 139856449679360 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:22.989266 139856449679360 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:22.989343 139856449679360 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:22.989710 139856449679360 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:22.990476 139856449679360 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:22.992957 139856449679360 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:22.993589 139856449679360 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:22.993674 139856449679360 transformer_layer.py:468] tlayer: End windows.
I0123 12:41:22.993709 139856449679360 transformer_layer.py:472] tlayer: final FFN.
I0123 12:41:22.993767 139856449679360 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:22.993895 139856449679360 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:41:22.994219 139856449679360 nn_components.py:325] mlp: activation = None
I0123 12:41:22.994261 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:22.996168 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:22.996262 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:22.998797 139856449679360 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:22.998877 139856449679360 transformer_base.py:443] tbase: final FFN
I0123 12:41:22.999310 139856449679360 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:41:23.001595 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:23.003513 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.003609 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:23.003897 139856449679360 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.003978 139856449679360 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 12:41:23.004085 139856449679360 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:41:23.004123 139856449679360 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:41:23.004152 139856449679360 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:41:23.005942 139856449679360 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.008322 139856449679360 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:41:23.013901 139856449679360 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.014160 139856449679360 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:41:23.016739 139856449679360 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:41:23.020457 139856449679360 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:41:23.020512 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:23.020546 139856449679360 transformer_layer.py:389] tlayer: self-attention.
I0123 12:41:23.020576 139856449679360 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.020691 139856449679360 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.021259 139856449679360 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.021334 139856449679360 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.021698 139856449679360 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.022464 139856449679360 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.024905 139856449679360 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.025525 139856449679360 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.025602 139856449679360 transformer_layer.py:468] tlayer: End windows.
I0123 12:41:23.025636 139856449679360 transformer_layer.py:472] tlayer: final FFN.
I0123 12:41:23.025701 139856449679360 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.025829 139856449679360 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:41:23.026149 139856449679360 nn_components.py:325] mlp: activation = None
I0123 12:41:23.026189 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:23.028075 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.028167 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:23.030659 139856449679360 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.030739 139856449679360 transformer_base.py:443] tbase: final FFN
I0123 12:41:23.031168 139856449679360 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:41:23.033420 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:23.035307 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.035402 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:23.035688 139856449679360 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.035769 139856449679360 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 12:41:23.035875 139856449679360 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:41:23.035913 139856449679360 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:41:23.035942 139856449679360 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:41:23.037743 139856449679360 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.040134 139856449679360 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:41:23.045650 139856449679360 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.045907 139856449679360 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:41:23.048450 139856449679360 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:41:23.052104 139856449679360 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:41:23.052166 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:23.052202 139856449679360 transformer_layer.py:389] tlayer: self-attention.
I0123 12:41:23.052232 139856449679360 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.052295 139856449679360 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.053335 139856449679360 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.053413 139856449679360 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.053781 139856449679360 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.054552 139856449679360 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.057016 139856449679360 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.057633 139856449679360 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.057717 139856449679360 transformer_layer.py:468] tlayer: End windows.
I0123 12:41:23.057750 139856449679360 transformer_layer.py:472] tlayer: final FFN.
I0123 12:41:23.057808 139856449679360 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.057936 139856449679360 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:41:23.058255 139856449679360 nn_components.py:325] mlp: activation = None
I0123 12:41:23.058297 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:23.060210 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.060302 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:23.063007 139856449679360 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.063086 139856449679360 transformer_base.py:443] tbase: final FFN
I0123 12:41:23.063514 139856449679360 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:41:23.065931 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:23.067842 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.067937 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:23.068228 139856449679360 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.068309 139856449679360 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 12:41:23.068416 139856449679360 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:41:23.068453 139856449679360 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:41:23.068482 139856449679360 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:41:23.070295 139856449679360 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.072672 139856449679360 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:41:23.078234 139856449679360 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.078486 139856449679360 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:41:23.081041 139856449679360 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:41:23.084697 139856449679360 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:41:23.084752 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:23.084786 139856449679360 transformer_layer.py:389] tlayer: self-attention.
I0123 12:41:23.084823 139856449679360 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.084888 139856449679360 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.085512 139856449679360 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.085588 139856449679360 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.085951 139856449679360 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.086740 139856449679360 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.089218 139856449679360 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.089839 139856449679360 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.089915 139856449679360 transformer_layer.py:468] tlayer: End windows.
I0123 12:41:23.089949 139856449679360 transformer_layer.py:472] tlayer: final FFN.
I0123 12:41:23.090006 139856449679360 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.090133 139856449679360 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:41:23.090451 139856449679360 nn_components.py:325] mlp: activation = None
I0123 12:41:23.090493 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:23.092392 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.092484 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:23.094993 139856449679360 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.095071 139856449679360 transformer_base.py:443] tbase: final FFN
I0123 12:41:23.095493 139856449679360 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:41:23.097740 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:23.099641 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.099734 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:23.100021 139856449679360 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.100102 139856449679360 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 12:41:23.100210 139856449679360 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:41:23.100248 139856449679360 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:41:23.100277 139856449679360 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:41:23.102072 139856449679360 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.104444 139856449679360 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:41:23.109989 139856449679360 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.110241 139856449679360 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:41:23.112784 139856449679360 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:41:23.116429 139856449679360 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:41:23.116482 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:23.116515 139856449679360 transformer_layer.py:389] tlayer: self-attention.
I0123 12:41:23.116544 139856449679360 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.116610 139856449679360 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.117217 139856449679360 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.117294 139856449679360 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.117651 139856449679360 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.118412 139856449679360 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.120870 139856449679360 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.121488 139856449679360 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.121567 139856449679360 transformer_layer.py:468] tlayer: End windows.
I0123 12:41:23.121600 139856449679360 transformer_layer.py:472] tlayer: final FFN.
I0123 12:41:23.121664 139856449679360 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.121794 139856449679360 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:41:23.122114 139856449679360 nn_components.py:325] mlp: activation = None
I0123 12:41:23.122155 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:23.124062 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.124154 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:23.126677 139856449679360 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.126756 139856449679360 transformer_base.py:443] tbase: final FFN
I0123 12:41:23.127179 139856449679360 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:41:23.129429 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:23.131337 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.131433 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:23.131719 139856449679360 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.131800 139856449679360 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 12:41:23.131908 139856449679360 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:41:23.131945 139856449679360 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:41:23.131975 139856449679360 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:41:23.133780 139856449679360 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.136168 139856449679360 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:41:23.141750 139856449679360 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.142006 139856449679360 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:41:23.144583 139856449679360 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:41:23.148230 139856449679360 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:41:23.148285 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:23.148319 139856449679360 transformer_layer.py:389] tlayer: self-attention.
I0123 12:41:23.148349 139856449679360 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.148416 139856449679360 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.149029 139856449679360 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.149107 139856449679360 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.149462 139856449679360 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.150227 139856449679360 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.152660 139856449679360 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.153270 139856449679360 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.153344 139856449679360 transformer_layer.py:468] tlayer: End windows.
I0123 12:41:23.153377 139856449679360 transformer_layer.py:472] tlayer: final FFN.
I0123 12:41:23.153434 139856449679360 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.153579 139856449679360 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:41:23.153900 139856449679360 nn_components.py:325] mlp: activation = None
I0123 12:41:23.153943 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:23.155831 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.155922 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:23.158419 139856449679360 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.158498 139856449679360 transformer_base.py:443] tbase: final FFN
I0123 12:41:23.158915 139856449679360 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:41:23.161145 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:23.163057 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.163151 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:23.163561 139856449679360 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.163640 139856449679360 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 12:41:23.163746 139856449679360 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:41:23.163783 139856449679360 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:41:23.163812 139856449679360 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:41:23.165762 139856449679360 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.168148 139856449679360 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:41:23.173752 139856449679360 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.174011 139856449679360 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:41:23.176562 139856449679360 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:41:23.180192 139856449679360 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:41:23.180247 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:23.180280 139856449679360 transformer_layer.py:389] tlayer: self-attention.
I0123 12:41:23.180310 139856449679360 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.180371 139856449679360 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.181383 139856449679360 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.181459 139856449679360 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.181818 139856449679360 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.182580 139856449679360 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.185005 139856449679360 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.185618 139856449679360 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.185704 139856449679360 transformer_layer.py:468] tlayer: End windows.
I0123 12:41:23.185738 139856449679360 transformer_layer.py:472] tlayer: final FFN.
I0123 12:41:23.185796 139856449679360 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.185923 139856449679360 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:41:23.186239 139856449679360 nn_components.py:325] mlp: activation = None
I0123 12:41:23.186281 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:23.188173 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.188266 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:23.190776 139856449679360 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.190855 139856449679360 transformer_base.py:443] tbase: final FFN
I0123 12:41:23.191280 139856449679360 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:41:23.193506 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:23.195401 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.195495 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:23.195781 139856449679360 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.195861 139856449679360 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 12:41:23.195967 139856449679360 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:41:23.196007 139856449679360 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:41:23.196036 139856449679360 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:41:23.197834 139856449679360 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.200209 139856449679360 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:41:23.205750 139856449679360 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.206009 139856449679360 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:41:23.208567 139856449679360 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:41:23.212369 139856449679360 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:41:23.212423 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:23.212457 139856449679360 transformer_layer.py:389] tlayer: self-attention.
I0123 12:41:23.212487 139856449679360 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.212549 139856449679360 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.213160 139856449679360 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.213242 139856449679360 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.213600 139856449679360 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.214364 139856449679360 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.216983 139856449679360 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.217593 139856449679360 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.217675 139856449679360 transformer_layer.py:468] tlayer: End windows.
I0123 12:41:23.217710 139856449679360 transformer_layer.py:472] tlayer: final FFN.
I0123 12:41:23.217767 139856449679360 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.217894 139856449679360 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:41:23.218210 139856449679360 nn_components.py:325] mlp: activation = None
I0123 12:41:23.218253 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:23.220156 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.220248 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:23.222742 139856449679360 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.222821 139856449679360 transformer_base.py:443] tbase: final FFN
I0123 12:41:23.223245 139856449679360 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:41:23.225494 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:23.227401 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.227496 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:23.227783 139856449679360 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.227864 139856449679360 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 12:41:23.227970 139856449679360 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:41:23.228008 139856449679360 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:41:23.228038 139856449679360 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:41:23.229835 139856449679360 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.232205 139856449679360 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:41:23.237773 139856449679360 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.238038 139856449679360 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:41:23.240606 139856449679360 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:41:23.244261 139856449679360 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:41:23.244315 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:23.244349 139856449679360 transformer_layer.py:389] tlayer: self-attention.
I0123 12:41:23.244379 139856449679360 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.244441 139856449679360 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.245058 139856449679360 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.245140 139856449679360 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.245498 139856449679360 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.246264 139856449679360 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.248678 139856449679360 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.249293 139856449679360 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.249370 139856449679360 transformer_layer.py:468] tlayer: End windows.
I0123 12:41:23.249403 139856449679360 transformer_layer.py:472] tlayer: final FFN.
I0123 12:41:23.249460 139856449679360 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.249588 139856449679360 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:41:23.249915 139856449679360 nn_components.py:325] mlp: activation = None
I0123 12:41:23.249958 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:23.251848 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.251940 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:23.254437 139856449679360 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.254516 139856449679360 transformer_base.py:443] tbase: final FFN
I0123 12:41:23.254934 139856449679360 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:41:23.257181 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:23.259093 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.259189 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:23.259476 139856449679360 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.259556 139856449679360 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 12:41:23.259663 139856449679360 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:41:23.259701 139856449679360 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:41:23.259730 139856449679360 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:41:23.261508 139856449679360 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.263895 139856449679360 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:41:23.269465 139856449679360 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.269725 139856449679360 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:41:23.272288 139856449679360 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:41:23.275921 139856449679360 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:41:23.275976 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:23.276011 139856449679360 transformer_layer.py:389] tlayer: self-attention.
I0123 12:41:23.276041 139856449679360 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.276102 139856449679360 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.276712 139856449679360 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.276787 139856449679360 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.277147 139856449679360 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.277914 139856449679360 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.280363 139856449679360 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.280988 139856449679360 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.281064 139856449679360 transformer_layer.py:468] tlayer: End windows.
I0123 12:41:23.281097 139856449679360 transformer_layer.py:472] tlayer: final FFN.
I0123 12:41:23.281154 139856449679360 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.281281 139856449679360 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:41:23.281597 139856449679360 nn_components.py:325] mlp: activation = None
I0123 12:41:23.281639 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:23.283563 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.283656 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:23.286188 139856449679360 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.286269 139856449679360 transformer_base.py:443] tbase: final FFN
I0123 12:41:23.286697 139856449679360 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:41:23.288952 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:23.290876 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.290970 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:23.291259 139856449679360 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.291339 139856449679360 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 12:41:23.291446 139856449679360 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:41:23.291484 139856449679360 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:41:23.291514 139856449679360 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:41:23.293299 139856449679360 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.295693 139856449679360 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:41:23.301290 139856449679360 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.301542 139856449679360 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:41:23.304105 139856449679360 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:41:23.307773 139856449679360 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:41:23.307827 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:23.307861 139856449679360 transformer_layer.py:389] tlayer: self-attention.
I0123 12:41:23.307890 139856449679360 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.307951 139856449679360 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.308932 139856449679360 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.309010 139856449679360 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.309371 139856449679360 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.310157 139856449679360 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.312598 139856449679360 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.313219 139856449679360 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.313296 139856449679360 transformer_layer.py:468] tlayer: End windows.
I0123 12:41:23.313330 139856449679360 transformer_layer.py:472] tlayer: final FFN.
I0123 12:41:23.313388 139856449679360 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.313516 139856449679360 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:41:23.313893 139856449679360 nn_components.py:325] mlp: activation = None
I0123 12:41:23.313938 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:23.315846 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.315939 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:23.318736 139856449679360 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.318815 139856449679360 transformer_base.py:443] tbase: final FFN
I0123 12:41:23.319237 139856449679360 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:41:23.321487 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:23.323401 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.323497 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:23.323786 139856449679360 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.323867 139856449679360 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 12:41:23.323975 139856449679360 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:41:23.324013 139856449679360 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:41:23.324043 139856449679360 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:41:23.325841 139856449679360 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.328224 139856449679360 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:41:23.333836 139856449679360 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.334095 139856449679360 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:41:23.336684 139856449679360 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:41:23.340378 139856449679360 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:41:23.340432 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:23.340466 139856449679360 transformer_layer.py:389] tlayer: self-attention.
I0123 12:41:23.340497 139856449679360 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.340559 139856449679360 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.341174 139856449679360 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.341250 139856449679360 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.341606 139856449679360 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.342386 139856449679360 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.344839 139856449679360 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.345461 139856449679360 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.345537 139856449679360 transformer_layer.py:468] tlayer: End windows.
I0123 12:41:23.345571 139856449679360 transformer_layer.py:472] tlayer: final FFN.
I0123 12:41:23.345628 139856449679360 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.345762 139856449679360 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:41:23.346081 139856449679360 nn_components.py:325] mlp: activation = None
I0123 12:41:23.346123 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:23.348017 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.348109 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:23.350625 139856449679360 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.350705 139856449679360 transformer_base.py:443] tbase: final FFN
I0123 12:41:23.351128 139856449679360 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:41:23.353380 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:23.355290 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.355386 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:23.355678 139856449679360 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.355924 139856449679360 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 12:41:23.355992 139856449679360 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 12:41:23.356048 139856449679360 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 12:41:23.356102 139856449679360 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 12:41:23.356154 139856449679360 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 12:41:23.356206 139856449679360 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 12:41:23.356258 139856449679360 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 12:41:23.356311 139856449679360 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 12:41:23.356363 139856449679360 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 12:41:23.356415 139856449679360 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 12:41:23.356464 139856449679360 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 12:41:23.356515 139856449679360 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 12:41:23.356551 139856449679360 decoder_stack.py:344] dstack: Final layernorm.
I0123 12:41:23.359456 139856449679360 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:41:23.404477 139856449679360 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.404559 139856449679360 decoder_stack.py:333] dstack: autoregressive generator.
I0123 12:41:23.404611 139856449679360 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 12:41:23.404714 139856449679360 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:41:23.404757 139856449679360 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:41:23.404788 139856449679360 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:41:23.404851 139856449679360 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.407212 139856449679360 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:41:23.412561 139856449679360 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.412818 139856449679360 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:41:23.415383 139856449679360 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:41:23.428048 139856449679360 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:41:23.428103 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:23.428138 139856449679360 transformer_layer.py:389] tlayer: self-attention.
I0123 12:41:23.428167 139856449679360 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.428229 139856449679360 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.428790 139856449679360 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.428865 139856449679360 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.429220 139856449679360 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.429913 139856449679360 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.432436 139856449679360 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.433050 139856449679360 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.433127 139856449679360 transformer_layer.py:468] tlayer: End windows.
I0123 12:41:23.433161 139856449679360 transformer_layer.py:472] tlayer: final FFN.
I0123 12:41:23.433219 139856449679360 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.433347 139856449679360 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:41:23.433455 139856449679360 nn_components.py:325] mlp: activation = None
I0123 12:41:23.433492 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:23.435674 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.435767 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:23.438151 139856449679360 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.438238 139856449679360 transformer_base.py:443] tbase: final FFN
I0123 12:41:23.438344 139856449679360 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:41:23.440590 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:23.442447 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.442543 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:23.442833 139856449679360 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.442916 139856449679360 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 12:41:23.443022 139856449679360 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:41:23.443059 139856449679360 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:41:23.443101 139856449679360 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:41:23.443166 139856449679360 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.445377 139856449679360 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:41:23.450745 139856449679360 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.451003 139856449679360 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:41:23.453759 139856449679360 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:41:23.466181 139856449679360 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:41:23.466236 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:23.466271 139856449679360 transformer_layer.py:389] tlayer: self-attention.
I0123 12:41:23.466301 139856449679360 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.466363 139856449679360 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.466918 139856449679360 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.466993 139856449679360 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.467347 139856449679360 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.468024 139856449679360 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.470533 139856449679360 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.471148 139856449679360 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.471228 139856449679360 transformer_layer.py:468] tlayer: End windows.
I0123 12:41:23.471262 139856449679360 transformer_layer.py:472] tlayer: final FFN.
I0123 12:41:23.471320 139856449679360 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.471447 139856449679360 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:41:23.471556 139856449679360 nn_components.py:325] mlp: activation = None
I0123 12:41:23.471594 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:23.473437 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.473530 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:23.475942 139856449679360 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.476021 139856449679360 transformer_base.py:443] tbase: final FFN
I0123 12:41:23.476128 139856449679360 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:41:23.478411 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:23.480268 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.480361 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:23.480648 139856449679360 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.480728 139856449679360 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 12:41:23.480836 139856449679360 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:41:23.480874 139856449679360 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:41:23.480909 139856449679360 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:41:23.480973 139856449679360 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.483207 139856449679360 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:41:23.488591 139856449679360 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.488846 139856449679360 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:41:23.491908 139856449679360 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:41:23.504420 139856449679360 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:41:23.504474 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:23.504508 139856449679360 transformer_layer.py:389] tlayer: self-attention.
I0123 12:41:23.504538 139856449679360 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.504598 139856449679360 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.505146 139856449679360 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.505221 139856449679360 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.505573 139856449679360 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.506251 139856449679360 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.508747 139856449679360 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.509359 139856449679360 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.509435 139856449679360 transformer_layer.py:468] tlayer: End windows.
I0123 12:41:23.509469 139856449679360 transformer_layer.py:472] tlayer: final FFN.
I0123 12:41:23.509527 139856449679360 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.509660 139856449679360 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:41:23.509770 139856449679360 nn_components.py:325] mlp: activation = None
I0123 12:41:23.509807 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:23.511648 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.511740 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:23.514137 139856449679360 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.514214 139856449679360 transformer_base.py:443] tbase: final FFN
I0123 12:41:23.514321 139856449679360 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:41:23.516567 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:23.518413 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.518506 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:23.518792 139856449679360 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.518872 139856449679360 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 12:41:23.518979 139856449679360 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:41:23.519016 139856449679360 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:41:23.519047 139856449679360 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:41:23.519114 139856449679360 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.521339 139856449679360 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:41:23.526700 139856449679360 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.526954 139856449679360 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:41:23.529611 139856449679360 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:41:23.541861 139856449679360 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:41:23.541914 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:23.541949 139856449679360 transformer_layer.py:389] tlayer: self-attention.
I0123 12:41:23.541978 139856449679360 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.542039 139856449679360 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.542595 139856449679360 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.542670 139856449679360 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.543023 139856449679360 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.543693 139856449679360 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.546212 139856449679360 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.546828 139856449679360 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.546905 139856449679360 transformer_layer.py:468] tlayer: End windows.
I0123 12:41:23.546938 139856449679360 transformer_layer.py:472] tlayer: final FFN.
I0123 12:41:23.546996 139856449679360 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.547123 139856449679360 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:41:23.547231 139856449679360 nn_components.py:325] mlp: activation = None
I0123 12:41:23.547268 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:23.549108 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.549199 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:23.551611 139856449679360 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.551689 139856449679360 transformer_base.py:443] tbase: final FFN
I0123 12:41:23.551795 139856449679360 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:41:23.554061 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:23.555894 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.555988 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:23.556282 139856449679360 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.556363 139856449679360 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 12:41:23.556469 139856449679360 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:41:23.556507 139856449679360 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:41:23.556537 139856449679360 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:41:23.556598 139856449679360 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.558834 139856449679360 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:41:23.564178 139856449679360 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.564435 139856449679360 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:41:23.567073 139856449679360 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:41:23.579364 139856449679360 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:41:23.579418 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:23.579453 139856449679360 transformer_layer.py:389] tlayer: self-attention.
I0123 12:41:23.579482 139856449679360 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.579543 139856449679360 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.580092 139856449679360 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.580167 139856449679360 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.580519 139856449679360 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.581188 139856449679360 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.583662 139856449679360 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.584273 139856449679360 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.584349 139856449679360 transformer_layer.py:468] tlayer: End windows.
I0123 12:41:23.584382 139856449679360 transformer_layer.py:472] tlayer: final FFN.
I0123 12:41:23.584439 139856449679360 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.584566 139856449679360 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:41:23.584674 139856449679360 nn_components.py:325] mlp: activation = None
I0123 12:41:23.584711 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:23.586545 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.586637 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:23.589008 139856449679360 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.589086 139856449679360 transformer_base.py:443] tbase: final FFN
I0123 12:41:23.589194 139856449679360 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:41:23.591468 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:23.593321 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.593415 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:23.593711 139856449679360 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.593792 139856449679360 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 12:41:23.593898 139856449679360 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:41:23.593935 139856449679360 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:41:23.593966 139856449679360 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:41:23.594028 139856449679360 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.596266 139856449679360 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:41:23.601793 139856449679360 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.602046 139856449679360 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:41:23.605082 139856449679360 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:41:23.617343 139856449679360 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:41:23.617397 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:23.617430 139856449679360 transformer_layer.py:389] tlayer: self-attention.
I0123 12:41:23.617460 139856449679360 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.617520 139856449679360 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.618075 139856449679360 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.618150 139856449679360 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.618497 139856449679360 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.619172 139856449679360 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.621702 139856449679360 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.622317 139856449679360 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.622392 139856449679360 transformer_layer.py:468] tlayer: End windows.
I0123 12:41:23.622425 139856449679360 transformer_layer.py:472] tlayer: final FFN.
I0123 12:41:23.622481 139856449679360 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.622608 139856449679360 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:41:23.622714 139856449679360 nn_components.py:325] mlp: activation = None
I0123 12:41:23.622751 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:23.624602 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.624694 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:23.627087 139856449679360 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.627166 139856449679360 transformer_base.py:443] tbase: final FFN
I0123 12:41:23.627272 139856449679360 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:41:23.629518 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:23.631353 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.631447 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:23.631736 139856449679360 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.631817 139856449679360 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 12:41:23.631924 139856449679360 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:41:23.631962 139856449679360 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:41:23.631992 139856449679360 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:41:23.632054 139856449679360 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.634259 139856449679360 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:41:23.639662 139856449679360 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.639918 139856449679360 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:41:23.642593 139856449679360 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:41:23.654910 139856449679360 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:41:23.654964 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:23.654999 139856449679360 transformer_layer.py:389] tlayer: self-attention.
I0123 12:41:23.655029 139856449679360 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.655089 139856449679360 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.655640 139856449679360 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.655714 139856449679360 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.656065 139856449679360 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.656744 139856449679360 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.659257 139856449679360 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.659874 139856449679360 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.659950 139856449679360 transformer_layer.py:468] tlayer: End windows.
I0123 12:41:23.659983 139856449679360 transformer_layer.py:472] tlayer: final FFN.
I0123 12:41:23.660041 139856449679360 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.660167 139856449679360 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:41:23.660274 139856449679360 nn_components.py:325] mlp: activation = None
I0123 12:41:23.660311 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:23.662153 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.662247 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:23.664630 139856449679360 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.664707 139856449679360 transformer_base.py:443] tbase: final FFN
I0123 12:41:23.664816 139856449679360 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:41:23.667067 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:23.668895 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.668988 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:23.669280 139856449679360 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.669360 139856449679360 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 12:41:23.669468 139856449679360 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:41:23.669506 139856449679360 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:41:23.669536 139856449679360 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:41:23.669599 139856449679360 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.671825 139856449679360 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:41:23.677179 139856449679360 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.677438 139856449679360 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:41:23.680080 139856449679360 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:41:23.692374 139856449679360 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:41:23.692429 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:23.692463 139856449679360 transformer_layer.py:389] tlayer: self-attention.
I0123 12:41:23.692493 139856449679360 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.692554 139856449679360 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.693109 139856449679360 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.693184 139856449679360 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.693536 139856449679360 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.694219 139856449679360 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.696726 139856449679360 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.697334 139856449679360 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.697410 139856449679360 transformer_layer.py:468] tlayer: End windows.
I0123 12:41:23.697443 139856449679360 transformer_layer.py:472] tlayer: final FFN.
I0123 12:41:23.697499 139856449679360 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.697624 139856449679360 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:41:23.697737 139856449679360 nn_components.py:325] mlp: activation = None
I0123 12:41:23.697774 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:23.699719 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.699810 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:23.702193 139856449679360 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.702270 139856449679360 transformer_base.py:443] tbase: final FFN
I0123 12:41:23.702375 139856449679360 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:41:23.704598 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:23.706440 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.706534 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:23.706823 139856449679360 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.706904 139856449679360 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 12:41:23.707010 139856449679360 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:41:23.707048 139856449679360 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:41:23.707078 139856449679360 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:41:23.707139 139856449679360 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.709357 139856449679360 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:41:23.714950 139856449679360 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.715213 139856449679360 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:41:23.718239 139856449679360 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:41:23.730468 139856449679360 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:41:23.730522 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:23.730556 139856449679360 transformer_layer.py:389] tlayer: self-attention.
I0123 12:41:23.730586 139856449679360 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.730646 139856449679360 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.731201 139856449679360 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.731276 139856449679360 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.731629 139856449679360 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.732303 139856449679360 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.734802 139856449679360 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.735424 139856449679360 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.735501 139856449679360 transformer_layer.py:468] tlayer: End windows.
I0123 12:41:23.735534 139856449679360 transformer_layer.py:472] tlayer: final FFN.
I0123 12:41:23.735591 139856449679360 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.735718 139856449679360 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:41:23.735827 139856449679360 nn_components.py:325] mlp: activation = None
I0123 12:41:23.735864 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:23.737695 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.737787 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:23.740176 139856449679360 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.740254 139856449679360 transformer_base.py:443] tbase: final FFN
I0123 12:41:23.740362 139856449679360 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:41:23.742613 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:23.744438 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.744532 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:23.744818 139856449679360 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.744899 139856449679360 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 12:41:23.745005 139856449679360 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:41:23.745042 139856449679360 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:41:23.745071 139856449679360 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:41:23.745133 139856449679360 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.747328 139856449679360 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:41:23.752675 139856449679360 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.752938 139856449679360 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:41:23.755594 139856449679360 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:41:23.767850 139856449679360 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:41:23.767909 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:23.767943 139856449679360 transformer_layer.py:389] tlayer: self-attention.
I0123 12:41:23.767972 139856449679360 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.768034 139856449679360 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.768591 139856449679360 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.768667 139856449679360 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.769022 139856449679360 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.769708 139856449679360 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.772215 139856449679360 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.772818 139856449679360 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.772893 139856449679360 transformer_layer.py:468] tlayer: End windows.
I0123 12:41:23.772926 139856449679360 transformer_layer.py:472] tlayer: final FFN.
I0123 12:41:23.772983 139856449679360 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.773108 139856449679360 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:41:23.773214 139856449679360 nn_components.py:325] mlp: activation = None
I0123 12:41:23.773251 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:23.775088 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.775181 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:23.777557 139856449679360 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.777635 139856449679360 transformer_base.py:443] tbase: final FFN
I0123 12:41:23.777750 139856449679360 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:41:23.779983 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:23.781818 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.781913 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:23.782198 139856449679360 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.782279 139856449679360 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 12:41:23.782383 139856449679360 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:41:23.782420 139856449679360 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:41:23.782449 139856449679360 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:41:23.782510 139856449679360 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.784717 139856449679360 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:41:23.790098 139856449679360 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.790355 139856449679360 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:41:23.793018 139856449679360 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:41:23.805295 139856449679360 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:41:23.805350 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:23.805384 139856449679360 transformer_layer.py:389] tlayer: self-attention.
I0123 12:41:23.805413 139856449679360 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.805473 139856449679360 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.806032 139856449679360 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.806107 139856449679360 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.806463 139856449679360 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.807139 139856449679360 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.809666 139856449679360 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.810431 139856449679360 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.810507 139856449679360 transformer_layer.py:468] tlayer: End windows.
I0123 12:41:23.810541 139856449679360 transformer_layer.py:472] tlayer: final FFN.
I0123 12:41:23.810597 139856449679360 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.810724 139856449679360 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:41:23.810833 139856449679360 nn_components.py:325] mlp: activation = None
I0123 12:41:23.810871 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:23.812886 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.812979 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:23.815370 139856449679360 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.815449 139856449679360 transformer_base.py:443] tbase: final FFN
I0123 12:41:23.815555 139856449679360 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:41:23.817795 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:23.819643 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.819735 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:23.820024 139856449679360 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.820106 139856449679360 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 12:41:23.820214 139856449679360 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:41:23.820252 139856449679360 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:41:23.820281 139856449679360 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:41:23.820343 139856449679360 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.822571 139856449679360 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:41:23.828043 139856449679360 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.828302 139856449679360 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:41:23.831333 139856449679360 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:41:23.843729 139856449679360 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:41:23.843783 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:23.843816 139856449679360 transformer_layer.py:389] tlayer: self-attention.
I0123 12:41:23.843845 139856449679360 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.843905 139856449679360 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.844456 139856449679360 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.844532 139856449679360 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.844886 139856449679360 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.845567 139856449679360 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.848060 139856449679360 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.848671 139856449679360 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.848747 139856449679360 transformer_layer.py:468] tlayer: End windows.
I0123 12:41:23.848779 139856449679360 transformer_layer.py:472] tlayer: final FFN.
I0123 12:41:23.848836 139856449679360 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.848964 139856449679360 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:41:23.849072 139856449679360 nn_components.py:325] mlp: activation = None
I0123 12:41:23.849109 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:23.850969 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.851063 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:23.853459 139856449679360 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.853538 139856449679360 transformer_base.py:443] tbase: final FFN
I0123 12:41:23.853650 139856449679360 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:41:23.855889 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:23.857728 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.857821 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:23.858116 139856449679360 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.858206 139856449679360 decoder_stack.py:344] dstack: Final layernorm.
I0123 12:41:23.861107 139856449679360 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:41:23.913156 139856449679360 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:23.913240 139856449679360 decoder_stack.py:333] dstack: autoregressive generator.
I0123 12:41:23.913291 139856449679360 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 12:41:23.913394 139856449679360 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:41:23.913431 139856449679360 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:41:23.913460 139856449679360 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:41:23.913522 139856449679360 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:23.915909 139856449679360 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:41:23.921470 139856449679360 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:23.921736 139856449679360 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:41:23.924393 139856449679360 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:41:23.937120 139856449679360 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:41:23.937175 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:23.937210 139856449679360 transformer_layer.py:389] tlayer: self-attention.
I0123 12:41:23.937240 139856449679360 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:23.937302 139856449679360 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:23.937870 139856449679360 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:23.937950 139856449679360 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:23.938329 139856449679360 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:23.939052 139856449679360 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:23.941545 139856449679360 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:23.942183 139856449679360 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:23.942264 139856449679360 transformer_layer.py:468] tlayer: End windows.
I0123 12:41:23.942299 139856449679360 transformer_layer.py:472] tlayer: final FFN.
I0123 12:41:23.942359 139856449679360 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:23.942493 139856449679360 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:41:23.942605 139856449679360 nn_components.py:325] mlp: activation = None
I0123 12:41:23.942645 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:23.944606 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:23.944697 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:23.947150 139856449679360 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:23.947232 139856449679360 transformer_base.py:443] tbase: final FFN
I0123 12:41:23.947344 139856449679360 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:41:23.949544 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:23.951446 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:23.951543 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:23.951845 139856449679360 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:23.951929 139856449679360 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 12:41:23.952037 139856449679360 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:41:23.952075 139856449679360 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:41:23.952105 139856449679360 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:41:23.952167 139856449679360 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:23.954413 139856449679360 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:41:23.959982 139856449679360 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:23.960242 139856449679360 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:41:23.962825 139856449679360 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:41:23.975211 139856449679360 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:41:23.975265 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:23.975299 139856449679360 transformer_layer.py:389] tlayer: self-attention.
I0123 12:41:23.975328 139856449679360 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:23.975389 139856449679360 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:23.975941 139856449679360 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:23.976018 139856449679360 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:23.976376 139856449679360 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:23.977051 139856449679360 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:23.979498 139856449679360 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:23.980115 139856449679360 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:23.980191 139856449679360 transformer_layer.py:468] tlayer: End windows.
I0123 12:41:23.980225 139856449679360 transformer_layer.py:472] tlayer: final FFN.
I0123 12:41:23.980282 139856449679360 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:23.980411 139856449679360 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:41:23.980519 139856449679360 nn_components.py:325] mlp: activation = None
I0123 12:41:23.980556 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:23.982473 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:23.982565 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:23.984957 139856449679360 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:23.985035 139856449679360 transformer_base.py:443] tbase: final FFN
I0123 12:41:23.985142 139856449679360 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:41:23.987319 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:23.989175 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:23.989270 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:23.989561 139856449679360 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:23.989647 139856449679360 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 12:41:23.989757 139856449679360 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:41:23.989794 139856449679360 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:41:23.989825 139856449679360 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:41:23.989886 139856449679360 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:23.992096 139856449679360 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:41:23.997551 139856449679360 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:23.997822 139856449679360 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:41:24.000395 139856449679360 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:41:24.013395 139856449679360 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:41:24.013451 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:24.013485 139856449679360 transformer_layer.py:389] tlayer: self-attention.
I0123 12:41:24.013515 139856449679360 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.013576 139856449679360 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.014137 139856449679360 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.014212 139856449679360 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.014732 139856449679360 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.015400 139856449679360 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.017809 139856449679360 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.018429 139856449679360 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.018506 139856449679360 transformer_layer.py:468] tlayer: End windows.
I0123 12:41:24.018541 139856449679360 transformer_layer.py:472] tlayer: final FFN.
I0123 12:41:24.018599 139856449679360 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.018727 139856449679360 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:41:24.018835 139856449679360 nn_components.py:325] mlp: activation = None
I0123 12:41:24.018873 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:24.020792 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.020887 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:24.023277 139856449679360 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.023356 139856449679360 transformer_base.py:443] tbase: final FFN
I0123 12:41:24.023463 139856449679360 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:41:24.025660 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:24.027507 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.027601 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:24.027889 139856449679360 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.027970 139856449679360 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 12:41:24.028078 139856449679360 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:41:24.028116 139856449679360 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:41:24.028145 139856449679360 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:41:24.028206 139856449679360 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.030442 139856449679360 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:41:24.035894 139856449679360 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.036159 139856449679360 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:41:24.038740 139856449679360 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:41:24.051050 139856449679360 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:41:24.051105 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:24.051139 139856449679360 transformer_layer.py:389] tlayer: self-attention.
I0123 12:41:24.051168 139856449679360 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.051229 139856449679360 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.051778 139856449679360 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.051853 139856449679360 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.052206 139856449679360 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.052888 139856449679360 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.055326 139856449679360 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.055945 139856449679360 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.056023 139856449679360 transformer_layer.py:468] tlayer: End windows.
I0123 12:41:24.056056 139856449679360 transformer_layer.py:472] tlayer: final FFN.
I0123 12:41:24.056113 139856449679360 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.056239 139856449679360 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:41:24.056346 139856449679360 nn_components.py:325] mlp: activation = None
I0123 12:41:24.056383 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:24.058295 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.058388 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:24.060756 139856449679360 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.060834 139856449679360 transformer_base.py:443] tbase: final FFN
I0123 12:41:24.060941 139856449679360 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:41:24.063123 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:24.064964 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.065059 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:24.065350 139856449679360 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.065431 139856449679360 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 12:41:24.065539 139856449679360 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:41:24.065577 139856449679360 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:41:24.065606 139856449679360 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:41:24.065674 139856449679360 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.067884 139856449679360 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:41:24.073333 139856449679360 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.073591 139856449679360 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:41:24.076194 139856449679360 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:41:24.088568 139856449679360 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:41:24.088623 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:24.088657 139856449679360 transformer_layer.py:389] tlayer: self-attention.
I0123 12:41:24.088687 139856449679360 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.088749 139856449679360 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.089303 139856449679360 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.089379 139856449679360 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.089743 139856449679360 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.090429 139856449679360 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.092857 139856449679360 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.093483 139856449679360 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.093560 139856449679360 transformer_layer.py:468] tlayer: End windows.
I0123 12:41:24.093594 139856449679360 transformer_layer.py:472] tlayer: final FFN.
I0123 12:41:24.093657 139856449679360 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.093789 139856449679360 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:41:24.093896 139856449679360 nn_components.py:325] mlp: activation = None
I0123 12:41:24.093934 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:24.095840 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.095933 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:24.098314 139856449679360 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.098392 139856449679360 transformer_base.py:443] tbase: final FFN
I0123 12:41:24.098498 139856449679360 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:41:24.100679 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:24.102527 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.102621 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:24.102908 139856449679360 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.102988 139856449679360 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 12:41:24.103097 139856449679360 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:41:24.103135 139856449679360 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:41:24.103164 139856449679360 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:41:24.103227 139856449679360 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.105436 139856449679360 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:41:24.110881 139856449679360 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.111141 139856449679360 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:41:24.113738 139856449679360 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:41:24.126992 139856449679360 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:41:24.127047 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:24.127081 139856449679360 transformer_layer.py:389] tlayer: self-attention.
I0123 12:41:24.127110 139856449679360 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.127171 139856449679360 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.127733 139856449679360 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.127809 139856449679360 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.128166 139856449679360 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.128841 139856449679360 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.131311 139856449679360 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.131932 139856449679360 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.132010 139856449679360 transformer_layer.py:468] tlayer: End windows.
I0123 12:41:24.132044 139856449679360 transformer_layer.py:472] tlayer: final FFN.
I0123 12:41:24.132102 139856449679360 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.132228 139856449679360 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:41:24.132340 139856449679360 nn_components.py:325] mlp: activation = None
I0123 12:41:24.132378 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:24.134304 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.134397 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:24.136773 139856449679360 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.136851 139856449679360 transformer_base.py:443] tbase: final FFN
I0123 12:41:24.136956 139856449679360 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:41:24.139170 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:24.141020 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.141114 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:24.141406 139856449679360 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.141486 139856449679360 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 12:41:24.141592 139856449679360 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:41:24.141629 139856449679360 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:41:24.141665 139856449679360 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:41:24.141729 139856449679360 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.143929 139856449679360 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:41:24.149374 139856449679360 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.149634 139856449679360 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:41:24.152206 139856449679360 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:41:24.164598 139856449679360 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:41:24.164652 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:24.164686 139856449679360 transformer_layer.py:389] tlayer: self-attention.
I0123 12:41:24.164716 139856449679360 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.164782 139856449679360 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.165337 139856449679360 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.165413 139856449679360 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.165769 139856449679360 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.166445 139856449679360 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.168866 139856449679360 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.169484 139856449679360 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.169561 139856449679360 transformer_layer.py:468] tlayer: End windows.
I0123 12:41:24.169594 139856449679360 transformer_layer.py:472] tlayer: final FFN.
I0123 12:41:24.169658 139856449679360 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.169786 139856449679360 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:41:24.169893 139856449679360 nn_components.py:325] mlp: activation = None
I0123 12:41:24.169929 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:24.171839 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.171932 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:24.174327 139856449679360 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.174405 139856449679360 transformer_base.py:443] tbase: final FFN
I0123 12:41:24.174512 139856449679360 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:41:24.176688 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:24.178534 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.178628 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:24.178915 139856449679360 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.178996 139856449679360 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 12:41:24.179101 139856449679360 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:41:24.179139 139856449679360 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:41:24.179168 139856449679360 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:41:24.179229 139856449679360 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.181430 139856449679360 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:41:24.186882 139856449679360 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.187140 139856449679360 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:41:24.189714 139856449679360 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:41:24.202129 139856449679360 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:41:24.202188 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:24.202223 139856449679360 transformer_layer.py:389] tlayer: self-attention.
I0123 12:41:24.202254 139856449679360 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.202316 139856449679360 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.202873 139856449679360 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.202950 139856449679360 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.203304 139856449679360 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.203977 139856449679360 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.206419 139856449679360 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.207036 139856449679360 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.207112 139856449679360 transformer_layer.py:468] tlayer: End windows.
I0123 12:41:24.207145 139856449679360 transformer_layer.py:472] tlayer: final FFN.
I0123 12:41:24.207202 139856449679360 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.207327 139856449679360 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:41:24.207435 139856449679360 nn_components.py:325] mlp: activation = None
I0123 12:41:24.207472 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:24.209392 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.209482 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:24.211874 139856449679360 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.211951 139856449679360 transformer_base.py:443] tbase: final FFN
I0123 12:41:24.212058 139856449679360 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:41:24.214246 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:24.216082 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.216354 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:24.216640 139856449679360 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.216718 139856449679360 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 12:41:24.216824 139856449679360 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:41:24.216862 139856449679360 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:41:24.216891 139856449679360 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:41:24.216951 139856449679360 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.219272 139856449679360 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:41:24.224808 139856449679360 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.225064 139856449679360 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:41:24.227663 139856449679360 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:41:24.240528 139856449679360 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:41:24.240582 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:24.240622 139856449679360 transformer_layer.py:389] tlayer: self-attention.
I0123 12:41:24.240653 139856449679360 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.240714 139856449679360 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.241267 139856449679360 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.241344 139856449679360 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.241706 139856449679360 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.242384 139856449679360 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.244812 139856449679360 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.245425 139856449679360 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.245501 139856449679360 transformer_layer.py:468] tlayer: End windows.
I0123 12:41:24.245534 139856449679360 transformer_layer.py:472] tlayer: final FFN.
I0123 12:41:24.245591 139856449679360 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.245722 139856449679360 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:41:24.245831 139856449679360 nn_components.py:325] mlp: activation = None
I0123 12:41:24.245868 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:24.247799 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.247890 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:24.250271 139856449679360 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.250348 139856449679360 transformer_base.py:443] tbase: final FFN
I0123 12:41:24.250455 139856449679360 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:41:24.252642 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:24.254499 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.254592 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:24.254878 139856449679360 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.254956 139856449679360 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 12:41:24.255064 139856449679360 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:41:24.255101 139856449679360 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:41:24.255131 139856449679360 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:41:24.255192 139856449679360 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.257407 139856449679360 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:41:24.262892 139856449679360 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.263151 139856449679360 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:41:24.265721 139856449679360 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:41:24.278115 139856449679360 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:41:24.278170 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:24.278210 139856449679360 transformer_layer.py:389] tlayer: self-attention.
I0123 12:41:24.278242 139856449679360 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.278301 139856449679360 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.278845 139856449679360 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.278918 139856449679360 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.279271 139856449679360 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.279952 139856449679360 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.282399 139856449679360 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.283007 139856449679360 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.283082 139856449679360 transformer_layer.py:468] tlayer: End windows.
I0123 12:41:24.283115 139856449679360 transformer_layer.py:472] tlayer: final FFN.
I0123 12:41:24.283172 139856449679360 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.283297 139856449679360 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:41:24.283402 139856449679360 nn_components.py:325] mlp: activation = None
I0123 12:41:24.283439 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:24.285366 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.285457 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:24.287845 139856449679360 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.287922 139856449679360 transformer_base.py:443] tbase: final FFN
I0123 12:41:24.288029 139856449679360 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:41:24.290220 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:24.292061 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.292154 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:24.292440 139856449679360 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.292518 139856449679360 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 12:41:24.292623 139856449679360 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:41:24.292661 139856449679360 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:41:24.292691 139856449679360 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:41:24.292753 139856449679360 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.294951 139856449679360 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:41:24.300395 139856449679360 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.300649 139856449679360 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:41:24.303196 139856449679360 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:41:24.315557 139856449679360 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:41:24.315611 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:24.315646 139856449679360 transformer_layer.py:389] tlayer: self-attention.
I0123 12:41:24.315683 139856449679360 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.315744 139856449679360 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.316298 139856449679360 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.316373 139856449679360 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.316722 139856449679360 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.317537 139856449679360 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.319974 139856449679360 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.320589 139856449679360 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.320665 139856449679360 transformer_layer.py:468] tlayer: End windows.
I0123 12:41:24.320699 139856449679360 transformer_layer.py:472] tlayer: final FFN.
I0123 12:41:24.320756 139856449679360 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.320882 139856449679360 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:41:24.320997 139856449679360 nn_components.py:325] mlp: activation = None
I0123 12:41:24.321036 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:24.322965 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.323056 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:24.325599 139856449679360 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.325683 139856449679360 transformer_base.py:443] tbase: final FFN
I0123 12:41:24.325796 139856449679360 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:41:24.327980 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:24.329812 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.329905 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:24.330194 139856449679360 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.330271 139856449679360 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 12:41:24.330378 139856449679360 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:41:24.330416 139856449679360 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:41:24.330446 139856449679360 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:41:24.330507 139856449679360 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.332704 139856449679360 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:41:24.338156 139856449679360 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.338410 139856449679360 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:41:24.340990 139856449679360 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:41:24.353767 139856449679360 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:41:24.353821 139856449679360 attention.py:418] Single window, no scan.
I0123 12:41:24.353856 139856449679360 transformer_layer.py:389] tlayer: self-attention.
I0123 12:41:24.353887 139856449679360 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.353951 139856449679360 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.354513 139856449679360 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.354587 139856449679360 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.354939 139856449679360 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.355623 139856449679360 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.358075 139856449679360 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.358690 139856449679360 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.358766 139856449679360 transformer_layer.py:468] tlayer: End windows.
I0123 12:41:24.358800 139856449679360 transformer_layer.py:472] tlayer: final FFN.
I0123 12:41:24.358856 139856449679360 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.358981 139856449679360 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:41:24.359093 139856449679360 nn_components.py:325] mlp: activation = None
I0123 12:41:24.359131 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:24.361070 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.361162 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:24.363576 139856449679360 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.363654 139856449679360 transformer_base.py:443] tbase: final FFN
I0123 12:41:24.363762 139856449679360 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:41:24.365961 139856449679360 nn_components.py:329] mlp: final activation = None
I0123 12:41:24.367805 139856449679360 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.367897 139856449679360 nn_components.py:261] mlp: residual
I0123 12:41:24.368183 139856449679360 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:24.368265 139856449679360 decoder_stack.py:344] dstack: Final layernorm.
I0123 12:41:24.371077 139856449679360 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:41:36.289116 139856449679360 alphageometry.py:566] LM output (score=-0.615056): "j : C c e j 11 T c e j i 12 ;"
I0123 12:41:36.289444 139856449679360 alphageometry.py:567] Translation: "j = on_line j c e, on_tline j i c e"

I0123 12:41:36.289510 139856449679360 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = on_circle e d b; f = incenter f a c b; g = incenter g c b e; h = incenter h b e a; i = incenter i e a c; j = on_line j c e, on_tline j i c e ? perp f g g h"
I0123 12:41:36.289653 139856449679360 graph.py:498] 
I0123 12:41:36.289711 139856449679360 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = on_circle e d b; f = incenter f a c b; g = incenter g c b e; h = incenter h b e a; i = incenter i e a c; j = on_line j c e, on_tline j i c e ? perp f g g h
I0123 12:41:36.509123 139856449679360 ddar.py:60] Depth 1/1000 time = 0.20096707344055176
I0123 12:41:36.750599 139856449679360 ddar.py:60] Depth 2/1000 time = 0.2413947582244873
I0123 12:41:37.003542 139856449679360 ddar.py:60] Depth 3/1000 time = 0.2516510486602783
I0123 12:41:37.388992 139856449679360 ddar.py:60] Depth 4/1000 time = 0.37198758125305176
I0123 12:41:37.389093 139856449679360 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:41:37.389188 139856449679360 alphageometry.py:566] LM output (score=-0.804433): "j : C e i j 11 D e i i j 12 ;"
I0123 12:41:37.389225 139856449679360 alphageometry.py:567] Translation: "j = on_line j e i, on_circle j i e"

I0123 12:41:37.389260 139856449679360 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = on_circle e d b; f = incenter f a c b; g = incenter g c b e; h = incenter h b e a; i = incenter i e a c; j = on_line j e i, on_circle j i e ? perp f g g h"
I0123 12:41:37.389370 139856449679360 graph.py:498] 
I0123 12:41:37.389416 139856449679360 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = on_circle e d b; f = incenter f a c b; g = incenter g c b e; h = incenter h b e a; i = incenter i e a c; j = on_line j e i, on_circle j i e ? perp f g g h
I0123 12:41:37.603918 139856449679360 ddar.py:60] Depth 1/1000 time = 0.1923353672027588
I0123 12:41:38.078078 139856449679360 ddar.py:60] Depth 2/1000 time = 0.47406768798828125
I0123 12:41:38.318782 139856449679360 ddar.py:60] Depth 3/1000 time = 0.24042105674743652
I0123 12:41:38.560824 139856449679360 ddar.py:60] Depth 4/1000 time = 0.24054312705993652
I0123 12:41:38.898841 139856449679360 ddar.py:60] Depth 5/1000 time = 0.32695841789245605
I0123 12:41:38.898939 139856449679360 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:41:38.899004 139856449679360 alphageometry.py:566] LM output (score=-0.906241): "j : C a e j 11 T a e j i 12 ;"
I0123 12:41:38.899042 139856449679360 alphageometry.py:567] Translation: "j = on_line j a e, on_tline j i a e"

I0123 12:41:38.899078 139856449679360 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = on_circle e d b; f = incenter f a c b; g = incenter g c b e; h = incenter h b e a; i = incenter i e a c; j = on_line j a e, on_tline j i a e ? perp f g g h"
I0123 12:41:38.899200 139856449679360 graph.py:498] 
I0123 12:41:38.899250 139856449679360 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = on_circle e d b; f = incenter f a c b; g = incenter g c b e; h = incenter h b e a; i = incenter i e a c; j = on_line j a e, on_tline j i a e ? perp f g g h
I0123 12:41:39.118525 139856449679360 ddar.py:60] Depth 1/1000 time = 0.1979231834411621
I0123 12:41:39.354321 139856449679360 ddar.py:60] Depth 2/1000 time = 0.23572039604187012
I0123 12:41:39.601907 139856449679360 ddar.py:60] Depth 3/1000 time = 0.24629640579223633
I0123 12:41:39.999945 139856449679360 ddar.py:60] Depth 4/1000 time = 0.381878137588501
I0123 12:41:40.000031 139856449679360 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:41:40.000085 139856449679360 alphageometry.py:566] LM output (score=-1.098592): "j : C e i j 11 D e i e j 12 ;"
I0123 12:41:40.000122 139856449679360 alphageometry.py:567] Translation: "j = on_line j e i, on_circle j e i"

I0123 12:41:40.000158 139856449679360 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = on_circle e d b; f = incenter f a c b; g = incenter g c b e; h = incenter h b e a; i = incenter i e a c; j = on_line j e i, on_circle j e i ? perp f g g h"
I0123 12:41:40.000273 139856449679360 graph.py:498] 
I0123 12:41:40.000320 139856449679360 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = on_circle e d b; f = incenter f a c b; g = incenter g c b e; h = incenter h b e a; i = incenter i e a c; j = on_line j e i, on_circle j e i ? perp f g g h
I0123 12:41:40.220958 139856449679360 ddar.py:60] Depth 1/1000 time = 0.19563746452331543
I0123 12:41:40.456513 139856449679360 ddar.py:60] Depth 2/1000 time = 0.23548412322998047
I0123 12:41:40.875602 139856449679360 ddar.py:60] Depth 3/1000 time = 0.41890382766723633
I0123 12:41:41.122855 139856449679360 ddar.py:60] Depth 4/1000 time = 0.2456979751586914
I0123 12:41:41.461280 139856449679360 ddar.py:60] Depth 5/1000 time = 0.3274075984954834
I0123 12:41:41.461379 139856449679360 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:41:41.461456 139856449679360 alphageometry.py:566] LM output (score=-1.342075): "j : C e i j 11 D e j i j 12 ;"
I0123 12:41:41.461494 139856449679360 alphageometry.py:567] Translation: "j = on_line j e i, on_bline j i e"

I0123 12:41:41.461531 139856449679360 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = on_circle e d b; f = incenter f a c b; g = incenter g c b e; h = incenter h b e a; i = incenter i e a c; j = on_line j e i, on_bline j i e ? perp f g g h"
I0123 12:41:41.461656 139856449679360 graph.py:498] 
I0123 12:41:41.461706 139856449679360 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = on_circle e d b; f = incenter f a c b; g = incenter g c b e; h = incenter h b e a; i = incenter i e a c; j = on_line j e i, on_bline j i e ? perp f g g h
I0123 12:41:41.708155 139856449679360 ddar.py:60] Depth 1/1000 time = 0.19816923141479492
I0123 12:41:41.939767 139856449679360 ddar.py:60] Depth 2/1000 time = 0.2315385341644287
I0123 12:41:42.174086 139856449679360 ddar.py:60] Depth 3/1000 time = 0.2341477870941162
I0123 12:41:42.424664 139856449679360 ddar.py:60] Depth 4/1000 time = 0.249129056930542
I0123 12:41:42.758290 139856449679360 ddar.py:60] Depth 5/1000 time = 0.3226487636566162
I0123 12:41:42.758373 139856449679360 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:41:42.758425 139856449679360 alphageometry.py:566] LM output (score=-1.736107): "j : C a c j 11 T a c j i 12 ;"
I0123 12:41:42.758461 139856449679360 alphageometry.py:567] Translation: "j = on_line j a c, on_tline j i a c"

I0123 12:41:42.758496 139856449679360 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = on_circle e d b; f = incenter f a c b; g = incenter g c b e; h = incenter h b e a; i = incenter i e a c; j = on_line j a c, on_tline j i a c ? perp f g g h"
I0123 12:41:42.758608 139856449679360 graph.py:498] 
I0123 12:41:42.758655 139856449679360 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = on_circle e d b; f = incenter f a c b; g = incenter g c b e; h = incenter h b e a; i = incenter i e a c; j = on_line j a c, on_tline j i a c ? perp f g g h
I0123 12:41:42.970074 139856449679360 ddar.py:60] Depth 1/1000 time = 0.19397544860839844
I0123 12:41:43.398148 139856449679360 ddar.py:60] Depth 2/1000 time = 0.42798447608947754
I0123 12:41:43.645755 139856449679360 ddar.py:60] Depth 3/1000 time = 0.24622845649719238
I0123 12:41:44.035499 139856449679360 ddar.py:60] Depth 4/1000 time = 0.37388086318969727
I0123 12:41:44.035598 139856449679360 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:41:44.035666 139856449679360 alphageometry.py:566] LM output (score=-2.072491): "j : C a b j 11 T a b j h 12 ;"
I0123 12:41:44.035702 139856449679360 alphageometry.py:567] Translation: "j = on_line j a b, on_tline j h a b"

I0123 12:41:44.035739 139856449679360 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = on_circle e d b; f = incenter f a c b; g = incenter g c b e; h = incenter h b e a; i = incenter i e a c; j = on_line j a b, on_tline j h a b ? perp f g g h"
I0123 12:41:44.035858 139856449679360 graph.py:498] 
I0123 12:41:44.035906 139856449679360 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = on_circle e d b; f = incenter f a c b; g = incenter g c b e; h = incenter h b e a; i = incenter i e a c; j = on_line j a b, on_tline j h a b ? perp f g g h
I0123 12:41:44.248640 139856449679360 ddar.py:60] Depth 1/1000 time = 0.1943647861480713
I0123 12:41:44.497622 139856449679360 ddar.py:60] Depth 2/1000 time = 0.24890422821044922
I0123 12:41:44.747948 139856449679360 ddar.py:60] Depth 3/1000 time = 0.2490243911743164
I0123 12:41:45.127516 139856449679360 ddar.py:60] Depth 4/1000 time = 0.36606454849243164
I0123 12:41:45.127599 139856449679360 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:41:45.127651 139856449679360 alphageometry.py:566] LM output (score=-2.098353): "j : C e i j 11 D c i c j 12 ;"
I0123 12:41:45.127687 139856449679360 alphageometry.py:567] Translation: "j = on_line j e i, on_circle j c i"

I0123 12:41:45.127735 139856449679360 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = on_circle e d b; f = incenter f a c b; g = incenter g c b e; h = incenter h b e a; i = incenter i e a c; j = on_line j e i, on_circle j c i ? perp f g g h"
I0123 12:41:45.127848 139856449679360 graph.py:498] 
I0123 12:41:45.127895 139856449679360 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = on_circle e d b; f = incenter f a c b; g = incenter g c b e; h = incenter h b e a; i = incenter i e a c; j = on_line j e i, on_circle j c i ? perp f g g h
I0123 12:41:45.350521 139856449679360 ddar.py:60] Depth 1/1000 time = 0.2042236328125
I0123 12:41:45.607593 139856449679360 ddar.py:60] Depth 2/1000 time = 0.2570030689239502
I0123 12:41:46.061676 139856449679360 ddar.py:60] Depth 3/1000 time = 0.4526028633117676
I0123 12:41:46.443027 139856449679360 ddar.py:60] Depth 4/1000 time = 0.36826157569885254
I0123 12:41:46.443152 139856449679360 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:41:46.443223 139856449679360 alphageometry.py:566] LM output (score=-2.235430): "j : C e g j 11 D e j g j 12 ;"
I0123 12:41:46.443262 139856449679360 alphageometry.py:567] Translation: "j = on_line j e g, on_bline j g e"

I0123 12:41:46.443298 139856449679360 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = on_circle e d b; f = incenter f a c b; g = incenter g c b e; h = incenter h b e a; i = incenter i e a c; j = on_line j e g, on_bline j g e ? perp f g g h"
I0123 12:41:46.443413 139856449679360 graph.py:498] 
I0123 12:41:46.443461 139856449679360 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = on_circle e d b; f = incenter f a c b; g = incenter g c b e; h = incenter h b e a; i = incenter i e a c; j = on_line j e g, on_bline j g e ? perp f g g h
I0123 12:41:46.673939 139856449679360 ddar.py:60] Depth 1/1000 time = 0.1905207633972168
I0123 12:41:46.905723 139856449679360 ddar.py:60] Depth 2/1000 time = 0.2317054271697998
I0123 12:41:47.139904 139856449679360 ddar.py:60] Depth 3/1000 time = 0.23400616645812988
I0123 12:41:47.389170 139856449679360 ddar.py:60] Depth 4/1000 time = 0.24779772758483887
I0123 12:41:47.719295 139856449679360 ddar.py:60] Depth 5/1000 time = 0.3189070224761963
I0123 12:41:47.719383 139856449679360 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:41:47.719435 139856449679360 alphageometry.py:566] LM output (score=-2.276661): "j : C a b j 11 T a b j i 12 ;"
I0123 12:41:47.719472 139856449679360 alphageometry.py:567] Translation: "j = on_line j a b, on_tline j i a b"

I0123 12:41:47.719507 139856449679360 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = on_circle e d b; f = incenter f a c b; g = incenter g c b e; h = incenter h b e a; i = incenter i e a c; j = on_line j a b, on_tline j i a b ? perp f g g h"
I0123 12:41:47.719619 139856449679360 graph.py:498] 
I0123 12:41:47.719666 139856449679360 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = on_circle e d b; f = incenter f a c b; g = incenter g c b e; h = incenter h b e a; i = incenter i e a c; j = on_line j a b, on_tline j i a b ? perp f g g h
I0123 12:41:47.997758 139856449679360 ddar.py:60] Depth 1/1000 time = 0.18972468376159668
I0123 12:41:48.243559 139856449679360 ddar.py:60] Depth 2/1000 time = 0.2457270622253418
I0123 12:41:48.488836 139856449679360 ddar.py:60] Depth 3/1000 time = 0.24398159980773926
I0123 12:41:49.061334 139856449679360 ddar.py:60] Depth 4/1000 time = 0.55902099609375
I0123 12:41:49.061507 139856449679360 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:41:49.061582 139856449679360 alphageometry.py:566] LM output (score=-2.321226): "j : C e g j 11 D e i i j 12 ;"
I0123 12:41:49.061621 139856449679360 alphageometry.py:567] Translation: "j = on_line j e g, on_circle j i e"

I0123 12:41:49.061666 139856449679360 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = on_circle e d b; f = incenter f a c b; g = incenter g c b e; h = incenter h b e a; i = incenter i e a c; j = on_line j e g, on_circle j i e ? perp f g g h"
I0123 12:41:49.061803 139856449679360 graph.py:498] 
I0123 12:41:49.061852 139856449679360 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = on_circle e d b; f = incenter f a c b; g = incenter g c b e; h = incenter h b e a; i = incenter i e a c; j = on_line j e g, on_circle j i e ? perp f g g h
I0123 12:41:49.296633 139856449679360 ddar.py:60] Depth 1/1000 time = 0.21549534797668457
I0123 12:41:49.546239 139856449679360 ddar.py:60] Depth 2/1000 time = 0.2495274543762207
I0123 12:41:49.805637 139856449679360 ddar.py:60] Depth 3/1000 time = 0.25792503356933594
I0123 12:41:50.182037 139856449679360 ddar.py:60] Depth 4/1000 time = 0.3633842468261719
I0123 12:41:50.182128 139856449679360 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:41:50.182180 139856449679360 alphageometry.py:566] LM output (score=-2.492757): "j : C b c j 11 T b c j i 12 ;"
I0123 12:41:50.182215 139856449679360 alphageometry.py:567] Translation: "j = on_line j b c, on_tline j i b c"

I0123 12:41:50.182248 139856449679360 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = on_circle e d b; f = incenter f a c b; g = incenter g c b e; h = incenter h b e a; i = incenter i e a c; j = on_line j b c, on_tline j i b c ? perp f g g h"
I0123 12:41:50.182359 139856449679360 graph.py:498] 
I0123 12:41:50.182404 139856449679360 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = on_circle e d b; f = incenter f a c b; g = incenter g c b e; h = incenter h b e a; i = incenter i e a c; j = on_line j b c, on_tline j i b c ? perp f g g h
I0123 12:41:50.395256 139856449679360 ddar.py:60] Depth 1/1000 time = 0.19848990440368652
I0123 12:41:50.628761 139856449679360 ddar.py:60] Depth 2/1000 time = 0.23343157768249512
I0123 12:41:50.873331 139856449679360 ddar.py:60] Depth 3/1000 time = 0.24329686164855957
I0123 12:41:51.268446 139856449679360 ddar.py:60] Depth 4/1000 time = 0.37939882278442383
I0123 12:41:51.268535 139856449679360 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:41:51.268587 139856449679360 alphageometry.py:566] LM output (score=-2.540334): "j : C b e j 11 T b e j i 12 ;"
I0123 12:41:51.268623 139856449679360 alphageometry.py:567] Translation: "j = on_line j b e, on_tline j i b e"

I0123 12:41:51.268658 139856449679360 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = on_circle e d b; f = incenter f a c b; g = incenter g c b e; h = incenter h b e a; i = incenter i e a c; j = on_line j b e, on_tline j i b e ? perp f g g h"
I0123 12:41:51.268771 139856449679360 graph.py:498] 
I0123 12:41:51.268818 139856449679360 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = on_circle e d b; f = incenter f a c b; g = incenter g c b e; h = incenter h b e a; i = incenter i e a c; j = on_line j b e, on_tline j i b e ? perp f g g h
I0123 12:41:51.693416 139856449679360 ddar.py:60] Depth 1/1000 time = 0.39249706268310547
I0123 12:41:51.930495 139856449679360 ddar.py:60] Depth 2/1000 time = 0.23692703247070312
I0123 12:41:52.177720 139856449679360 ddar.py:60] Depth 3/1000 time = 0.24585270881652832
I0123 12:41:52.574391 139856449679360 ddar.py:60] Depth 4/1000 time = 0.380695104598999
I0123 12:41:52.574485 139856449679360 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:41:52.574542 139856449679360 alphageometry.py:566] LM output (score=-2.589665): "j : C e f j 11 D e j f j 12 ;"
I0123 12:41:52.574579 139856449679360 alphageometry.py:567] Translation: "j = on_line j e f, on_bline j f e"

I0123 12:41:52.574615 139856449679360 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = on_circle e d b; f = incenter f a c b; g = incenter g c b e; h = incenter h b e a; i = incenter i e a c; j = on_line j e f, on_bline j f e ? perp f g g h"
I0123 12:41:52.574732 139856449679360 graph.py:498] 
I0123 12:41:52.574780 139856449679360 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = on_circle e d b; f = incenter f a c b; g = incenter g c b e; h = incenter h b e a; i = incenter i e a c; j = on_line j e f, on_bline j f e ? perp f g g h
I0123 12:41:52.791917 139856449679360 ddar.py:60] Depth 1/1000 time = 0.19512128829956055
I0123 12:41:53.028158 139856449679360 ddar.py:60] Depth 2/1000 time = 0.23616719245910645
I0123 12:41:53.269400 139856449679360 ddar.py:60] Depth 3/1000 time = 0.24106383323669434
I0123 12:41:53.515492 139856449679360 ddar.py:60] Depth 4/1000 time = 0.2446424961090088
I0123 12:41:53.850041 139856449679360 ddar.py:60] Depth 5/1000 time = 0.32361483573913574
I0123 12:41:53.850131 139856449679360 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:41:53.850184 139856449679360 alphageometry.py:566] LM output (score=-2.926874): "j : C e g j 11 D e g e j 12 ;"
I0123 12:41:53.850221 139856449679360 alphageometry.py:567] Translation: "j = on_line j e g, on_circle j e g"

I0123 12:41:53.850256 139856449679360 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = on_circle e d b; f = incenter f a c b; g = incenter g c b e; h = incenter h b e a; i = incenter i e a c; j = on_line j e g, on_circle j e g ? perp f g g h"
I0123 12:41:53.850368 139856449679360 graph.py:498] 
I0123 12:41:53.850415 139856449679360 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = on_circle e d b; f = incenter f a c b; g = incenter g c b e; h = incenter h b e a; i = incenter i e a c; j = on_line j e g, on_circle j e g ? perp f g g h
I0123 12:41:54.288913 139856449679360 ddar.py:60] Depth 1/1000 time = 0.4049866199493408
I0123 12:41:54.521568 139856449679360 ddar.py:60] Depth 2/1000 time = 0.2325115203857422
I0123 12:41:54.756720 139856449679360 ddar.py:60] Depth 3/1000 time = 0.23493146896362305
I0123 12:41:55.001450 139856449679360 ddar.py:60] Depth 4/1000 time = 0.24325919151306152
I0123 12:41:55.339585 139856449679360 ddar.py:60] Depth 5/1000 time = 0.327282190322876
I0123 12:41:55.339674 139856449679360 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:41:55.339728 139856449679360 alphageometry.py:566] LM output (score=-2.987022): "j : C e i j 11 D e e e j 12 ;"
I0123 12:41:55.339763 139856449679360 alphageometry.py:567] Translation: "ERROR: Invalid predicate D e e e j"

I0123 12:41:55.339797 139856449679360 alphageometry.py:566] LM output (score=-3.217209): "j : D c e e j 11 D c i i j 12 ;"
I0123 12:41:55.339826 139856449679360 alphageometry.py:567] Translation: "j = on_circle j e c, on_circle j i c"

I0123 12:41:55.339856 139856449679360 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = on_circle e d b; f = incenter f a c b; g = incenter g c b e; h = incenter h b e a; i = incenter i e a c; j = on_circle j e c, on_circle j i c ? perp f g g h"
I0123 12:41:55.339968 139856449679360 graph.py:498] 
I0123 12:41:55.340014 139856449679360 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = on_circle e d b; f = incenter f a c b; g = incenter g c b e; h = incenter h b e a; i = incenter i e a c; j = on_circle j e c, on_circle j i c ? perp f g g h
I0123 12:41:55.618948 139856449679360 ddar.py:60] Depth 1/1000 time = 0.2224445343017578
I0123 12:41:55.917222 139856449679360 ddar.py:60] Depth 2/1000 time = 0.2981991767883301
I0123 12:41:56.217390 139856449679360 ddar.py:60] Depth 3/1000 time = 0.3000912666320801
I0123 12:41:56.566805 139856449679360 ddar.py:60] Depth 4/1000 time = 0.34934377670288086
I0123 12:41:56.941898 139856449679360 ddar.py:60] Depth 5/1000 time = 0.3750157356262207
I0123 12:41:57.590000 139856449679360 ddar.py:60] Depth 6/1000 time = 0.6480050086975098
I0123 12:41:58.012502 139856449679360 ddar.py:60] Depth 7/1000 time = 0.42235326766967773
I0123 12:41:58.451239 139856449679360 ddar.py:60] Depth 8/1000 time = 0.4366164207458496
I0123 12:41:59.092839 139856449679360 ddar.py:60] Depth 9/1000 time = 0.6198933124542236
I0123 12:41:59.738311 139856449679360 ddar.py:60] Depth 10/1000 time = 0.6453778743743896
I0123 12:41:59.744565 139856449679360 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:41:59.744654 139856449679360 alphageometry.py:566] LM output (score=-3.262432): "j : T c e j i 11 ;"
I0123 12:41:59.744692 139856449679360 alphageometry.py:567] Translation: "j = on_tline j i c e"

I0123 12:41:59.744728 139856449679360 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = on_circle e d b; f = incenter f a c b; g = incenter g c b e; h = incenter h b e a; i = incenter i e a c; j = on_tline j i c e ? perp f g g h"
I0123 12:41:59.744840 139856449679360 graph.py:498] 
I0123 12:41:59.744888 139856449679360 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = on_circle e d b; f = incenter f a c b; g = incenter g c b e; h = incenter h b e a; i = incenter i e a c; j = on_tline j i c e ? perp f g g h
I0123 12:41:59.929088 139856449679360 ddar.py:60] Depth 1/1000 time = 0.1582193374633789
I0123 12:42:00.128582 139856449679360 ddar.py:60] Depth 2/1000 time = 0.19942307472229004
I0123 12:42:00.334058 139856449679360 ddar.py:60] Depth 3/1000 time = 0.20420384407043457
I0123 12:42:00.648424 139856449679360 ddar.py:60] Depth 4/1000 time = 0.3007831573486328
I0123 12:42:00.648506 139856449679360 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:42:00.648556 139856449679360 alphageometry.py:566] LM output (score=-3.324681): "j : T a b j h 11 ;"
I0123 12:42:00.648591 139856449679360 alphageometry.py:567] Translation: "j = on_tline j h a b"

I0123 12:42:00.648624 139856449679360 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = on_circle e d b; f = incenter f a c b; g = incenter g c b e; h = incenter h b e a; i = incenter i e a c; j = on_tline j h a b ? perp f g g h"
I0123 12:42:00.648729 139856449679360 graph.py:498] 
I0123 12:42:00.648774 139856449679360 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = on_circle e d b; f = incenter f a c b; g = incenter g c b e; h = incenter h b e a; i = incenter i e a c; j = on_tline j h a b ? perp f g g h
I0123 12:42:00.818479 139856449679360 ddar.py:60] Depth 1/1000 time = 0.15532612800598145
I0123 12:42:01.012665 139856449679360 ddar.py:60] Depth 2/1000 time = 0.19411730766296387
I0123 12:42:01.216130 139856449679360 ddar.py:60] Depth 3/1000 time = 0.20218706130981445
I0123 12:42:01.530150 139856449679360 ddar.py:60] Depth 4/1000 time = 0.3006775379180908
I0123 12:42:01.530236 139856449679360 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:42:01.530292 139856449679360 alphageometry.py:566] LM output (score=-3.416040): "j : C e g j 11 D c g c j 12 ;"
I0123 12:42:01.530328 139856449679360 alphageometry.py:567] Translation: "j = on_line j e g, on_circle j c g"

I0123 12:42:01.530363 139856449679360 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = on_circle e d b; f = incenter f a c b; g = incenter g c b e; h = incenter h b e a; i = incenter i e a c; j = on_line j e g, on_circle j c g ? perp f g g h"
I0123 12:42:01.530471 139856449679360 graph.py:498] 
I0123 12:42:01.530516 139856449679360 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = on_circle e d b; f = incenter f a c b; g = incenter g c b e; h = incenter h b e a; i = incenter i e a c; j = on_line j e g, on_circle j c g ? perp f g g h
I0123 12:42:02.012279 139856449679360 ddar.py:60] Depth 1/1000 time = 0.4637010097503662
I0123 12:42:02.270187 139856449679360 ddar.py:60] Depth 2/1000 time = 0.25773191452026367
I0123 12:42:02.537572 139856449679360 ddar.py:60] Depth 3/1000 time = 0.2658250331878662
I0123 12:42:02.919914 139856449679360 ddar.py:60] Depth 4/1000 time = 0.3691842555999756
I0123 12:42:02.920006 139856449679360 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:42:02.920063 139856449679360 alphageometry.py:566] LM output (score=-3.418067): "j : C e h j 11 D e j h j 12 ;"
I0123 12:42:02.920100 139856449679360 alphageometry.py:567] Translation: "j = on_line j e h, on_bline j h e"

I0123 12:42:02.920140 139856449679360 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = on_circle e d b; f = incenter f a c b; g = incenter g c b e; h = incenter h b e a; i = incenter i e a c; j = on_line j e h, on_bline j h e ? perp f g g h"
I0123 12:42:02.920274 139856449679360 graph.py:498] 
I0123 12:42:02.920328 139856449679360 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = on_circle e d b; f = incenter f a c b; g = incenter g c b e; h = incenter h b e a; i = incenter i e a c; j = on_line j e h, on_bline j h e ? perp f g g h
I0123 12:42:03.136758 139856449679360 ddar.py:60] Depth 1/1000 time = 0.19724392890930176
I0123 12:42:03.370969 139856449679360 ddar.py:60] Depth 2/1000 time = 0.2341141700744629
I0123 12:42:03.609034 139856449679360 ddar.py:60] Depth 3/1000 time = 0.2378854751586914
I0123 12:42:03.854714 139856449679360 ddar.py:60] Depth 4/1000 time = 0.24422836303710938
I0123 12:42:04.191511 139856449679360 ddar.py:60] Depth 5/1000 time = 0.32584214210510254
I0123 12:42:04.191597 139856449679360 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:42:04.191651 139856449679360 alphageometry.py:566] LM output (score=-3.435454): "j : C e g j 11 D e g g j 12 ;"
I0123 12:42:04.191686 139856449679360 alphageometry.py:567] Translation: "j = on_line j e g, on_circle j g e"

I0123 12:42:04.191723 139856449679360 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = on_circle e d b; f = incenter f a c b; g = incenter g c b e; h = incenter h b e a; i = incenter i e a c; j = on_line j e g, on_circle j g e ? perp f g g h"
I0123 12:42:04.191832 139856449679360 graph.py:498] 
I0123 12:42:04.191877 139856449679360 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = on_circle e d b; f = incenter f a c b; g = incenter g c b e; h = incenter h b e a; i = incenter i e a c; j = on_line j e g, on_circle j g e ? perp f g g h
I0123 12:42:04.439280 139856449679360 ddar.py:60] Depth 1/1000 time = 0.1904609203338623
I0123 12:42:04.916724 139856449679360 ddar.py:60] Depth 2/1000 time = 0.47735118865966797
I0123 12:42:05.149712 139856449679360 ddar.py:60] Depth 3/1000 time = 0.2327289581298828
I0123 12:42:05.392832 139856449679360 ddar.py:60] Depth 4/1000 time = 0.24165844917297363
I0123 12:42:05.734268 139856449679360 ddar.py:60] Depth 5/1000 time = 0.33040571212768555
I0123 12:42:05.734356 139856449679360 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:42:05.734426 139856449679360 alphageometry.py:566] LM output (score=-3.696141): "j : C c e j 11 T c e j g 12 ;"
I0123 12:42:05.734462 139856449679360 alphageometry.py:567] Translation: "j = on_line j c e, on_tline j g c e"

I0123 12:42:05.734500 139856449679360 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = on_circle e d b; f = incenter f a c b; g = incenter g c b e; h = incenter h b e a; i = incenter i e a c; j = on_line j c e, on_tline j g c e ? perp f g g h"
I0123 12:42:05.734620 139856449679360 graph.py:498] 
I0123 12:42:05.734668 139856449679360 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = on_circle e d b; f = incenter f a c b; g = incenter g c b e; h = incenter h b e a; i = incenter i e a c; j = on_line j c e, on_tline j g c e ? perp f g g h
I0123 12:42:05.942953 139856449679360 ddar.py:60] Depth 1/1000 time = 0.19309544563293457
I0123 12:42:06.182183 139856449679360 ddar.py:60] Depth 2/1000 time = 0.23915576934814453
I0123 12:42:06.432651 139856449679360 ddar.py:60] Depth 3/1000 time = 0.2491912841796875
I0123 12:42:06.808919 139856449679360 ddar.py:60] Depth 4/1000 time = 0.36292481422424316
I0123 12:42:06.809001 139856449679360 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:42:06.809052 139856449679360 alphageometry.py:566] LM output (score=-3.708567): "j : C e i j 11 D c d d j 12 ;"
I0123 12:42:06.809087 139856449679360 alphageometry.py:567] Translation: "j = on_line j e i, on_circle j d c"

I0123 12:42:06.809121 139856449679360 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = on_circle e d b; f = incenter f a c b; g = incenter g c b e; h = incenter h b e a; i = incenter i e a c; j = on_line j e i, on_circle j d c ? perp f g g h"
I0123 12:42:06.809228 139856449679360 graph.py:498] 
I0123 12:42:06.809283 139856449679360 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = on_circle e d b; f = incenter f a c b; g = incenter g c b e; h = incenter h b e a; i = incenter i e a c; j = on_line j e i, on_circle j d c ? perp f g g h
I0123 12:42:07.284765 139856449679360 ddar.py:60] Depth 1/1000 time = 0.4584534168243408
I0123 12:42:08.218181 139856449679360 ddar.py:60] Depth 2/1000 time = 0.9333391189575195
I0123 12:42:09.250719 139856449679360 ddar.py:60] Depth 3/1000 time = 1.0324363708496094
I0123 12:42:10.064693 139856449679360 ddar.py:60] Depth 4/1000 time = 0.8138129711151123
I0123 12:42:10.903901 139856449679360 ddar.py:60] Depth 5/1000 time = 0.8388674259185791
I0123 12:42:11.765810 139856449679360 ddar.py:60] Depth 6/1000 time = 0.8618209362030029
I0123 12:42:12.891829 139856449679360 ddar.py:60] Depth 7/1000 time = 1.1240251064300537
I0123 12:42:13.961931 139856449679360 ddar.py:60] Depth 8/1000 time = 1.0424995422363281
I0123 12:42:15.430074 139856449679360 ddar.py:60] Depth 9/1000 time = 1.467944622039795
I0123 12:42:17.395841 139856449679360 ddar.py:60] Depth 10/1000 time = 1.9655084609985352
I0123 12:42:19.021266 139856449679360 ddar.py:60] Depth 11/1000 time = 1.6252729892730713
I0123 12:42:21.358697 139856449679360 ddar.py:60] Depth 12/1000 time = 2.303537130355835
I0123 12:42:23.691845 139856449679360 ddar.py:60] Depth 13/1000 time = 2.332977771759033
I0123 12:42:26.843225 139856449679360 ddar.py:60] Depth 14/1000 time = 3.1512041091918945
I0123 12:42:31.183442 139856449679360 ddar.py:60] Depth 15/1000 time = 4.340028285980225
I0123 12:42:37.670943 139856449679360 ddar.py:60] Depth 16/1000 time = 6.487305402755737
I0123 12:42:42.938641 139856449679360 ddar.py:60] Depth 17/1000 time = 5.267492771148682
I0123 12:42:50.356415 139856449679360 ddar.py:60] Depth 18/1000 time = 7.4169511795043945
I0123 12:42:52.727774 139856449679360 alphageometry.py:191] 
==========================
 * From theorem premises:
A B C D E F G H : Points
DB = DC [00]
DA = DB [01]
DE = DB [02]
ABF = FBC [03]
CAF = FAB [04]
CEG = GEB [05]
BCG = GCE [06]
BAH = HAE [07]
EBH = HBA [08]

 * Auxiliary Constructions:
I J : Points
AEI = IEC [09]
ECI = ICA [10]
J,I,E are collinear [11]
DJ = DC [12]

 * Proof steps:
001. DB = DC [00] & DJ = DC [12] & DE = DB [02]   J,C,B,E are concyclic [13]
002. DB = DC [00] & DJ = DC [12] & DE = DB [02]   DE = DJ [14]
003. J,C,B,E are concyclic [13] & DB = DC [00] & DE = DB [02] & DA = DB [01]   J,E,A,C are concyclic [15]
004. J,C,B,E are concyclic [13] & DB = DC [00] & DE = DB [02] & DA = DB [01]   J,E,A,B are concyclic [16]
005. J,E,A,C are concyclic [15]   JEA = JCA [17]
006. J,E,A,C are concyclic [15]   JAE = JCE [18]
007. JEA = JCA [17] & J,I,E are collinear [11]   IEA = JCA [19]
008. J,E,A,B are concyclic [16]   JEA = JBA [20]
009. J,E,A,B are concyclic [16]   JAE = JBE [21]
010. JEA = JBA [20] & J,I,E are collinear [11]   IEA = JBA [22]
011. J,E,C,B are concyclic [13]   JEC = JBC [23]
012. JEC = JBC [23] & J,I,E are collinear [11]   IEC = JBC [24]
013. J,I,E are collinear [11] & IEA = CEI [09]   JEA = CEJ [25]
014. J,E,A,C are concyclic [15] & JEA = CEJ [25]   JA = CJ [26]
015. ABF = FBC [03] & AEI = IEC [09] & IEA = JBA [22] & IEC = JBC [24] (Angle chase)  BF  BJ [27]
016. ABF = FBC [03] & AEI = IEC [09] & IEA = JBA [22] & IEC = JBC [24] (Angle chase)  FBC = AEI [28]
017. BF  BJ [27]   J,B,F are collinear [29]
018. CAF = FAB [04] & AEI = IEC [09] & JAE = JCE [18] & IEA = JCA [19] & IEA = JBA [22] (Angle chase)  JAF = (AF-BJ) [30]
019. J,B,F are collinear [29] & JAF = (AF-BJ) [30]   JAF = AFJ [31]
020. JAF = AFJ [31]   JA = JF [32]
021. ABF = FBC [03] & EBH = HBA [08] & AEI = IEC [09] & ECI = ICA [10] & JAE = JCE [18] & JAE = JBE [21] & IEA = JCA [19] & IEC = JBC [24] (Angle chase)  FBH = JCI [33]
022. ABF = FBC [03] & EBH = HBA [08] & AEI = IEC [09] & ECI = ICA [10] & JAE = JCE [18] & JAE = JBE [21] & IEA = JCA [19] & IEC = JBC [24] (Angle chase)  FBH = CIE [34]
023. J,I,E are collinear [11] & FBH = JCI [33] & FBH = CIE [34]   CIJ = JCI [35]
024. CIJ = JCI [35]   JC = JI [36]
025. JA = CJ [26] & JA = JF [32] & JC = JI [36]   A,I,C,F are concyclic [37]
026. A,I,C,F are concyclic [37]   AIC = AFC [38]
027. A,I,C,F are concyclic [37]   ACI = AFI [39]
028. A,I,C,F are concyclic [37]   FAC = FIC [40]
029. JA = CJ [26] & JA = JF [32]   JF = JC [41]
030. JF = JC [41]   JFC = FCJ [42]
031. JFC = FCJ [42] & J,B,F are collinear [29]   BFC = FCJ [43]
032. ABF = FBC [03] & BAH = HAE [07] & AEI = IEC [09] & ECI = ICA [10] & IEA = JCA [19] & IEA = JBA [22] & IEC = JBC [24] & AIC = AFC [38] & BFC = FCJ [43] (Angle chase)  BAH = FAI [44]
033. ABF = FBC [03] & BAH = HAE [07] & AEI = IEC [09] & ECI = ICA [10] & IEA = JCA [19] & IEA = JBA [22] & IEC = JBC [24] & AIC = AFC [38] & BFC = FCJ [43] (Angle chase)  BAF = HAI [45]
034. ABF = FBC [03] & EBH = HBA [08] & BAH = HAE [07] & AEI = IEC [09] & JAE = JCE [18] & JAE = JBE [21] & IEA = JCA [19] & IEA = JBA [22] & IEC = JBC [24] & ACI = AFI [39] & AIC = AFC [38] & BFC = FCJ [43] (Angle chase)  FIA = BHA [46]
035. BAH = FAI [44] & FIA = BHA [46] (Similar Triangles)  AF:AB = AI:AH [47]
036. AF:AB = AI:AH [47] & BAF = HAI [45] (Similar Triangles)  AIH = AFB [48]
037. AF:AB = AI:AH [47] & BAF = HAI [45] (Similar Triangles)  AHI = ABF [49]
038. CAF = FAB [04] & ABF = FBC [03] & CEG = GEB [05] & AEI = IEC [09] & JAE = JCE [18] & JAE = JBE [21] & IEA = JCA [19] & IEC = JBC [24] (Angle chase)  AFB = IEG [50]
039. J,I,E are collinear [11] & AIH = AFB [48] & AFB = IEG [50]   (JI-GE) = AIH [51]
040. DB = DC [00] & DE = DB [02]   DE = DC [52]
041. DE = DC [52]   DEC = ECD [53]
042. DE = DJ [14]   DEJ = EJD [54]
043. DEJ = EJD [54] & J,I,E are collinear [11]   DEI = (EI-DJ) [55]
044. DJ = DC [12]   DCJ = CJD [56]
045. CAF = FAB [04] & ABF = FBC [03] & AEI = IEC [09] & ECI = ICA [10] & IEA = JCA [19] & IEA = JBA [22] & IEC = JBC [24] & DEC = ECD [53] & DEI = (EI-DJ) [55] & DCJ = CJD [56] & AIC = AFC [38] & BFC = FCJ [43] (Angle chase)  CIE = (DJ-AI) [57]
046. J,I,E are collinear [11] & (DJ-AI) = CIE [57]   (DJ-AI) = CIJ [58]
047. (JI-GE) = AIH [51] & (DJ-AI) = CIJ [58]   (EG-HI) = (CI-DJ) [59]
048. CAF = FAB [04] & ABF = FBC [03] & BCG = GCE [06] & CEG = GEB [05] & AEI = IEC [09] & ECI = ICA [10] & JAE = JCE [18] & JAE = JBE [21] & IEA = JBA [22] & IEC = JBC [24] & AIC = AFC [38] & BFC = FCJ [43] (Angle chase)  CAI = CGE [60]
049. CAF = FAB [04] & ABF = FBC [03] & AEI = IEC [09] & ECI = ICA [10] & IEA = JCA [19] & IEA = JBA [22] & IEC = JBC [24] & AIC = AFC [38] & BFC = FCJ [43] (Angle chase)  CAI = IAE [61]
050. FBC = AEI [28] & ABF = FBC [03] & AHI = ABF [49]   AHI = AEI [62]
051. AHI = AEI [62]   A,I,H,E are concyclic [63]
052. A,I,H,E are concyclic [63]   IAE = IHE [64]
053. A,I,H,E are concyclic [63]   IAH = IEH [65]
054. A,I,H,E are concyclic [63]   AIE = AHE [66]
055. A,I,H,E are concyclic [63]   AIH = AEH [67]
056. CAF = FAB [04] & CEG = GEB [05] & JAE = JCE [18] & JAE = JBE [21] & IEA = JCA [19] & IEA = JBA [22] (Angle chase)  FAC = BEG [68]
057. CAF = FAB [04] & CEG = GEB [05] & JAE = JCE [18] & JAE = JBE [21] & IEA = JCA [19] & IEA = JBA [22] (Angle chase)  FAB = CEG [69]
058. FAC = FIC [40] & FAC = BEG [68] & GEC = BEG [05]   GEC = FIC [70]
059. CAF = FAB [04] & ABF = FBC [03] & BCG = GCE [06] & CEG = GEB [05] & AEI = IEC [09] & ECI = ICA [10] & JAE = JCE [18] & JAE = JBE [21] & IEA = JBA [22] & IEC = JBC [24] & ACI = AFI [39] & BFC = FCJ [43] (Angle chase)  CFI = CGE [71]
060. GEC = FIC [70] & CFI = CGE [71] (Similar Triangles)  CE:IC = GC:CF [72]
061. CAF = CIF [40] & ECI = ICA [10]   IFA = ECI [73]
062. ABF = FBC [03] & BCG = GCE [06] & AEI = IEC [09] & ECI = ICA [10] & IEA = JCA [19] & IEA = JBA [22] & IEC = JBC [24] & ACI = AFI [39] & BFC = FCJ [43] (Angle chase)  IFA = GCF [74]
063. IFA = ECI [73] & IFA = GCF [74]   GCF = ECI [75]
064. CE:IC = GC:CF [72] & GCF = ECI [75] (Similar Triangles)  FGC = IEC [76]
065. CE:IC = GC:CF [72] & GCF = ECI [75] (Similar Triangles)  GC:GF = CE:IE [77]
066. J,I,E are collinear [11] & IEC = AEI [09]   (JI-CE) = (AE-JI) [78]
067. J,I,E are collinear [11] & JEA = JCA [17]   CAE = CJI [79]
068. (JI-CE) = (AE-JI) [78] & CAE = CJI [79]   ECJ = (JI-AC) [80]
069. FGC = IEC [76] & ECJ = (JI-AC) [80] & J,I,E are collinear [11]   ACJ = FGC [81]
070. CAF = FAB [04] & BCG = GCE [06] & AEI = IEC [09] & ECI = ICA [10] & IEA = JBA [22] & IEC = JBC [24] & DEC = ECD [53] & DEI = (EI-DJ) [55] & DCJ = CJD [56] & ACI = AFI [39] (Angle chase)  (CG-FI) = CJD [82]
071. ACJ = FGC [81] & (CG-FI) = CJD [82]   (DJ-AC) = IFG [83]
072. DA = DB [01] & DB = DC [00]   DC = DA [84]
073. DC = DA [84] & JA = CJ [26]   AC  DJ [85]
074. (DJ-AC) = IFG [83] & AC  DJ [85]   GF  IF [86]
075. AEI = IEC [09] & JAE = JCE [18] & IEA = JCA [19] & IEA = JBA [22] (Angle chase)  BAJ = (BJ-AC) [87]
076. IHA = FBA [49] & BAJ = (BJ-AC) [87] & J,B,F are collinear [29]   CAJ = IHA [88]
077. CAF = FAB [04] & BAH = HAE [07] & AEI = IEC [09] & ECI = ICA [10] & DEC = ECD [53] & DEI = (EI-DJ) [55] & DCJ = CJD [56] & ACI = AFI [39] (Angle chase)  (FI-AH) = DCJ [89]
078. DA = DB [01] & DB = DC [00] & DJ = DC [12]   DJ = DA [90]
079. DJ = DC [12] & JA = CJ [26] & DJ = DA [90] (SSS)  DCJ = DJA [91]
080. (FI-AH) = DCJ [89] & DCJ = DJA [91]   DJA = (IF-AH) [92]
081. CAJ = IHA [88] & DJA = (IF-AH) [92]   (AC-DJ) = HIF [93]
082. (AC-DJ) = HIF [93] & AC  DJ [85]   IF  IH [94]
083. GF  IF [86] & IF  IH [94]   GF  IH [95]
084. CAI = CGE [60] & CAI = IAE [61] & IAE = IHE [64] & FG  HI [95]   IHE = CGE [96]
085. IAH = IEH [65] & FAB = IAH [45] & FAB = CEG [69]   CEG = IEH [97]
086. IHE = CGE [96] & CEG = IEH [97] (Similar Triangles)  CE:IE = GE:HE [98]
087. CAF = FAB [04] & BCG = GCE [06] & CEG = GEB [05] & BAH = HAE [07] & JAE = JCE [18] & JAE = JBE [21] & IEA = JCA [19] & IEC = JBC [24] (Angle chase)  (CG-AF) = (EG-AH) [99]
088. ABF = FBC [03] & BCG = GCE [06] & AEI = IEC [09] & ECI = ICA [10] & IEA = JCA [19] & IEA = JBA [22] & IEC = JBC [24] & AIC = AFC [38] & BFC = FCJ [43] (Angle chase)  (CG-AF) = (CE-AI) [100]
089. ABF = FBC [03] & BCG = GCE [06] & AEI = IEC [09] & ECI = ICA [10] & IEA = JCA [19] & IEA = JBA [22] & IEC = JBC [24] & AIC = AFC [38] & BFC = FCJ [43] (Angle chase)  (AF-BC) = (AI-CG) [101]
090. (CG-AF) = (EG-AH) [99] & (CG-AF) = (CE-AI) [100]   (CE-AI) = (GE-AH) [102]
091. J,I,E are collinear [11] & AHE = AIE [66]   AHE = AIJ [103]
092. (CE-AI) = (GE-AH) [102] & AHE = AIJ [103]   GEH = (CE-JI) [104]
093. GEH = (CE-JI) [104] & J,I,E are collinear [11]   GEH = CEI [105]
094. CE:IE = GE:HE [98] & GEH = CEI [105] (Similar Triangles)  EGH = ECI [106]
095. EGH = ECI [106] & ECI = ICA [10]   ICA = EGH [107]
096. (EG-HI) = (CI-DJ) [59] & ICA = EGH [107]   IHG = (DJ-AC) [108]
097. GC:GF = CE:IE [77] & CE:IE = GE:HE [98]   GE:HE = GC:GF [109]
098. CAF = FAB [04] & CEG = GEB [05] & JAE = JCE [18] & JAE = JBE [21] & IEA = JCA [19] & IEA = JBA [22] & IEC = JBC [24] (Angle chase)  (AF-BC) = AEG [110]
099. (AF-BC) = AEG [110] & (AF-BC) = (AI-CG) [101]   (AI-GC) = AEG [111]
100. (AI-GC) = AEG [111] & AIH = AEH [67]   (CG-HI) = GEH [112]
101. (CG-HI) = GEH [112] & FG  HI [95]   CGF = GEH [113]
102. GE:HE = GC:GF [109] & CGF = GEH [113] (Similar Triangles)  EGC = (GH-CF) [114]
103. IFC = EGC [71] & EGC = (GH-CF) [114]   (GH-CF) = IFC [115]
104. (GH-CF) = IFC [115]   GH  IF [116]
105. IHG = (DJ-AC) [108] & FG  HI [95] & GH  FI [116] & (AC-DJ) = GFI [83]   FG  GH
==========================

I0123 12:42:52.727922 139856449679360 alphageometry.py:582] Solved.
