I0123 11:57:42.452957 139754068353024 inference_utils.py:69] Parsing gin configuration.
I0123 11:57:42.453061 139754068353024 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0123 11:57:42.453272 139754068353024 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0123 11:57:42.453306 139754068353024 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0123 11:57:42.453335 139754068353024 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0123 11:57:42.453361 139754068353024 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0123 11:57:42.453387 139754068353024 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0123 11:57:42.453412 139754068353024 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0123 11:57:42.453438 139754068353024 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0123 11:57:42.453464 139754068353024 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=32
I0123 11:57:42.453490 139754068353024 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0123 11:57:42.453515 139754068353024 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0123 11:57:42.453561 139754068353024 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0123 11:57:42.453720 139754068353024 resource_reader.py:55] Path not found: base_htrans.gin
I0123 11:57:42.453929 139754068353024 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0123 11:57:42.454032 139754068353024 resource_reader.py:55] Path not found: trainer_configuration.gin
I0123 11:57:42.460402 139754068353024 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0123 11:57:42.460525 139754068353024 resource_reader.py:55] Path not found: size/medium_150M.gin
I0123 11:57:42.460850 139754068353024 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0123 11:57:42.460955 139754068353024 resource_reader.py:55] Path not found: options/positions_t5.gin
I0123 11:57:42.461232 139754068353024 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0123 11:57:42.461332 139754068353024 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0123 11:57:42.461750 139754068353024 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0123 11:57:42.461850 139754068353024 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0123 11:57:42.465552 139754068353024 training_loop.py:334] ==== Training loop: initializing model ====
I0123 11:57:42.559430 139754068353024 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0123 11:57:42.560149 139754068353024 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0123 11:57:42.566863 139754068353024 training_loop.py:335] Process 0 of 1
I0123 11:57:42.566917 139754068353024 training_loop.py:336] Local device count = 1
I0123 11:57:42.566955 139754068353024 training_loop.py:337] Number of replicas = 1
I0123 11:57:42.566986 139754068353024 training_loop.py:339] Using random number seed 42
I0123 11:57:43.038571 139754068353024 training_loop.py:359] Initializing the model.
I0123 11:57:43.447782 139754068353024 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.448071 139754068353024 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 11:57:43.448179 139754068353024 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:57:43.448259 139754068353024 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:57:43.448336 139754068353024 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:57:43.448418 139754068353024 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:57:43.448493 139754068353024 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:57:43.448566 139754068353024 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:57:43.448638 139754068353024 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:57:43.448709 139754068353024 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:57:43.448779 139754068353024 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:57:43.448849 139754068353024 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:57:43.448919 139754068353024 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:57:43.448987 139754068353024 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:57:43.449027 139754068353024 attention.py:418] Single window, no scan.
I0123 11:57:43.449074 139754068353024 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 11:57:43.449190 139754068353024 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:57:43.449230 139754068353024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:57:43.449261 139754068353024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:57:43.451320 139754068353024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.456722 139754068353024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:57:43.467566 139754068353024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.467853 139754068353024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:57:43.472290 139754068353024 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:57:43.483214 139754068353024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:57:43.483271 139754068353024 attention.py:418] Single window, no scan.
I0123 11:57:43.483309 139754068353024 transformer_layer.py:389] tlayer: self-attention.
I0123 11:57:43.483342 139754068353024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.483406 139754068353024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.484590 139754068353024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.484668 139754068353024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.485386 139754068353024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.487881 139754068353024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.493766 139754068353024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.495497 139754068353024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.495579 139754068353024 transformer_layer.py:468] tlayer: End windows.
I0123 11:57:43.495615 139754068353024 transformer_layer.py:472] tlayer: final FFN.
I0123 11:57:43.495677 139754068353024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.495813 139754068353024 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:57:43.496152 139754068353024 nn_components.py:325] mlp: activation = None
I0123 11:57:43.496199 139754068353024 nn_components.py:329] mlp: final activation = None
I0123 11:57:43.498135 139754068353024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.498239 139754068353024 nn_components.py:261] mlp: residual
I0123 11:57:43.501159 139754068353024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.501239 139754068353024 transformer_base.py:443] tbase: final FFN
I0123 11:57:43.501736 139754068353024 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:57:43.512073 139754068353024 nn_components.py:329] mlp: final activation = None
I0123 11:57:43.521075 139754068353024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.521175 139754068353024 nn_components.py:261] mlp: residual
I0123 11:57:43.521475 139754068353024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.521556 139754068353024 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 11:57:43.521673 139754068353024 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:57:43.521715 139754068353024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:57:43.521746 139754068353024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:57:43.523623 139754068353024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.526138 139754068353024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:57:43.531755 139754068353024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.532029 139754068353024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:57:43.535356 139754068353024 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:57:43.539409 139754068353024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:57:43.539467 139754068353024 attention.py:418] Single window, no scan.
I0123 11:57:43.539504 139754068353024 transformer_layer.py:389] tlayer: self-attention.
I0123 11:57:43.539536 139754068353024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.539607 139754068353024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.540199 139754068353024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.540277 139754068353024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.540646 139754068353024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.541432 139754068353024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.543976 139754068353024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.544611 139754068353024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.544689 139754068353024 transformer_layer.py:468] tlayer: End windows.
I0123 11:57:43.544724 139754068353024 transformer_layer.py:472] tlayer: final FFN.
I0123 11:57:43.544784 139754068353024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.544914 139754068353024 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:57:43.545248 139754068353024 nn_components.py:325] mlp: activation = None
I0123 11:57:43.545294 139754068353024 nn_components.py:329] mlp: final activation = None
I0123 11:57:43.547267 139754068353024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.547366 139754068353024 nn_components.py:261] mlp: residual
I0123 11:57:43.549917 139754068353024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.550000 139754068353024 transformer_base.py:443] tbase: final FFN
I0123 11:57:43.550438 139754068353024 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:57:43.552792 139754068353024 nn_components.py:329] mlp: final activation = None
I0123 11:57:43.554700 139754068353024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.554797 139754068353024 nn_components.py:261] mlp: residual
I0123 11:57:43.555100 139754068353024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.555182 139754068353024 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 11:57:43.555293 139754068353024 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:57:43.555332 139754068353024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:57:43.555362 139754068353024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:57:43.557297 139754068353024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.559689 139754068353024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:57:43.565693 139754068353024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.565956 139754068353024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:57:43.568644 139754068353024 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:57:43.572503 139754068353024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:57:43.572559 139754068353024 attention.py:418] Single window, no scan.
I0123 11:57:43.572594 139754068353024 transformer_layer.py:389] tlayer: self-attention.
I0123 11:57:43.572625 139754068353024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.572687 139754068353024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.573422 139754068353024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.573499 139754068353024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.573877 139754068353024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.574662 139754068353024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.577345 139754068353024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.578022 139754068353024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.578101 139754068353024 transformer_layer.py:468] tlayer: End windows.
I0123 11:57:43.578136 139754068353024 transformer_layer.py:472] tlayer: final FFN.
I0123 11:57:43.578195 139754068353024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.578327 139754068353024 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:57:43.578649 139754068353024 nn_components.py:325] mlp: activation = None
I0123 11:57:43.578693 139754068353024 nn_components.py:329] mlp: final activation = None
I0123 11:57:43.580605 139754068353024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.580698 139754068353024 nn_components.py:261] mlp: residual
I0123 11:57:43.583240 139754068353024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.583328 139754068353024 transformer_base.py:443] tbase: final FFN
I0123 11:57:43.583814 139754068353024 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:57:43.586125 139754068353024 nn_components.py:329] mlp: final activation = None
I0123 11:57:43.588052 139754068353024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.588147 139754068353024 nn_components.py:261] mlp: residual
I0123 11:57:43.588440 139754068353024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.588521 139754068353024 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 11:57:43.588631 139754068353024 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:57:43.588671 139754068353024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:57:43.588710 139754068353024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:57:43.590623 139754068353024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.593033 139754068353024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:57:43.598687 139754068353024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.598950 139754068353024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:57:43.601617 139754068353024 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:57:43.605470 139754068353024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:57:43.605525 139754068353024 attention.py:418] Single window, no scan.
I0123 11:57:43.605561 139754068353024 transformer_layer.py:389] tlayer: self-attention.
I0123 11:57:43.605593 139754068353024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.605664 139754068353024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.606235 139754068353024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.606315 139754068353024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.606682 139754068353024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.607455 139754068353024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.610041 139754068353024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.610669 139754068353024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.610750 139754068353024 transformer_layer.py:468] tlayer: End windows.
I0123 11:57:43.610789 139754068353024 transformer_layer.py:472] tlayer: final FFN.
I0123 11:57:43.610852 139754068353024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.610982 139754068353024 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:57:43.611306 139754068353024 nn_components.py:325] mlp: activation = None
I0123 11:57:43.611351 139754068353024 nn_components.py:329] mlp: final activation = None
I0123 11:57:43.613274 139754068353024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.613370 139754068353024 nn_components.py:261] mlp: residual
I0123 11:57:43.615950 139754068353024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.616036 139754068353024 transformer_base.py:443] tbase: final FFN
I0123 11:57:43.616473 139754068353024 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:57:43.618751 139754068353024 nn_components.py:329] mlp: final activation = None
I0123 11:57:43.620672 139754068353024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.620766 139754068353024 nn_components.py:261] mlp: residual
I0123 11:57:43.621062 139754068353024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.621143 139754068353024 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 11:57:43.621252 139754068353024 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:57:43.621292 139754068353024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:57:43.621323 139754068353024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:57:43.623249 139754068353024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.625690 139754068353024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:57:43.631403 139754068353024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.631669 139754068353024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:57:43.634428 139754068353024 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:57:43.638232 139754068353024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:57:43.638288 139754068353024 attention.py:418] Single window, no scan.
I0123 11:57:43.638324 139754068353024 transformer_layer.py:389] tlayer: self-attention.
I0123 11:57:43.638357 139754068353024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.638420 139754068353024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.638994 139754068353024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.639071 139754068353024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.639442 139754068353024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.640229 139754068353024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.643194 139754068353024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.643829 139754068353024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.643909 139754068353024 transformer_layer.py:468] tlayer: End windows.
I0123 11:57:43.643945 139754068353024 transformer_layer.py:472] tlayer: final FFN.
I0123 11:57:43.644006 139754068353024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.644144 139754068353024 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:57:43.644471 139754068353024 nn_components.py:325] mlp: activation = None
I0123 11:57:43.644515 139754068353024 nn_components.py:329] mlp: final activation = None
I0123 11:57:43.646449 139754068353024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.646545 139754068353024 nn_components.py:261] mlp: residual
I0123 11:57:43.649132 139754068353024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.649213 139754068353024 transformer_base.py:443] tbase: final FFN
I0123 11:57:43.649656 139754068353024 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:57:43.651965 139754068353024 nn_components.py:329] mlp: final activation = None
I0123 11:57:43.653957 139754068353024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.654052 139754068353024 nn_components.py:261] mlp: residual
I0123 11:57:43.654347 139754068353024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.654427 139754068353024 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 11:57:43.654537 139754068353024 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:57:43.654576 139754068353024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:57:43.654606 139754068353024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:57:43.656462 139754068353024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.658935 139754068353024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:57:43.664719 139754068353024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.665016 139754068353024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:57:43.667759 139754068353024 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:57:43.671566 139754068353024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:57:43.671621 139754068353024 attention.py:418] Single window, no scan.
I0123 11:57:43.671658 139754068353024 transformer_layer.py:389] tlayer: self-attention.
I0123 11:57:43.671689 139754068353024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.671753 139754068353024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.672363 139754068353024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.672440 139754068353024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.672811 139754068353024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.673603 139754068353024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.676347 139754068353024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.676975 139754068353024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.677052 139754068353024 transformer_layer.py:468] tlayer: End windows.
I0123 11:57:43.677088 139754068353024 transformer_layer.py:472] tlayer: final FFN.
I0123 11:57:43.677148 139754068353024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.677444 139754068353024 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:57:43.677779 139754068353024 nn_components.py:325] mlp: activation = None
I0123 11:57:43.677831 139754068353024 nn_components.py:329] mlp: final activation = None
I0123 11:57:43.679779 139754068353024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.679877 139754068353024 nn_components.py:261] mlp: residual
I0123 11:57:43.682502 139754068353024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.682582 139754068353024 transformer_base.py:443] tbase: final FFN
I0123 11:57:43.683017 139754068353024 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:57:43.685391 139754068353024 nn_components.py:329] mlp: final activation = None
I0123 11:57:43.687359 139754068353024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.687457 139754068353024 nn_components.py:261] mlp: residual
I0123 11:57:43.687756 139754068353024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.687839 139754068353024 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 11:57:43.687949 139754068353024 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:57:43.687989 139754068353024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:57:43.688021 139754068353024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:57:43.689902 139754068353024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.692410 139754068353024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:57:43.698101 139754068353024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.698368 139754068353024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:57:43.701031 139754068353024 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:57:43.704866 139754068353024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:57:43.704922 139754068353024 attention.py:418] Single window, no scan.
I0123 11:57:43.704957 139754068353024 transformer_layer.py:389] tlayer: self-attention.
I0123 11:57:43.704989 139754068353024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.705051 139754068353024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.705617 139754068353024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.705703 139754068353024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.706068 139754068353024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.706859 139754068353024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.709379 139754068353024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.710024 139754068353024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.710102 139754068353024 transformer_layer.py:468] tlayer: End windows.
I0123 11:57:43.710137 139754068353024 transformer_layer.py:472] tlayer: final FFN.
I0123 11:57:43.710197 139754068353024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.710325 139754068353024 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:57:43.710650 139754068353024 nn_components.py:325] mlp: activation = None
I0123 11:57:43.710693 139754068353024 nn_components.py:329] mlp: final activation = None
I0123 11:57:43.712678 139754068353024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.712776 139754068353024 nn_components.py:261] mlp: residual
I0123 11:57:43.715312 139754068353024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.715393 139754068353024 transformer_base.py:443] tbase: final FFN
I0123 11:57:43.715833 139754068353024 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:57:43.718507 139754068353024 nn_components.py:329] mlp: final activation = None
I0123 11:57:43.720450 139754068353024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.720554 139754068353024 nn_components.py:261] mlp: residual
I0123 11:57:43.720853 139754068353024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.720935 139754068353024 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 11:57:43.721049 139754068353024 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:57:43.721089 139754068353024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:57:43.721121 139754068353024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:57:43.863943 139754068353024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.867181 139754068353024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:57:43.873423 139754068353024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.873748 139754068353024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:57:43.876531 139754068353024 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:57:43.880513 139754068353024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:57:43.880570 139754068353024 attention.py:418] Single window, no scan.
I0123 11:57:43.880612 139754068353024 transformer_layer.py:389] tlayer: self-attention.
I0123 11:57:43.880645 139754068353024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.880709 139754068353024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.881342 139754068353024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.881420 139754068353024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.881797 139754068353024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.882594 139754068353024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.885213 139754068353024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.885864 139754068353024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.885948 139754068353024 transformer_layer.py:468] tlayer: End windows.
I0123 11:57:43.885985 139754068353024 transformer_layer.py:472] tlayer: final FFN.
I0123 11:57:43.886045 139754068353024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.886176 139754068353024 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:57:43.886514 139754068353024 nn_components.py:325] mlp: activation = None
I0123 11:57:43.886558 139754068353024 nn_components.py:329] mlp: final activation = None
I0123 11:57:43.888495 139754068353024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.888589 139754068353024 nn_components.py:261] mlp: residual
I0123 11:57:43.891278 139754068353024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.891366 139754068353024 transformer_base.py:443] tbase: final FFN
I0123 11:57:43.891841 139754068353024 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:57:43.894294 139754068353024 nn_components.py:329] mlp: final activation = None
I0123 11:57:43.896274 139754068353024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.896378 139754068353024 nn_components.py:261] mlp: residual
I0123 11:57:43.896677 139754068353024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.896761 139754068353024 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 11:57:43.896874 139754068353024 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:57:43.896914 139754068353024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:57:43.896944 139754068353024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:57:43.898955 139754068353024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.901410 139754068353024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:57:43.907213 139754068353024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.907492 139754068353024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:57:43.910241 139754068353024 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:57:43.914137 139754068353024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:57:43.914196 139754068353024 attention.py:418] Single window, no scan.
I0123 11:57:43.914234 139754068353024 transformer_layer.py:389] tlayer: self-attention.
I0123 11:57:43.914268 139754068353024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.914333 139754068353024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.914930 139754068353024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.915010 139754068353024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.915387 139754068353024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.916197 139754068353024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.918828 139754068353024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.919463 139754068353024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.919541 139754068353024 transformer_layer.py:468] tlayer: End windows.
I0123 11:57:43.919577 139754068353024 transformer_layer.py:472] tlayer: final FFN.
I0123 11:57:43.919637 139754068353024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.919767 139754068353024 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:57:43.920095 139754068353024 nn_components.py:325] mlp: activation = None
I0123 11:57:43.920139 139754068353024 nn_components.py:329] mlp: final activation = None
I0123 11:57:43.922085 139754068353024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.922180 139754068353024 nn_components.py:261] mlp: residual
I0123 11:57:43.924796 139754068353024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.924877 139754068353024 transformer_base.py:443] tbase: final FFN
I0123 11:57:43.925309 139754068353024 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:57:43.927631 139754068353024 nn_components.py:329] mlp: final activation = None
I0123 11:57:43.929620 139754068353024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.929726 139754068353024 nn_components.py:261] mlp: residual
I0123 11:57:43.930018 139754068353024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.930106 139754068353024 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 11:57:43.930217 139754068353024 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:57:43.930256 139754068353024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:57:43.930287 139754068353024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:57:43.932161 139754068353024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.934664 139754068353024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:57:43.940307 139754068353024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.940573 139754068353024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:57:43.943665 139754068353024 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:57:43.947494 139754068353024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:57:43.947551 139754068353024 attention.py:418] Single window, no scan.
I0123 11:57:43.947587 139754068353024 transformer_layer.py:389] tlayer: self-attention.
I0123 11:57:43.947619 139754068353024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.947682 139754068353024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.948297 139754068353024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.948375 139754068353024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.948749 139754068353024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.949529 139754068353024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.952071 139754068353024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.952708 139754068353024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.952786 139754068353024 transformer_layer.py:468] tlayer: End windows.
I0123 11:57:43.952821 139754068353024 transformer_layer.py:472] tlayer: final FFN.
I0123 11:57:43.952882 139754068353024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.953012 139754068353024 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:57:43.953338 139754068353024 nn_components.py:325] mlp: activation = None
I0123 11:57:43.953382 139754068353024 nn_components.py:329] mlp: final activation = None
I0123 11:57:43.955314 139754068353024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.955409 139754068353024 nn_components.py:261] mlp: residual
I0123 11:57:43.958007 139754068353024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.958093 139754068353024 transformer_base.py:443] tbase: final FFN
I0123 11:57:43.958522 139754068353024 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:57:43.960843 139754068353024 nn_components.py:329] mlp: final activation = None
I0123 11:57:43.962791 139754068353024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.962888 139754068353024 nn_components.py:261] mlp: residual
I0123 11:57:43.963307 139754068353024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.963395 139754068353024 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 11:57:43.963509 139754068353024 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:57:43.963549 139754068353024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:57:43.963580 139754068353024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:57:43.965616 139754068353024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.968128 139754068353024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:57:43.973814 139754068353024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.974082 139754068353024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:57:43.976750 139754068353024 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:57:43.980559 139754068353024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:57:43.980620 139754068353024 attention.py:418] Single window, no scan.
I0123 11:57:43.980657 139754068353024 transformer_layer.py:389] tlayer: self-attention.
I0123 11:57:43.980690 139754068353024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.980752 139754068353024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.981320 139754068353024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.981397 139754068353024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.981775 139754068353024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.982560 139754068353024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.985080 139754068353024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.985709 139754068353024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.985787 139754068353024 transformer_layer.py:468] tlayer: End windows.
I0123 11:57:43.985823 139754068353024 transformer_layer.py:472] tlayer: final FFN.
I0123 11:57:43.985884 139754068353024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.986014 139754068353024 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:57:43.986335 139754068353024 nn_components.py:325] mlp: activation = None
I0123 11:57:43.986379 139754068353024 nn_components.py:329] mlp: final activation = None
I0123 11:57:43.988378 139754068353024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.988476 139754068353024 nn_components.py:261] mlp: residual
I0123 11:57:43.991306 139754068353024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.991387 139754068353024 transformer_base.py:443] tbase: final FFN
I0123 11:57:43.991815 139754068353024 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:57:43.994172 139754068353024 nn_components.py:329] mlp: final activation = None
I0123 11:57:43.996077 139754068353024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.996172 139754068353024 nn_components.py:261] mlp: residual
I0123 11:57:43.996468 139754068353024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:43.996549 139754068353024 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 11:57:43.996666 139754068353024 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:57:43.996707 139754068353024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:57:43.996738 139754068353024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:57:43.998664 139754068353024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:44.001078 139754068353024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:57:44.006720 139754068353024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:44.006987 139754068353024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:57:44.009651 139754068353024 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:57:44.013489 139754068353024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:57:44.013544 139754068353024 attention.py:418] Single window, no scan.
I0123 11:57:44.013580 139754068353024 transformer_layer.py:389] tlayer: self-attention.
I0123 11:57:44.013612 139754068353024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:44.013680 139754068353024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:44.014247 139754068353024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:44.014323 139754068353024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:44.014688 139754068353024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:44.015466 139754068353024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:44.017999 139754068353024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:44.018985 139754068353024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:44.019064 139754068353024 transformer_layer.py:468] tlayer: End windows.
I0123 11:57:44.019100 139754068353024 transformer_layer.py:472] tlayer: final FFN.
I0123 11:57:44.019164 139754068353024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:44.019298 139754068353024 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:57:44.019620 139754068353024 nn_components.py:325] mlp: activation = None
I0123 11:57:44.019663 139754068353024 nn_components.py:329] mlp: final activation = None
I0123 11:57:44.021580 139754068353024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:44.021682 139754068353024 nn_components.py:261] mlp: residual
I0123 11:57:44.024214 139754068353024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:44.024298 139754068353024 transformer_base.py:443] tbase: final FFN
I0123 11:57:44.024779 139754068353024 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:57:44.027058 139754068353024 nn_components.py:329] mlp: final activation = None
I0123 11:57:44.028969 139754068353024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:44.029068 139754068353024 nn_components.py:261] mlp: residual
I0123 11:57:44.029361 139754068353024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:44.029648 139754068353024 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:57:44.029721 139754068353024 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:57:44.029789 139754068353024 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:57:44.029848 139754068353024 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:57:44.029903 139754068353024 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:57:44.029957 139754068353024 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:57:44.030011 139754068353024 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:57:44.030064 139754068353024 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:57:44.030117 139754068353024 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:57:44.030169 139754068353024 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:57:44.030222 139754068353024 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:57:44.030275 139754068353024 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:57:44.030312 139754068353024 decoder_stack.py:344] dstack: Final layernorm.
I0123 11:57:44.033874 139754068353024 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:57:44.082164 139754068353024 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.082249 139754068353024 decoder_stack.py:333] dstack: autoregressive generator.
I0123 11:57:44.082305 139754068353024 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 11:57:44.082410 139754068353024 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:57:44.082449 139754068353024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:57:44.082479 139754068353024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:57:44.082543 139754068353024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.085017 139754068353024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:57:44.090564 139754068353024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.090829 139754068353024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:57:44.093475 139754068353024 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:57:44.110065 139754068353024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:57:44.110121 139754068353024 attention.py:418] Single window, no scan.
I0123 11:57:44.110159 139754068353024 transformer_layer.py:389] tlayer: self-attention.
I0123 11:57:44.110190 139754068353024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.110253 139754068353024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.111392 139754068353024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.111471 139754068353024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.112191 139754068353024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.114242 139754068353024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.119071 139754068353024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.120393 139754068353024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.120480 139754068353024 transformer_layer.py:468] tlayer: End windows.
I0123 11:57:44.120516 139754068353024 transformer_layer.py:472] tlayer: final FFN.
I0123 11:57:44.120577 139754068353024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.120712 139754068353024 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:57:44.120829 139754068353024 nn_components.py:325] mlp: activation = None
I0123 11:57:44.120869 139754068353024 nn_components.py:329] mlp: final activation = None
I0123 11:57:44.122798 139754068353024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.122895 139754068353024 nn_components.py:261] mlp: residual
I0123 11:57:44.125350 139754068353024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.125431 139754068353024 transformer_base.py:443] tbase: final FFN
I0123 11:57:44.125542 139754068353024 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:57:44.127805 139754068353024 nn_components.py:329] mlp: final activation = None
I0123 11:57:44.129773 139754068353024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.129870 139754068353024 nn_components.py:261] mlp: residual
I0123 11:57:44.130166 139754068353024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.130249 139754068353024 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 11:57:44.130360 139754068353024 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:57:44.130399 139754068353024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:57:44.130431 139754068353024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:57:44.130496 139754068353024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.132777 139754068353024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:57:44.138309 139754068353024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.138572 139754068353024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:57:44.141295 139754068353024 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:57:44.154544 139754068353024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:57:44.154601 139754068353024 attention.py:418] Single window, no scan.
I0123 11:57:44.154638 139754068353024 transformer_layer.py:389] tlayer: self-attention.
I0123 11:57:44.154670 139754068353024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.154732 139754068353024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.155301 139754068353024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.155378 139754068353024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.155741 139754068353024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.156443 139754068353024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.158983 139754068353024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.159609 139754068353024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.159686 139754068353024 transformer_layer.py:468] tlayer: End windows.
I0123 11:57:44.159727 139754068353024 transformer_layer.py:472] tlayer: final FFN.
I0123 11:57:44.159788 139754068353024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.159924 139754068353024 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:57:44.160034 139754068353024 nn_components.py:325] mlp: activation = None
I0123 11:57:44.160073 139754068353024 nn_components.py:329] mlp: final activation = None
I0123 11:57:44.162025 139754068353024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.162120 139754068353024 nn_components.py:261] mlp: residual
I0123 11:57:44.164564 139754068353024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.164645 139754068353024 transformer_base.py:443] tbase: final FFN
I0123 11:57:44.164755 139754068353024 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:57:44.166994 139754068353024 nn_components.py:329] mlp: final activation = None
I0123 11:57:44.169020 139754068353024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.169115 139754068353024 nn_components.py:261] mlp: residual
I0123 11:57:44.169567 139754068353024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.169654 139754068353024 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 11:57:44.169767 139754068353024 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:57:44.169807 139754068353024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:57:44.169839 139754068353024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:57:44.169902 139754068353024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.172161 139754068353024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:57:44.177636 139754068353024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.177906 139754068353024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:57:44.180612 139754068353024 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:57:44.193333 139754068353024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:57:44.193390 139754068353024 attention.py:418] Single window, no scan.
I0123 11:57:44.193428 139754068353024 transformer_layer.py:389] tlayer: self-attention.
I0123 11:57:44.193460 139754068353024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.193523 139754068353024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.197966 139754068353024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.198085 139754068353024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.198493 139754068353024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.199242 139754068353024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.201865 139754068353024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.202505 139754068353024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.202587 139754068353024 transformer_layer.py:468] tlayer: End windows.
I0123 11:57:44.202624 139754068353024 transformer_layer.py:472] tlayer: final FFN.
I0123 11:57:44.202703 139754068353024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.202841 139754068353024 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:57:44.202961 139754068353024 nn_components.py:325] mlp: activation = None
I0123 11:57:44.203000 139754068353024 nn_components.py:329] mlp: final activation = None
I0123 11:57:44.205057 139754068353024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.205151 139754068353024 nn_components.py:261] mlp: residual
I0123 11:57:44.207690 139754068353024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.207771 139754068353024 transformer_base.py:443] tbase: final FFN
I0123 11:57:44.207883 139754068353024 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:57:44.210187 139754068353024 nn_components.py:329] mlp: final activation = None
I0123 11:57:44.212134 139754068353024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.212229 139754068353024 nn_components.py:261] mlp: residual
I0123 11:57:44.212520 139754068353024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.212606 139754068353024 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 11:57:44.212720 139754068353024 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:57:44.212762 139754068353024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:57:44.212794 139754068353024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:57:44.212861 139754068353024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.215164 139754068353024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:57:44.220641 139754068353024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.220905 139754068353024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:57:44.223667 139754068353024 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:57:44.236729 139754068353024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:57:44.236787 139754068353024 attention.py:418] Single window, no scan.
I0123 11:57:44.236824 139754068353024 transformer_layer.py:389] tlayer: self-attention.
I0123 11:57:44.236856 139754068353024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.236918 139754068353024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.237484 139754068353024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.237561 139754068353024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.237936 139754068353024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.238643 139754068353024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.241182 139754068353024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.241816 139754068353024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.241894 139754068353024 transformer_layer.py:468] tlayer: End windows.
I0123 11:57:44.241930 139754068353024 transformer_layer.py:472] tlayer: final FFN.
I0123 11:57:44.241990 139754068353024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.242135 139754068353024 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:57:44.242247 139754068353024 nn_components.py:325] mlp: activation = None
I0123 11:57:44.242285 139754068353024 nn_components.py:329] mlp: final activation = None
I0123 11:57:44.244259 139754068353024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.244354 139754068353024 nn_components.py:261] mlp: residual
I0123 11:57:44.246806 139754068353024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.246891 139754068353024 transformer_base.py:443] tbase: final FFN
I0123 11:57:44.247002 139754068353024 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:57:44.249228 139754068353024 nn_components.py:329] mlp: final activation = None
I0123 11:57:44.251114 139754068353024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.251210 139754068353024 nn_components.py:261] mlp: residual
I0123 11:57:44.251499 139754068353024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.251580 139754068353024 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 11:57:44.251689 139754068353024 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:57:44.251729 139754068353024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:57:44.251761 139754068353024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:57:44.251827 139754068353024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.254450 139754068353024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:57:44.259997 139754068353024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.260267 139754068353024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:57:44.263026 139754068353024 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:57:44.276292 139754068353024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:57:44.276348 139754068353024 attention.py:418] Single window, no scan.
I0123 11:57:44.276385 139754068353024 transformer_layer.py:389] tlayer: self-attention.
I0123 11:57:44.276417 139754068353024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.276483 139754068353024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.277051 139754068353024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.277129 139754068353024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.277497 139754068353024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.278213 139754068353024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.280822 139754068353024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.281455 139754068353024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.281536 139754068353024 transformer_layer.py:468] tlayer: End windows.
I0123 11:57:44.281573 139754068353024 transformer_layer.py:472] tlayer: final FFN.
I0123 11:57:44.281633 139754068353024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.281783 139754068353024 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:57:44.281897 139754068353024 nn_components.py:325] mlp: activation = None
I0123 11:57:44.281937 139754068353024 nn_components.py:329] mlp: final activation = None
I0123 11:57:44.283849 139754068353024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.283944 139754068353024 nn_components.py:261] mlp: residual
I0123 11:57:44.286399 139754068353024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.286479 139754068353024 transformer_base.py:443] tbase: final FFN
I0123 11:57:44.286588 139754068353024 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:57:44.288901 139754068353024 nn_components.py:329] mlp: final activation = None
I0123 11:57:44.290955 139754068353024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.291051 139754068353024 nn_components.py:261] mlp: residual
I0123 11:57:44.291365 139754068353024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.291447 139754068353024 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 11:57:44.291559 139754068353024 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:57:44.291599 139754068353024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:57:44.291631 139754068353024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:57:44.291696 139754068353024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.294008 139754068353024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:57:44.299523 139754068353024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.299788 139754068353024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:57:44.302523 139754068353024 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:57:44.315351 139754068353024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:57:44.315407 139754068353024 attention.py:418] Single window, no scan.
I0123 11:57:44.315443 139754068353024 transformer_layer.py:389] tlayer: self-attention.
I0123 11:57:44.315475 139754068353024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.315537 139754068353024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.316110 139754068353024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.316188 139754068353024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.316551 139754068353024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.317259 139754068353024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.319816 139754068353024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.320436 139754068353024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.320515 139754068353024 transformer_layer.py:468] tlayer: End windows.
I0123 11:57:44.320551 139754068353024 transformer_layer.py:472] tlayer: final FFN.
I0123 11:57:44.320610 139754068353024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.320743 139754068353024 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:57:44.320858 139754068353024 nn_components.py:325] mlp: activation = None
I0123 11:57:44.320898 139754068353024 nn_components.py:329] mlp: final activation = None
I0123 11:57:44.322858 139754068353024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.322953 139754068353024 nn_components.py:261] mlp: residual
I0123 11:57:44.325395 139754068353024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.325476 139754068353024 transformer_base.py:443] tbase: final FFN
I0123 11:57:44.325584 139754068353024 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:57:44.327841 139754068353024 nn_components.py:329] mlp: final activation = None
I0123 11:57:44.329722 139754068353024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.329819 139754068353024 nn_components.py:261] mlp: residual
I0123 11:57:44.330109 139754068353024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.330193 139754068353024 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 11:57:44.330302 139754068353024 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:57:44.330343 139754068353024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:57:44.330374 139754068353024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:57:44.330438 139754068353024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.332698 139754068353024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:57:44.338315 139754068353024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.338580 139754068353024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:57:44.341231 139754068353024 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:57:44.353986 139754068353024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:57:44.354043 139754068353024 attention.py:418] Single window, no scan.
I0123 11:57:44.354080 139754068353024 transformer_layer.py:389] tlayer: self-attention.
I0123 11:57:44.354112 139754068353024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.354175 139754068353024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.354745 139754068353024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.354822 139754068353024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.355185 139754068353024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.355874 139754068353024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.358409 139754068353024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.359413 139754068353024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.359492 139754068353024 transformer_layer.py:468] tlayer: End windows.
I0123 11:57:44.359527 139754068353024 transformer_layer.py:472] tlayer: final FFN.
I0123 11:57:44.359588 139754068353024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.359717 139754068353024 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:57:44.359828 139754068353024 nn_components.py:325] mlp: activation = None
I0123 11:57:44.359872 139754068353024 nn_components.py:329] mlp: final activation = None
I0123 11:57:44.361799 139754068353024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.361896 139754068353024 nn_components.py:261] mlp: residual
I0123 11:57:44.364337 139754068353024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.364418 139754068353024 transformer_base.py:443] tbase: final FFN
I0123 11:57:44.364526 139754068353024 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:57:44.366772 139754068353024 nn_components.py:329] mlp: final activation = None
I0123 11:57:44.368725 139754068353024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.368822 139754068353024 nn_components.py:261] mlp: residual
I0123 11:57:44.369112 139754068353024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.369194 139754068353024 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 11:57:44.369302 139754068353024 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:57:44.369342 139754068353024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:57:44.369373 139754068353024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:57:44.369437 139754068353024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.371711 139754068353024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:57:44.377230 139754068353024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.377508 139754068353024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:57:44.380224 139754068353024 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:57:44.393080 139754068353024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:57:44.393136 139754068353024 attention.py:418] Single window, no scan.
I0123 11:57:44.393172 139754068353024 transformer_layer.py:389] tlayer: self-attention.
I0123 11:57:44.393203 139754068353024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.393265 139754068353024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.393885 139754068353024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.393964 139754068353024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.394332 139754068353024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.395036 139754068353024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.397544 139754068353024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.398180 139754068353024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.398258 139754068353024 transformer_layer.py:468] tlayer: End windows.
I0123 11:57:44.398294 139754068353024 transformer_layer.py:472] tlayer: final FFN.
I0123 11:57:44.398355 139754068353024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.398488 139754068353024 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:57:44.398599 139754068353024 nn_components.py:325] mlp: activation = None
I0123 11:57:44.398643 139754068353024 nn_components.py:329] mlp: final activation = None
I0123 11:57:44.400535 139754068353024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.400630 139754068353024 nn_components.py:261] mlp: residual
I0123 11:57:44.403125 139754068353024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.403204 139754068353024 transformer_base.py:443] tbase: final FFN
I0123 11:57:44.403312 139754068353024 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:57:44.405530 139754068353024 nn_components.py:329] mlp: final activation = None
I0123 11:57:44.407402 139754068353024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.407498 139754068353024 nn_components.py:261] mlp: residual
I0123 11:57:44.407787 139754068353024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.407869 139754068353024 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 11:57:44.407979 139754068353024 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:57:44.408018 139754068353024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:57:44.408049 139754068353024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:57:44.408112 139754068353024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.410372 139754068353024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:57:44.415908 139754068353024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.416174 139754068353024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:57:44.418824 139754068353024 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:57:44.431611 139754068353024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:57:44.431668 139754068353024 attention.py:418] Single window, no scan.
I0123 11:57:44.431703 139754068353024 transformer_layer.py:389] tlayer: self-attention.
I0123 11:57:44.431734 139754068353024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.431796 139754068353024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.432369 139754068353024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.432446 139754068353024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.432811 139754068353024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.433502 139754068353024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.436017 139754068353024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.436688 139754068353024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.436766 139754068353024 transformer_layer.py:468] tlayer: End windows.
I0123 11:57:44.436802 139754068353024 transformer_layer.py:472] tlayer: final FFN.
I0123 11:57:44.436868 139754068353024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.437000 139754068353024 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:57:44.437110 139754068353024 nn_components.py:325] mlp: activation = None
I0123 11:57:44.437149 139754068353024 nn_components.py:329] mlp: final activation = None
I0123 11:57:44.439043 139754068353024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.439137 139754068353024 nn_components.py:261] mlp: residual
I0123 11:57:44.441556 139754068353024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.441639 139754068353024 transformer_base.py:443] tbase: final FFN
I0123 11:57:44.441755 139754068353024 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:57:44.443980 139754068353024 nn_components.py:329] mlp: final activation = None
I0123 11:57:44.445933 139754068353024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.446029 139754068353024 nn_components.py:261] mlp: residual
I0123 11:57:44.446318 139754068353024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.446400 139754068353024 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 11:57:44.446510 139754068353024 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:57:44.446550 139754068353024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:57:44.446582 139754068353024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:57:44.446646 139754068353024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.448898 139754068353024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:57:44.454356 139754068353024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.454618 139754068353024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:57:44.457309 139754068353024 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:57:44.470277 139754068353024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:57:44.470335 139754068353024 attention.py:418] Single window, no scan.
I0123 11:57:44.470371 139754068353024 transformer_layer.py:389] tlayer: self-attention.
I0123 11:57:44.470402 139754068353024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.470464 139754068353024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.471074 139754068353024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.471151 139754068353024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.471510 139754068353024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.472211 139754068353024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.474735 139754068353024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.475364 139754068353024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.475442 139754068353024 transformer_layer.py:468] tlayer: End windows.
I0123 11:57:44.475478 139754068353024 transformer_layer.py:472] tlayer: final FFN.
I0123 11:57:44.475536 139754068353024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.475666 139754068353024 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:57:44.475778 139754068353024 nn_components.py:325] mlp: activation = None
I0123 11:57:44.475817 139754068353024 nn_components.py:329] mlp: final activation = None
I0123 11:57:44.477716 139754068353024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.477819 139754068353024 nn_components.py:261] mlp: residual
I0123 11:57:44.480303 139754068353024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.480384 139754068353024 transformer_base.py:443] tbase: final FFN
I0123 11:57:44.480495 139754068353024 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:57:44.482729 139754068353024 nn_components.py:329] mlp: final activation = None
I0123 11:57:44.484586 139754068353024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.484681 139754068353024 nn_components.py:261] mlp: residual
I0123 11:57:44.484967 139754068353024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.485048 139754068353024 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 11:57:44.485155 139754068353024 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:57:44.485194 139754068353024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:57:44.485225 139754068353024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:57:44.485288 139754068353024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.487539 139754068353024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:57:44.493052 139754068353024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.493314 139754068353024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:57:44.495968 139754068353024 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:57:44.508653 139754068353024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:57:44.508712 139754068353024 attention.py:418] Single window, no scan.
I0123 11:57:44.508752 139754068353024 transformer_layer.py:389] tlayer: self-attention.
I0123 11:57:44.508785 139754068353024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.508846 139754068353024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.509403 139754068353024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.509478 139754068353024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.509845 139754068353024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.510547 139754068353024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.513050 139754068353024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.513719 139754068353024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.513797 139754068353024 transformer_layer.py:468] tlayer: End windows.
I0123 11:57:44.513832 139754068353024 transformer_layer.py:472] tlayer: final FFN.
I0123 11:57:44.513892 139754068353024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.514025 139754068353024 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:57:44.514138 139754068353024 nn_components.py:325] mlp: activation = None
I0123 11:57:44.514178 139754068353024 nn_components.py:329] mlp: final activation = None
I0123 11:57:44.516091 139754068353024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.516196 139754068353024 nn_components.py:261] mlp: residual
I0123 11:57:44.518646 139754068353024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.518728 139754068353024 transformer_base.py:443] tbase: final FFN
I0123 11:57:44.518837 139754068353024 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:57:44.521045 139754068353024 nn_components.py:329] mlp: final activation = None
I0123 11:57:44.522992 139754068353024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.523089 139754068353024 nn_components.py:261] mlp: residual
I0123 11:57:44.523381 139754068353024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.523463 139754068353024 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 11:57:44.523572 139754068353024 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:57:44.523612 139754068353024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:57:44.523644 139754068353024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:57:44.523708 139754068353024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.525976 139754068353024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:57:44.531419 139754068353024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.531686 139754068353024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:57:44.534404 139754068353024 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:57:44.547039 139754068353024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:57:44.547097 139754068353024 attention.py:418] Single window, no scan.
I0123 11:57:44.547134 139754068353024 transformer_layer.py:389] tlayer: self-attention.
I0123 11:57:44.547166 139754068353024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.547233 139754068353024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.547796 139754068353024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.547872 139754068353024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.548227 139754068353024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.548970 139754068353024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.551496 139754068353024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.552126 139754068353024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.552204 139754068353024 transformer_layer.py:468] tlayer: End windows.
I0123 11:57:44.552240 139754068353024 transformer_layer.py:472] tlayer: final FFN.
I0123 11:57:44.552300 139754068353024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.552428 139754068353024 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:57:44.552536 139754068353024 nn_components.py:325] mlp: activation = None
I0123 11:57:44.552575 139754068353024 nn_components.py:329] mlp: final activation = None
I0123 11:57:44.554468 139754068353024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.554564 139754068353024 nn_components.py:261] mlp: residual
I0123 11:57:44.556993 139754068353024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.557073 139754068353024 transformer_base.py:443] tbase: final FFN
I0123 11:57:44.557184 139754068353024 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:57:44.559471 139754068353024 nn_components.py:329] mlp: final activation = None
I0123 11:57:44.561332 139754068353024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.561429 139754068353024 nn_components.py:261] mlp: residual
I0123 11:57:44.561724 139754068353024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.561816 139754068353024 decoder_stack.py:344] dstack: Final layernorm.
I0123 11:57:44.564773 139754068353024 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:57:44.620233 139754068353024 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.620321 139754068353024 decoder_stack.py:333] dstack: autoregressive generator.
I0123 11:57:44.620377 139754068353024 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 11:57:44.620482 139754068353024 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:57:44.620521 139754068353024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:57:44.620551 139754068353024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:57:44.620615 139754068353024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.623317 139754068353024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:57:44.628743 139754068353024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.629005 139754068353024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:57:44.631653 139754068353024 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:57:44.644013 139754068353024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:57:44.644070 139754068353024 attention.py:418] Single window, no scan.
I0123 11:57:44.644105 139754068353024 transformer_layer.py:389] tlayer: self-attention.
I0123 11:57:44.644135 139754068353024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.644197 139754068353024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.644759 139754068353024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.644835 139754068353024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.645190 139754068353024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.645873 139754068353024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.648398 139754068353024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.649017 139754068353024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.649095 139754068353024 transformer_layer.py:468] tlayer: End windows.
I0123 11:57:44.649130 139754068353024 transformer_layer.py:472] tlayer: final FFN.
I0123 11:57:44.649189 139754068353024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.649318 139754068353024 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:57:44.649436 139754068353024 nn_components.py:325] mlp: activation = None
I0123 11:57:44.649477 139754068353024 nn_components.py:329] mlp: final activation = None
I0123 11:57:44.651329 139754068353024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.651427 139754068353024 nn_components.py:261] mlp: residual
I0123 11:57:44.653811 139754068353024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.653893 139754068353024 transformer_base.py:443] tbase: final FFN
I0123 11:57:44.654004 139754068353024 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:57:44.656246 139754068353024 nn_components.py:329] mlp: final activation = None
I0123 11:57:44.658307 139754068353024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.658405 139754068353024 nn_components.py:261] mlp: residual
I0123 11:57:44.658694 139754068353024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.658778 139754068353024 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 11:57:44.658887 139754068353024 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:57:44.658927 139754068353024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:57:44.658957 139754068353024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:57:44.659021 139754068353024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.661245 139754068353024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:57:44.666649 139754068353024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.666907 139754068353024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:57:44.669562 139754068353024 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:57:44.681778 139754068353024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:57:44.681834 139754068353024 attention.py:418] Single window, no scan.
I0123 11:57:44.681869 139754068353024 transformer_layer.py:389] tlayer: self-attention.
I0123 11:57:44.681901 139754068353024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.681963 139754068353024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.682522 139754068353024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.682600 139754068353024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.682959 139754068353024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.683639 139754068353024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.686153 139754068353024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.686768 139754068353024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.686845 139754068353024 transformer_layer.py:468] tlayer: End windows.
I0123 11:57:44.686882 139754068353024 transformer_layer.py:472] tlayer: final FFN.
I0123 11:57:44.686941 139754068353024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.687070 139754068353024 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:57:44.687179 139754068353024 nn_components.py:325] mlp: activation = None
I0123 11:57:44.687224 139754068353024 nn_components.py:329] mlp: final activation = None
I0123 11:57:44.689068 139754068353024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.689163 139754068353024 nn_components.py:261] mlp: residual
I0123 11:57:44.691572 139754068353024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.691653 139754068353024 transformer_base.py:443] tbase: final FFN
I0123 11:57:44.691764 139754068353024 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:57:44.694017 139754068353024 nn_components.py:329] mlp: final activation = None
I0123 11:57:44.695863 139754068353024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.695958 139754068353024 nn_components.py:261] mlp: residual
I0123 11:57:44.696244 139754068353024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.696325 139754068353024 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 11:57:44.696433 139754068353024 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:57:44.696472 139754068353024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:57:44.696503 139754068353024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:57:44.696567 139754068353024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.698809 139754068353024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:57:44.704177 139754068353024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.704437 139754068353024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:57:44.707113 139754068353024 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:57:44.719738 139754068353024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:57:44.719795 139754068353024 attention.py:418] Single window, no scan.
I0123 11:57:44.719830 139754068353024 transformer_layer.py:389] tlayer: self-attention.
I0123 11:57:44.719861 139754068353024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.719923 139754068353024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.720474 139754068353024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.720551 139754068353024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.720909 139754068353024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.721593 139754068353024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.724147 139754068353024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.724764 139754068353024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.724843 139754068353024 transformer_layer.py:468] tlayer: End windows.
I0123 11:57:44.724879 139754068353024 transformer_layer.py:472] tlayer: final FFN.
I0123 11:57:44.724941 139754068353024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.725071 139754068353024 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:57:44.725180 139754068353024 nn_components.py:325] mlp: activation = None
I0123 11:57:44.725220 139754068353024 nn_components.py:329] mlp: final activation = None
I0123 11:57:44.727076 139754068353024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.727171 139754068353024 nn_components.py:261] mlp: residual
I0123 11:57:44.729589 139754068353024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.729675 139754068353024 transformer_base.py:443] tbase: final FFN
I0123 11:57:44.729789 139754068353024 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:57:44.732475 139754068353024 nn_components.py:329] mlp: final activation = None
I0123 11:57:44.734334 139754068353024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.734431 139754068353024 nn_components.py:261] mlp: residual
I0123 11:57:44.734717 139754068353024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.734798 139754068353024 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 11:57:44.734908 139754068353024 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:57:44.734947 139754068353024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:57:44.734978 139754068353024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:57:44.735041 139754068353024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.737258 139754068353024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:57:44.742637 139754068353024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.742899 139754068353024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:57:44.745557 139754068353024 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:57:44.757958 139754068353024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:57:44.758014 139754068353024 attention.py:418] Single window, no scan.
I0123 11:57:44.758052 139754068353024 transformer_layer.py:389] tlayer: self-attention.
I0123 11:57:44.758095 139754068353024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.758160 139754068353024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.758718 139754068353024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.758793 139754068353024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.759157 139754068353024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.759846 139754068353024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.762401 139754068353024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.763019 139754068353024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.763096 139754068353024 transformer_layer.py:468] tlayer: End windows.
I0123 11:57:44.763131 139754068353024 transformer_layer.py:472] tlayer: final FFN.
I0123 11:57:44.763189 139754068353024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.763318 139754068353024 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:57:44.763425 139754068353024 nn_components.py:325] mlp: activation = None
I0123 11:57:44.763464 139754068353024 nn_components.py:329] mlp: final activation = None
I0123 11:57:44.765335 139754068353024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.765428 139754068353024 nn_components.py:261] mlp: residual
I0123 11:57:44.767856 139754068353024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.767935 139754068353024 transformer_base.py:443] tbase: final FFN
I0123 11:57:44.768041 139754068353024 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:57:44.770310 139754068353024 nn_components.py:329] mlp: final activation = None
I0123 11:57:44.772177 139754068353024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.772271 139754068353024 nn_components.py:261] mlp: residual
I0123 11:57:44.772562 139754068353024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.772642 139754068353024 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 11:57:44.772749 139754068353024 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:57:44.772788 139754068353024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:57:44.772818 139754068353024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:57:44.772880 139754068353024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.775128 139754068353024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:57:44.780521 139754068353024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.780780 139754068353024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:57:44.783473 139754068353024 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:57:44.796019 139754068353024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:57:44.796073 139754068353024 attention.py:418] Single window, no scan.
I0123 11:57:44.796108 139754068353024 transformer_layer.py:389] tlayer: self-attention.
I0123 11:57:44.796139 139754068353024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.796200 139754068353024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.796759 139754068353024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.796835 139754068353024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.797195 139754068353024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.797884 139754068353024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.800446 139754068353024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.801063 139754068353024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.801138 139754068353024 transformer_layer.py:468] tlayer: End windows.
I0123 11:57:44.801172 139754068353024 transformer_layer.py:472] tlayer: final FFN.
I0123 11:57:44.801234 139754068353024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.801380 139754068353024 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:57:44.801489 139754068353024 nn_components.py:325] mlp: activation = None
I0123 11:57:44.801526 139754068353024 nn_components.py:329] mlp: final activation = None
I0123 11:57:44.803402 139754068353024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.803501 139754068353024 nn_components.py:261] mlp: residual
I0123 11:57:44.805922 139754068353024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.806001 139754068353024 transformer_base.py:443] tbase: final FFN
I0123 11:57:44.806110 139754068353024 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:57:44.808393 139754068353024 nn_components.py:329] mlp: final activation = None
I0123 11:57:44.810263 139754068353024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.810360 139754068353024 nn_components.py:261] mlp: residual
I0123 11:57:44.810647 139754068353024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.810728 139754068353024 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 11:57:44.810836 139754068353024 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:57:44.810874 139754068353024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:57:44.810903 139754068353024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:57:44.810965 139754068353024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.813225 139754068353024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:57:44.818739 139754068353024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.818997 139754068353024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:57:44.821697 139754068353024 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:57:44.834375 139754068353024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:57:44.834429 139754068353024 attention.py:418] Single window, no scan.
I0123 11:57:44.834463 139754068353024 transformer_layer.py:389] tlayer: self-attention.
I0123 11:57:44.834492 139754068353024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.834553 139754068353024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.835104 139754068353024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.835178 139754068353024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.835530 139754068353024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.836222 139754068353024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.838785 139754068353024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.839397 139754068353024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.839471 139754068353024 transformer_layer.py:468] tlayer: End windows.
I0123 11:57:44.839505 139754068353024 transformer_layer.py:472] tlayer: final FFN.
I0123 11:57:44.839564 139754068353024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.839689 139754068353024 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:57:44.839797 139754068353024 nn_components.py:325] mlp: activation = None
I0123 11:57:44.839834 139754068353024 nn_components.py:329] mlp: final activation = None
I0123 11:57:44.841714 139754068353024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.841813 139754068353024 nn_components.py:261] mlp: residual
I0123 11:57:44.844220 139754068353024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.844300 139754068353024 transformer_base.py:443] tbase: final FFN
I0123 11:57:44.844407 139754068353024 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:57:44.847096 139754068353024 nn_components.py:329] mlp: final activation = None
I0123 11:57:44.848965 139754068353024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.849059 139754068353024 nn_components.py:261] mlp: residual
I0123 11:57:44.849348 139754068353024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.849429 139754068353024 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 11:57:44.849537 139754068353024 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:57:44.849574 139754068353024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:57:44.849604 139754068353024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:57:44.849671 139754068353024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.851908 139754068353024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:57:44.857288 139754068353024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.857548 139754068353024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:57:44.860307 139754068353024 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:57:44.873311 139754068353024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:57:44.873366 139754068353024 attention.py:418] Single window, no scan.
I0123 11:57:44.873401 139754068353024 transformer_layer.py:389] tlayer: self-attention.
I0123 11:57:44.873430 139754068353024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.873492 139754068353024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.874065 139754068353024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.874141 139754068353024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.874499 139754068353024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.875215 139754068353024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.877756 139754068353024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.878381 139754068353024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.878458 139754068353024 transformer_layer.py:468] tlayer: End windows.
I0123 11:57:44.878492 139754068353024 transformer_layer.py:472] tlayer: final FFN.
I0123 11:57:44.878549 139754068353024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.878676 139754068353024 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:57:44.878783 139754068353024 nn_components.py:325] mlp: activation = None
I0123 11:57:44.878821 139754068353024 nn_components.py:329] mlp: final activation = None
I0123 11:57:44.880700 139754068353024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.880794 139754068353024 nn_components.py:261] mlp: residual
I0123 11:57:44.883217 139754068353024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.883299 139754068353024 transformer_base.py:443] tbase: final FFN
I0123 11:57:44.883409 139754068353024 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:57:44.885794 139754068353024 nn_components.py:329] mlp: final activation = None
I0123 11:57:44.887882 139754068353024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.887994 139754068353024 nn_components.py:261] mlp: residual
I0123 11:57:44.888281 139754068353024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.888360 139754068353024 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 11:57:44.888465 139754068353024 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:57:44.888503 139754068353024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:57:44.888533 139754068353024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:57:44.888596 139754068353024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.890889 139754068353024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:57:44.896472 139754068353024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.896730 139754068353024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:57:44.899483 139754068353024 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:57:44.912326 139754068353024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:57:44.912380 139754068353024 attention.py:418] Single window, no scan.
I0123 11:57:44.912415 139754068353024 transformer_layer.py:389] tlayer: self-attention.
I0123 11:57:44.912444 139754068353024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.912505 139754068353024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.913060 139754068353024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.913135 139754068353024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.913495 139754068353024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.914193 139754068353024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.916827 139754068353024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.917449 139754068353024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.917527 139754068353024 transformer_layer.py:468] tlayer: End windows.
I0123 11:57:44.917561 139754068353024 transformer_layer.py:472] tlayer: final FFN.
I0123 11:57:44.917620 139754068353024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.917757 139754068353024 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:57:44.917870 139754068353024 nn_components.py:325] mlp: activation = None
I0123 11:57:44.917908 139754068353024 nn_components.py:329] mlp: final activation = None
I0123 11:57:44.919835 139754068353024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.919945 139754068353024 nn_components.py:261] mlp: residual
I0123 11:57:44.922383 139754068353024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.922469 139754068353024 transformer_base.py:443] tbase: final FFN
I0123 11:57:44.922576 139754068353024 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:57:44.924910 139754068353024 nn_components.py:329] mlp: final activation = None
I0123 11:57:44.926787 139754068353024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.926902 139754068353024 nn_components.py:261] mlp: residual
I0123 11:57:44.927196 139754068353024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.927277 139754068353024 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 11:57:44.927383 139754068353024 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:57:44.927420 139754068353024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:57:44.927448 139754068353024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:57:44.927509 139754068353024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.929757 139754068353024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:57:44.935242 139754068353024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.935501 139754068353024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:57:44.938213 139754068353024 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:57:44.950757 139754068353024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:57:44.950811 139754068353024 attention.py:418] Single window, no scan.
I0123 11:57:44.950846 139754068353024 transformer_layer.py:389] tlayer: self-attention.
I0123 11:57:44.950877 139754068353024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.950942 139754068353024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.951507 139754068353024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.951581 139754068353024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.951937 139754068353024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.952627 139754068353024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.955216 139754068353024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.955837 139754068353024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.955912 139754068353024 transformer_layer.py:468] tlayer: End windows.
I0123 11:57:44.955947 139754068353024 transformer_layer.py:472] tlayer: final FFN.
I0123 11:57:44.956004 139754068353024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.956129 139754068353024 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:57:44.956235 139754068353024 nn_components.py:325] mlp: activation = None
I0123 11:57:44.956272 139754068353024 nn_components.py:329] mlp: final activation = None
I0123 11:57:44.958147 139754068353024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.958240 139754068353024 nn_components.py:261] mlp: residual
I0123 11:57:44.960660 139754068353024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.960744 139754068353024 transformer_base.py:443] tbase: final FFN
I0123 11:57:44.960855 139754068353024 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:57:44.963524 139754068353024 nn_components.py:329] mlp: final activation = None
I0123 11:57:44.965399 139754068353024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.965494 139754068353024 nn_components.py:261] mlp: residual
I0123 11:57:44.965788 139754068353024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.965868 139754068353024 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 11:57:44.965976 139754068353024 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:57:44.966014 139754068353024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:57:44.966044 139754068353024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:57:44.966106 139754068353024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.968344 139754068353024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:57:44.973812 139754068353024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.974074 139754068353024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:57:44.976754 139754068353024 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:57:44.989495 139754068353024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:57:44.989549 139754068353024 attention.py:418] Single window, no scan.
I0123 11:57:44.989584 139754068353024 transformer_layer.py:389] tlayer: self-attention.
I0123 11:57:44.989614 139754068353024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.989683 139754068353024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.990245 139754068353024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.990321 139754068353024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.990676 139754068353024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.991366 139754068353024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.993913 139754068353024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.994531 139754068353024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.994607 139754068353024 transformer_layer.py:468] tlayer: End windows.
I0123 11:57:44.994642 139754068353024 transformer_layer.py:472] tlayer: final FFN.
I0123 11:57:44.994699 139754068353024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.994827 139754068353024 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:57:44.994934 139754068353024 nn_components.py:325] mlp: activation = None
I0123 11:57:44.994972 139754068353024 nn_components.py:329] mlp: final activation = None
I0123 11:57:44.997320 139754068353024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.997415 139754068353024 nn_components.py:261] mlp: residual
I0123 11:57:44.999812 139754068353024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:44.999893 139754068353024 transformer_base.py:443] tbase: final FFN
I0123 11:57:45.000006 139754068353024 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:57:45.002251 139754068353024 nn_components.py:329] mlp: final activation = None
I0123 11:57:45.004079 139754068353024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:45.004173 139754068353024 nn_components.py:261] mlp: residual
I0123 11:57:45.004460 139754068353024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:45.004539 139754068353024 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 11:57:45.004644 139754068353024 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:57:45.004682 139754068353024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:57:45.004711 139754068353024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:57:45.004772 139754068353024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:45.007019 139754068353024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:57:45.012398 139754068353024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:45.012655 139754068353024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:57:45.015345 139754068353024 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:57:45.027827 139754068353024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:57:45.027882 139754068353024 attention.py:418] Single window, no scan.
I0123 11:57:45.027916 139754068353024 transformer_layer.py:389] tlayer: self-attention.
I0123 11:57:45.027945 139754068353024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:45.028007 139754068353024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:45.028568 139754068353024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:45.028643 139754068353024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:45.028995 139754068353024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:45.029687 139754068353024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:45.032246 139754068353024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:45.032869 139754068353024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:45.032947 139754068353024 transformer_layer.py:468] tlayer: End windows.
I0123 11:57:45.032982 139754068353024 transformer_layer.py:472] tlayer: final FFN.
I0123 11:57:45.033040 139754068353024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:45.033172 139754068353024 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:57:45.033281 139754068353024 nn_components.py:325] mlp: activation = None
I0123 11:57:45.033318 139754068353024 nn_components.py:329] mlp: final activation = None
I0123 11:57:45.035211 139754068353024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:45.035305 139754068353024 nn_components.py:261] mlp: residual
I0123 11:57:45.037706 139754068353024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:45.037785 139754068353024 transformer_base.py:443] tbase: final FFN
I0123 11:57:45.037892 139754068353024 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:57:45.040176 139754068353024 nn_components.py:329] mlp: final activation = None
I0123 11:57:45.042038 139754068353024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:45.042133 139754068353024 nn_components.py:261] mlp: residual
I0123 11:57:45.042422 139754068353024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:45.042503 139754068353024 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 11:57:45.042610 139754068353024 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:57:45.042648 139754068353024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:57:45.042678 139754068353024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:57:45.042739 139754068353024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:45.044981 139754068353024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:57:45.050371 139754068353024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:45.050628 139754068353024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:57:45.053353 139754068353024 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:57:45.065886 139754068353024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:57:45.065940 139754068353024 attention.py:418] Single window, no scan.
I0123 11:57:45.065974 139754068353024 transformer_layer.py:389] tlayer: self-attention.
I0123 11:57:45.066004 139754068353024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:45.066064 139754068353024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:45.066621 139754068353024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:45.066699 139754068353024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:45.067064 139754068353024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:45.067762 139754068353024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:45.070314 139754068353024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:45.070925 139754068353024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:45.071000 139754068353024 transformer_layer.py:468] tlayer: End windows.
I0123 11:57:45.071033 139754068353024 transformer_layer.py:472] tlayer: final FFN.
I0123 11:57:45.071088 139754068353024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:45.071219 139754068353024 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:57:45.071326 139754068353024 nn_components.py:325] mlp: activation = None
I0123 11:57:45.071363 139754068353024 nn_components.py:329] mlp: final activation = None
I0123 11:57:45.073230 139754068353024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:45.073322 139754068353024 nn_components.py:261] mlp: residual
I0123 11:57:45.075741 139754068353024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:45.075820 139754068353024 transformer_base.py:443] tbase: final FFN
I0123 11:57:45.075926 139754068353024 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:57:45.078599 139754068353024 nn_components.py:329] mlp: final activation = None
I0123 11:57:45.080448 139754068353024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:45.080542 139754068353024 nn_components.py:261] mlp: residual
I0123 11:57:45.080828 139754068353024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:45.080913 139754068353024 decoder_stack.py:344] dstack: Final layernorm.
I0123 11:57:45.083746 139754068353024 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:57:49.497823 139754068353024 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
I0123 11:57:50.002001 139754068353024 training_loop.py:409] No working directory specified.
I0123 11:57:50.002130 139754068353024 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0123 11:57:50.002943 139754068353024 checkpoints.py:1062] Restoring legacy Flax checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0123 11:57:53.129754 139754068353024 training_loop.py:447] Only restoring trainable parameters.
I0123 11:57:53.130522 139754068353024 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0123 11:57:53.130582 139754068353024 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0123 11:57:53.130630 139754068353024 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:57:53.130676 139754068353024 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:57:53.130717 139754068353024 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:57:53.130757 139754068353024 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:57:53.130796 139754068353024 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:57:53.130835 139754068353024 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:57:53.130876 139754068353024 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:57:53.130915 139754068353024 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:57:53.130952 139754068353024 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:57:53.130991 139754068353024 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:57:53.131029 139754068353024 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:57:53.131066 139754068353024 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:57:53.131103 139754068353024 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:57:53.131140 139754068353024 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:57:53.131177 139754068353024 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:57:53.131214 139754068353024 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:57:53.131250 139754068353024 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:57:53.131287 139754068353024 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:57:53.131337 139754068353024 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:57:53.131376 139754068353024 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:57:53.131415 139754068353024 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:57:53.131453 139754068353024 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:57:53.131490 139754068353024 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:57:53.131527 139754068353024 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:57:53.131563 139754068353024 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:57:53.131600 139754068353024 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:57:53.131637 139754068353024 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:57:53.131674 139754068353024 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:57:53.131709 139754068353024 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:57:53.131746 139754068353024 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:57:53.131782 139754068353024 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:57:53.131819 139754068353024 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:57:53.131855 139754068353024 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:57:53.131892 139754068353024 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:57:53.131928 139754068353024 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:57:53.131964 139754068353024 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:57:53.132000 139754068353024 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:57:53.132037 139754068353024 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:57:53.132072 139754068353024 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:57:53.132109 139754068353024 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:57:53.132146 139754068353024 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:57:53.132183 139754068353024 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:57:53.132219 139754068353024 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:57:53.132255 139754068353024 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:57:53.132297 139754068353024 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:57:53.132336 139754068353024 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:57:53.132373 139754068353024 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:57:53.132410 139754068353024 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:57:53.132446 139754068353024 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:57:53.132482 139754068353024 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:57:53.132518 139754068353024 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:57:53.132554 139754068353024 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:57:53.132589 139754068353024 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:57:53.132625 139754068353024 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:57:53.132663 139754068353024 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:57:53.132700 139754068353024 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:57:53.132737 139754068353024 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:57:53.132773 139754068353024 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:57:53.132809 139754068353024 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:57:53.132845 139754068353024 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:57:53.132880 139754068353024 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:57:53.132916 139754068353024 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:57:53.132951 139754068353024 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:57:53.132987 139754068353024 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:57:53.133023 139754068353024 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:57:53.133059 139754068353024 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:57:53.133094 139754068353024 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:57:53.133129 139754068353024 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:57:53.133165 139754068353024 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:57:53.133202 139754068353024 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:57:53.133239 139754068353024 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:57:53.133280 139754068353024 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:57:53.133317 139754068353024 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:57:53.133353 139754068353024 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:57:53.133390 139754068353024 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:57:53.133426 139754068353024 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:57:53.133463 139754068353024 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:57:53.133498 139754068353024 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:57:53.133533 139754068353024 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:57:53.133567 139754068353024 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:57:53.133603 139754068353024 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:57:53.133638 139754068353024 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:57:53.133682 139754068353024 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:57:53.133718 139754068353024 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:57:53.133754 139754068353024 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:57:53.133790 139754068353024 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:57:53.133827 139754068353024 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:57:53.133863 139754068353024 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:57:53.133898 139754068353024 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:57:53.133935 139754068353024 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:57:53.133972 139754068353024 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:57:53.134009 139754068353024 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:57:53.134045 139754068353024 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:57:53.134081 139754068353024 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:57:53.134117 139754068353024 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:57:53.134153 139754068353024 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:57:53.134188 139754068353024 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:57:53.134223 139754068353024 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:57:53.134263 139754068353024 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:57:53.134301 139754068353024 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:57:53.134337 139754068353024 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:57:53.134373 139754068353024 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:57:53.134409 139754068353024 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:57:53.134444 139754068353024 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:57:53.134481 139754068353024 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:57:53.134517 139754068353024 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:57:53.134552 139754068353024 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:57:53.134587 139754068353024 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:57:53.134623 139754068353024 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:57:53.134658 139754068353024 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:57:53.134693 139754068353024 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:57:53.134728 139754068353024 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:57:53.134763 139754068353024 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:57:53.134800 139754068353024 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:57:53.134838 139754068353024 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:57:53.134874 139754068353024 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:57:53.134909 139754068353024 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:57:53.134944 139754068353024 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:57:53.134979 139754068353024 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:57:53.135015 139754068353024 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:57:53.135043 139754068353024 training_loop.py:725] Total parameters: 152072288
I0123 11:57:53.135278 139754068353024 training_loop.py:739] Total state size: 0
I0123 11:57:53.157477 139754068353024 training_loop.py:492] Training loop: creating task for mode beam_search
I0123 11:57:53.157766 139754068353024 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0123 11:57:53.158144 139754068353024 training_loop.py:652] Compiling mode beam_search with jit.
I0123 11:57:53.158486 139754068353024 training_loop.py:89] registering functions: dict_keys([])
I0123 11:57:53.175202 139754068353024 graph.py:499] a b c = triangle a b c; d = midpoint d c b; e = midpoint e c a; f = midpoint f b a; g = midpoint g f e; h = on_line h c g, on_line h d f ? para a g b h
I0123 11:57:53.290981 139754068353024 ddar.py:60] Depth 1/1000 time = 0.09247207641601562
I0123 11:57:53.594159 139754068353024 ddar.py:60] Depth 2/1000 time = 0.30309414863586426
I0123 11:57:54.445360 139754068353024 ddar.py:60] Depth 3/1000 time = 0.8511166572570801
I0123 11:57:54.447645 139754068353024 alphageometry.py:191] 
==========================
 * From theorem premises:
A B C D E F G H : Points
B,C,D are collinear [00]
DC = DB [01]
E,C,A are collinear [02]
EC = EA [03]
B,F,A are collinear [04]
FB = FA [05]
E,F,G are collinear [06]
GF = GE [07]
C,H,G are collinear [08]
F,H,D are collinear [09]

 * Auxiliary Constructions:
: Points


 * Proof steps:
001. E,F,G are collinear [06] & GF = GE [07]   G is midpoint of EF [10]
002. B,C,D are collinear [00] & DC = DB [01]   D is midpoint of BC [11]
003. B,F,A are collinear [04] & FB = FA [05]   F is midpoint of BA [12]
004. D is midpoint of BC [11] & F is midpoint of BA [12]   DF  CA [13]
005. E,C,A are collinear [02] & DF  AC [13]   FD  EA [14]
006. E,C,A are collinear [02] & EC = EA [03]   E is midpoint of CA [15]
007. D is midpoint of BC [11] & E is midpoint of CA [15]   DE  BA [16]
008. B,F,A are collinear [04] & DE  AB [16]   FA  ED [17]
009. G is midpoint of EF [10] & FD  EA [14] & FA  ED [17]   G,A,D are collinear [18]
010. F,H,D are collinear [09] & E,C,A are collinear [02] & DF  AC [13]   FH  EC [19]
011. FH  EC [19] & E,F,G are collinear [06] & C,H,G are collinear [08]   GF:GE = GH:GC [20]
012. C,H,G are collinear [08] & GF:GE = GH:GC [20] & GF = GE [07]   G is midpoint of CH [21]
013. D is midpoint of BC [11] & G is midpoint of CH [21]   DG  BH [22]
014. G,A,D are collinear [18] & DG  BH [22]   AG  BH
==========================

