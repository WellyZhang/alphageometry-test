I0123 12:49:56.701500 139698650726400 inference_utils.py:69] Parsing gin configuration.
I0123 12:49:56.701626 139698650726400 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0123 12:49:56.701856 139698650726400 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0123 12:49:56.701892 139698650726400 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0123 12:49:56.701923 139698650726400 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0123 12:49:56.701951 139698650726400 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0123 12:49:56.701978 139698650726400 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0123 12:49:56.702004 139698650726400 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0123 12:49:56.702030 139698650726400 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0123 12:49:56.702056 139698650726400 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=32
I0123 12:49:56.702081 139698650726400 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0123 12:49:56.702106 139698650726400 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0123 12:49:56.702155 139698650726400 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0123 12:49:56.702301 139698650726400 resource_reader.py:55] Path not found: base_htrans.gin
I0123 12:49:56.702546 139698650726400 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0123 12:49:56.702653 139698650726400 resource_reader.py:55] Path not found: trainer_configuration.gin
I0123 12:49:56.709158 139698650726400 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0123 12:49:56.709286 139698650726400 resource_reader.py:55] Path not found: size/medium_150M.gin
I0123 12:49:56.709619 139698650726400 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0123 12:49:56.709738 139698650726400 resource_reader.py:55] Path not found: options/positions_t5.gin
I0123 12:49:56.710029 139698650726400 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0123 12:49:56.710133 139698650726400 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0123 12:49:56.710549 139698650726400 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0123 12:49:56.710650 139698650726400 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0123 12:49:56.714365 139698650726400 training_loop.py:334] ==== Training loop: initializing model ====
I0123 12:49:56.814568 139698650726400 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0123 12:49:56.815735 139698650726400 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0123 12:49:56.826688 139698650726400 training_loop.py:335] Process 0 of 1
I0123 12:49:56.826783 139698650726400 training_loop.py:336] Local device count = 1
I0123 12:49:56.826853 139698650726400 training_loop.py:337] Number of replicas = 1
I0123 12:49:56.826912 139698650726400 training_loop.py:339] Using random number seed 42
I0123 12:49:57.343652 139698650726400 training_loop.py:359] Initializing the model.
I0123 12:49:57.728327 139698650726400 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.728677 139698650726400 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 12:49:57.728786 139698650726400 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:49:57.728868 139698650726400 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:49:57.728945 139698650726400 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:49:57.729029 139698650726400 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:49:57.729101 139698650726400 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:49:57.729171 139698650726400 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:49:57.729238 139698650726400 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:49:57.729307 139698650726400 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:49:57.729375 139698650726400 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:49:57.729443 139698650726400 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:49:57.729510 139698650726400 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:49:57.729581 139698650726400 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:49:57.729623 139698650726400 attention.py:418] Single window, no scan.
I0123 12:49:57.729681 139698650726400 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 12:49:57.729798 139698650726400 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:49:57.729838 139698650726400 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:49:57.729870 139698650726400 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:49:57.731939 139698650726400 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.737385 139698650726400 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:49:57.748207 139698650726400 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.748489 139698650726400 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:49:57.752904 139698650726400 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:49:57.763689 139698650726400 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:49:57.763750 139698650726400 attention.py:418] Single window, no scan.
I0123 12:49:57.763789 139698650726400 transformer_layer.py:389] tlayer: self-attention.
I0123 12:49:57.763823 139698650726400 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.763887 139698650726400 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.765094 139698650726400 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.765173 139698650726400 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.765911 139698650726400 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.768417 139698650726400 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.774688 139698650726400 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.776016 139698650726400 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.776100 139698650726400 transformer_layer.py:468] tlayer: End windows.
I0123 12:49:57.776137 139698650726400 transformer_layer.py:472] tlayer: final FFN.
I0123 12:49:57.776200 139698650726400 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.776330 139698650726400 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:49:57.776671 139698650726400 nn_components.py:325] mlp: activation = None
I0123 12:49:57.776718 139698650726400 nn_components.py:329] mlp: final activation = None
I0123 12:49:57.778653 139698650726400 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.778756 139698650726400 nn_components.py:261] mlp: residual
I0123 12:49:57.781661 139698650726400 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.781747 139698650726400 transformer_base.py:443] tbase: final FFN
I0123 12:49:57.782242 139698650726400 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:49:57.792562 139698650726400 nn_components.py:329] mlp: final activation = None
I0123 12:49:57.801507 139698650726400 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.801608 139698650726400 nn_components.py:261] mlp: residual
I0123 12:49:57.801915 139698650726400 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.801999 139698650726400 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 12:49:57.802110 139698650726400 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:49:57.802150 139698650726400 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:49:57.802183 139698650726400 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:49:57.804054 139698650726400 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.806583 139698650726400 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:49:57.812211 139698650726400 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.812472 139698650726400 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:49:57.815138 139698650726400 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:49:57.819152 139698650726400 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:49:57.819209 139698650726400 attention.py:418] Single window, no scan.
I0123 12:49:57.819247 139698650726400 transformer_layer.py:389] tlayer: self-attention.
I0123 12:49:57.819279 139698650726400 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.819345 139698650726400 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.820098 139698650726400 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.820174 139698650726400 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.820539 139698650726400 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.821315 139698650726400 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.823834 139698650726400 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.824460 139698650726400 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.824537 139698650726400 transformer_layer.py:468] tlayer: End windows.
I0123 12:49:57.824572 139698650726400 transformer_layer.py:472] tlayer: final FFN.
I0123 12:49:57.824631 139698650726400 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.824760 139698650726400 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:49:57.825085 139698650726400 nn_components.py:325] mlp: activation = None
I0123 12:49:57.825128 139698650726400 nn_components.py:329] mlp: final activation = None
I0123 12:49:57.827079 139698650726400 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.827174 139698650726400 nn_components.py:261] mlp: residual
I0123 12:49:57.829722 139698650726400 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.829802 139698650726400 transformer_base.py:443] tbase: final FFN
I0123 12:49:57.830233 139698650726400 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:49:57.832559 139698650726400 nn_components.py:329] mlp: final activation = None
I0123 12:49:57.834484 139698650726400 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.834580 139698650726400 nn_components.py:261] mlp: residual
I0123 12:49:57.834878 139698650726400 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.834960 139698650726400 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 12:49:57.835072 139698650726400 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:49:57.835113 139698650726400 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:49:57.835145 139698650726400 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:49:57.837398 139698650726400 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.839809 139698650726400 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:49:57.845417 139698650726400 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.845692 139698650726400 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:49:57.848376 139698650726400 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:49:57.852240 139698650726400 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:49:57.852296 139698650726400 attention.py:418] Single window, no scan.
I0123 12:49:57.852333 139698650726400 transformer_layer.py:389] tlayer: self-attention.
I0123 12:49:57.852365 139698650726400 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.852427 139698650726400 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.852990 139698650726400 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.853066 139698650726400 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.853428 139698650726400 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.854204 139698650726400 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.856729 139698650726400 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.857400 139698650726400 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.857478 139698650726400 transformer_layer.py:468] tlayer: End windows.
I0123 12:49:57.857513 139698650726400 transformer_layer.py:472] tlayer: final FFN.
I0123 12:49:57.857574 139698650726400 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.857711 139698650726400 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:49:57.858038 139698650726400 nn_components.py:325] mlp: activation = None
I0123 12:49:57.858083 139698650726400 nn_components.py:329] mlp: final activation = None
I0123 12:49:57.859999 139698650726400 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.860093 139698650726400 nn_components.py:261] mlp: residual
I0123 12:49:57.862644 139698650726400 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.862743 139698650726400 transformer_base.py:443] tbase: final FFN
I0123 12:49:57.863232 139698650726400 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:49:57.865527 139698650726400 nn_components.py:329] mlp: final activation = None
I0123 12:49:57.867472 139698650726400 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.867569 139698650726400 nn_components.py:261] mlp: residual
I0123 12:49:57.867864 139698650726400 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.867945 139698650726400 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 12:49:57.868057 139698650726400 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:49:57.868097 139698650726400 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:49:57.868129 139698650726400 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:49:57.870071 139698650726400 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.872481 139698650726400 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:49:57.878197 139698650726400 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.878464 139698650726400 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:49:57.881111 139698650726400 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:49:57.884923 139698650726400 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:49:57.884982 139698650726400 attention.py:418] Single window, no scan.
I0123 12:49:57.885020 139698650726400 transformer_layer.py:389] tlayer: self-attention.
I0123 12:49:57.885052 139698650726400 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.885116 139698650726400 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.885683 139698650726400 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.885760 139698650726400 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.886123 139698650726400 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.886908 139698650726400 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.889478 139698650726400 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.890112 139698650726400 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.890190 139698650726400 transformer_layer.py:468] tlayer: End windows.
I0123 12:49:57.890225 139698650726400 transformer_layer.py:472] tlayer: final FFN.
I0123 12:49:57.890286 139698650726400 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.890413 139698650726400 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:49:57.890740 139698650726400 nn_components.py:325] mlp: activation = None
I0123 12:49:57.890784 139698650726400 nn_components.py:329] mlp: final activation = None
I0123 12:49:57.892704 139698650726400 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.892798 139698650726400 nn_components.py:261] mlp: residual
I0123 12:49:57.895389 139698650726400 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.895474 139698650726400 transformer_base.py:443] tbase: final FFN
I0123 12:49:57.895911 139698650726400 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:49:57.898174 139698650726400 nn_components.py:329] mlp: final activation = None
I0123 12:49:57.900095 139698650726400 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.900192 139698650726400 nn_components.py:261] mlp: residual
I0123 12:49:57.900491 139698650726400 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.900572 139698650726400 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 12:49:57.900681 139698650726400 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:49:57.900720 139698650726400 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:49:57.900752 139698650726400 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:49:57.902696 139698650726400 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.905112 139698650726400 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:49:57.910770 139698650726400 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.911030 139698650726400 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:49:57.914108 139698650726400 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:49:57.917896 139698650726400 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:49:57.917953 139698650726400 attention.py:418] Single window, no scan.
I0123 12:49:57.917996 139698650726400 transformer_layer.py:389] tlayer: self-attention.
I0123 12:49:57.918030 139698650726400 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.918093 139698650726400 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.918666 139698650726400 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.918743 139698650726400 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.919107 139698650726400 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.919884 139698650726400 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.922464 139698650726400 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.923088 139698650726400 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.923166 139698650726400 transformer_layer.py:468] tlayer: End windows.
I0123 12:49:57.923202 139698650726400 transformer_layer.py:472] tlayer: final FFN.
I0123 12:49:57.923263 139698650726400 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.923402 139698650726400 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:49:57.923732 139698650726400 nn_components.py:325] mlp: activation = None
I0123 12:49:57.923776 139698650726400 nn_components.py:329] mlp: final activation = None
I0123 12:49:57.925703 139698650726400 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.925797 139698650726400 nn_components.py:261] mlp: residual
I0123 12:49:57.928380 139698650726400 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.928459 139698650726400 transformer_base.py:443] tbase: final FFN
I0123 12:49:57.928892 139698650726400 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:49:57.931162 139698650726400 nn_components.py:329] mlp: final activation = None
I0123 12:49:57.933109 139698650726400 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.933208 139698650726400 nn_components.py:261] mlp: residual
I0123 12:49:57.933501 139698650726400 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.933583 139698650726400 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 12:49:57.933701 139698650726400 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:49:57.933742 139698650726400 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:49:57.933775 139698650726400 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:49:57.935646 139698650726400 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.938076 139698650726400 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:49:57.943751 139698650726400 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.944016 139698650726400 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:49:57.946727 139698650726400 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:49:57.950453 139698650726400 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:49:57.950509 139698650726400 attention.py:418] Single window, no scan.
I0123 12:49:57.950546 139698650726400 transformer_layer.py:389] tlayer: self-attention.
I0123 12:49:57.950577 139698650726400 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.950639 139698650726400 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.951251 139698650726400 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.951330 139698650726400 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.951700 139698650726400 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.952485 139698650726400 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.954995 139698650726400 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.955618 139698650726400 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.955696 139698650726400 transformer_layer.py:468] tlayer: End windows.
I0123 12:49:57.955732 139698650726400 transformer_layer.py:472] tlayer: final FFN.
I0123 12:49:57.955793 139698650726400 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.955927 139698650726400 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:49:57.956247 139698650726400 nn_components.py:325] mlp: activation = None
I0123 12:49:57.956290 139698650726400 nn_components.py:329] mlp: final activation = None
I0123 12:49:57.958200 139698650726400 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.958296 139698650726400 nn_components.py:261] mlp: residual
I0123 12:49:57.960856 139698650726400 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.960935 139698650726400 transformer_base.py:443] tbase: final FFN
I0123 12:49:57.961362 139698650726400 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:49:57.963678 139698650726400 nn_components.py:329] mlp: final activation = None
I0123 12:49:57.965600 139698650726400 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.965710 139698650726400 nn_components.py:261] mlp: residual
I0123 12:49:57.966008 139698650726400 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.966090 139698650726400 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 12:49:57.966201 139698650726400 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:49:57.966241 139698650726400 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:49:57.966273 139698650726400 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:49:57.968136 139698650726400 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.970608 139698650726400 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:49:57.976244 139698650726400 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.976512 139698650726400 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:49:57.979166 139698650726400 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:49:57.982950 139698650726400 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:49:57.983005 139698650726400 attention.py:418] Single window, no scan.
I0123 12:49:57.983041 139698650726400 transformer_layer.py:389] tlayer: self-attention.
I0123 12:49:57.983073 139698650726400 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.983135 139698650726400 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.983700 139698650726400 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.983777 139698650726400 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.984135 139698650726400 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.984920 139698650726400 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.987428 139698650726400 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.988056 139698650726400 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.988136 139698650726400 transformer_layer.py:468] tlayer: End windows.
I0123 12:49:57.988172 139698650726400 transformer_layer.py:472] tlayer: final FFN.
I0123 12:49:57.988231 139698650726400 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.988357 139698650726400 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:49:57.988675 139698650726400 nn_components.py:325] mlp: activation = None
I0123 12:49:57.988718 139698650726400 nn_components.py:329] mlp: final activation = None
I0123 12:49:57.991011 139698650726400 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.991112 139698650726400 nn_components.py:261] mlp: residual
I0123 12:49:57.993598 139698650726400 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:57.993686 139698650726400 transformer_base.py:443] tbase: final FFN
I0123 12:49:57.994116 139698650726400 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:49:58.135738 139698650726400 nn_components.py:329] mlp: final activation = None
I0123 12:49:58.138057 139698650726400 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.138237 139698650726400 nn_components.py:261] mlp: residual
I0123 12:49:58.138564 139698650726400 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.138661 139698650726400 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 12:49:58.138778 139698650726400 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:49:58.138819 139698650726400 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:49:58.138852 139698650726400 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:49:58.140924 139698650726400 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.143689 139698650726400 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:49:58.149399 139698650726400 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.149685 139698650726400 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:49:58.152378 139698650726400 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:49:58.156352 139698650726400 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:49:58.156412 139698650726400 attention.py:418] Single window, no scan.
I0123 12:49:58.156451 139698650726400 transformer_layer.py:389] tlayer: self-attention.
I0123 12:49:58.156484 139698650726400 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.156547 139698650726400 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.157172 139698650726400 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.157251 139698650726400 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.157621 139698650726400 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.158417 139698650726400 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.161021 139698650726400 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.161673 139698650726400 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.161753 139698650726400 transformer_layer.py:468] tlayer: End windows.
I0123 12:49:58.161789 139698650726400 transformer_layer.py:472] tlayer: final FFN.
I0123 12:49:58.161851 139698650726400 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.161978 139698650726400 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:49:58.162303 139698650726400 nn_components.py:325] mlp: activation = None
I0123 12:49:58.162347 139698650726400 nn_components.py:329] mlp: final activation = None
I0123 12:49:58.164307 139698650726400 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.164402 139698650726400 nn_components.py:261] mlp: residual
I0123 12:49:58.166994 139698650726400 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.167074 139698650726400 transformer_base.py:443] tbase: final FFN
I0123 12:49:58.167571 139698650726400 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:49:58.169908 139698650726400 nn_components.py:329] mlp: final activation = None
I0123 12:49:58.171845 139698650726400 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.171949 139698650726400 nn_components.py:261] mlp: residual
I0123 12:49:58.172248 139698650726400 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.172331 139698650726400 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 12:49:58.172446 139698650726400 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:49:58.172485 139698650726400 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:49:58.172517 139698650726400 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:49:58.174469 139698650726400 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.176876 139698650726400 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:49:58.182592 139698650726400 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.182857 139698650726400 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:49:58.185570 139698650726400 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:49:58.189368 139698650726400 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:49:58.189425 139698650726400 attention.py:418] Single window, no scan.
I0123 12:49:58.189462 139698650726400 transformer_layer.py:389] tlayer: self-attention.
I0123 12:49:58.189493 139698650726400 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.189554 139698650726400 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.190135 139698650726400 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.190214 139698650726400 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.190583 139698650726400 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.191358 139698650726400 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.193946 139698650726400 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.194578 139698650726400 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.194655 139698650726400 transformer_layer.py:468] tlayer: End windows.
I0123 12:49:58.194691 139698650726400 transformer_layer.py:472] tlayer: final FFN.
I0123 12:49:58.194752 139698650726400 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.194883 139698650726400 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:49:58.195210 139698650726400 nn_components.py:325] mlp: activation = None
I0123 12:49:58.195254 139698650726400 nn_components.py:329] mlp: final activation = None
I0123 12:49:58.197179 139698650726400 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.197274 139698650726400 nn_components.py:261] mlp: residual
I0123 12:49:58.199865 139698650726400 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.199947 139698650726400 transformer_base.py:443] tbase: final FFN
I0123 12:49:58.200385 139698650726400 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:49:58.202693 139698650726400 nn_components.py:329] mlp: final activation = None
I0123 12:49:58.204633 139698650726400 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.204730 139698650726400 nn_components.py:261] mlp: residual
I0123 12:49:58.205024 139698650726400 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.205112 139698650726400 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 12:49:58.205227 139698650726400 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:49:58.205268 139698650726400 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:49:58.205300 139698650726400 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:49:58.207236 139698650726400 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.209629 139698650726400 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:49:58.215608 139698650726400 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.215876 139698650726400 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:49:58.218598 139698650726400 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:49:58.222383 139698650726400 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:49:58.222440 139698650726400 attention.py:418] Single window, no scan.
I0123 12:49:58.222478 139698650726400 transformer_layer.py:389] tlayer: self-attention.
I0123 12:49:58.222510 139698650726400 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.222573 139698650726400 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.223130 139698650726400 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.223211 139698650726400 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.223577 139698650726400 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.224398 139698650726400 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.226953 139698650726400 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.227582 139698650726400 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.227662 139698650726400 transformer_layer.py:468] tlayer: End windows.
I0123 12:49:58.227698 139698650726400 transformer_layer.py:472] tlayer: final FFN.
I0123 12:49:58.227758 139698650726400 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.227890 139698650726400 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:49:58.228214 139698650726400 nn_components.py:325] mlp: activation = None
I0123 12:49:58.228258 139698650726400 nn_components.py:329] mlp: final activation = None
I0123 12:49:58.230184 139698650726400 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.230280 139698650726400 nn_components.py:261] mlp: residual
I0123 12:49:58.232857 139698650726400 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.232937 139698650726400 transformer_base.py:443] tbase: final FFN
I0123 12:49:58.233373 139698650726400 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:49:58.235659 139698650726400 nn_components.py:329] mlp: final activation = None
I0123 12:49:58.237636 139698650726400 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.237744 139698650726400 nn_components.py:261] mlp: residual
I0123 12:49:58.238044 139698650726400 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.238132 139698650726400 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 12:49:58.238247 139698650726400 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:49:58.238287 139698650726400 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:49:58.238319 139698650726400 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:49:58.240184 139698650726400 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.242686 139698650726400 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:49:58.252432 139698650726400 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.252741 139698650726400 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:49:58.255584 139698650726400 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:49:58.259513 139698650726400 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:49:58.259569 139698650726400 attention.py:418] Single window, no scan.
I0123 12:49:58.259607 139698650726400 transformer_layer.py:389] tlayer: self-attention.
I0123 12:49:58.259640 139698650726400 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.259744 139698650726400 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.260347 139698650726400 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.260424 139698650726400 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.260799 139698650726400 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.261599 139698650726400 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.264204 139698650726400 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.264840 139698650726400 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.264919 139698650726400 transformer_layer.py:468] tlayer: End windows.
I0123 12:49:58.264956 139698650726400 transformer_layer.py:472] tlayer: final FFN.
I0123 12:49:58.265020 139698650726400 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.265149 139698650726400 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:49:58.265485 139698650726400 nn_components.py:325] mlp: activation = None
I0123 12:49:58.265529 139698650726400 nn_components.py:329] mlp: final activation = None
I0123 12:49:58.267541 139698650726400 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.267641 139698650726400 nn_components.py:261] mlp: residual
I0123 12:49:58.270990 139698650726400 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.271074 139698650726400 transformer_base.py:443] tbase: final FFN
I0123 12:49:58.271519 139698650726400 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:49:58.273876 139698650726400 nn_components.py:329] mlp: final activation = None
I0123 12:49:58.275804 139698650726400 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.275899 139698650726400 nn_components.py:261] mlp: residual
I0123 12:49:58.276191 139698650726400 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.276280 139698650726400 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 12:49:58.276398 139698650726400 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:49:58.276442 139698650726400 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:49:58.276474 139698650726400 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:49:58.278398 139698650726400 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.280897 139698650726400 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:49:58.286582 139698650726400 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.286847 139698650726400 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:49:58.289512 139698650726400 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:49:58.293667 139698650726400 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:49:58.293724 139698650726400 attention.py:418] Single window, no scan.
I0123 12:49:58.293762 139698650726400 transformer_layer.py:389] tlayer: self-attention.
I0123 12:49:58.293794 139698650726400 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.293858 139698650726400 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.294422 139698650726400 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.294497 139698650726400 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.294863 139698650726400 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.295637 139698650726400 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.298164 139698650726400 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.298793 139698650726400 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.298871 139698650726400 transformer_layer.py:468] tlayer: End windows.
I0123 12:49:58.298907 139698650726400 transformer_layer.py:472] tlayer: final FFN.
I0123 12:49:58.298971 139698650726400 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.299100 139698650726400 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:49:58.299424 139698650726400 nn_components.py:325] mlp: activation = None
I0123 12:49:58.299468 139698650726400 nn_components.py:329] mlp: final activation = None
I0123 12:49:58.301437 139698650726400 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.301532 139698650726400 nn_components.py:261] mlp: residual
I0123 12:49:58.304056 139698650726400 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.304138 139698650726400 transformer_base.py:443] tbase: final FFN
I0123 12:49:58.304576 139698650726400 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:49:58.306880 139698650726400 nn_components.py:329] mlp: final activation = None
I0123 12:49:58.308818 139698650726400 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.308914 139698650726400 nn_components.py:261] mlp: residual
I0123 12:49:58.309209 139698650726400 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.309507 139698650726400 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:49:58.309583 139698650726400 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:49:58.309662 139698650726400 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:49:58.309724 139698650726400 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:49:58.309781 139698650726400 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:49:58.309836 139698650726400 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:49:58.309890 139698650726400 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:49:58.309943 139698650726400 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:49:58.309996 139698650726400 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:49:58.310049 139698650726400 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:49:58.310101 139698650726400 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:49:58.310153 139698650726400 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:49:58.310190 139698650726400 decoder_stack.py:344] dstack: Final layernorm.
I0123 12:49:58.313769 139698650726400 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:49:58.361811 139698650726400 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.361897 139698650726400 decoder_stack.py:333] dstack: autoregressive generator.
I0123 12:49:58.361955 139698650726400 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 12:49:58.362063 139698650726400 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:49:58.362102 139698650726400 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:49:58.362133 139698650726400 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:49:58.362200 139698650726400 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.364665 139698650726400 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:49:58.370202 139698650726400 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.370464 139698650726400 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:49:58.373138 139698650726400 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:49:58.389851 139698650726400 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:49:58.389909 139698650726400 attention.py:418] Single window, no scan.
I0123 12:49:58.389946 139698650726400 transformer_layer.py:389] tlayer: self-attention.
I0123 12:49:58.389977 139698650726400 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.390044 139698650726400 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.391201 139698650726400 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.391280 139698650726400 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.391997 139698650726400 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.394035 139698650726400 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.398874 139698650726400 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.400198 139698650726400 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.400285 139698650726400 transformer_layer.py:468] tlayer: End windows.
I0123 12:49:58.400321 139698650726400 transformer_layer.py:472] tlayer: final FFN.
I0123 12:49:58.400382 139698650726400 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.400512 139698650726400 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:49:58.400622 139698650726400 nn_components.py:325] mlp: activation = None
I0123 12:49:58.400661 139698650726400 nn_components.py:329] mlp: final activation = None
I0123 12:49:58.402595 139698650726400 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.402689 139698650726400 nn_components.py:261] mlp: residual
I0123 12:49:58.405139 139698650726400 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.405218 139698650726400 transformer_base.py:443] tbase: final FFN
I0123 12:49:58.405325 139698650726400 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:49:58.407591 139698650726400 nn_components.py:329] mlp: final activation = None
I0123 12:49:58.409537 139698650726400 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.409634 139698650726400 nn_components.py:261] mlp: residual
I0123 12:49:58.409937 139698650726400 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.410019 139698650726400 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 12:49:58.410130 139698650726400 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:49:58.410170 139698650726400 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:49:58.410201 139698650726400 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:49:58.410266 139698650726400 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.412531 139698650726400 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:49:58.418029 139698650726400 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.418290 139698650726400 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:49:58.421028 139698650726400 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:49:58.434167 139698650726400 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:49:58.434225 139698650726400 attention.py:418] Single window, no scan.
I0123 12:49:58.434262 139698650726400 transformer_layer.py:389] tlayer: self-attention.
I0123 12:49:58.434293 139698650726400 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.434355 139698650726400 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.434919 139698650726400 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.434997 139698650726400 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.435362 139698650726400 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.436068 139698650726400 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.438598 139698650726400 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.439215 139698650726400 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.439295 139698650726400 transformer_layer.py:468] tlayer: End windows.
I0123 12:49:58.439336 139698650726400 transformer_layer.py:472] tlayer: final FFN.
I0123 12:49:58.439398 139698650726400 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.439531 139698650726400 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:49:58.439641 139698650726400 nn_components.py:325] mlp: activation = None
I0123 12:49:58.439680 139698650726400 nn_components.py:329] mlp: final activation = None
I0123 12:49:58.441606 139698650726400 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.441709 139698650726400 nn_components.py:261] mlp: residual
I0123 12:49:58.444135 139698650726400 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.444215 139698650726400 transformer_base.py:443] tbase: final FFN
I0123 12:49:58.444323 139698650726400 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:49:58.446556 139698650726400 nn_components.py:329] mlp: final activation = None
I0123 12:49:58.448505 139698650726400 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.448602 139698650726400 nn_components.py:261] mlp: residual
I0123 12:49:58.448894 139698650726400 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.448977 139698650726400 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 12:49:58.449088 139698650726400 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:49:58.449128 139698650726400 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:49:58.449161 139698650726400 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:49:58.449227 139698650726400 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.451521 139698650726400 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:49:58.457004 139698650726400 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.457264 139698650726400 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:49:58.459993 139698650726400 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:49:58.472830 139698650726400 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:49:58.472886 139698650726400 attention.py:418] Single window, no scan.
I0123 12:49:58.472923 139698650726400 transformer_layer.py:389] tlayer: self-attention.
I0123 12:49:58.472954 139698650726400 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.473016 139698650726400 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.473579 139698650726400 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.473667 139698650726400 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.474035 139698650726400 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.474735 139698650726400 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.477242 139698650726400 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.477876 139698650726400 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.477954 139698650726400 transformer_layer.py:468] tlayer: End windows.
I0123 12:49:58.477989 139698650726400 transformer_layer.py:472] tlayer: final FFN.
I0123 12:49:58.478055 139698650726400 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.478182 139698650726400 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:49:58.478290 139698650726400 nn_components.py:325] mlp: activation = None
I0123 12:49:58.478328 139698650726400 nn_components.py:329] mlp: final activation = None
I0123 12:49:58.480270 139698650726400 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.480365 139698650726400 nn_components.py:261] mlp: residual
I0123 12:49:58.482836 139698650726400 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.482919 139698650726400 transformer_base.py:443] tbase: final FFN
I0123 12:49:58.483028 139698650726400 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:49:58.485270 139698650726400 nn_components.py:329] mlp: final activation = None
I0123 12:49:58.487207 139698650726400 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.487304 139698650726400 nn_components.py:261] mlp: residual
I0123 12:49:58.487596 139698650726400 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.487677 139698650726400 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 12:49:58.487787 139698650726400 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:49:58.487826 139698650726400 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:49:58.487858 139698650726400 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:49:58.487923 139698650726400 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.490192 139698650726400 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:49:58.495683 139698650726400 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.495947 139698650726400 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:49:58.498676 139698650726400 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:49:58.511495 139698650726400 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:49:58.511551 139698650726400 attention.py:418] Single window, no scan.
I0123 12:49:58.511588 139698650726400 transformer_layer.py:389] tlayer: self-attention.
I0123 12:49:58.511620 139698650726400 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.511682 139698650726400 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.512243 139698650726400 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.512320 139698650726400 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.512682 139698650726400 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.513385 139698650726400 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.515905 139698650726400 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.516544 139698650726400 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.516622 139698650726400 transformer_layer.py:468] tlayer: End windows.
I0123 12:49:58.516657 139698650726400 transformer_layer.py:472] tlayer: final FFN.
I0123 12:49:58.516718 139698650726400 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.516857 139698650726400 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:49:58.516970 139698650726400 nn_components.py:325] mlp: activation = None
I0123 12:49:58.517010 139698650726400 nn_components.py:329] mlp: final activation = None
I0123 12:49:58.519256 139698650726400 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.519353 139698650726400 nn_components.py:261] mlp: residual
I0123 12:49:58.521816 139698650726400 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.521900 139698650726400 transformer_base.py:443] tbase: final FFN
I0123 12:49:58.522010 139698650726400 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:49:58.524257 139698650726400 nn_components.py:329] mlp: final activation = None
I0123 12:49:58.526140 139698650726400 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.526238 139698650726400 nn_components.py:261] mlp: residual
I0123 12:49:58.526529 139698650726400 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.526611 139698650726400 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 12:49:58.526721 139698650726400 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:49:58.526760 139698650726400 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:49:58.526792 139698650726400 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:49:58.526858 139698650726400 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.529197 139698650726400 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:49:58.534687 139698650726400 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.534959 139698650726400 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:49:58.537603 139698650726400 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:49:58.550405 139698650726400 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:49:58.550461 139698650726400 attention.py:418] Single window, no scan.
I0123 12:49:58.550498 139698650726400 transformer_layer.py:389] tlayer: self-attention.
I0123 12:49:58.550529 139698650726400 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.550592 139698650726400 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.551158 139698650726400 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.551235 139698650726400 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.551601 139698650726400 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.552315 139698650726400 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.554883 139698650726400 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.555516 139698650726400 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.555594 139698650726400 transformer_layer.py:468] tlayer: End windows.
I0123 12:49:58.555630 139698650726400 transformer_layer.py:472] tlayer: final FFN.
I0123 12:49:58.555691 139698650726400 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.555827 139698650726400 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:49:58.555936 139698650726400 nn_components.py:325] mlp: activation = None
I0123 12:49:58.555975 139698650726400 nn_components.py:329] mlp: final activation = None
I0123 12:49:58.557886 139698650726400 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.557982 139698650726400 nn_components.py:261] mlp: residual
I0123 12:49:58.560428 139698650726400 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.560507 139698650726400 transformer_base.py:443] tbase: final FFN
I0123 12:49:58.560615 139698650726400 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:49:58.562929 139698650726400 nn_components.py:329] mlp: final activation = None
I0123 12:49:58.564820 139698650726400 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.564916 139698650726400 nn_components.py:261] mlp: residual
I0123 12:49:58.565205 139698650726400 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.565288 139698650726400 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 12:49:58.565398 139698650726400 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:49:58.565437 139698650726400 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:49:58.565469 139698650726400 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:49:58.565534 139698650726400 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.567840 139698650726400 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:49:58.573322 139698650726400 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.573582 139698650726400 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:49:58.576314 139698650726400 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:49:58.589102 139698650726400 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:49:58.589158 139698650726400 attention.py:418] Single window, no scan.
I0123 12:49:58.589199 139698650726400 transformer_layer.py:389] tlayer: self-attention.
I0123 12:49:58.589231 139698650726400 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.589294 139698650726400 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.589860 139698650726400 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.589940 139698650726400 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.590305 139698650726400 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.591010 139698650726400 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.593519 139698650726400 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.594155 139698650726400 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.594236 139698650726400 transformer_layer.py:468] tlayer: End windows.
I0123 12:49:58.594271 139698650726400 transformer_layer.py:472] tlayer: final FFN.
I0123 12:49:58.594331 139698650726400 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.594460 139698650726400 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:49:58.594578 139698650726400 nn_components.py:325] mlp: activation = None
I0123 12:49:58.594618 139698650726400 nn_components.py:329] mlp: final activation = None
I0123 12:49:58.596578 139698650726400 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.596672 139698650726400 nn_components.py:261] mlp: residual
I0123 12:49:58.599142 139698650726400 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.599223 139698650726400 transformer_base.py:443] tbase: final FFN
I0123 12:49:58.599331 139698650726400 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:49:58.601572 139698650726400 nn_components.py:329] mlp: final activation = None
I0123 12:49:58.603464 139698650726400 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.603561 139698650726400 nn_components.py:261] mlp: residual
I0123 12:49:58.603850 139698650726400 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.603932 139698650726400 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 12:49:58.604040 139698650726400 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:49:58.604079 139698650726400 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:49:58.604110 139698650726400 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:49:58.604175 139698650726400 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.606469 139698650726400 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:49:58.612048 139698650726400 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.612314 139698650726400 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:49:58.614981 139698650726400 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:49:58.628135 139698650726400 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:49:58.628194 139698650726400 attention.py:418] Single window, no scan.
I0123 12:49:58.628231 139698650726400 transformer_layer.py:389] tlayer: self-attention.
I0123 12:49:58.628263 139698650726400 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.628325 139698650726400 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.628889 139698650726400 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.628964 139698650726400 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.629327 139698650726400 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.630034 139698650726400 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.632555 139698650726400 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.633220 139698650726400 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.633298 139698650726400 transformer_layer.py:468] tlayer: End windows.
I0123 12:49:58.633333 139698650726400 transformer_layer.py:472] tlayer: final FFN.
I0123 12:49:58.633394 139698650726400 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.633526 139698650726400 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:49:58.633635 139698650726400 nn_components.py:325] mlp: activation = None
I0123 12:49:58.633687 139698650726400 nn_components.py:329] mlp: final activation = None
I0123 12:49:58.635593 139698650726400 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.635689 139698650726400 nn_components.py:261] mlp: residual
I0123 12:49:58.638149 139698650726400 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.638230 139698650726400 transformer_base.py:443] tbase: final FFN
I0123 12:49:58.638338 139698650726400 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:49:58.640574 139698650726400 nn_components.py:329] mlp: final activation = None
I0123 12:49:58.642524 139698650726400 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.642621 139698650726400 nn_components.py:261] mlp: residual
I0123 12:49:58.642915 139698650726400 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.642997 139698650726400 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 12:49:58.643107 139698650726400 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:49:58.643147 139698650726400 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:49:58.643178 139698650726400 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:49:58.643243 139698650726400 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.645519 139698650726400 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:49:58.651044 139698650726400 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.651319 139698650726400 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:49:58.654052 139698650726400 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:49:58.666854 139698650726400 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:49:58.666912 139698650726400 attention.py:418] Single window, no scan.
I0123 12:49:58.666949 139698650726400 transformer_layer.py:389] tlayer: self-attention.
I0123 12:49:58.666982 139698650726400 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.667045 139698650726400 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.667652 139698650726400 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.667729 139698650726400 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.668094 139698650726400 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.668809 139698650726400 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.671352 139698650726400 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.671982 139698650726400 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.672060 139698650726400 transformer_layer.py:468] tlayer: End windows.
I0123 12:49:58.672095 139698650726400 transformer_layer.py:472] tlayer: final FFN.
I0123 12:49:58.672155 139698650726400 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.672291 139698650726400 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:49:58.672404 139698650726400 nn_components.py:325] mlp: activation = None
I0123 12:49:58.672450 139698650726400 nn_components.py:329] mlp: final activation = None
I0123 12:49:58.674360 139698650726400 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.674458 139698650726400 nn_components.py:261] mlp: residual
I0123 12:49:58.676955 139698650726400 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.677034 139698650726400 transformer_base.py:443] tbase: final FFN
I0123 12:49:58.677143 139698650726400 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:49:58.679384 139698650726400 nn_components.py:329] mlp: final activation = None
I0123 12:49:58.681262 139698650726400 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.681357 139698650726400 nn_components.py:261] mlp: residual
I0123 12:49:58.681656 139698650726400 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.681739 139698650726400 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 12:49:58.681848 139698650726400 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:49:58.681888 139698650726400 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:49:58.681920 139698650726400 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:49:58.681983 139698650726400 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.684269 139698650726400 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:49:58.689826 139698650726400 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.690086 139698650726400 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:49:58.692735 139698650726400 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:49:58.705546 139698650726400 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:49:58.705602 139698650726400 attention.py:418] Single window, no scan.
I0123 12:49:58.705647 139698650726400 transformer_layer.py:389] tlayer: self-attention.
I0123 12:49:58.705683 139698650726400 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.705746 139698650726400 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.706315 139698650726400 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.706394 139698650726400 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.706759 139698650726400 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.707465 139698650726400 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.710010 139698650726400 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.710686 139698650726400 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.710764 139698650726400 transformer_layer.py:468] tlayer: End windows.
I0123 12:49:58.710800 139698650726400 transformer_layer.py:472] tlayer: final FFN.
I0123 12:49:58.710859 139698650726400 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.710990 139698650726400 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:49:58.711100 139698650726400 nn_components.py:325] mlp: activation = None
I0123 12:49:58.711139 139698650726400 nn_components.py:329] mlp: final activation = None
I0123 12:49:58.713040 139698650726400 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.713135 139698650726400 nn_components.py:261] mlp: residual
I0123 12:49:58.715605 139698650726400 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.715685 139698650726400 transformer_base.py:443] tbase: final FFN
I0123 12:49:58.715794 139698650726400 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:49:58.718059 139698650726400 nn_components.py:329] mlp: final activation = None
I0123 12:49:58.720010 139698650726400 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.720105 139698650726400 nn_components.py:261] mlp: residual
I0123 12:49:58.720399 139698650726400 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.720482 139698650726400 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 12:49:58.720593 139698650726400 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:49:58.720633 139698650726400 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:49:58.720666 139698650726400 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:49:58.720731 139698650726400 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.723025 139698650726400 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:49:58.728541 139698650726400 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.728803 139698650726400 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:49:58.731863 139698650726400 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:49:58.744620 139698650726400 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:49:58.744676 139698650726400 attention.py:418] Single window, no scan.
I0123 12:49:58.744713 139698650726400 transformer_layer.py:389] tlayer: self-attention.
I0123 12:49:58.744745 139698650726400 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.744806 139698650726400 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.745411 139698650726400 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.745489 139698650726400 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.745869 139698650726400 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.746561 139698650726400 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.749055 139698650726400 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.749685 139698650726400 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.749766 139698650726400 transformer_layer.py:468] tlayer: End windows.
I0123 12:49:58.749802 139698650726400 transformer_layer.py:472] tlayer: final FFN.
I0123 12:49:58.749861 139698650726400 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.749994 139698650726400 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:49:58.750105 139698650726400 nn_components.py:325] mlp: activation = None
I0123 12:49:58.750144 139698650726400 nn_components.py:329] mlp: final activation = None
I0123 12:49:58.752043 139698650726400 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.752143 139698650726400 nn_components.py:261] mlp: residual
I0123 12:49:58.754651 139698650726400 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.754731 139698650726400 transformer_base.py:443] tbase: final FFN
I0123 12:49:58.754840 139698650726400 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:49:58.757083 139698650726400 nn_components.py:329] mlp: final activation = None
I0123 12:49:58.758965 139698650726400 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.759063 139698650726400 nn_components.py:261] mlp: residual
I0123 12:49:58.759357 139698650726400 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.759439 139698650726400 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 12:49:58.759548 139698650726400 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:49:58.759587 139698650726400 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:49:58.759619 139698650726400 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:49:58.759683 139698650726400 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.761960 139698650726400 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:49:58.767560 139698650726400 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.767829 139698650726400 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:49:58.770552 139698650726400 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:49:58.783373 139698650726400 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:49:58.783428 139698650726400 attention.py:418] Single window, no scan.
I0123 12:49:58.783465 139698650726400 transformer_layer.py:389] tlayer: self-attention.
I0123 12:49:58.783496 139698650726400 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.783559 139698650726400 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.784131 139698650726400 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.784208 139698650726400 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.784577 139698650726400 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.785274 139698650726400 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.787836 139698650726400 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.788508 139698650726400 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.788585 139698650726400 transformer_layer.py:468] tlayer: End windows.
I0123 12:49:58.788620 139698650726400 transformer_layer.py:472] tlayer: final FFN.
I0123 12:49:58.788681 139698650726400 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.788813 139698650726400 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:49:58.788924 139698650726400 nn_components.py:325] mlp: activation = None
I0123 12:49:58.788963 139698650726400 nn_components.py:329] mlp: final activation = None
I0123 12:49:58.790877 139698650726400 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.790979 139698650726400 nn_components.py:261] mlp: residual
I0123 12:49:58.793460 139698650726400 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.793540 139698650726400 transformer_base.py:443] tbase: final FFN
I0123 12:49:58.793659 139698650726400 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:49:58.795903 139698650726400 nn_components.py:329] mlp: final activation = None
I0123 12:49:58.797858 139698650726400 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.797955 139698650726400 nn_components.py:261] mlp: residual
I0123 12:49:58.798251 139698650726400 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.798335 139698650726400 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 12:49:58.798447 139698650726400 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:49:58.798486 139698650726400 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:49:58.798518 139698650726400 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:49:58.798583 139698650726400 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.800867 139698650726400 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:49:58.806407 139698650726400 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.806672 139698650726400 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:49:58.809355 139698650726400 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:49:58.822118 139698650726400 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:49:58.822173 139698650726400 attention.py:418] Single window, no scan.
I0123 12:49:58.822211 139698650726400 transformer_layer.py:389] tlayer: self-attention.
I0123 12:49:58.822243 139698650726400 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.822306 139698650726400 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.822872 139698650726400 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.822948 139698650726400 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.823306 139698650726400 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.824004 139698650726400 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.826595 139698650726400 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.827222 139698650726400 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.827300 139698650726400 transformer_layer.py:468] tlayer: End windows.
I0123 12:49:58.827335 139698650726400 transformer_layer.py:472] tlayer: final FFN.
I0123 12:49:58.827395 139698650726400 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.827522 139698650726400 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:49:58.827632 139698650726400 nn_components.py:325] mlp: activation = None
I0123 12:49:58.827671 139698650726400 nn_components.py:329] mlp: final activation = None
I0123 12:49:58.829564 139698650726400 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.829665 139698650726400 nn_components.py:261] mlp: residual
I0123 12:49:58.832111 139698650726400 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.832191 139698650726400 transformer_base.py:443] tbase: final FFN
I0123 12:49:58.832305 139698650726400 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:49:58.834959 139698650726400 nn_components.py:329] mlp: final activation = None
I0123 12:49:58.836852 139698650726400 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.836949 139698650726400 nn_components.py:261] mlp: residual
I0123 12:49:58.837242 139698650726400 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.837329 139698650726400 decoder_stack.py:344] dstack: Final layernorm.
I0123 12:49:58.840250 139698650726400 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:58.895868 139698650726400 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:58.895955 139698650726400 decoder_stack.py:333] dstack: autoregressive generator.
I0123 12:49:58.896011 139698650726400 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 12:49:58.896115 139698650726400 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:49:58.896153 139698650726400 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:49:58.896185 139698650726400 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:49:58.896250 139698650726400 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:58.898629 139698650726400 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:49:58.904021 139698650726400 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:58.904281 139698650726400 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:49:58.906885 139698650726400 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:49:58.919258 139698650726400 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:49:58.919313 139698650726400 attention.py:418] Single window, no scan.
I0123 12:49:58.919349 139698650726400 transformer_layer.py:389] tlayer: self-attention.
I0123 12:49:58.919381 139698650726400 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:58.919444 139698650726400 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:58.920006 139698650726400 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:58.920083 139698650726400 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:58.920444 139698650726400 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:58.921131 139698650726400 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:58.923671 139698650726400 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:58.924295 139698650726400 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:58.924373 139698650726400 transformer_layer.py:468] tlayer: End windows.
I0123 12:49:58.924408 139698650726400 transformer_layer.py:472] tlayer: final FFN.
I0123 12:49:58.924468 139698650726400 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:58.924595 139698650726400 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:49:58.924709 139698650726400 nn_components.py:325] mlp: activation = None
I0123 12:49:58.924750 139698650726400 nn_components.py:329] mlp: final activation = None
I0123 12:49:58.926614 139698650726400 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:58.926709 139698650726400 nn_components.py:261] mlp: residual
I0123 12:49:58.929097 139698650726400 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:58.929176 139698650726400 transformer_base.py:443] tbase: final FFN
I0123 12:49:58.929282 139698650726400 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:49:58.931536 139698650726400 nn_components.py:329] mlp: final activation = None
I0123 12:49:58.933397 139698650726400 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:58.933493 139698650726400 nn_components.py:261] mlp: residual
I0123 12:49:58.933793 139698650726400 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:58.933876 139698650726400 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 12:49:58.933983 139698650726400 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:49:58.934022 139698650726400 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:49:58.934054 139698650726400 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:49:58.934119 139698650726400 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:58.936380 139698650726400 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:49:58.941806 139698650726400 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:58.942066 139698650726400 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:49:58.944739 139698650726400 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:49:58.956999 139698650726400 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:49:58.957053 139698650726400 attention.py:418] Single window, no scan.
I0123 12:49:58.957089 139698650726400 transformer_layer.py:389] tlayer: self-attention.
I0123 12:49:58.957120 139698650726400 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:58.957182 139698650726400 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:58.957749 139698650726400 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:58.957826 139698650726400 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:58.958184 139698650726400 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:58.958859 139698650726400 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:58.961382 139698650726400 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:58.962043 139698650726400 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:58.962122 139698650726400 transformer_layer.py:468] tlayer: End windows.
I0123 12:49:58.962157 139698650726400 transformer_layer.py:472] tlayer: final FFN.
I0123 12:49:58.962217 139698650726400 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:58.962343 139698650726400 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:49:58.962452 139698650726400 nn_components.py:325] mlp: activation = None
I0123 12:49:58.962497 139698650726400 nn_components.py:329] mlp: final activation = None
I0123 12:49:58.964355 139698650726400 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:58.964448 139698650726400 nn_components.py:261] mlp: residual
I0123 12:49:58.966876 139698650726400 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:58.966956 139698650726400 transformer_base.py:443] tbase: final FFN
I0123 12:49:58.967067 139698650726400 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:49:58.969345 139698650726400 nn_components.py:329] mlp: final activation = None
I0123 12:49:58.971212 139698650726400 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:58.971309 139698650726400 nn_components.py:261] mlp: residual
I0123 12:49:58.971597 139698650726400 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:58.971679 139698650726400 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 12:49:58.971787 139698650726400 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:49:58.971827 139698650726400 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:49:58.971859 139698650726400 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:49:58.971924 139698650726400 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:58.974180 139698650726400 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:49:58.979568 139698650726400 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:58.979827 139698650726400 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:49:58.982529 139698650726400 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:49:58.994853 139698650726400 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:49:58.994909 139698650726400 attention.py:418] Single window, no scan.
I0123 12:49:58.994945 139698650726400 transformer_layer.py:389] tlayer: self-attention.
I0123 12:49:58.994977 139698650726400 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:58.995040 139698650726400 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:58.995599 139698650726400 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:58.995676 139698650726400 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:58.996035 139698650726400 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:58.996719 139698650726400 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:58.999679 139698650726400 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.000298 139698650726400 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.000375 139698650726400 transformer_layer.py:468] tlayer: End windows.
I0123 12:49:59.000411 139698650726400 transformer_layer.py:472] tlayer: final FFN.
I0123 12:49:59.000471 139698650726400 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.000598 139698650726400 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:49:59.000705 139698650726400 nn_components.py:325] mlp: activation = None
I0123 12:49:59.000744 139698650726400 nn_components.py:329] mlp: final activation = None
I0123 12:49:59.002628 139698650726400 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.002723 139698650726400 nn_components.py:261] mlp: residual
I0123 12:49:59.005124 139698650726400 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.005204 139698650726400 transformer_base.py:443] tbase: final FFN
I0123 12:49:59.005315 139698650726400 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:49:59.007591 139698650726400 nn_components.py:329] mlp: final activation = None
I0123 12:49:59.009446 139698650726400 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.009542 139698650726400 nn_components.py:261] mlp: residual
I0123 12:49:59.009840 139698650726400 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.009923 139698650726400 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 12:49:59.010032 139698650726400 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:49:59.010071 139698650726400 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:49:59.010103 139698650726400 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:49:59.010169 139698650726400 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.012420 139698650726400 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:49:59.017819 139698650726400 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.018082 139698650726400 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:49:59.020766 139698650726400 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:49:59.033188 139698650726400 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:49:59.033244 139698650726400 attention.py:418] Single window, no scan.
I0123 12:49:59.033288 139698650726400 transformer_layer.py:389] tlayer: self-attention.
I0123 12:49:59.033329 139698650726400 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.033393 139698650726400 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.033961 139698650726400 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.034038 139698650726400 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.034402 139698650726400 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.035089 139698650726400 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.037657 139698650726400 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.038280 139698650726400 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.038356 139698650726400 transformer_layer.py:468] tlayer: End windows.
I0123 12:49:59.038390 139698650726400 transformer_layer.py:472] tlayer: final FFN.
I0123 12:49:59.038450 139698650726400 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.038575 139698650726400 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:49:59.038682 139698650726400 nn_components.py:325] mlp: activation = None
I0123 12:49:59.038722 139698650726400 nn_components.py:329] mlp: final activation = None
I0123 12:49:59.040592 139698650726400 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.040684 139698650726400 nn_components.py:261] mlp: residual
I0123 12:49:59.043096 139698650726400 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.043174 139698650726400 transformer_base.py:443] tbase: final FFN
I0123 12:49:59.043279 139698650726400 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:49:59.045572 139698650726400 nn_components.py:329] mlp: final activation = None
I0123 12:49:59.047429 139698650726400 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.047526 139698650726400 nn_components.py:261] mlp: residual
I0123 12:49:59.047816 139698650726400 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.047896 139698650726400 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 12:49:59.048003 139698650726400 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:49:59.048041 139698650726400 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:49:59.048072 139698650726400 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:49:59.048136 139698650726400 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.050384 139698650726400 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:49:59.055830 139698650726400 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.056088 139698650726400 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:49:59.058812 139698650726400 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:49:59.071321 139698650726400 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:49:59.071377 139698650726400 attention.py:418] Single window, no scan.
I0123 12:49:59.071412 139698650726400 transformer_layer.py:389] tlayer: self-attention.
I0123 12:49:59.071443 139698650726400 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.071505 139698650726400 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.072062 139698650726400 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.072137 139698650726400 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.072497 139698650726400 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.073183 139698650726400 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.075752 139698650726400 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.076381 139698650726400 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.076457 139698650726400 transformer_layer.py:468] tlayer: End windows.
I0123 12:49:59.076491 139698650726400 transformer_layer.py:472] tlayer: final FFN.
I0123 12:49:59.076551 139698650726400 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.076677 139698650726400 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:49:59.076784 139698650726400 nn_components.py:325] mlp: activation = None
I0123 12:49:59.076822 139698650726400 nn_components.py:329] mlp: final activation = None
I0123 12:49:59.078713 139698650726400 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.078811 139698650726400 nn_components.py:261] mlp: residual
I0123 12:49:59.081225 139698650726400 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.081302 139698650726400 transformer_base.py:443] tbase: final FFN
I0123 12:49:59.081411 139698650726400 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:49:59.083717 139698650726400 nn_components.py:329] mlp: final activation = None
I0123 12:49:59.085591 139698650726400 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.085694 139698650726400 nn_components.py:261] mlp: residual
I0123 12:49:59.085986 139698650726400 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.086066 139698650726400 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 12:49:59.086175 139698650726400 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:49:59.086212 139698650726400 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:49:59.086243 139698650726400 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:49:59.086307 139698650726400 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.088566 139698650726400 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:49:59.094051 139698650726400 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.094307 139698650726400 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:49:59.097030 139698650726400 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:49:59.109578 139698650726400 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:49:59.109632 139698650726400 attention.py:418] Single window, no scan.
I0123 12:49:59.109678 139698650726400 transformer_layer.py:389] tlayer: self-attention.
I0123 12:49:59.109709 139698650726400 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.109769 139698650726400 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.110327 139698650726400 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.110402 139698650726400 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.110768 139698650726400 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.111467 139698650726400 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.114433 139698650726400 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.115061 139698650726400 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.115138 139698650726400 transformer_layer.py:468] tlayer: End windows.
I0123 12:49:59.115172 139698650726400 transformer_layer.py:472] tlayer: final FFN.
I0123 12:49:59.115232 139698650726400 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.115357 139698650726400 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:49:59.115465 139698650726400 nn_components.py:325] mlp: activation = None
I0123 12:49:59.115503 139698650726400 nn_components.py:329] mlp: final activation = None
I0123 12:49:59.117370 139698650726400 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.117470 139698650726400 nn_components.py:261] mlp: residual
I0123 12:49:59.119883 139698650726400 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.119963 139698650726400 transformer_base.py:443] tbase: final FFN
I0123 12:49:59.120071 139698650726400 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:49:59.122356 139698650726400 nn_components.py:329] mlp: final activation = None
I0123 12:49:59.124232 139698650726400 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.124326 139698650726400 nn_components.py:261] mlp: residual
I0123 12:49:59.124615 139698650726400 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.124695 139698650726400 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 12:49:59.124802 139698650726400 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:49:59.124840 139698650726400 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:49:59.124870 139698650726400 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:49:59.124934 139698650726400 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.127182 139698650726400 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:49:59.132606 139698650726400 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.132864 139698650726400 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:49:59.135583 139698650726400 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:49:59.148148 139698650726400 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:49:59.148204 139698650726400 attention.py:418] Single window, no scan.
I0123 12:49:59.148239 139698650726400 transformer_layer.py:389] tlayer: self-attention.
I0123 12:49:59.148270 139698650726400 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.148330 139698650726400 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.148892 139698650726400 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.148967 139698650726400 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.149325 139698650726400 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.150014 139698650726400 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.152570 139698650726400 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.153201 139698650726400 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.153278 139698650726400 transformer_layer.py:468] tlayer: End windows.
I0123 12:49:59.153312 139698650726400 transformer_layer.py:472] tlayer: final FFN.
I0123 12:49:59.153372 139698650726400 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.153498 139698650726400 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:49:59.153609 139698650726400 nn_components.py:325] mlp: activation = None
I0123 12:49:59.153654 139698650726400 nn_components.py:329] mlp: final activation = None
I0123 12:49:59.155557 139698650726400 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.155649 139698650726400 nn_components.py:261] mlp: residual
I0123 12:49:59.158094 139698650726400 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.158172 139698650726400 transformer_base.py:443] tbase: final FFN
I0123 12:49:59.158282 139698650726400 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:49:59.160578 139698650726400 nn_components.py:329] mlp: final activation = None
I0123 12:49:59.162470 139698650726400 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.162566 139698650726400 nn_components.py:261] mlp: residual
I0123 12:49:59.162858 139698650726400 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.162938 139698650726400 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 12:49:59.163046 139698650726400 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:49:59.163084 139698650726400 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:49:59.163116 139698650726400 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:49:59.163178 139698650726400 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.165444 139698650726400 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:49:59.170930 139698650726400 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.171197 139698650726400 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:49:59.173952 139698650726400 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:49:59.186593 139698650726400 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:49:59.186646 139698650726400 attention.py:418] Single window, no scan.
I0123 12:49:59.186682 139698650726400 transformer_layer.py:389] tlayer: self-attention.
I0123 12:49:59.186712 139698650726400 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.186774 139698650726400 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.187341 139698650726400 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.187418 139698650726400 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.187786 139698650726400 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.188479 139698650726400 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.191061 139698650726400 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.191694 139698650726400 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.191771 139698650726400 transformer_layer.py:468] tlayer: End windows.
I0123 12:49:59.191805 139698650726400 transformer_layer.py:472] tlayer: final FFN.
I0123 12:49:59.191864 139698650726400 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.191991 139698650726400 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:49:59.192102 139698650726400 nn_components.py:325] mlp: activation = None
I0123 12:49:59.192142 139698650726400 nn_components.py:329] mlp: final activation = None
I0123 12:49:59.194031 139698650726400 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.194125 139698650726400 nn_components.py:261] mlp: residual
I0123 12:49:59.196563 139698650726400 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.196654 139698650726400 transformer_base.py:443] tbase: final FFN
I0123 12:49:59.196765 139698650726400 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:49:59.199082 139698650726400 nn_components.py:329] mlp: final activation = None
I0123 12:49:59.200949 139698650726400 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.201043 139698650726400 nn_components.py:261] mlp: residual
I0123 12:49:59.201332 139698650726400 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.201412 139698650726400 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 12:49:59.201518 139698650726400 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:49:59.201555 139698650726400 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:49:59.201585 139698650726400 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:49:59.201659 139698650726400 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.203928 139698650726400 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:49:59.209390 139698650726400 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.209657 139698650726400 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:49:59.212359 139698650726400 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:49:59.225005 139698650726400 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:49:59.225059 139698650726400 attention.py:418] Single window, no scan.
I0123 12:49:59.225095 139698650726400 transformer_layer.py:389] tlayer: self-attention.
I0123 12:49:59.225125 139698650726400 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.225189 139698650726400 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.225760 139698650726400 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.225836 139698650726400 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.226198 139698650726400 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.226894 139698650726400 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.229850 139698650726400 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.230479 139698650726400 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.230556 139698650726400 transformer_layer.py:468] tlayer: End windows.
I0123 12:49:59.230590 139698650726400 transformer_layer.py:472] tlayer: final FFN.
I0123 12:49:59.230648 139698650726400 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.230775 139698650726400 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:49:59.230883 139698650726400 nn_components.py:325] mlp: activation = None
I0123 12:49:59.230921 139698650726400 nn_components.py:329] mlp: final activation = None
I0123 12:49:59.232807 139698650726400 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.232899 139698650726400 nn_components.py:261] mlp: residual
I0123 12:49:59.235359 139698650726400 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.235444 139698650726400 transformer_base.py:443] tbase: final FFN
I0123 12:49:59.235554 139698650726400 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:49:59.237854 139698650726400 nn_components.py:329] mlp: final activation = None
I0123 12:49:59.239726 139698650726400 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.239820 139698650726400 nn_components.py:261] mlp: residual
I0123 12:49:59.240107 139698650726400 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.240186 139698650726400 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 12:49:59.240294 139698650726400 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:49:59.240331 139698650726400 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:49:59.240361 139698650726400 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:49:59.240422 139698650726400 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.242687 139698650726400 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:49:59.248161 139698650726400 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.248423 139698650726400 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:49:59.251137 139698650726400 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:49:59.263718 139698650726400 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:49:59.263772 139698650726400 attention.py:418] Single window, no scan.
I0123 12:49:59.263807 139698650726400 transformer_layer.py:389] tlayer: self-attention.
I0123 12:49:59.263838 139698650726400 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.263898 139698650726400 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.264460 139698650726400 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.264534 139698650726400 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.264890 139698650726400 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.265582 139698650726400 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.268159 139698650726400 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.268791 139698650726400 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.268867 139698650726400 transformer_layer.py:468] tlayer: End windows.
I0123 12:49:59.268901 139698650726400 transformer_layer.py:472] tlayer: final FFN.
I0123 12:49:59.268960 139698650726400 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.269090 139698650726400 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:49:59.269199 139698650726400 nn_components.py:325] mlp: activation = None
I0123 12:49:59.269237 139698650726400 nn_components.py:329] mlp: final activation = None
I0123 12:49:59.271458 139698650726400 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.271553 139698650726400 nn_components.py:261] mlp: residual
I0123 12:49:59.273962 139698650726400 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.274041 139698650726400 transformer_base.py:443] tbase: final FFN
I0123 12:49:59.274156 139698650726400 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:49:59.276406 139698650726400 nn_components.py:329] mlp: final activation = None
I0123 12:49:59.278271 139698650726400 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.278367 139698650726400 nn_components.py:261] mlp: residual
I0123 12:49:59.278658 139698650726400 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.278738 139698650726400 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 12:49:59.278844 139698650726400 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:49:59.278882 139698650726400 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:49:59.278913 139698650726400 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:49:59.278975 139698650726400 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.281215 139698650726400 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:49:59.286651 139698650726400 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.286912 139698650726400 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:49:59.289605 139698650726400 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:49:59.302061 139698650726400 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:49:59.302114 139698650726400 attention.py:418] Single window, no scan.
I0123 12:49:59.302148 139698650726400 transformer_layer.py:389] tlayer: self-attention.
I0123 12:49:59.302178 139698650726400 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.302241 139698650726400 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.302802 139698650726400 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.302877 139698650726400 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.303239 139698650726400 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.303930 139698650726400 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.306499 139698650726400 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.307122 139698650726400 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.307198 139698650726400 transformer_layer.py:468] tlayer: End windows.
I0123 12:49:59.307232 139698650726400 transformer_layer.py:472] tlayer: final FFN.
I0123 12:49:59.307290 139698650726400 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.307416 139698650726400 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:49:59.307523 139698650726400 nn_components.py:325] mlp: activation = None
I0123 12:49:59.307560 139698650726400 nn_components.py:329] mlp: final activation = None
I0123 12:49:59.309466 139698650726400 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.309559 139698650726400 nn_components.py:261] mlp: residual
I0123 12:49:59.311964 139698650726400 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.312044 139698650726400 transformer_base.py:443] tbase: final FFN
I0123 12:49:59.312151 139698650726400 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:49:59.314436 139698650726400 nn_components.py:329] mlp: final activation = None
I0123 12:49:59.316291 139698650726400 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.316385 139698650726400 nn_components.py:261] mlp: residual
I0123 12:49:59.316671 139698650726400 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.316750 139698650726400 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 12:49:59.316858 139698650726400 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:49:59.316895 139698650726400 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:49:59.316926 139698650726400 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:49:59.316988 139698650726400 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.319258 139698650726400 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:49:59.324719 139698650726400 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.324978 139698650726400 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:49:59.327696 139698650726400 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:49:59.340162 139698650726400 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:49:59.340216 139698650726400 attention.py:418] Single window, no scan.
I0123 12:49:59.340251 139698650726400 transformer_layer.py:389] tlayer: self-attention.
I0123 12:49:59.340287 139698650726400 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.340348 139698650726400 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.340900 139698650726400 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.340978 139698650726400 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.341342 139698650726400 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.342047 139698650726400 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.344957 139698650726400 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.345575 139698650726400 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.345658 139698650726400 transformer_layer.py:468] tlayer: End windows.
I0123 12:49:59.345694 139698650726400 transformer_layer.py:472] tlayer: final FFN.
I0123 12:49:59.345753 139698650726400 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.345885 139698650726400 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:49:59.345994 139698650726400 nn_components.py:325] mlp: activation = None
I0123 12:49:59.346032 139698650726400 nn_components.py:329] mlp: final activation = None
I0123 12:49:59.347972 139698650726400 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.348067 139698650726400 nn_components.py:261] mlp: residual
I0123 12:49:59.350507 139698650726400 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.350585 139698650726400 transformer_base.py:443] tbase: final FFN
I0123 12:49:59.350693 139698650726400 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:49:59.352991 139698650726400 nn_components.py:329] mlp: final activation = None
I0123 12:49:59.354896 139698650726400 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.354991 139698650726400 nn_components.py:261] mlp: residual
I0123 12:49:59.355279 139698650726400 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:59.355365 139698650726400 decoder_stack.py:344] dstack: Final layernorm.
I0123 12:49:59.358235 139698650726400 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:03.744649 139698650726400 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
I0123 12:50:04.253189 139698650726400 training_loop.py:409] No working directory specified.
I0123 12:50:04.253343 139698650726400 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0123 12:50:04.254206 139698650726400 checkpoints.py:1062] Restoring legacy Flax checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0123 12:50:07.490672 139698650726400 training_loop.py:447] Only restoring trainable parameters.
I0123 12:50:07.491415 139698650726400 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0123 12:50:07.491497 139698650726400 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0123 12:50:07.491547 139698650726400 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:50:07.491591 139698650726400 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:50:07.491632 139698650726400 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:07.491674 139698650726400 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:50:07.491714 139698650726400 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:07.491753 139698650726400 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:07.491791 139698650726400 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:50:07.491828 139698650726400 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:50:07.491865 139698650726400 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:07.491903 139698650726400 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:50:07.491940 139698650726400 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:50:07.491976 139698650726400 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:50:07.492013 139698650726400 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:07.492049 139698650726400 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:50:07.492085 139698650726400 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:07.492121 139698650726400 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:07.492163 139698650726400 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:50:07.492198 139698650726400 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:50:07.492260 139698650726400 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:07.492298 139698650726400 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:50:07.492335 139698650726400 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:50:07.492371 139698650726400 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:50:07.492406 139698650726400 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:07.492442 139698650726400 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:50:07.492477 139698650726400 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:07.492511 139698650726400 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:07.492546 139698650726400 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:50:07.492581 139698650726400 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:50:07.492615 139698650726400 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:07.492649 139698650726400 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:50:07.492684 139698650726400 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:50:07.492718 139698650726400 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:50:07.492754 139698650726400 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:07.492789 139698650726400 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:50:07.492824 139698650726400 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:07.492858 139698650726400 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:07.492892 139698650726400 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:50:07.492927 139698650726400 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:50:07.492961 139698650726400 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:07.492995 139698650726400 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:50:07.493030 139698650726400 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:50:07.493064 139698650726400 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:50:07.493099 139698650726400 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:07.493135 139698650726400 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:50:07.493177 139698650726400 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:07.493214 139698650726400 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:07.493250 139698650726400 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:50:07.493285 139698650726400 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:50:07.493320 139698650726400 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:07.493354 139698650726400 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:50:07.493389 139698650726400 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:50:07.493422 139698650726400 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:50:07.493457 139698650726400 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:07.493492 139698650726400 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:50:07.493526 139698650726400 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:07.493561 139698650726400 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:07.493596 139698650726400 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:50:07.493630 139698650726400 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:50:07.493680 139698650726400 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:07.493716 139698650726400 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:50:07.493752 139698650726400 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:50:07.493786 139698650726400 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:50:07.493821 139698650726400 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:07.493855 139698650726400 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:50:07.493890 139698650726400 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:07.493924 139698650726400 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:07.493959 139698650726400 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:50:07.493994 139698650726400 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:50:07.494028 139698650726400 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:07.494063 139698650726400 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:50:07.494098 139698650726400 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:50:07.494138 139698650726400 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:50:07.494174 139698650726400 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:07.494209 139698650726400 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:50:07.494245 139698650726400 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:07.494280 139698650726400 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:07.494315 139698650726400 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:50:07.494350 139698650726400 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:50:07.494385 139698650726400 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:07.494420 139698650726400 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:50:07.494456 139698650726400 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:50:07.494491 139698650726400 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:50:07.494526 139698650726400 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:07.494561 139698650726400 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:50:07.494597 139698650726400 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:07.494632 139698650726400 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:07.494666 139698650726400 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:50:07.494701 139698650726400 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:50:07.494736 139698650726400 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:07.494771 139698650726400 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:50:07.494807 139698650726400 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:50:07.494842 139698650726400 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:50:07.494876 139698650726400 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:07.494911 139698650726400 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:50:07.494945 139698650726400 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:07.494980 139698650726400 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:07.495014 139698650726400 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:50:07.495050 139698650726400 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:50:07.495090 139698650726400 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:07.495126 139698650726400 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:50:07.495162 139698650726400 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:50:07.495196 139698650726400 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:50:07.495231 139698650726400 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:07.495265 139698650726400 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:50:07.495301 139698650726400 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:07.495337 139698650726400 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:07.495372 139698650726400 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:50:07.495407 139698650726400 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:50:07.495442 139698650726400 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:07.495476 139698650726400 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:50:07.495512 139698650726400 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:50:07.495546 139698650726400 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:50:07.495580 139698650726400 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:07.495615 139698650726400 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:50:07.495649 139698650726400 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:07.495684 139698650726400 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:07.495719 139698650726400 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:50:07.495754 139698650726400 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:50:07.495788 139698650726400 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:07.495822 139698650726400 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:50:07.495850 139698650726400 training_loop.py:725] Total parameters: 152072288
I0123 12:50:07.496077 139698650726400 training_loop.py:739] Total state size: 0
I0123 12:50:07.521726 139698650726400 training_loop.py:492] Training loop: creating task for mode beam_search
I0123 12:50:07.522012 139698650726400 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0123 12:50:07.522576 139698650726400 training_loop.py:652] Compiling mode beam_search with jit.
I0123 12:50:07.522943 139698650726400 training_loop.py:89] registering functions: dict_keys([])
I0123 12:50:07.544008 139698650726400 graph.py:499] a b c = triangle a b c; d = on_line d b a; e = on_line e c b; f = lc_tangent f d b, on_line f c b; g = lc_tangent g e b, on_line g a b; h = on_line h f d, on_line h e g; i = foot i h c a; j = foot j c b h; k = foot k c a h; l = on_line l j i, on_line l k e ? coll a b l
I0123 12:50:08.013732 139698650726400 ddar.py:60] Depth 1/1000 time = 0.44698381423950195
I0123 12:50:09.076357 139698650726400 ddar.py:60] Depth 2/1000 time = 1.0624926090240479
I0123 12:50:12.282558 139698650726400 ddar.py:60] Depth 3/1000 time = 3.2059879302978516
I0123 12:50:15.537013 139698650726400 ddar.py:60] Depth 4/1000 time = 3.2542243003845215
I0123 12:50:18.862735 139698650726400 ddar.py:60] Depth 5/1000 time = 3.3253986835479736
I0123 12:50:22.376870 139698650726400 ddar.py:60] Depth 6/1000 time = 3.5138015747070312
I0123 12:50:25.781824 139698650726400 ddar.py:60] Depth 7/1000 time = 3.4046785831451416
I0123 12:50:25.797791 139698650726400 alphageometry.py:191] 
==========================
 * From theorem premises:
A B C D E F G H I J K L : Points
A,B,D are collinear [00]
B,C,E are collinear [01]
DF  DB [02]
EG  EB [03]
D,H,F are collinear [04]
G,H,E are collinear [05]
A,I,C are collinear [06]
HI  AC [07]
B,J,H are collinear [08]
CJ  BH [09]
A,K,H are collinear [10]
CK  AH [11]
L,I,J are collinear [12]
L,K,E are collinear [13]

 * Auxiliary Constructions:
: Points


 * Proof steps:
001. A,D,B are collinear [00] & D,H,F are collinear [04] & A,I,C are collinear [06] & HI  AC [07] & DF  DB [02]   ADH = AIH [14]
002. ADH = AIH [14]   A,D,I,H are concyclic [15]
003. A,D,I,H are concyclic [15]   ADI = AHI [16]
004. A,K,H are collinear [10] & A,I,C are collinear [06] & HI  AC [07] & CK  AH [11]   HKC = HIC [17]
005. HKC = HIC [17]   I,K,H,C are concyclic [18]
006. A,K,H are collinear [10] & B,J,H are collinear [08] & CJ  BH [09] & CK  AH [11]   HKC = HJC [19]
007. HKC = HJC [19]   K,J,H,C are concyclic [20]
008. I,K,H,C are concyclic [18] & K,J,H,C are concyclic [20]   K,J,H,I are concyclic [21]
009. I,K,J,H are concyclic [21]   IJK = IHK [22]
010. I,L,J are collinear [12] & ADI = AHI [16] & A,B,D are collinear [00] & IJK = IHK [22] & A,K,H are collinear [10]   (LI-KJ) = (DI-AB) [23]
011. A,D,B are collinear [00] & D,H,F are collinear [04] & B,C,E are collinear [01] & G,H,E are collinear [05] & EG  EB [03] & DF  DB [02]   BDH = BEH [24]
012. BDH = BEH [24]   D,B,H,E are concyclic [25]
013. D,B,H,E are concyclic [25]   DBH = DEH [26]
014. A,K,H are collinear [10] & G,H,E are collinear [05] & B,C,E are collinear [01] & EG  EB [03] & CK  AH [11]   HKC = HEC [27]
015. HKC = HEC [27]   K,H,C,E are concyclic [28]
016. K,H,C,E are concyclic [28] & K,H,C,J are concyclic [20]   K,J,H,E are concyclic [29]
017. K,J,H,E are concyclic [29]   KJH = KEH [30]
018. L,K,E are collinear [13] & DBH = DEH [26] & A,B,D are collinear [00] & G,H,E are collinear [05] & KJH = KEH [30] & B,J,H are collinear [08]   JKL = (AB-DE) [31]
019. (LI-KJ) = (DI-AB) [23] & JKL = (AB-DE) [31]   ILK = IDE [32]
020. I,L,J are collinear [12] & L,K,E are collinear [13] & ILK = IDE [32]   IDE = ILE [33]
021. IDE = ILE [33]   D,I,L,E are concyclic [34]
022. D,I,L,E are concyclic [34]   DIE = DLE [35]
023. A,K,H are collinear [10] & ADI = AHI [16] & A,B,D are collinear [00]   KAB = HID [36]
024. K,H,C,E are concyclic [28] & I,K,H,C are concyclic [18]   I,K,H,E are concyclic [37]
025. I,K,H,E are concyclic [37]   IHK = IEK [38]
026. A,K,H are collinear [10] & L,K,E are collinear [13] & IHK = IEK [38]   AKL = HIE [39]
027. KAB = HID [36] & AKL = HIE [39]   DIE = (AB-LK) [40]
028. DIE = DLE [35] & L,K,E are collinear [13] & DIE = (AB-LK) [40]   (AB-DI) = LDI [41]
029. (AB-DI) = LDI [41]   AB  DL [42]
030. A,D,B are collinear [00] & AB  DL [42]   DL  DA [43]
031. DL  DA [43]   A,D,L are collinear [44]
032. A,D,L are collinear [44] & A,B,D are collinear [00]   A,B,L are collinear
==========================

