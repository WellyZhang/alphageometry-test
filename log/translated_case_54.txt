I0123 12:32:42.380743 139680014180352 inference_utils.py:69] Parsing gin configuration.
I0123 12:32:42.380835 139680014180352 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0123 12:32:42.381034 139680014180352 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0123 12:32:42.381066 139680014180352 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0123 12:32:42.381095 139680014180352 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0123 12:32:42.381122 139680014180352 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0123 12:32:42.381148 139680014180352 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0123 12:32:42.381174 139680014180352 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0123 12:32:42.381200 139680014180352 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0123 12:32:42.381226 139680014180352 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=32
I0123 12:32:42.381251 139680014180352 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0123 12:32:42.381278 139680014180352 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0123 12:32:42.381321 139680014180352 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0123 12:32:42.381450 139680014180352 resource_reader.py:55] Path not found: base_htrans.gin
I0123 12:32:42.381649 139680014180352 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0123 12:32:42.381749 139680014180352 resource_reader.py:55] Path not found: trainer_configuration.gin
I0123 12:32:42.387944 139680014180352 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0123 12:32:42.388057 139680014180352 resource_reader.py:55] Path not found: size/medium_150M.gin
I0123 12:32:42.388368 139680014180352 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0123 12:32:42.388469 139680014180352 resource_reader.py:55] Path not found: options/positions_t5.gin
I0123 12:32:42.388741 139680014180352 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0123 12:32:42.388839 139680014180352 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0123 12:32:42.389242 139680014180352 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0123 12:32:42.389341 139680014180352 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0123 12:32:42.392986 139680014180352 training_loop.py:334] ==== Training loop: initializing model ====
I0123 12:32:42.482261 139680014180352 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0123 12:32:42.482976 139680014180352 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0123 12:32:42.489446 139680014180352 training_loop.py:335] Process 0 of 1
I0123 12:32:42.489500 139680014180352 training_loop.py:336] Local device count = 1
I0123 12:32:42.489541 139680014180352 training_loop.py:337] Number of replicas = 1
I0123 12:32:42.489572 139680014180352 training_loop.py:339] Using random number seed 42
I0123 12:32:42.943511 139680014180352 training_loop.py:359] Initializing the model.
I0123 12:32:43.320887 139680014180352 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.321361 139680014180352 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 12:32:43.321475 139680014180352 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:32:43.321553 139680014180352 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:32:43.321629 139680014180352 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:32:43.321719 139680014180352 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:32:43.321792 139680014180352 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:32:43.321863 139680014180352 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:32:43.321933 139680014180352 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:32:43.322004 139680014180352 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:32:43.322073 139680014180352 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:32:43.322143 139680014180352 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:32:43.322211 139680014180352 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:32:43.322282 139680014180352 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:32:43.322325 139680014180352 attention.py:418] Single window, no scan.
I0123 12:32:43.322375 139680014180352 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 12:32:43.322496 139680014180352 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:43.322536 139680014180352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:43.322567 139680014180352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:43.324654 139680014180352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.330171 139680014180352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:43.341141 139680014180352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.341444 139680014180352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:43.345916 139680014180352 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:32:43.357136 139680014180352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:43.357195 139680014180352 attention.py:418] Single window, no scan.
I0123 12:32:43.357234 139680014180352 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:43.357269 139680014180352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.357333 139680014180352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.358653 139680014180352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.358734 139680014180352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.359452 139680014180352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.361924 139680014180352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.367767 139680014180352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.369508 139680014180352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.369590 139680014180352 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:43.369626 139680014180352 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:43.369702 139680014180352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.369834 139680014180352 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:43.370178 139680014180352 nn_components.py:325] mlp: activation = None
I0123 12:32:43.370226 139680014180352 nn_components.py:329] mlp: final activation = None
I0123 12:32:43.372159 139680014180352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.372264 139680014180352 nn_components.py:261] mlp: residual
I0123 12:32:43.375181 139680014180352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.375263 139680014180352 transformer_base.py:443] tbase: final FFN
I0123 12:32:43.375763 139680014180352 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:43.386045 139680014180352 nn_components.py:329] mlp: final activation = None
I0123 12:32:43.394888 139680014180352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.394989 139680014180352 nn_components.py:261] mlp: residual
I0123 12:32:43.395288 139680014180352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.395371 139680014180352 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 12:32:43.395482 139680014180352 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:43.395523 139680014180352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:43.395554 139680014180352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:43.397390 139680014180352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.399870 139680014180352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:43.405485 139680014180352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.405759 139680014180352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:43.408419 139680014180352 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:32:43.412512 139680014180352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:43.412568 139680014180352 attention.py:418] Single window, no scan.
I0123 12:32:43.412602 139680014180352 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:43.412632 139680014180352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.412699 139680014180352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.413260 139680014180352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.413335 139680014180352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.413703 139680014180352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.414466 139680014180352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.416907 139680014180352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.417529 139680014180352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.417606 139680014180352 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:43.417646 139680014180352 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:43.417706 139680014180352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.417832 139680014180352 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:43.418155 139680014180352 nn_components.py:325] mlp: activation = None
I0123 12:32:43.418197 139680014180352 nn_components.py:329] mlp: final activation = None
I0123 12:32:43.420122 139680014180352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.420219 139680014180352 nn_components.py:261] mlp: residual
I0123 12:32:43.422708 139680014180352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.422791 139680014180352 transformer_base.py:443] tbase: final FFN
I0123 12:32:43.423216 139680014180352 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:43.425514 139680014180352 nn_components.py:329] mlp: final activation = None
I0123 12:32:43.427397 139680014180352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.427496 139680014180352 nn_components.py:261] mlp: residual
I0123 12:32:43.427788 139680014180352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.427868 139680014180352 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 12:32:43.427978 139680014180352 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:43.428017 139680014180352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:43.428047 139680014180352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:43.429956 139680014180352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.432307 139680014180352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:43.441157 139680014180352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.442003 139680014180352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:43.444795 139680014180352 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:32:43.449150 139680014180352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:43.449220 139680014180352 attention.py:418] Single window, no scan.
I0123 12:32:43.449258 139680014180352 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:43.449289 139680014180352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.449351 139680014180352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.450011 139680014180352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.450099 139680014180352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.450477 139680014180352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.451241 139680014180352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.453759 139680014180352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.454435 139680014180352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.454513 139680014180352 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:43.454548 139680014180352 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:43.454607 139680014180352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.454733 139680014180352 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:43.455099 139680014180352 nn_components.py:325] mlp: activation = None
I0123 12:32:43.455144 139680014180352 nn_components.py:329] mlp: final activation = None
I0123 12:32:43.457088 139680014180352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.457181 139680014180352 nn_components.py:261] mlp: residual
I0123 12:32:43.459743 139680014180352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.459830 139680014180352 transformer_base.py:443] tbase: final FFN
I0123 12:32:43.460328 139680014180352 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:43.462661 139680014180352 nn_components.py:329] mlp: final activation = None
I0123 12:32:43.464618 139680014180352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.464713 139680014180352 nn_components.py:261] mlp: residual
I0123 12:32:43.465004 139680014180352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.465085 139680014180352 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 12:32:43.465193 139680014180352 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:43.465233 139680014180352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:43.465263 139680014180352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:43.467257 139680014180352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.469713 139680014180352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:43.475435 139680014180352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.475731 139680014180352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:43.478443 139680014180352 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:32:43.482339 139680014180352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:43.482396 139680014180352 attention.py:418] Single window, no scan.
I0123 12:32:43.482432 139680014180352 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:43.482463 139680014180352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.482525 139680014180352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.483099 139680014180352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.483181 139680014180352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.483546 139680014180352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.484326 139680014180352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.486918 139680014180352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.487551 139680014180352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.487630 139680014180352 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:43.487668 139680014180352 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:43.487733 139680014180352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.487863 139680014180352 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:43.488195 139680014180352 nn_components.py:325] mlp: activation = None
I0123 12:32:43.488237 139680014180352 nn_components.py:329] mlp: final activation = None
I0123 12:32:43.490162 139680014180352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.490256 139680014180352 nn_components.py:261] mlp: residual
I0123 12:32:43.492854 139680014180352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.492939 139680014180352 transformer_base.py:443] tbase: final FFN
I0123 12:32:43.493365 139680014180352 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:43.495677 139680014180352 nn_components.py:329] mlp: final activation = None
I0123 12:32:43.497628 139680014180352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.497739 139680014180352 nn_components.py:261] mlp: residual
I0123 12:32:43.498036 139680014180352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.498118 139680014180352 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 12:32:43.498229 139680014180352 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:43.498268 139680014180352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:43.498299 139680014180352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:43.500233 139680014180352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.502663 139680014180352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:43.508406 139680014180352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.508682 139680014180352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:43.511682 139680014180352 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:32:43.515525 139680014180352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:43.515590 139680014180352 attention.py:418] Single window, no scan.
I0123 12:32:43.515626 139680014180352 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:43.515656 139680014180352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.515720 139680014180352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.516296 139680014180352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.516374 139680014180352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.516732 139680014180352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.517514 139680014180352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.520392 139680014180352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.521021 139680014180352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.521099 139680014180352 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:43.521135 139680014180352 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:43.521199 139680014180352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.521332 139680014180352 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:43.521670 139680014180352 nn_components.py:325] mlp: activation = None
I0123 12:32:43.521715 139680014180352 nn_components.py:329] mlp: final activation = None
I0123 12:32:43.523609 139680014180352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.523704 139680014180352 nn_components.py:261] mlp: residual
I0123 12:32:43.526281 139680014180352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.526362 139680014180352 transformer_base.py:443] tbase: final FFN
I0123 12:32:43.526795 139680014180352 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:43.529060 139680014180352 nn_components.py:329] mlp: final activation = None
I0123 12:32:43.531017 139680014180352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.531114 139680014180352 nn_components.py:261] mlp: residual
I0123 12:32:43.531408 139680014180352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.531488 139680014180352 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 12:32:43.531598 139680014180352 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:43.531638 139680014180352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:43.531669 139680014180352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:43.533534 139680014180352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.535945 139680014180352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:43.541632 139680014180352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.541899 139680014180352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:43.544603 139680014180352 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:32:43.548411 139680014180352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:43.548467 139680014180352 attention.py:418] Single window, no scan.
I0123 12:32:43.548503 139680014180352 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:43.548533 139680014180352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.548595 139680014180352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.549207 139680014180352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.549284 139680014180352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.549651 139680014180352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.550432 139680014180352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.552946 139680014180352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.553585 139680014180352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.553671 139680014180352 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:43.553707 139680014180352 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:43.553766 139680014180352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.553894 139680014180352 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:43.554218 139680014180352 nn_components.py:325] mlp: activation = None
I0123 12:32:43.554261 139680014180352 nn_components.py:329] mlp: final activation = None
I0123 12:32:43.556177 139680014180352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.556270 139680014180352 nn_components.py:261] mlp: residual
I0123 12:32:43.558833 139680014180352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.558917 139680014180352 transformer_base.py:443] tbase: final FFN
I0123 12:32:43.559351 139680014180352 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:43.561689 139680014180352 nn_components.py:329] mlp: final activation = None
I0123 12:32:43.563597 139680014180352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.563695 139680014180352 nn_components.py:261] mlp: residual
I0123 12:32:43.563987 139680014180352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.564068 139680014180352 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 12:32:43.564176 139680014180352 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:43.564214 139680014180352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:43.564246 139680014180352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:43.566112 139680014180352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.568564 139680014180352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:43.574235 139680014180352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.574499 139680014180352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:43.577151 139680014180352 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:32:43.580947 139680014180352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:43.581003 139680014180352 attention.py:418] Single window, no scan.
I0123 12:32:43.581039 139680014180352 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:43.581070 139680014180352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.581133 139680014180352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.581702 139680014180352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.581783 139680014180352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.582144 139680014180352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.582914 139680014180352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.585390 139680014180352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.586013 139680014180352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.586093 139680014180352 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:43.586128 139680014180352 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:43.586185 139680014180352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.586335 139680014180352 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:43.586653 139680014180352 nn_components.py:325] mlp: activation = None
I0123 12:32:43.586696 139680014180352 nn_components.py:329] mlp: final activation = None
I0123 12:32:43.588644 139680014180352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.588740 139680014180352 nn_components.py:261] mlp: residual
I0123 12:32:43.591248 139680014180352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.591331 139680014180352 transformer_base.py:443] tbase: final FFN
I0123 12:32:43.591761 139680014180352 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:43.594411 139680014180352 nn_components.py:329] mlp: final activation = None
I0123 12:32:43.596320 139680014180352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.596424 139680014180352 nn_components.py:261] mlp: residual
I0123 12:32:43.596718 139680014180352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.596799 139680014180352 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 12:32:43.596909 139680014180352 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:43.596948 139680014180352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:43.596979 139680014180352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:43.959142 139680014180352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.963240 139680014180352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:43.969563 139680014180352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.969938 139680014180352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:43.972843 139680014180352 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:32:43.977264 139680014180352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:43.977327 139680014180352 attention.py:418] Single window, no scan.
I0123 12:32:43.977368 139680014180352 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:43.977403 139680014180352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.977475 139680014180352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.978223 139680014180352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.978303 139680014180352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.978702 139680014180352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.979530 139680014180352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.982304 139680014180352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.982965 139680014180352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.983045 139680014180352 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:43.983082 139680014180352 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:43.983148 139680014180352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.983277 139680014180352 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:43.983644 139680014180352 nn_components.py:325] mlp: activation = None
I0123 12:32:43.983690 139680014180352 nn_components.py:329] mlp: final activation = None
I0123 12:32:43.985673 139680014180352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.985772 139680014180352 nn_components.py:261] mlp: residual
I0123 12:32:43.988507 139680014180352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.988589 139680014180352 transformer_base.py:443] tbase: final FFN
I0123 12:32:43.989054 139680014180352 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:43.991550 139680014180352 nn_components.py:329] mlp: final activation = None
I0123 12:32:43.993545 139680014180352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.993661 139680014180352 nn_components.py:261] mlp: residual
I0123 12:32:43.993961 139680014180352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.994046 139680014180352 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 12:32:43.994163 139680014180352 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:43.994213 139680014180352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:43.994247 139680014180352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:43.996306 139680014180352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:43.998755 139680014180352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:44.004595 139680014180352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:44.004874 139680014180352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:44.007672 139680014180352 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:32:44.011630 139680014180352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:44.011688 139680014180352 attention.py:418] Single window, no scan.
I0123 12:32:44.011724 139680014180352 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:44.011757 139680014180352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:44.011821 139680014180352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:44.012395 139680014180352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:44.012472 139680014180352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:44.012841 139680014180352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:44.013625 139680014180352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:44.016260 139680014180352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:44.016887 139680014180352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:44.016965 139680014180352 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:44.017001 139680014180352 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:44.017063 139680014180352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:44.017194 139680014180352 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:44.017524 139680014180352 nn_components.py:325] mlp: activation = None
I0123 12:32:44.017570 139680014180352 nn_components.py:329] mlp: final activation = None
I0123 12:32:44.019698 139680014180352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:44.019798 139680014180352 nn_components.py:261] mlp: residual
I0123 12:32:44.022456 139680014180352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:44.022538 139680014180352 transformer_base.py:443] tbase: final FFN
I0123 12:32:44.022970 139680014180352 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:44.025329 139680014180352 nn_components.py:329] mlp: final activation = None
I0123 12:32:44.027357 139680014180352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:44.027459 139680014180352 nn_components.py:261] mlp: residual
I0123 12:32:44.027759 139680014180352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:44.027848 139680014180352 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 12:32:44.027962 139680014180352 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:44.028003 139680014180352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:44.028035 139680014180352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:44.029919 139680014180352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:44.032419 139680014180352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:44.038105 139680014180352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:44.038416 139680014180352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:44.041556 139680014180352 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:32:44.045466 139680014180352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:44.045523 139680014180352 attention.py:418] Single window, no scan.
I0123 12:32:44.045560 139680014180352 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:44.045592 139680014180352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:44.045663 139680014180352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:44.046285 139680014180352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:44.046366 139680014180352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:44.046739 139680014180352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:44.047529 139680014180352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:44.050049 139680014180352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:44.050689 139680014180352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:44.050767 139680014180352 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:44.050803 139680014180352 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:44.050862 139680014180352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:44.050990 139680014180352 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:44.051324 139680014180352 nn_components.py:325] mlp: activation = None
I0123 12:32:44.051369 139680014180352 nn_components.py:329] mlp: final activation = None
I0123 12:32:44.053347 139680014180352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:44.053443 139680014180352 nn_components.py:261] mlp: residual
I0123 12:32:44.056088 139680014180352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:44.056173 139680014180352 transformer_base.py:443] tbase: final FFN
I0123 12:32:44.056605 139680014180352 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:44.058982 139680014180352 nn_components.py:329] mlp: final activation = None
I0123 12:32:44.060971 139680014180352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:44.061068 139680014180352 nn_components.py:261] mlp: residual
I0123 12:32:44.061366 139680014180352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:44.061454 139680014180352 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 12:32:44.061568 139680014180352 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:44.061610 139680014180352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:44.061648 139680014180352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:44.063558 139680014180352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:44.066054 139680014180352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:44.071900 139680014180352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:44.072168 139680014180352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:44.074841 139680014180352 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:32:44.078686 139680014180352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:44.078743 139680014180352 attention.py:418] Single window, no scan.
I0123 12:32:44.078781 139680014180352 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:44.078813 139680014180352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:44.078876 139680014180352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:44.079442 139680014180352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:44.079519 139680014180352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:44.079878 139680014180352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:44.080661 139680014180352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:44.083205 139680014180352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:44.083844 139680014180352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:44.083923 139680014180352 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:44.083959 139680014180352 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:44.084018 139680014180352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:44.084148 139680014180352 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:44.084471 139680014180352 nn_components.py:325] mlp: activation = None
I0123 12:32:44.084515 139680014180352 nn_components.py:329] mlp: final activation = None
I0123 12:32:44.086512 139680014180352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:44.086607 139680014180352 nn_components.py:261] mlp: residual
I0123 12:32:44.090243 139680014180352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:44.090324 139680014180352 transformer_base.py:443] tbase: final FFN
I0123 12:32:44.090757 139680014180352 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:44.093107 139680014180352 nn_components.py:329] mlp: final activation = None
I0123 12:32:44.095034 139680014180352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:44.095130 139680014180352 nn_components.py:261] mlp: residual
I0123 12:32:44.095424 139680014180352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:44.095507 139680014180352 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 12:32:44.095625 139680014180352 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:44.095666 139680014180352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:44.095698 139680014180352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:44.097620 139680014180352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:44.100044 139680014180352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:44.105703 139680014180352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:44.105974 139680014180352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:44.108624 139680014180352 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:32:44.112474 139680014180352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:44.112531 139680014180352 attention.py:418] Single window, no scan.
I0123 12:32:44.112568 139680014180352 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:44.112600 139680014180352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:44.112663 139680014180352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:44.113234 139680014180352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:44.113311 139680014180352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:44.113678 139680014180352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:44.114464 139680014180352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:44.116951 139680014180352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:44.117953 139680014180352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:44.118034 139680014180352 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:44.118071 139680014180352 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:44.118131 139680014180352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:44.118453 139680014180352 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:44.118786 139680014180352 nn_components.py:325] mlp: activation = None
I0123 12:32:44.118830 139680014180352 nn_components.py:329] mlp: final activation = None
I0123 12:32:44.120919 139680014180352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:44.121013 139680014180352 nn_components.py:261] mlp: residual
I0123 12:32:44.123608 139680014180352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:44.123690 139680014180352 transformer_base.py:443] tbase: final FFN
I0123 12:32:44.124177 139680014180352 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:44.126468 139680014180352 nn_components.py:329] mlp: final activation = None
I0123 12:32:44.128396 139680014180352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:44.128491 139680014180352 nn_components.py:261] mlp: residual
I0123 12:32:44.128782 139680014180352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:44.129131 139680014180352 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:32:44.129203 139680014180352 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:32:44.129271 139680014180352 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:32:44.129331 139680014180352 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:32:44.129388 139680014180352 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:32:44.129442 139680014180352 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:32:44.129497 139680014180352 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:32:44.129550 139680014180352 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:32:44.129604 139680014180352 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:32:44.129665 139680014180352 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:32:44.129721 139680014180352 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:32:44.129774 139680014180352 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:32:44.129815 139680014180352 decoder_stack.py:344] dstack: Final layernorm.
I0123 12:32:44.133451 139680014180352 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:32:44.182914 139680014180352 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.183005 139680014180352 decoder_stack.py:333] dstack: autoregressive generator.
I0123 12:32:44.183062 139680014180352 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 12:32:44.183179 139680014180352 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:44.183220 139680014180352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:44.183252 139680014180352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:44.183319 139680014180352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.185802 139680014180352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:44.191391 139680014180352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.191658 139680014180352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:44.194377 139680014180352 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:44.211822 139680014180352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:44.211884 139680014180352 attention.py:418] Single window, no scan.
I0123 12:32:44.211923 139680014180352 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:44.211956 139680014180352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.212020 139680014180352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.213188 139680014180352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.213274 139680014180352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.214019 139680014180352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.216152 139680014180352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.221062 139680014180352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.222410 139680014180352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.222499 139680014180352 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:44.222536 139680014180352 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:44.222597 139680014180352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.222728 139680014180352 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:44.222838 139680014180352 nn_components.py:325] mlp: activation = None
I0123 12:32:44.222878 139680014180352 nn_components.py:329] mlp: final activation = None
I0123 12:32:44.224829 139680014180352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.224925 139680014180352 nn_components.py:261] mlp: residual
I0123 12:32:44.227428 139680014180352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.227515 139680014180352 transformer_base.py:443] tbase: final FFN
I0123 12:32:44.227627 139680014180352 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:44.229902 139680014180352 nn_components.py:329] mlp: final activation = None
I0123 12:32:44.231907 139680014180352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.232004 139680014180352 nn_components.py:261] mlp: residual
I0123 12:32:44.232297 139680014180352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.232380 139680014180352 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 12:32:44.232491 139680014180352 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:44.232531 139680014180352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:44.232565 139680014180352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:44.232630 139680014180352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.234941 139680014180352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:44.240542 139680014180352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.240805 139680014180352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:44.243573 139680014180352 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:44.257112 139680014180352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:44.257171 139680014180352 attention.py:418] Single window, no scan.
I0123 12:32:44.257209 139680014180352 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:44.257240 139680014180352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.257304 139680014180352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.257879 139680014180352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.257957 139680014180352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.258319 139680014180352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.259026 139680014180352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.261564 139680014180352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.262206 139680014180352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.262285 139680014180352 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:44.262328 139680014180352 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:44.262395 139680014180352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.262526 139680014180352 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:44.262638 139680014180352 nn_components.py:325] mlp: activation = None
I0123 12:32:44.262678 139680014180352 nn_components.py:329] mlp: final activation = None
I0123 12:32:44.264643 139680014180352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.264739 139680014180352 nn_components.py:261] mlp: residual
I0123 12:32:44.267224 139680014180352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.267306 139680014180352 transformer_base.py:443] tbase: final FFN
I0123 12:32:44.267415 139680014180352 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:44.269689 139680014180352 nn_components.py:329] mlp: final activation = None
I0123 12:32:44.271643 139680014180352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.271739 139680014180352 nn_components.py:261] mlp: residual
I0123 12:32:44.272030 139680014180352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.272111 139680014180352 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 12:32:44.272222 139680014180352 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:44.272261 139680014180352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:44.272293 139680014180352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:44.272357 139680014180352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.274655 139680014180352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:44.280207 139680014180352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.280470 139680014180352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:44.283427 139680014180352 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:44.296663 139680014180352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:44.296722 139680014180352 attention.py:418] Single window, no scan.
I0123 12:32:44.296759 139680014180352 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:44.296790 139680014180352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.296853 139680014180352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.297434 139680014180352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.297512 139680014180352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.297889 139680014180352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.298737 139680014180352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.301444 139680014180352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.302088 139680014180352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.302168 139680014180352 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:44.302203 139680014180352 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:44.302270 139680014180352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.302401 139680014180352 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:44.302511 139680014180352 nn_components.py:325] mlp: activation = None
I0123 12:32:44.302551 139680014180352 nn_components.py:329] mlp: final activation = None
I0123 12:32:44.304515 139680014180352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.304610 139680014180352 nn_components.py:261] mlp: residual
I0123 12:32:44.307105 139680014180352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.307186 139680014180352 transformer_base.py:443] tbase: final FFN
I0123 12:32:44.307297 139680014180352 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:44.309673 139680014180352 nn_components.py:329] mlp: final activation = None
I0123 12:32:44.311638 139680014180352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.311735 139680014180352 nn_components.py:261] mlp: residual
I0123 12:32:44.312025 139680014180352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.312108 139680014180352 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 12:32:44.312217 139680014180352 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:44.312257 139680014180352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:44.312290 139680014180352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:44.312355 139680014180352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.314670 139680014180352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:44.320225 139680014180352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.320486 139680014180352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:44.323233 139680014180352 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:44.336416 139680014180352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:44.336475 139680014180352 attention.py:418] Single window, no scan.
I0123 12:32:44.336512 139680014180352 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:44.336544 139680014180352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.336607 139680014180352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.337170 139680014180352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.337247 139680014180352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.337604 139680014180352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.338320 139680014180352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.340839 139680014180352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.341464 139680014180352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.341542 139680014180352 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:44.341577 139680014180352 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:44.341651 139680014180352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.341792 139680014180352 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:44.341904 139680014180352 nn_components.py:325] mlp: activation = None
I0123 12:32:44.341943 139680014180352 nn_components.py:329] mlp: final activation = None
I0123 12:32:44.343938 139680014180352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.344034 139680014180352 nn_components.py:261] mlp: residual
I0123 12:32:44.346489 139680014180352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.346570 139680014180352 transformer_base.py:443] tbase: final FFN
I0123 12:32:44.346679 139680014180352 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:44.348975 139680014180352 nn_components.py:329] mlp: final activation = None
I0123 12:32:44.350864 139680014180352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.350961 139680014180352 nn_components.py:261] mlp: residual
I0123 12:32:44.351250 139680014180352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.351331 139680014180352 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 12:32:44.351442 139680014180352 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:44.351482 139680014180352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:44.351514 139680014180352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:44.351579 139680014180352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.354239 139680014180352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:44.359832 139680014180352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.360105 139680014180352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:44.362798 139680014180352 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:44.375910 139680014180352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:44.375970 139680014180352 attention.py:418] Single window, no scan.
I0123 12:32:44.376008 139680014180352 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:44.376041 139680014180352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.376105 139680014180352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.376674 139680014180352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.376753 139680014180352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.377110 139680014180352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.377825 139680014180352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.380404 139680014180352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.381042 139680014180352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.381121 139680014180352 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:44.381157 139680014180352 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:44.381218 139680014180352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.381356 139680014180352 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:44.381473 139680014180352 nn_components.py:325] mlp: activation = None
I0123 12:32:44.381513 139680014180352 nn_components.py:329] mlp: final activation = None
I0123 12:32:44.383463 139680014180352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.383560 139680014180352 nn_components.py:261] mlp: residual
I0123 12:32:44.386036 139680014180352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.386118 139680014180352 transformer_base.py:443] tbase: final FFN
I0123 12:32:44.386226 139680014180352 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:44.388566 139680014180352 nn_components.py:329] mlp: final activation = None
I0123 12:32:44.390491 139680014180352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.390593 139680014180352 nn_components.py:261] mlp: residual
I0123 12:32:44.390883 139680014180352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.390964 139680014180352 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 12:32:44.391077 139680014180352 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:44.391117 139680014180352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:44.391149 139680014180352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:44.391212 139680014180352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.393499 139680014180352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:44.399057 139680014180352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.399397 139680014180352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:44.402308 139680014180352 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:44.415330 139680014180352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:44.415388 139680014180352 attention.py:418] Single window, no scan.
I0123 12:32:44.415426 139680014180352 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:44.415457 139680014180352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.415520 139680014180352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.416093 139680014180352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.416172 139680014180352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.416528 139680014180352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.417219 139680014180352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.419723 139680014180352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.420341 139680014180352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.420418 139680014180352 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:44.420454 139680014180352 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:44.420514 139680014180352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.420649 139680014180352 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:44.420768 139680014180352 nn_components.py:325] mlp: activation = None
I0123 12:32:44.420808 139680014180352 nn_components.py:329] mlp: final activation = None
I0123 12:32:44.422779 139680014180352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.422875 139680014180352 nn_components.py:261] mlp: residual
I0123 12:32:44.425325 139680014180352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.425405 139680014180352 transformer_base.py:443] tbase: final FFN
I0123 12:32:44.425516 139680014180352 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:44.427769 139680014180352 nn_components.py:329] mlp: final activation = None
I0123 12:32:44.429621 139680014180352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.429725 139680014180352 nn_components.py:261] mlp: residual
I0123 12:32:44.430016 139680014180352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.430099 139680014180352 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 12:32:44.430209 139680014180352 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:44.430249 139680014180352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:44.430280 139680014180352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:44.430345 139680014180352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.432578 139680014180352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:44.438182 139680014180352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.438445 139680014180352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:44.441062 139680014180352 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:44.454092 139680014180352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:44.454149 139680014180352 attention.py:418] Single window, no scan.
I0123 12:32:44.454187 139680014180352 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:44.454220 139680014180352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.454287 139680014180352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.454849 139680014180352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.454925 139680014180352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.455283 139680014180352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.455990 139680014180352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.458527 139680014180352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.459522 139680014180352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.459607 139680014180352 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:44.459643 139680014180352 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:44.459705 139680014180352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.459841 139680014180352 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:44.459953 139680014180352 nn_components.py:325] mlp: activation = None
I0123 12:32:44.459997 139680014180352 nn_components.py:329] mlp: final activation = None
I0123 12:32:44.461930 139680014180352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.462027 139680014180352 nn_components.py:261] mlp: residual
I0123 12:32:44.464471 139680014180352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.464552 139680014180352 transformer_base.py:443] tbase: final FFN
I0123 12:32:44.464661 139680014180352 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:44.466918 139680014180352 nn_components.py:329] mlp: final activation = None
I0123 12:32:44.468846 139680014180352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.468942 139680014180352 nn_components.py:261] mlp: residual
I0123 12:32:44.469234 139680014180352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.469318 139680014180352 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 12:32:44.469428 139680014180352 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:44.469468 139680014180352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:44.469500 139680014180352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:44.469565 139680014180352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.471837 139680014180352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:44.477344 139680014180352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.477618 139680014180352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:44.480335 139680014180352 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:44.493261 139680014180352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:44.493318 139680014180352 attention.py:418] Single window, no scan.
I0123 12:32:44.493355 139680014180352 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:44.493387 139680014180352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.493449 139680014180352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.494070 139680014180352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.494149 139680014180352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.494507 139680014180352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.495199 139680014180352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.497729 139680014180352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.498379 139680014180352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.498459 139680014180352 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:44.498495 139680014180352 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:44.498555 139680014180352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.498685 139680014180352 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:44.498796 139680014180352 nn_components.py:325] mlp: activation = None
I0123 12:32:44.498841 139680014180352 nn_components.py:329] mlp: final activation = None
I0123 12:32:44.500895 139680014180352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.500989 139680014180352 nn_components.py:261] mlp: residual
I0123 12:32:44.503588 139680014180352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.503670 139680014180352 transformer_base.py:443] tbase: final FFN
I0123 12:32:44.503784 139680014180352 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:44.506026 139680014180352 nn_components.py:329] mlp: final activation = None
I0123 12:32:44.507906 139680014180352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.508003 139680014180352 nn_components.py:261] mlp: residual
I0123 12:32:44.508292 139680014180352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.508374 139680014180352 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 12:32:44.508484 139680014180352 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:44.508524 139680014180352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:44.508555 139680014180352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:44.508619 139680014180352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.510884 139680014180352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:44.516433 139680014180352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.516698 139680014180352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:44.519361 139680014180352 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:44.532361 139680014180352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:44.532419 139680014180352 attention.py:418] Single window, no scan.
I0123 12:32:44.532456 139680014180352 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:44.532488 139680014180352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.532551 139680014180352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.533125 139680014180352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.533206 139680014180352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.533564 139680014180352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.534273 139680014180352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.536792 139680014180352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.537466 139680014180352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.537546 139680014180352 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:44.537582 139680014180352 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:44.537649 139680014180352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.537790 139680014180352 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:44.537899 139680014180352 nn_components.py:325] mlp: activation = None
I0123 12:32:44.537939 139680014180352 nn_components.py:329] mlp: final activation = None
I0123 12:32:44.539835 139680014180352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.539929 139680014180352 nn_components.py:261] mlp: residual
I0123 12:32:44.542361 139680014180352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.542443 139680014180352 transformer_base.py:443] tbase: final FFN
I0123 12:32:44.542551 139680014180352 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:44.544796 139680014180352 nn_components.py:329] mlp: final activation = None
I0123 12:32:44.546751 139680014180352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.546850 139680014180352 nn_components.py:261] mlp: residual
I0123 12:32:44.547140 139680014180352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.547223 139680014180352 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 12:32:44.547334 139680014180352 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:44.547374 139680014180352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:44.547406 139680014180352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:44.547471 139680014180352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.549737 139680014180352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:44.555242 139680014180352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.555503 139680014180352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:44.558239 139680014180352 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:44.571622 139680014180352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:44.571679 139680014180352 attention.py:418] Single window, no scan.
I0123 12:32:44.571716 139680014180352 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:44.571748 139680014180352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.571812 139680014180352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.572427 139680014180352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.572504 139680014180352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.572865 139680014180352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.573564 139680014180352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.576089 139680014180352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.576708 139680014180352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.576786 139680014180352 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:44.576822 139680014180352 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:44.576881 139680014180352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.577010 139680014180352 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:44.577120 139680014180352 nn_components.py:325] mlp: activation = None
I0123 12:32:44.577159 139680014180352 nn_components.py:329] mlp: final activation = None
I0123 12:32:44.579055 139680014180352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.579155 139680014180352 nn_components.py:261] mlp: residual
I0123 12:32:44.581647 139680014180352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.581728 139680014180352 transformer_base.py:443] tbase: final FFN
I0123 12:32:44.581836 139680014180352 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:44.584078 139680014180352 nn_components.py:329] mlp: final activation = None
I0123 12:32:44.585956 139680014180352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.586053 139680014180352 nn_components.py:261] mlp: residual
I0123 12:32:44.586344 139680014180352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.586425 139680014180352 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 12:32:44.586536 139680014180352 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:44.586576 139680014180352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:44.586609 139680014180352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:44.586674 139680014180352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.588941 139680014180352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:44.594506 139680014180352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.594771 139680014180352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:44.597428 139680014180352 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:44.610375 139680014180352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:44.610433 139680014180352 attention.py:418] Single window, no scan.
I0123 12:32:44.610470 139680014180352 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:44.610501 139680014180352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.610564 139680014180352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.611128 139680014180352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.611204 139680014180352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.611563 139680014180352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.612267 139680014180352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.614815 139680014180352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.615488 139680014180352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.615567 139680014180352 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:44.615602 139680014180352 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:44.615660 139680014180352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.615789 139680014180352 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:44.615899 139680014180352 nn_components.py:325] mlp: activation = None
I0123 12:32:44.615942 139680014180352 nn_components.py:329] mlp: final activation = None
I0123 12:32:44.617852 139680014180352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.617953 139680014180352 nn_components.py:261] mlp: residual
I0123 12:32:44.620416 139680014180352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.620500 139680014180352 transformer_base.py:443] tbase: final FFN
I0123 12:32:44.620608 139680014180352 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:44.622846 139680014180352 nn_components.py:329] mlp: final activation = None
I0123 12:32:44.624810 139680014180352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.624907 139680014180352 nn_components.py:261] mlp: residual
I0123 12:32:44.625199 139680014180352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.625282 139680014180352 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 12:32:44.625395 139680014180352 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:44.625435 139680014180352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:44.625468 139680014180352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:44.625534 139680014180352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.627844 139680014180352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:44.633347 139680014180352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.633611 139680014180352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:44.636348 139680014180352 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:44.649236 139680014180352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:44.649292 139680014180352 attention.py:418] Single window, no scan.
I0123 12:32:44.649329 139680014180352 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:44.649359 139680014180352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.649422 139680014180352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.649997 139680014180352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.650076 139680014180352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.650432 139680014180352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.651173 139680014180352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.653685 139680014180352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.654320 139680014180352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.654398 139680014180352 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:44.654434 139680014180352 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:44.654494 139680014180352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.654626 139680014180352 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:44.654735 139680014180352 nn_components.py:325] mlp: activation = None
I0123 12:32:44.654775 139680014180352 nn_components.py:329] mlp: final activation = None
I0123 12:32:44.656664 139680014180352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.656758 139680014180352 nn_components.py:261] mlp: residual
I0123 12:32:44.659213 139680014180352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.659294 139680014180352 transformer_base.py:443] tbase: final FFN
I0123 12:32:44.659403 139680014180352 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:44.661713 139680014180352 nn_components.py:329] mlp: final activation = None
I0123 12:32:44.663586 139680014180352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.663683 139680014180352 nn_components.py:261] mlp: residual
I0123 12:32:44.663972 139680014180352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.664058 139680014180352 decoder_stack.py:344] dstack: Final layernorm.
I0123 12:32:44.666951 139680014180352 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:32:44.723022 139680014180352 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.723113 139680014180352 decoder_stack.py:333] dstack: autoregressive generator.
I0123 12:32:44.723168 139680014180352 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 12:32:44.723274 139680014180352 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:44.723313 139680014180352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:44.723344 139680014180352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:44.723409 139680014180352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.726111 139680014180352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:44.731567 139680014180352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.731832 139680014180352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:44.734456 139680014180352 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:44.747057 139680014180352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:44.747114 139680014180352 attention.py:418] Single window, no scan.
I0123 12:32:44.747153 139680014180352 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:44.747184 139680014180352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.747248 139680014180352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.747812 139680014180352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.747889 139680014180352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.748248 139680014180352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.748932 139680014180352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.751449 139680014180352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.752075 139680014180352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.752153 139680014180352 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:44.752188 139680014180352 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:44.752249 139680014180352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.752379 139680014180352 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:44.752497 139680014180352 nn_components.py:325] mlp: activation = None
I0123 12:32:44.752537 139680014180352 nn_components.py:329] mlp: final activation = None
I0123 12:32:44.754409 139680014180352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.754505 139680014180352 nn_components.py:261] mlp: residual
I0123 12:32:44.756903 139680014180352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.756983 139680014180352 transformer_base.py:443] tbase: final FFN
I0123 12:32:44.757093 139680014180352 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:44.759371 139680014180352 nn_components.py:329] mlp: final activation = None
I0123 12:32:44.761224 139680014180352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.761319 139680014180352 nn_components.py:261] mlp: residual
I0123 12:32:44.761607 139680014180352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.761696 139680014180352 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 12:32:44.761805 139680014180352 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:44.761844 139680014180352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:44.761876 139680014180352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:44.761941 139680014180352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.764194 139680014180352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:44.769579 139680014180352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.769852 139680014180352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:44.772545 139680014180352 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:44.785152 139680014180352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:44.785210 139680014180352 attention.py:418] Single window, no scan.
I0123 12:32:44.785248 139680014180352 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:44.785280 139680014180352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.785342 139680014180352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.785915 139680014180352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.785995 139680014180352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.786356 139680014180352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.787042 139680014180352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.789573 139680014180352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.790208 139680014180352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.790288 139680014180352 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:44.790325 139680014180352 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:44.790388 139680014180352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.790519 139680014180352 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:44.790629 139680014180352 nn_components.py:325] mlp: activation = None
I0123 12:32:44.790674 139680014180352 nn_components.py:329] mlp: final activation = None
I0123 12:32:44.792541 139680014180352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.792635 139680014180352 nn_components.py:261] mlp: residual
I0123 12:32:44.795052 139680014180352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.795133 139680014180352 transformer_base.py:443] tbase: final FFN
I0123 12:32:44.795244 139680014180352 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:44.797508 139680014180352 nn_components.py:329] mlp: final activation = None
I0123 12:32:44.799377 139680014180352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.799474 139680014180352 nn_components.py:261] mlp: residual
I0123 12:32:44.799764 139680014180352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.799848 139680014180352 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 12:32:44.799958 139680014180352 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:44.799998 139680014180352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:44.800030 139680014180352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:44.800094 139680014180352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.802361 139680014180352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:44.807740 139680014180352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.808002 139680014180352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:44.810698 139680014180352 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:44.823341 139680014180352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:44.823397 139680014180352 attention.py:418] Single window, no scan.
I0123 12:32:44.823435 139680014180352 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:44.823468 139680014180352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.823531 139680014180352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.824092 139680014180352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.824169 139680014180352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.824527 139680014180352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.825203 139680014180352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.827738 139680014180352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.828356 139680014180352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.828435 139680014180352 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:44.828471 139680014180352 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:44.828533 139680014180352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.828661 139680014180352 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:44.828770 139680014180352 nn_components.py:325] mlp: activation = None
I0123 12:32:44.828809 139680014180352 nn_components.py:329] mlp: final activation = None
I0123 12:32:44.830675 139680014180352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.830770 139680014180352 nn_components.py:261] mlp: residual
I0123 12:32:44.833177 139680014180352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.833257 139680014180352 transformer_base.py:443] tbase: final FFN
I0123 12:32:44.833369 139680014180352 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:44.836078 139680014180352 nn_components.py:329] mlp: final activation = None
I0123 12:32:44.837939 139680014180352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.838037 139680014180352 nn_components.py:261] mlp: residual
I0123 12:32:44.838327 139680014180352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.838409 139680014180352 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 12:32:44.838520 139680014180352 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:44.838560 139680014180352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:44.838593 139680014180352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:44.838657 139680014180352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.840884 139680014180352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:44.846276 139680014180352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.846538 139680014180352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:44.849251 139680014180352 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:44.861859 139680014180352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:44.861915 139680014180352 attention.py:418] Single window, no scan.
I0123 12:32:44.861954 139680014180352 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:44.861994 139680014180352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.862058 139680014180352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.862626 139680014180352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.862701 139680014180352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.863064 139680014180352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.863761 139680014180352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.866312 139680014180352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.866942 139680014180352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.867019 139680014180352 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:44.867053 139680014180352 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:44.867113 139680014180352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.867237 139680014180352 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:44.867345 139680014180352 nn_components.py:325] mlp: activation = None
I0123 12:32:44.867384 139680014180352 nn_components.py:329] mlp: final activation = None
I0123 12:32:44.869262 139680014180352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.869356 139680014180352 nn_components.py:261] mlp: residual
I0123 12:32:44.871778 139680014180352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.871857 139680014180352 transformer_base.py:443] tbase: final FFN
I0123 12:32:44.871966 139680014180352 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:44.874277 139680014180352 nn_components.py:329] mlp: final activation = None
I0123 12:32:44.876137 139680014180352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.876231 139680014180352 nn_components.py:261] mlp: residual
I0123 12:32:44.876518 139680014180352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.876599 139680014180352 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 12:32:44.876705 139680014180352 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:44.876743 139680014180352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:44.876773 139680014180352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:44.876836 139680014180352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.879069 139680014180352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:44.884457 139680014180352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.884718 139680014180352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:44.887458 139680014180352 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:44.900467 139680014180352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:44.900525 139680014180352 attention.py:418] Single window, no scan.
I0123 12:32:44.900562 139680014180352 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:44.900594 139680014180352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.900658 139680014180352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.901225 139680014180352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.901300 139680014180352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.901669 139680014180352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.902366 139680014180352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.904930 139680014180352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.905560 139680014180352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.905637 139680014180352 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:44.905681 139680014180352 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:44.905740 139680014180352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.905867 139680014180352 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:44.905975 139680014180352 nn_components.py:325] mlp: activation = None
I0123 12:32:44.906013 139680014180352 nn_components.py:329] mlp: final activation = None
I0123 12:32:44.907977 139680014180352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.908082 139680014180352 nn_components.py:261] mlp: residual
I0123 12:32:44.910564 139680014180352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.910643 139680014180352 transformer_base.py:443] tbase: final FFN
I0123 12:32:44.910751 139680014180352 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:44.913049 139680014180352 nn_components.py:329] mlp: final activation = None
I0123 12:32:44.914936 139680014180352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.915032 139680014180352 nn_components.py:261] mlp: residual
I0123 12:32:44.915318 139680014180352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.915398 139680014180352 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 12:32:44.915507 139680014180352 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:44.915545 139680014180352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:44.915576 139680014180352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:44.915641 139680014180352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.917900 139680014180352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:44.923562 139680014180352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.923827 139680014180352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:44.926583 139680014180352 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:44.939299 139680014180352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:44.939355 139680014180352 attention.py:418] Single window, no scan.
I0123 12:32:44.939391 139680014180352 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:44.939423 139680014180352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.939484 139680014180352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.940037 139680014180352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.940113 139680014180352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.940472 139680014180352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.941163 139680014180352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.943748 139680014180352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.944376 139680014180352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.944454 139680014180352 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:44.944489 139680014180352 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:44.944547 139680014180352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.944674 139680014180352 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:44.944781 139680014180352 nn_components.py:325] mlp: activation = None
I0123 12:32:44.944819 139680014180352 nn_components.py:329] mlp: final activation = None
I0123 12:32:44.946716 139680014180352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.946815 139680014180352 nn_components.py:261] mlp: residual
I0123 12:32:44.949220 139680014180352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.949299 139680014180352 transformer_base.py:443] tbase: final FFN
I0123 12:32:44.949408 139680014180352 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:44.952117 139680014180352 nn_components.py:329] mlp: final activation = None
I0123 12:32:44.954011 139680014180352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.954107 139680014180352 nn_components.py:261] mlp: residual
I0123 12:32:44.954395 139680014180352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.954475 139680014180352 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 12:32:44.954583 139680014180352 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:44.954620 139680014180352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:44.954651 139680014180352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:44.954713 139680014180352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.956974 139680014180352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:44.962478 139680014180352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.962743 139680014180352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:44.965497 139680014180352 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:44.978599 139680014180352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:44.978654 139680014180352 attention.py:418] Single window, no scan.
I0123 12:32:44.978690 139680014180352 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:44.978722 139680014180352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.978783 139680014180352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.979345 139680014180352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.979421 139680014180352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.979784 139680014180352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.980472 139680014180352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.983022 139680014180352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.983664 139680014180352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.983741 139680014180352 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:44.983776 139680014180352 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:44.983834 139680014180352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.983963 139680014180352 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:44.984071 139680014180352 nn_components.py:325] mlp: activation = None
I0123 12:32:44.984108 139680014180352 nn_components.py:329] mlp: final activation = None
I0123 12:32:44.985995 139680014180352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.986087 139680014180352 nn_components.py:261] mlp: residual
I0123 12:32:44.988502 139680014180352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.988580 139680014180352 transformer_base.py:443] tbase: final FFN
I0123 12:32:44.988689 139680014180352 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:44.990975 139680014180352 nn_components.py:329] mlp: final activation = None
I0123 12:32:44.992831 139680014180352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.992926 139680014180352 nn_components.py:261] mlp: residual
I0123 12:32:44.993212 139680014180352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.993293 139680014180352 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 12:32:44.993400 139680014180352 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:44.993438 139680014180352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:44.993469 139680014180352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:44.993532 139680014180352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:44.995796 139680014180352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:45.001280 139680014180352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.001542 139680014180352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:45.004307 139680014180352 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:45.017110 139680014180352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:45.017165 139680014180352 attention.py:418] Single window, no scan.
I0123 12:32:45.017201 139680014180352 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:45.017233 139680014180352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.017295 139680014180352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.017873 139680014180352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.017950 139680014180352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.018302 139680014180352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.018996 139680014180352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.021547 139680014180352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.022192 139680014180352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.022271 139680014180352 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:45.022306 139680014180352 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:45.022367 139680014180352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.022495 139680014180352 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:45.022606 139680014180352 nn_components.py:325] mlp: activation = None
I0123 12:32:45.022644 139680014180352 nn_components.py:329] mlp: final activation = None
I0123 12:32:45.024540 139680014180352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.024633 139680014180352 nn_components.py:261] mlp: residual
I0123 12:32:45.027076 139680014180352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.027161 139680014180352 transformer_base.py:443] tbase: final FFN
I0123 12:32:45.027269 139680014180352 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:45.029587 139680014180352 nn_components.py:329] mlp: final activation = None
I0123 12:32:45.031470 139680014180352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.031566 139680014180352 nn_components.py:261] mlp: residual
I0123 12:32:45.031853 139680014180352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.031936 139680014180352 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 12:32:45.032043 139680014180352 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:45.032081 139680014180352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:45.032112 139680014180352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:45.032175 139680014180352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.034429 139680014180352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:45.039900 139680014180352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.040163 139680014180352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:45.042955 139680014180352 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:45.055778 139680014180352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:45.055833 139680014180352 attention.py:418] Single window, no scan.
I0123 12:32:45.055870 139680014180352 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:45.055902 139680014180352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.055964 139680014180352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.056539 139680014180352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.056615 139680014180352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.056975 139680014180352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.057669 139680014180352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.060227 139680014180352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.060849 139680014180352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.060926 139680014180352 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:45.060961 139680014180352 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:45.061020 139680014180352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.061146 139680014180352 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:45.061261 139680014180352 nn_components.py:325] mlp: activation = None
I0123 12:32:45.061300 139680014180352 nn_components.py:329] mlp: final activation = None
I0123 12:32:45.063195 139680014180352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.063289 139680014180352 nn_components.py:261] mlp: residual
I0123 12:32:45.065720 139680014180352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.065806 139680014180352 transformer_base.py:443] tbase: final FFN
I0123 12:32:45.065918 139680014180352 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:45.068592 139680014180352 nn_components.py:329] mlp: final activation = None
I0123 12:32:45.070486 139680014180352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.070582 139680014180352 nn_components.py:261] mlp: residual
I0123 12:32:45.070870 139680014180352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.070951 139680014180352 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 12:32:45.071059 139680014180352 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:45.071097 139680014180352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:45.071128 139680014180352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:45.071191 139680014180352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.073444 139680014180352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:45.078905 139680014180352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.079166 139680014180352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:45.081898 139680014180352 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:45.094707 139680014180352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:45.094762 139680014180352 attention.py:418] Single window, no scan.
I0123 12:32:45.094797 139680014180352 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:45.094826 139680014180352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.094891 139680014180352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.095454 139680014180352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.095531 139680014180352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.095895 139680014180352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.096587 139680014180352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.099159 139680014180352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.099789 139680014180352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.099866 139680014180352 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:45.099900 139680014180352 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:45.099957 139680014180352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.100086 139680014180352 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:45.100193 139680014180352 nn_components.py:325] mlp: activation = None
I0123 12:32:45.100230 139680014180352 nn_components.py:329] mlp: final activation = None
I0123 12:32:45.102466 139680014180352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.102561 139680014180352 nn_components.py:261] mlp: residual
I0123 12:32:45.104962 139680014180352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.105041 139680014180352 transformer_base.py:443] tbase: final FFN
I0123 12:32:45.105157 139680014180352 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:45.107440 139680014180352 nn_components.py:329] mlp: final activation = None
I0123 12:32:45.109290 139680014180352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.109384 139680014180352 nn_components.py:261] mlp: residual
I0123 12:32:45.109679 139680014180352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.109760 139680014180352 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 12:32:45.109872 139680014180352 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:45.109911 139680014180352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:45.109941 139680014180352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:45.110003 139680014180352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.112251 139680014180352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:45.117712 139680014180352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.117975 139680014180352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:45.120687 139680014180352 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:45.133400 139680014180352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:45.133455 139680014180352 attention.py:418] Single window, no scan.
I0123 12:32:45.133491 139680014180352 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:45.133522 139680014180352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.133584 139680014180352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.134153 139680014180352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.134229 139680014180352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.134586 139680014180352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.135273 139680014180352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.137850 139680014180352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.138480 139680014180352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.138557 139680014180352 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:45.138591 139680014180352 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:45.138648 139680014180352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.138773 139680014180352 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:45.138879 139680014180352 nn_components.py:325] mlp: activation = None
I0123 12:32:45.138916 139680014180352 nn_components.py:329] mlp: final activation = None
I0123 12:32:45.140969 139680014180352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.141060 139680014180352 nn_components.py:261] mlp: residual
I0123 12:32:45.143464 139680014180352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.143543 139680014180352 transformer_base.py:443] tbase: final FFN
I0123 12:32:45.143648 139680014180352 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:45.145941 139680014180352 nn_components.py:329] mlp: final activation = None
I0123 12:32:45.147787 139680014180352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.147879 139680014180352 nn_components.py:261] mlp: residual
I0123 12:32:45.148164 139680014180352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.148243 139680014180352 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 12:32:45.148351 139680014180352 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:32:45.148389 139680014180352 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:32:45.148419 139680014180352 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:32:45.148481 139680014180352 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.150716 139680014180352 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:32:45.156152 139680014180352 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.156415 139680014180352 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:32:45.159124 139680014180352 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:32:45.171823 139680014180352 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:32:45.171880 139680014180352 attention.py:418] Single window, no scan.
I0123 12:32:45.171916 139680014180352 transformer_layer.py:389] tlayer: self-attention.
I0123 12:32:45.171946 139680014180352 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.172008 139680014180352 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.172579 139680014180352 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.172656 139680014180352 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.173014 139680014180352 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.173716 139680014180352 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.176269 139680014180352 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.176895 139680014180352 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.176976 139680014180352 transformer_layer.py:468] tlayer: End windows.
I0123 12:32:45.177011 139680014180352 transformer_layer.py:472] tlayer: final FFN.
I0123 12:32:45.177070 139680014180352 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.177196 139680014180352 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:32:45.177305 139680014180352 nn_components.py:325] mlp: activation = None
I0123 12:32:45.177343 139680014180352 nn_components.py:329] mlp: final activation = None
I0123 12:32:45.179226 139680014180352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.179319 139680014180352 nn_components.py:261] mlp: residual
I0123 12:32:45.181745 139680014180352 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.181823 139680014180352 transformer_base.py:443] tbase: final FFN
I0123 12:32:45.181929 139680014180352 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:32:45.184587 139680014180352 nn_components.py:329] mlp: final activation = None
I0123 12:32:45.186465 139680014180352 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.186561 139680014180352 nn_components.py:261] mlp: residual
I0123 12:32:45.186844 139680014180352 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:45.186928 139680014180352 decoder_stack.py:344] dstack: Final layernorm.
I0123 12:32:45.189820 139680014180352 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:32:49.827992 139680014180352 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
I0123 12:32:50.437962 139680014180352 training_loop.py:409] No working directory specified.
I0123 12:32:50.438268 139680014180352 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0123 12:32:50.439502 139680014180352 checkpoints.py:1062] Restoring legacy Flax checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0123 12:32:53.528418 139680014180352 training_loop.py:447] Only restoring trainable parameters.
I0123 12:32:53.529311 139680014180352 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0123 12:32:53.529392 139680014180352 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0123 12:32:53.529445 139680014180352 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:32:53.529490 139680014180352 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:32:53.529532 139680014180352 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:53.529572 139680014180352 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:53.529611 139680014180352 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:53.529670 139680014180352 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:53.529724 139680014180352 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:32:53.529767 139680014180352 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:32:53.529806 139680014180352 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:53.529845 139680014180352 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:53.529883 139680014180352 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:32:53.529920 139680014180352 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:32:53.529958 139680014180352 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:53.529996 139680014180352 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:53.530035 139680014180352 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:53.530072 139680014180352 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:53.530115 139680014180352 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:32:53.530177 139680014180352 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:32:53.530243 139680014180352 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:53.530286 139680014180352 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:53.530325 139680014180352 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:32:53.530363 139680014180352 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:32:53.530400 139680014180352 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:53.530437 139680014180352 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:53.530475 139680014180352 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:53.530512 139680014180352 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:53.530549 139680014180352 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:32:53.530586 139680014180352 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:32:53.530623 139680014180352 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:53.530659 139680014180352 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:53.530697 139680014180352 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:32:53.530735 139680014180352 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:32:53.530775 139680014180352 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:53.530835 139680014180352 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:53.530877 139680014180352 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:53.530915 139680014180352 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:53.530952 139680014180352 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:32:53.530989 139680014180352 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:32:53.531025 139680014180352 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:53.531063 139680014180352 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:53.531101 139680014180352 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:32:53.531137 139680014180352 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:32:53.531175 139680014180352 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:53.531212 139680014180352 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:53.531255 139680014180352 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:53.531295 139680014180352 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:53.531333 139680014180352 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:32:53.531370 139680014180352 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:32:53.531407 139680014180352 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:53.531462 139680014180352 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:53.531514 139680014180352 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:32:53.531564 139680014180352 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:32:53.531611 139680014180352 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:53.531650 139680014180352 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:53.531687 139680014180352 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:53.531724 139680014180352 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:53.531760 139680014180352 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:32:53.531797 139680014180352 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:32:53.531834 139680014180352 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:53.531870 139680014180352 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:53.531910 139680014180352 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:32:53.531959 139680014180352 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:32:53.531998 139680014180352 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:53.532036 139680014180352 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:53.532073 139680014180352 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:53.532128 139680014180352 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:53.532188 139680014180352 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:32:53.532250 139680014180352 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:32:53.532295 139680014180352 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:53.532333 139680014180352 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:53.532371 139680014180352 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:32:53.532413 139680014180352 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:32:53.532452 139680014180352 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:53.532490 139680014180352 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:53.532528 139680014180352 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:53.532564 139680014180352 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:53.532602 139680014180352 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:32:53.532638 139680014180352 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:32:53.532674 139680014180352 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:53.532710 139680014180352 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:53.532746 139680014180352 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:32:53.532782 139680014180352 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:32:53.532819 139680014180352 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:53.532855 139680014180352 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:53.532892 139680014180352 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:53.532929 139680014180352 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:53.532965 139680014180352 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:32:53.533002 139680014180352 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:32:53.533059 139680014180352 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:53.533102 139680014180352 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:53.533141 139680014180352 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:32:53.533178 139680014180352 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:32:53.533217 139680014180352 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:53.533279 139680014180352 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:53.533337 139680014180352 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:53.533378 139680014180352 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:53.533417 139680014180352 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:32:53.533453 139680014180352 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:32:53.533496 139680014180352 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:53.533534 139680014180352 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:53.533572 139680014180352 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:32:53.533609 139680014180352 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:32:53.533661 139680014180352 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:53.533710 139680014180352 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:53.533754 139680014180352 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:53.533792 139680014180352 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:53.533830 139680014180352 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:32:53.533867 139680014180352 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:32:53.533903 139680014180352 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:53.533950 139680014180352 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:53.533990 139680014180352 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:32:53.534028 139680014180352 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:32:53.534065 139680014180352 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:53.534102 139680014180352 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:53.534138 139680014180352 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:53.534176 139680014180352 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:53.534213 139680014180352 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:32:53.534251 139680014180352 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:32:53.534288 139680014180352 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:32:53.534324 139680014180352 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:32:53.534353 139680014180352 training_loop.py:725] Total parameters: 152072288
I0123 12:32:53.534632 139680014180352 training_loop.py:739] Total state size: 0
I0123 12:32:53.573199 139680014180352 training_loop.py:492] Training loop: creating task for mode beam_search
I0123 12:32:53.573664 139680014180352 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0123 12:32:53.574903 139680014180352 training_loop.py:652] Compiling mode beam_search with jit.
I0123 12:32:53.575331 139680014180352 training_loop.py:89] registering functions: dict_keys([])
I0123 12:32:53.593832 139680014180352 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = mirror e b a; f = on_line f a e; g = circle g c a f; h = on_circle h g c, on_line h b c; i = mirror i h f; j = on_line j f i; k = on_circle k d b, on_line k j b ? cyclic k f j a
