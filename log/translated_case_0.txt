I0123 16:23:17.749587 139735039037440 inference_utils.py:69] Parsing gin configuration.
I0123 16:23:17.749711 139735039037440 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0123 16:23:17.749917 139735039037440 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0123 16:23:17.749952 139735039037440 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0123 16:23:17.749983 139735039037440 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0123 16:23:17.750012 139735039037440 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0123 16:23:17.750040 139735039037440 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0123 16:23:17.750067 139735039037440 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0123 16:23:17.750095 139735039037440 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0123 16:23:17.750122 139735039037440 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=32
I0123 16:23:17.750152 139735039037440 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0123 16:23:17.750181 139735039037440 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0123 16:23:17.750230 139735039037440 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0123 16:23:17.750366 139735039037440 resource_reader.py:55] Path not found: base_htrans.gin
I0123 16:23:17.750574 139735039037440 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0123 16:23:17.750677 139735039037440 resource_reader.py:55] Path not found: trainer_configuration.gin
I0123 16:23:17.756952 139735039037440 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0123 16:23:17.757075 139735039037440 resource_reader.py:55] Path not found: size/medium_150M.gin
I0123 16:23:17.757402 139735039037440 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0123 16:23:17.757510 139735039037440 resource_reader.py:55] Path not found: options/positions_t5.gin
I0123 16:23:17.757800 139735039037440 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0123 16:23:17.757906 139735039037440 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0123 16:23:17.758313 139735039037440 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0123 16:23:17.758417 139735039037440 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0123 16:23:17.762044 139735039037440 training_loop.py:334] ==== Training loop: initializing model ====
I0123 16:23:17.856153 139735039037440 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0123 16:23:17.856879 139735039037440 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0123 16:23:17.863644 139735039037440 training_loop.py:335] Process 0 of 1
I0123 16:23:17.863703 139735039037440 training_loop.py:336] Local device count = 1
I0123 16:23:17.863744 139735039037440 training_loop.py:337] Number of replicas = 1
I0123 16:23:17.863777 139735039037440 training_loop.py:339] Using random number seed 42
I0123 16:23:18.363699 139735039037440 training_loop.py:359] Initializing the model.
I0123 16:23:18.792836 139735039037440 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.793081 139735039037440 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 16:23:18.793185 139735039037440 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 16:23:18.793265 139735039037440 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 16:23:18.793342 139735039037440 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 16:23:18.793426 139735039037440 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 16:23:18.793498 139735039037440 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 16:23:18.793569 139735039037440 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 16:23:18.793645 139735039037440 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 16:23:18.793720 139735039037440 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 16:23:18.793788 139735039037440 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 16:23:18.793855 139735039037440 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 16:23:18.793922 139735039037440 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 16:23:18.793989 139735039037440 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 16:23:18.794029 139735039037440 attention.py:418] Single window, no scan.
I0123 16:23:18.794074 139735039037440 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 16:23:18.794191 139735039037440 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:23:18.794233 139735039037440 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:23:18.794265 139735039037440 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:23:18.796261 139735039037440 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.801506 139735039037440 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:23:18.812093 139735039037440 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.812371 139735039037440 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:23:18.816718 139735039037440 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:23:18.827291 139735039037440 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:23:18.827352 139735039037440 attention.py:418] Single window, no scan.
I0123 16:23:18.827390 139735039037440 transformer_layer.py:389] tlayer: self-attention.
I0123 16:23:18.827423 139735039037440 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.827487 139735039037440 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.828668 139735039037440 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.828750 139735039037440 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.829465 139735039037440 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.831909 139735039037440 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.837564 139735039037440 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.839942 139735039037440 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.840071 139735039037440 transformer_layer.py:468] tlayer: End windows.
I0123 16:23:18.840111 139735039037440 transformer_layer.py:472] tlayer: final FFN.
I0123 16:23:18.840178 139735039037440 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.840319 139735039037440 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:23:18.840683 139735039037440 nn_components.py:325] mlp: activation = None
I0123 16:23:18.840735 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:23:18.842722 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.842830 139735039037440 nn_components.py:261] mlp: residual
I0123 16:23:18.845709 139735039037440 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.845793 139735039037440 transformer_base.py:443] tbase: final FFN
I0123 16:23:18.846290 139735039037440 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:23:18.856347 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:23:18.865192 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.865294 139735039037440 nn_components.py:261] mlp: residual
I0123 16:23:18.865589 139735039037440 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.865687 139735039037440 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 16:23:18.865803 139735039037440 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:23:18.865843 139735039037440 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:23:18.865875 139735039037440 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:23:18.867787 139735039037440 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.870265 139735039037440 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:23:18.875929 139735039037440 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.876194 139735039037440 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:23:18.878848 139735039037440 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:23:18.882734 139735039037440 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:23:18.882797 139735039037440 attention.py:418] Single window, no scan.
I0123 16:23:18.882835 139735039037440 transformer_layer.py:389] tlayer: self-attention.
I0123 16:23:18.882868 139735039037440 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.882933 139735039037440 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.883511 139735039037440 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.883588 139735039037440 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.883958 139735039037440 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.884760 139735039037440 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.887329 139735039037440 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.888170 139735039037440 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.888254 139735039037440 transformer_layer.py:468] tlayer: End windows.
I0123 16:23:18.888291 139735039037440 transformer_layer.py:472] tlayer: final FFN.
I0123 16:23:18.888353 139735039037440 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.888485 139735039037440 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:23:18.888815 139735039037440 nn_components.py:325] mlp: activation = None
I0123 16:23:18.888862 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:23:18.890918 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.891019 139735039037440 nn_components.py:261] mlp: residual
I0123 16:23:18.893518 139735039037440 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.893600 139735039037440 transformer_base.py:443] tbase: final FFN
I0123 16:23:18.894046 139735039037440 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:23:18.896602 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:23:18.898589 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.898692 139735039037440 nn_components.py:261] mlp: residual
I0123 16:23:18.898997 139735039037440 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.899084 139735039037440 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 16:23:18.899200 139735039037440 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:23:18.899243 139735039037440 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:23:18.899276 139735039037440 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:23:18.901177 139735039037440 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.903588 139735039037440 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:23:18.909741 139735039037440 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.910015 139735039037440 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:23:18.912694 139735039037440 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:23:18.916605 139735039037440 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:23:18.916664 139735039037440 attention.py:418] Single window, no scan.
I0123 16:23:18.916700 139735039037440 transformer_layer.py:389] tlayer: self-attention.
I0123 16:23:18.916733 139735039037440 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.916796 139735039037440 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.917357 139735039037440 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.917435 139735039037440 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.917807 139735039037440 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.918599 139735039037440 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.921123 139735039037440 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.921807 139735039037440 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.921888 139735039037440 transformer_layer.py:468] tlayer: End windows.
I0123 16:23:18.921927 139735039037440 transformer_layer.py:472] tlayer: final FFN.
I0123 16:23:18.921989 139735039037440 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.922129 139735039037440 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:23:18.922472 139735039037440 nn_components.py:325] mlp: activation = None
I0123 16:23:18.922520 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:23:18.924475 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.924570 139735039037440 nn_components.py:261] mlp: residual
I0123 16:23:18.927096 139735039037440 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.927189 139735039037440 transformer_base.py:443] tbase: final FFN
I0123 16:23:18.927714 139735039037440 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:23:18.929987 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:23:18.931961 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.932057 139735039037440 nn_components.py:261] mlp: residual
I0123 16:23:18.932345 139735039037440 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.932428 139735039037440 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 16:23:18.932539 139735039037440 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:23:18.932582 139735039037440 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:23:18.932615 139735039037440 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:23:18.934523 139735039037440 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.936929 139735039037440 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:23:18.942574 139735039037440 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.942858 139735039037440 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:23:18.945487 139735039037440 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:23:18.949372 139735039037440 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:23:18.949431 139735039037440 attention.py:418] Single window, no scan.
I0123 16:23:18.949469 139735039037440 transformer_layer.py:389] tlayer: self-attention.
I0123 16:23:18.949501 139735039037440 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.949565 139735039037440 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.950145 139735039037440 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.950227 139735039037440 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.950589 139735039037440 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.951381 139735039037440 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.953921 139735039037440 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.954554 139735039037440 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.954637 139735039037440 transformer_layer.py:468] tlayer: End windows.
I0123 16:23:18.954673 139735039037440 transformer_layer.py:472] tlayer: final FFN.
I0123 16:23:18.954734 139735039037440 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.954864 139735039037440 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:23:18.955194 139735039037440 nn_components.py:325] mlp: activation = None
I0123 16:23:18.955242 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:23:18.957124 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.957219 139735039037440 nn_components.py:261] mlp: residual
I0123 16:23:18.959742 139735039037440 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.959833 139735039037440 transformer_base.py:443] tbase: final FFN
I0123 16:23:18.960266 139735039037440 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:23:18.962530 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:23:18.964413 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.964510 139735039037440 nn_components.py:261] mlp: residual
I0123 16:23:18.964797 139735039037440 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.964880 139735039037440 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 16:23:18.964991 139735039037440 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:23:18.965033 139735039037440 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:23:18.965064 139735039037440 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:23:18.966964 139735039037440 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.969328 139735039037440 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:23:18.974874 139735039037440 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.975140 139735039037440 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:23:18.977797 139735039037440 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:23:18.981499 139735039037440 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:23:18.981562 139735039037440 attention.py:418] Single window, no scan.
I0123 16:23:18.981599 139735039037440 transformer_layer.py:389] tlayer: self-attention.
I0123 16:23:18.981633 139735039037440 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.981711 139735039037440 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.982285 139735039037440 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.982365 139735039037440 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.982732 139735039037440 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.983496 139735039037440 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.986333 139735039037440 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.986957 139735039037440 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.987037 139735039037440 transformer_layer.py:468] tlayer: End windows.
I0123 16:23:18.987074 139735039037440 transformer_layer.py:472] tlayer: final FFN.
I0123 16:23:18.987134 139735039037440 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.987271 139735039037440 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:23:18.987593 139735039037440 nn_components.py:325] mlp: activation = None
I0123 16:23:18.987639 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:23:18.989499 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.989593 139735039037440 nn_components.py:261] mlp: residual
I0123 16:23:18.992119 139735039037440 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.992201 139735039037440 transformer_base.py:443] tbase: final FFN
I0123 16:23:18.992632 139735039037440 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:23:18.994879 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:23:18.996899 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.996996 139735039037440 nn_components.py:261] mlp: residual
I0123 16:23:18.997288 139735039037440 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:18.997374 139735039037440 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 16:23:18.997485 139735039037440 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:23:18.997528 139735039037440 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:23:18.997559 139735039037440 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:23:18.999395 139735039037440 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.001758 139735039037440 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:23:19.007320 139735039037440 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.007691 139735039037440 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:23:19.010424 139735039037440 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:23:19.014278 139735039037440 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:23:19.014338 139735039037440 attention.py:418] Single window, no scan.
I0123 16:23:19.014377 139735039037440 transformer_layer.py:389] tlayer: self-attention.
I0123 16:23:19.014410 139735039037440 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.014472 139735039037440 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.015086 139735039037440 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.015167 139735039037440 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.015527 139735039037440 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.016304 139735039037440 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.018763 139735039037440 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.019385 139735039037440 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.019466 139735039037440 transformer_layer.py:468] tlayer: End windows.
I0123 16:23:19.019502 139735039037440 transformer_layer.py:472] tlayer: final FFN.
I0123 16:23:19.019560 139735039037440 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.019690 139735039037440 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:23:19.020015 139735039037440 nn_components.py:325] mlp: activation = None
I0123 16:23:19.020062 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:23:19.021959 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.022056 139735039037440 nn_components.py:261] mlp: residual
I0123 16:23:19.024744 139735039037440 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.024827 139735039037440 transformer_base.py:443] tbase: final FFN
I0123 16:23:19.025256 139735039037440 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:23:19.027734 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:23:19.029800 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.029899 139735039037440 nn_components.py:261] mlp: residual
I0123 16:23:19.030184 139735039037440 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.030266 139735039037440 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 16:23:19.030376 139735039037440 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:23:19.030418 139735039037440 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:23:19.030450 139735039037440 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:23:19.032298 139735039037440 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.034783 139735039037440 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:23:19.040319 139735039037440 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.040599 139735039037440 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:23:19.043210 139735039037440 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:23:19.046990 139735039037440 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:23:19.047050 139735039037440 attention.py:418] Single window, no scan.
I0123 16:23:19.047088 139735039037440 transformer_layer.py:389] tlayer: self-attention.
I0123 16:23:19.047122 139735039037440 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.047185 139735039037440 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.047751 139735039037440 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.047831 139735039037440 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.048191 139735039037440 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.048958 139735039037440 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.051441 139735039037440 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.052079 139735039037440 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.052161 139735039037440 transformer_layer.py:468] tlayer: End windows.
I0123 16:23:19.052198 139735039037440 transformer_layer.py:472] tlayer: final FFN.
I0123 16:23:19.052260 139735039037440 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.052394 139735039037440 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:23:19.052724 139735039037440 nn_components.py:325] mlp: activation = None
I0123 16:23:19.052772 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:23:19.054785 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.054884 139735039037440 nn_components.py:261] mlp: residual
I0123 16:23:19.057395 139735039037440 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.057478 139735039037440 transformer_base.py:443] tbase: final FFN
I0123 16:23:19.057928 139735039037440 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:23:19.060591 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:23:19.062547 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.062656 139735039037440 nn_components.py:261] mlp: residual
I0123 16:23:19.062959 139735039037440 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.063045 139735039037440 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 16:23:19.063159 139735039037440 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:23:19.063203 139735039037440 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:23:19.063235 139735039037440 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:23:19.204231 139735039037440 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.207510 139735039037440 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:23:19.213498 139735039037440 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.213821 139735039037440 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:23:19.216593 139735039037440 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:23:19.220631 139735039037440 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:23:19.220695 139735039037440 attention.py:418] Single window, no scan.
I0123 16:23:19.220735 139735039037440 transformer_layer.py:389] tlayer: self-attention.
I0123 16:23:19.220769 139735039037440 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.220840 139735039037440 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.221479 139735039037440 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.221562 139735039037440 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.221943 139735039037440 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.222750 139735039037440 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.225368 139735039037440 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.226033 139735039037440 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.226117 139735039037440 transformer_layer.py:468] tlayer: End windows.
I0123 16:23:19.226155 139735039037440 transformer_layer.py:472] tlayer: final FFN.
I0123 16:23:19.226219 139735039037440 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.226357 139735039037440 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:23:19.226705 139735039037440 nn_components.py:325] mlp: activation = None
I0123 16:23:19.226753 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:23:19.228686 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.228787 139735039037440 nn_components.py:261] mlp: residual
I0123 16:23:19.231375 139735039037440 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.231460 139735039037440 transformer_base.py:443] tbase: final FFN
I0123 16:23:19.231909 139735039037440 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:23:19.234269 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:23:19.236379 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.236494 139735039037440 nn_components.py:261] mlp: residual
I0123 16:23:19.236799 139735039037440 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.236891 139735039037440 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 16:23:19.237008 139735039037440 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:23:19.237051 139735039037440 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:23:19.237084 139735039037440 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:23:19.239076 139735039037440 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.241674 139735039037440 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:23:19.247333 139735039037440 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.247601 139735039037440 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:23:19.250289 139735039037440 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:23:19.254122 139735039037440 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:23:19.254182 139735039037440 attention.py:418] Single window, no scan.
I0123 16:23:19.254222 139735039037440 transformer_layer.py:389] tlayer: self-attention.
I0123 16:23:19.254257 139735039037440 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.254322 139735039037440 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.254899 139735039037440 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.254979 139735039037440 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.255343 139735039037440 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.256129 139735039037440 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.258703 139735039037440 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.259337 139735039037440 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.259419 139735039037440 transformer_layer.py:468] tlayer: End windows.
I0123 16:23:19.259455 139735039037440 transformer_layer.py:472] tlayer: final FFN.
I0123 16:23:19.259517 139735039037440 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.259648 139735039037440 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:23:19.259974 139735039037440 nn_components.py:325] mlp: activation = None
I0123 16:23:19.260022 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:23:19.261913 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.262016 139735039037440 nn_components.py:261] mlp: residual
I0123 16:23:19.264592 139735039037440 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.264678 139735039037440 transformer_base.py:443] tbase: final FFN
I0123 16:23:19.265110 139735039037440 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:23:19.267402 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:23:19.269360 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.269460 139735039037440 nn_components.py:261] mlp: residual
I0123 16:23:19.269756 139735039037440 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.269852 139735039037440 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 16:23:19.269972 139735039037440 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:23:19.270015 139735039037440 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:23:19.270049 139735039037440 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:23:19.271894 139735039037440 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.274338 139735039037440 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:23:19.279892 139735039037440 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.280167 139735039037440 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:23:19.283193 139735039037440 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:23:19.286966 139735039037440 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:23:19.287025 139735039037440 attention.py:418] Single window, no scan.
I0123 16:23:19.287063 139735039037440 transformer_layer.py:389] tlayer: self-attention.
I0123 16:23:19.287096 139735039037440 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.287161 139735039037440 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.287769 139735039037440 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.287849 139735039037440 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.288207 139735039037440 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.288988 139735039037440 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.291471 139735039037440 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.292109 139735039037440 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.292196 139735039037440 transformer_layer.py:468] tlayer: End windows.
I0123 16:23:19.292235 139735039037440 transformer_layer.py:472] tlayer: final FFN.
I0123 16:23:19.292299 139735039037440 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.292440 139735039037440 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:23:19.292772 139735039037440 nn_components.py:325] mlp: activation = None
I0123 16:23:19.292819 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:23:19.294737 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.294835 139735039037440 nn_components.py:261] mlp: residual
I0123 16:23:19.297365 139735039037440 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.297451 139735039037440 transformer_base.py:443] tbase: final FFN
I0123 16:23:19.297899 139735039037440 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:23:19.300211 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:23:19.302114 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.302214 139735039037440 nn_components.py:261] mlp: residual
I0123 16:23:19.302506 139735039037440 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.302603 139735039037440 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 16:23:19.302720 139735039037440 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:23:19.302765 139735039037440 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:23:19.302798 139735039037440 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:23:19.304647 139735039037440 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.307125 139735039037440 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:23:19.312712 139735039037440 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.312990 139735039037440 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:23:19.315647 139735039037440 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:23:19.319482 139735039037440 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:23:19.319542 139735039037440 attention.py:418] Single window, no scan.
I0123 16:23:19.319581 139735039037440 transformer_layer.py:389] tlayer: self-attention.
I0123 16:23:19.319616 139735039037440 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.319680 139735039037440 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.320255 139735039037440 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.320336 139735039037440 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.320699 139735039037440 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.321509 139735039037440 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.324012 139735039037440 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.324650 139735039037440 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.324735 139735039037440 transformer_layer.py:468] tlayer: End windows.
I0123 16:23:19.324773 139735039037440 transformer_layer.py:472] tlayer: final FFN.
I0123 16:23:19.324836 139735039037440 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.324974 139735039037440 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:23:19.325302 139735039037440 nn_components.py:325] mlp: activation = None
I0123 16:23:19.325349 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:23:19.327337 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.327442 139735039037440 nn_components.py:261] mlp: residual
I0123 16:23:19.330443 139735039037440 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.330529 139735039037440 transformer_base.py:443] tbase: final FFN
I0123 16:23:19.330970 139735039037440 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:23:19.333472 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:23:19.335378 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.335477 139735039037440 nn_components.py:261] mlp: residual
I0123 16:23:19.335765 139735039037440 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.335850 139735039037440 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 16:23:19.335976 139735039037440 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:23:19.336022 139735039037440 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:23:19.336058 139735039037440 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:23:19.337983 139735039037440 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.340355 139735039037440 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:23:19.345978 139735039037440 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.346242 139735039037440 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:23:19.348891 139735039037440 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:23:19.352765 139735039037440 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:23:19.352825 139735039037440 attention.py:418] Single window, no scan.
I0123 16:23:19.352863 139735039037440 transformer_layer.py:389] tlayer: self-attention.
I0123 16:23:19.352897 139735039037440 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.352961 139735039037440 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.353532 139735039037440 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.353613 139735039037440 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.353985 139735039037440 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.354775 139735039037440 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.357257 139735039037440 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.358264 139735039037440 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.358349 139735039037440 transformer_layer.py:468] tlayer: End windows.
I0123 16:23:19.358387 139735039037440 transformer_layer.py:472] tlayer: final FFN.
I0123 16:23:19.358449 139735039037440 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.358590 139735039037440 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:23:19.358917 139735039037440 nn_components.py:325] mlp: activation = None
I0123 16:23:19.358965 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:23:19.360882 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.360980 139735039037440 nn_components.py:261] mlp: residual
I0123 16:23:19.363496 139735039037440 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.363578 139735039037440 transformer_base.py:443] tbase: final FFN
I0123 16:23:19.364075 139735039037440 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:23:19.366361 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:23:19.368278 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.368379 139735039037440 nn_components.py:261] mlp: residual
I0123 16:23:19.368670 139735039037440 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.368962 139735039037440 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 16:23:19.369037 139735039037440 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 16:23:19.369108 139735039037440 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 16:23:19.369169 139735039037440 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 16:23:19.369225 139735039037440 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 16:23:19.369282 139735039037440 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 16:23:19.369336 139735039037440 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 16:23:19.369391 139735039037440 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 16:23:19.369447 139735039037440 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 16:23:19.369501 139735039037440 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 16:23:19.369554 139735039037440 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 16:23:19.369608 139735039037440 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 16:23:19.369653 139735039037440 decoder_stack.py:344] dstack: Final layernorm.
I0123 16:23:19.373169 139735039037440 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:23:19.420943 139735039037440 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.421033 139735039037440 decoder_stack.py:333] dstack: autoregressive generator.
I0123 16:23:19.421090 139735039037440 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 16:23:19.421197 139735039037440 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:23:19.421240 139735039037440 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:23:19.421274 139735039037440 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:23:19.421340 139735039037440 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.423768 139735039037440 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:23:19.429203 139735039037440 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.429468 139735039037440 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:23:19.432304 139735039037440 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:23:19.449544 139735039037440 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:23:19.449606 139735039037440 attention.py:418] Single window, no scan.
I0123 16:23:19.449649 139735039037440 transformer_layer.py:389] tlayer: self-attention.
I0123 16:23:19.449685 139735039037440 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.449749 139735039037440 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.450885 139735039037440 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.450967 139735039037440 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.451669 139735039037440 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.453789 139735039037440 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.458528 139735039037440 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.459862 139735039037440 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.459955 139735039037440 transformer_layer.py:468] tlayer: End windows.
I0123 16:23:19.459995 139735039037440 transformer_layer.py:472] tlayer: final FFN.
I0123 16:23:19.460057 139735039037440 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.460193 139735039037440 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:23:19.460309 139735039037440 nn_components.py:325] mlp: activation = None
I0123 16:23:19.460352 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:23:19.462268 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.462367 139735039037440 nn_components.py:261] mlp: residual
I0123 16:23:19.464777 139735039037440 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.464860 139735039037440 transformer_base.py:443] tbase: final FFN
I0123 16:23:19.464972 139735039037440 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:23:19.467223 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:23:19.469191 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.469290 139735039037440 nn_components.py:261] mlp: residual
I0123 16:23:19.469581 139735039037440 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.469673 139735039037440 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 16:23:19.469789 139735039037440 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:23:19.469832 139735039037440 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:23:19.469864 139735039037440 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:23:19.469931 139735039037440 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.472195 139735039037440 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:23:19.477711 139735039037440 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.477975 139735039037440 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:23:19.480664 139735039037440 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:23:19.498634 139735039037440 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:23:19.498719 139735039037440 attention.py:418] Single window, no scan.
I0123 16:23:19.498760 139735039037440 transformer_layer.py:389] tlayer: self-attention.
I0123 16:23:19.498794 139735039037440 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.498872 139735039037440 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.499514 139735039037440 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.499594 139735039037440 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.499977 139735039037440 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.500695 139735039037440 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.503286 139735039037440 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.503915 139735039037440 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.503996 139735039037440 transformer_layer.py:468] tlayer: End windows.
I0123 16:23:19.504040 139735039037440 transformer_layer.py:472] tlayer: final FFN.
I0123 16:23:19.504104 139735039037440 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.504242 139735039037440 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:23:19.504362 139735039037440 nn_components.py:325] mlp: activation = None
I0123 16:23:19.504405 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:23:19.506435 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.506533 139735039037440 nn_components.py:261] mlp: residual
I0123 16:23:19.508966 139735039037440 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.509048 139735039037440 transformer_base.py:443] tbase: final FFN
I0123 16:23:19.509165 139735039037440 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:23:19.511427 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:23:19.513378 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.513477 139735039037440 nn_components.py:261] mlp: residual
I0123 16:23:19.513769 139735039037440 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.513856 139735039037440 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 16:23:19.513972 139735039037440 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:23:19.514017 139735039037440 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:23:19.514052 139735039037440 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:23:19.514119 139735039037440 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.516378 139735039037440 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:23:19.521775 139735039037440 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.522040 139735039037440 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:23:19.524786 139735039037440 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:23:19.537732 139735039037440 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:23:19.537795 139735039037440 attention.py:418] Single window, no scan.
I0123 16:23:19.537833 139735039037440 transformer_layer.py:389] tlayer: self-attention.
I0123 16:23:19.537866 139735039037440 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.537930 139735039037440 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.538500 139735039037440 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.538582 139735039037440 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.538938 139735039037440 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.539638 139735039037440 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.542128 139735039037440 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.542760 139735039037440 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.542841 139735039037440 transformer_layer.py:468] tlayer: End windows.
I0123 16:23:19.542877 139735039037440 transformer_layer.py:472] tlayer: final FFN.
I0123 16:23:19.542944 139735039037440 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.543084 139735039037440 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:23:19.543201 139735039037440 nn_components.py:325] mlp: activation = None
I0123 16:23:19.543244 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:23:19.545167 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.545264 139735039037440 nn_components.py:261] mlp: residual
I0123 16:23:19.547693 139735039037440 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.547777 139735039037440 transformer_base.py:443] tbase: final FFN
I0123 16:23:19.547888 139735039037440 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:23:19.550109 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:23:19.552014 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.552113 139735039037440 nn_components.py:261] mlp: residual
I0123 16:23:19.552401 139735039037440 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.552486 139735039037440 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 16:23:19.552597 139735039037440 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:23:19.552638 139735039037440 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:23:19.552669 139735039037440 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:23:19.552731 139735039037440 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.554972 139735039037440 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:23:19.560395 139735039037440 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.560661 139735039037440 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:23:19.563361 139735039037440 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:23:19.576188 139735039037440 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:23:19.576252 139735039037440 attention.py:418] Single window, no scan.
I0123 16:23:19.576290 139735039037440 transformer_layer.py:389] tlayer: self-attention.
I0123 16:23:19.576323 139735039037440 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.576389 139735039037440 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.576952 139735039037440 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.577031 139735039037440 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.577389 139735039037440 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.578103 139735039037440 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.580573 139735039037440 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.581195 139735039037440 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.581278 139735039037440 transformer_layer.py:468] tlayer: End windows.
I0123 16:23:19.581315 139735039037440 transformer_layer.py:472] tlayer: final FFN.
I0123 16:23:19.581374 139735039037440 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.581516 139735039037440 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:23:19.581631 139735039037440 nn_components.py:325] mlp: activation = None
I0123 16:23:19.581681 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:23:19.583628 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.583726 139735039037440 nn_components.py:261] mlp: residual
I0123 16:23:19.586108 139735039037440 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.586191 139735039037440 transformer_base.py:443] tbase: final FFN
I0123 16:23:19.586301 139735039037440 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:23:19.588518 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:23:19.590376 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.590476 139735039037440 nn_components.py:261] mlp: residual
I0123 16:23:19.590757 139735039037440 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.590841 139735039037440 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 16:23:19.590951 139735039037440 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:23:19.590992 139735039037440 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:23:19.591025 139735039037440 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:23:19.591090 139735039037440 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.593658 139735039037440 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:23:19.599122 139735039037440 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.599392 139735039037440 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:23:19.602020 139735039037440 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:23:19.614952 139735039037440 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:23:19.615013 139735039037440 attention.py:418] Single window, no scan.
I0123 16:23:19.615051 139735039037440 transformer_layer.py:389] tlayer: self-attention.
I0123 16:23:19.615082 139735039037440 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.615145 139735039037440 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.615706 139735039037440 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.615786 139735039037440 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.616141 139735039037440 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.616834 139735039037440 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.619355 139735039037440 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.619992 139735039037440 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.620073 139735039037440 transformer_layer.py:468] tlayer: End windows.
I0123 16:23:19.620109 139735039037440 transformer_layer.py:472] tlayer: final FFN.
I0123 16:23:19.620168 139735039037440 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.620308 139735039037440 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:23:19.620425 139735039037440 nn_components.py:325] mlp: activation = None
I0123 16:23:19.620467 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:23:19.622356 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.622454 139735039037440 nn_components.py:261] mlp: residual
I0123 16:23:19.624827 139735039037440 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.624909 139735039037440 transformer_base.py:443] tbase: final FFN
I0123 16:23:19.625019 139735039037440 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:23:19.627290 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:23:19.629145 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.629244 139735039037440 nn_components.py:261] mlp: residual
I0123 16:23:19.629528 139735039037440 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.629612 139735039037440 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 16:23:19.629729 139735039037440 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:23:19.629773 139735039037440 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:23:19.629804 139735039037440 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:23:19.629868 139735039037440 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.632081 139735039037440 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:23:19.637453 139735039037440 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.637720 139735039037440 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:23:19.640367 139735039037440 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:23:19.653053 139735039037440 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:23:19.653113 139735039037440 attention.py:418] Single window, no scan.
I0123 16:23:19.653155 139735039037440 transformer_layer.py:389] tlayer: self-attention.
I0123 16:23:19.653188 139735039037440 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.653253 139735039037440 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.653833 139735039037440 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.653917 139735039037440 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.654292 139735039037440 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.655025 139735039037440 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.657532 139735039037440 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.658165 139735039037440 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.658248 139735039037440 transformer_layer.py:468] tlayer: End windows.
I0123 16:23:19.658285 139735039037440 transformer_layer.py:472] tlayer: final FFN.
I0123 16:23:19.658344 139735039037440 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.658478 139735039037440 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:23:19.658604 139735039037440 nn_components.py:325] mlp: activation = None
I0123 16:23:19.658649 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:23:19.660595 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.660694 139735039037440 nn_components.py:261] mlp: residual
I0123 16:23:19.663090 139735039037440 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.663173 139735039037440 transformer_base.py:443] tbase: final FFN
I0123 16:23:19.663283 139735039037440 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:23:19.665467 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:23:19.667308 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.667407 139735039037440 nn_components.py:261] mlp: residual
I0123 16:23:19.667692 139735039037440 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.667777 139735039037440 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 16:23:19.667889 139735039037440 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:23:19.667932 139735039037440 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:23:19.667964 139735039037440 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:23:19.668028 139735039037440 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.670243 139735039037440 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:23:19.675720 139735039037440 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.675983 139735039037440 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:23:19.678583 139735039037440 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:23:19.691469 139735039037440 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:23:19.691529 139735039037440 attention.py:418] Single window, no scan.
I0123 16:23:19.691567 139735039037440 transformer_layer.py:389] tlayer: self-attention.
I0123 16:23:19.691600 139735039037440 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.691664 139735039037440 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.692229 139735039037440 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.692311 139735039037440 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.692674 139735039037440 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.693367 139735039037440 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.695855 139735039037440 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.696848 139735039037440 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.696932 139735039037440 transformer_layer.py:468] tlayer: End windows.
I0123 16:23:19.696969 139735039037440 transformer_layer.py:472] tlayer: final FFN.
I0123 16:23:19.697029 139735039037440 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.697170 139735039037440 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:23:19.697285 139735039037440 nn_components.py:325] mlp: activation = None
I0123 16:23:19.697335 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:23:19.699241 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.699341 139735039037440 nn_components.py:261] mlp: residual
I0123 16:23:19.701748 139735039037440 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.701831 139735039037440 transformer_base.py:443] tbase: final FFN
I0123 16:23:19.701942 139735039037440 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:23:19.704166 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:23:19.706168 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.706268 139735039037440 nn_components.py:261] mlp: residual
I0123 16:23:19.706562 139735039037440 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.706649 139735039037440 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 16:23:19.706761 139735039037440 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:23:19.706804 139735039037440 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:23:19.706836 139735039037440 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:23:19.706901 139735039037440 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.709135 139735039037440 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:23:19.714577 139735039037440 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.714851 139735039037440 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:23:19.717530 139735039037440 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:23:19.730262 139735039037440 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:23:19.730322 139735039037440 attention.py:418] Single window, no scan.
I0123 16:23:19.730360 139735039037440 transformer_layer.py:389] tlayer: self-attention.
I0123 16:23:19.730392 139735039037440 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.730456 139735039037440 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.731065 139735039037440 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.731145 139735039037440 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.731502 139735039037440 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.732194 139735039037440 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.734679 139735039037440 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.735314 139735039037440 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.735396 139735039037440 transformer_layer.py:468] tlayer: End windows.
I0123 16:23:19.735432 139735039037440 transformer_layer.py:472] tlayer: final FFN.
I0123 16:23:19.735496 139735039037440 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.735631 139735039037440 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:23:19.735747 139735039037440 nn_components.py:325] mlp: activation = None
I0123 16:23:19.735796 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:23:19.737693 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.737791 139735039037440 nn_components.py:261] mlp: residual
I0123 16:23:19.740206 139735039037440 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.740287 139735039037440 transformer_base.py:443] tbase: final FFN
I0123 16:23:19.740398 139735039037440 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:23:19.742617 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:23:19.744481 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.744579 139735039037440 nn_components.py:261] mlp: residual
I0123 16:23:19.744865 139735039037440 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.744949 139735039037440 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 16:23:19.745061 139735039037440 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:23:19.745104 139735039037440 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:23:19.745136 139735039037440 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:23:19.745200 139735039037440 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.747438 139735039037440 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:23:19.752924 139735039037440 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.753187 139735039037440 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:23:19.755782 139735039037440 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:23:19.768508 139735039037440 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:23:19.768568 139735039037440 attention.py:418] Single window, no scan.
I0123 16:23:19.768608 139735039037440 transformer_layer.py:389] tlayer: self-attention.
I0123 16:23:19.768643 139735039037440 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.768706 139735039037440 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.769273 139735039037440 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.769353 139735039037440 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.769714 139735039037440 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.770409 139735039037440 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.772891 139735039037440 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.773571 139735039037440 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.773662 139735039037440 transformer_layer.py:468] tlayer: End windows.
I0123 16:23:19.773702 139735039037440 transformer_layer.py:472] tlayer: final FFN.
I0123 16:23:19.773764 139735039037440 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.773899 139735039037440 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:23:19.774010 139735039037440 nn_components.py:325] mlp: activation = None
I0123 16:23:19.774053 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:23:19.775927 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.776024 139735039037440 nn_components.py:261] mlp: residual
I0123 16:23:19.778431 139735039037440 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.778514 139735039037440 transformer_base.py:443] tbase: final FFN
I0123 16:23:19.778631 139735039037440 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:23:19.780852 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:23:19.782797 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.782898 139735039037440 nn_components.py:261] mlp: residual
I0123 16:23:19.783190 139735039037440 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.783276 139735039037440 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 16:23:19.783388 139735039037440 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:23:19.783431 139735039037440 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:23:19.783463 139735039037440 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:23:19.783527 139735039037440 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.785777 139735039037440 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:23:19.791166 139735039037440 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.791428 139735039037440 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:23:19.794079 139735039037440 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:23:19.807224 139735039037440 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:23:19.807285 139735039037440 attention.py:418] Single window, no scan.
I0123 16:23:19.807323 139735039037440 transformer_layer.py:389] tlayer: self-attention.
I0123 16:23:19.807358 139735039037440 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.807423 139735039037440 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.808039 139735039037440 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.808120 139735039037440 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.808484 139735039037440 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.809180 139735039037440 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.811646 139735039037440 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.812279 139735039037440 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.812361 139735039037440 transformer_layer.py:468] tlayer: End windows.
I0123 16:23:19.812397 139735039037440 transformer_layer.py:472] tlayer: final FFN.
I0123 16:23:19.812456 139735039037440 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.812592 139735039037440 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:23:19.812707 139735039037440 nn_components.py:325] mlp: activation = None
I0123 16:23:19.812750 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:23:19.814646 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.814752 139735039037440 nn_components.py:261] mlp: residual
I0123 16:23:19.817200 139735039037440 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.817286 139735039037440 transformer_base.py:443] tbase: final FFN
I0123 16:23:19.817398 139735039037440 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:23:19.819619 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:23:19.821471 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.821569 139735039037440 nn_components.py:261] mlp: residual
I0123 16:23:19.821859 139735039037440 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.821945 139735039037440 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 16:23:19.822056 139735039037440 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:23:19.822098 139735039037440 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:23:19.822131 139735039037440 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:23:19.822195 139735039037440 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.824419 139735039037440 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:23:19.829909 139735039037440 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.830173 139735039037440 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:23:19.832790 139735039037440 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:23:19.845495 139735039037440 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:23:19.845556 139735039037440 attention.py:418] Single window, no scan.
I0123 16:23:19.845594 139735039037440 transformer_layer.py:389] tlayer: self-attention.
I0123 16:23:19.845628 139735039037440 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.845699 139735039037440 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.846265 139735039037440 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.846347 139735039037440 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.846707 139735039037440 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.847403 139735039037440 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.849876 139735039037440 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.850555 139735039037440 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.850637 139735039037440 transformer_layer.py:468] tlayer: End windows.
I0123 16:23:19.850673 139735039037440 transformer_layer.py:472] tlayer: final FFN.
I0123 16:23:19.850732 139735039037440 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.850867 139735039037440 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:23:19.850980 139735039037440 nn_components.py:325] mlp: activation = None
I0123 16:23:19.851023 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:23:19.852897 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.853003 139735039037440 nn_components.py:261] mlp: residual
I0123 16:23:19.855438 139735039037440 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.855522 139735039037440 transformer_base.py:443] tbase: final FFN
I0123 16:23:19.855634 139735039037440 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:23:19.857853 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:23:19.859803 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.859907 139735039037440 nn_components.py:261] mlp: residual
I0123 16:23:19.860207 139735039037440 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.860297 139735039037440 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 16:23:19.860414 139735039037440 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:23:19.860459 139735039037440 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:23:19.860494 139735039037440 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:23:19.860563 139735039037440 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.862910 139735039037440 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:23:19.868404 139735039037440 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.868668 139735039037440 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:23:19.871402 139735039037440 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:23:19.884265 139735039037440 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:23:19.884330 139735039037440 attention.py:418] Single window, no scan.
I0123 16:23:19.884371 139735039037440 transformer_layer.py:389] tlayer: self-attention.
I0123 16:23:19.884404 139735039037440 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.884476 139735039037440 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.885072 139735039037440 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.885157 139735039037440 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.885532 139735039037440 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.886318 139735039037440 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.888901 139735039037440 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.889531 139735039037440 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.889613 139735039037440 transformer_layer.py:468] tlayer: End windows.
I0123 16:23:19.889657 139735039037440 transformer_layer.py:472] tlayer: final FFN.
I0123 16:23:19.889721 139735039037440 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.889864 139735039037440 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:23:19.889985 139735039037440 nn_components.py:325] mlp: activation = None
I0123 16:23:19.890032 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:23:19.891968 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.892066 139735039037440 nn_components.py:261] mlp: residual
I0123 16:23:19.894504 139735039037440 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.894592 139735039037440 transformer_base.py:443] tbase: final FFN
I0123 16:23:19.894706 139735039037440 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:23:19.897017 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:23:19.898943 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.899047 139735039037440 nn_components.py:261] mlp: residual
I0123 16:23:19.899345 139735039037440 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.899441 139735039037440 decoder_stack.py:344] dstack: Final layernorm.
I0123 16:23:19.902322 139735039037440 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:23:19.959142 139735039037440 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:19.959243 139735039037440 decoder_stack.py:333] dstack: autoregressive generator.
I0123 16:23:19.959315 139735039037440 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 16:23:19.959439 139735039037440 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:23:19.959483 139735039037440 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:23:19.959516 139735039037440 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:23:19.959582 139735039037440 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:19.962268 139735039037440 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:23:19.967819 139735039037440 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:19.968088 139735039037440 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:23:19.970704 139735039037440 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:23:19.983266 139735039037440 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:23:19.983329 139735039037440 attention.py:418] Single window, no scan.
I0123 16:23:19.983368 139735039037440 transformer_layer.py:389] tlayer: self-attention.
I0123 16:23:19.983401 139735039037440 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:19.983465 139735039037440 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:19.984023 139735039037440 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:19.984103 139735039037440 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:19.984454 139735039037440 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:19.985134 139735039037440 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:19.987616 139735039037440 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:19.988236 139735039037440 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:19.988318 139735039037440 transformer_layer.py:468] tlayer: End windows.
I0123 16:23:19.988355 139735039037440 transformer_layer.py:472] tlayer: final FFN.
I0123 16:23:19.988417 139735039037440 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:19.988550 139735039037440 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:23:19.988692 139735039037440 nn_components.py:325] mlp: activation = None
I0123 16:23:19.988737 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:23:19.990598 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:19.990698 139735039037440 nn_components.py:261] mlp: residual
I0123 16:23:19.993088 139735039037440 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:19.993171 139735039037440 transformer_base.py:443] tbase: final FFN
I0123 16:23:19.993284 139735039037440 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:23:19.995536 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:23:19.997415 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:19.997515 139735039037440 nn_components.py:261] mlp: residual
I0123 16:23:19.997813 139735039037440 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:19.997902 139735039037440 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 16:23:19.998016 139735039037440 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:23:19.998059 139735039037440 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:23:19.998094 139735039037440 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:23:19.998161 139735039037440 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.000401 139735039037440 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:23:20.005814 139735039037440 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.006079 139735039037440 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:23:20.008742 139735039037440 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:23:20.021343 139735039037440 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:23:20.021406 139735039037440 attention.py:418] Single window, no scan.
I0123 16:23:20.021445 139735039037440 transformer_layer.py:389] tlayer: self-attention.
I0123 16:23:20.021479 139735039037440 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.021543 139735039037440 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.022111 139735039037440 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.022195 139735039037440 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.022552 139735039037440 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.023237 139735039037440 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.025784 139735039037440 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.026417 139735039037440 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.026501 139735039037440 transformer_layer.py:468] tlayer: End windows.
I0123 16:23:20.026540 139735039037440 transformer_layer.py:472] tlayer: final FFN.
I0123 16:23:20.026603 139735039037440 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.026739 139735039037440 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:23:20.026857 139735039037440 nn_components.py:325] mlp: activation = None
I0123 16:23:20.026910 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:23:20.028769 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.028869 139735039037440 nn_components.py:261] mlp: residual
I0123 16:23:20.031267 139735039037440 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.031352 139735039037440 transformer_base.py:443] tbase: final FFN
I0123 16:23:20.031466 139735039037440 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:23:20.033733 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:23:20.035587 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.035688 139735039037440 nn_components.py:261] mlp: residual
I0123 16:23:20.035975 139735039037440 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.036062 139735039037440 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 16:23:20.036175 139735039037440 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:23:20.036218 139735039037440 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:23:20.036252 139735039037440 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:23:20.036318 139735039037440 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.038572 139735039037440 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:23:20.043971 139735039037440 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.044237 139735039037440 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:23:20.046915 139735039037440 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:23:20.059365 139735039037440 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:23:20.059428 139735039037440 attention.py:418] Single window, no scan.
I0123 16:23:20.059466 139735039037440 transformer_layer.py:389] tlayer: self-attention.
I0123 16:23:20.059499 139735039037440 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.059565 139735039037440 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.060124 139735039037440 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.060205 139735039037440 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.060561 139735039037440 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.061248 139735039037440 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.063742 139735039037440 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.064365 139735039037440 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.064448 139735039037440 transformer_layer.py:468] tlayer: End windows.
I0123 16:23:20.064486 139735039037440 transformer_layer.py:472] tlayer: final FFN.
I0123 16:23:20.064548 139735039037440 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.064679 139735039037440 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:23:20.064794 139735039037440 nn_components.py:325] mlp: activation = None
I0123 16:23:20.064838 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:23:20.066700 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.066801 139735039037440 nn_components.py:261] mlp: residual
I0123 16:23:20.069204 139735039037440 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.069288 139735039037440 transformer_base.py:443] tbase: final FFN
I0123 16:23:20.069403 139735039037440 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:23:20.072123 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:23:20.074019 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.074121 139735039037440 nn_components.py:261] mlp: residual
I0123 16:23:20.074412 139735039037440 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.074498 139735039037440 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 16:23:20.074612 139735039037440 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:23:20.074655 139735039037440 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:23:20.074688 139735039037440 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:23:20.074755 139735039037440 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.077001 139735039037440 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:23:20.082436 139735039037440 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.082701 139735039037440 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:23:20.085391 139735039037440 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:23:20.098002 139735039037440 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:23:20.098064 139735039037440 attention.py:418] Single window, no scan.
I0123 16:23:20.098104 139735039037440 transformer_layer.py:389] tlayer: self-attention.
I0123 16:23:20.098152 139735039037440 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.098218 139735039037440 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.098783 139735039037440 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.098862 139735039037440 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.099221 139735039037440 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.099914 139735039037440 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.102441 139735039037440 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.103070 139735039037440 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.103150 139735039037440 transformer_layer.py:468] tlayer: End windows.
I0123 16:23:20.103186 139735039037440 transformer_layer.py:472] tlayer: final FFN.
I0123 16:23:20.103247 139735039037440 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.103378 139735039037440 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:23:20.103490 139735039037440 nn_components.py:325] mlp: activation = None
I0123 16:23:20.103537 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:23:20.105456 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.105554 139735039037440 nn_components.py:261] mlp: residual
I0123 16:23:20.107975 139735039037440 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.108059 139735039037440 transformer_base.py:443] tbase: final FFN
I0123 16:23:20.108172 139735039037440 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:23:20.110486 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:23:20.112404 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.112502 139735039037440 nn_components.py:261] mlp: residual
I0123 16:23:20.112787 139735039037440 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.112871 139735039037440 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 16:23:20.112982 139735039037440 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:23:20.113022 139735039037440 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:23:20.113055 139735039037440 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:23:20.113120 139735039037440 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.115387 139735039037440 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:23:20.120787 139735039037440 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.121050 139735039037440 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:23:20.123762 139735039037440 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:23:20.136408 139735039037440 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:23:20.136467 139735039037440 attention.py:418] Single window, no scan.
I0123 16:23:20.136505 139735039037440 transformer_layer.py:389] tlayer: self-attention.
I0123 16:23:20.136539 139735039037440 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.136608 139735039037440 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.137166 139735039037440 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.137246 139735039037440 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.137607 139735039037440 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.138309 139735039037440 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.140828 139735039037440 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.141463 139735039037440 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.141543 139735039037440 transformer_layer.py:468] tlayer: End windows.
I0123 16:23:20.141580 139735039037440 transformer_layer.py:472] tlayer: final FFN.
I0123 16:23:20.141645 139735039037440 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.141779 139735039037440 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:23:20.141896 139735039037440 nn_components.py:325] mlp: activation = None
I0123 16:23:20.141938 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:23:20.143814 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.143918 139735039037440 nn_components.py:261] mlp: residual
I0123 16:23:20.146321 139735039037440 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.146404 139735039037440 transformer_base.py:443] tbase: final FFN
I0123 16:23:20.146516 139735039037440 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:23:20.148799 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:23:20.150670 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.150769 139735039037440 nn_components.py:261] mlp: residual
I0123 16:23:20.151057 139735039037440 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.151142 139735039037440 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 16:23:20.151252 139735039037440 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:23:20.151293 139735039037440 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:23:20.151329 139735039037440 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:23:20.151395 139735039037440 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.153655 139735039037440 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:23:20.159054 139735039037440 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.159320 139735039037440 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:23:20.162021 139735039037440 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:23:20.174689 139735039037440 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:23:20.174747 139735039037440 attention.py:418] Single window, no scan.
I0123 16:23:20.174783 139735039037440 transformer_layer.py:389] tlayer: self-attention.
I0123 16:23:20.174815 139735039037440 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.174877 139735039037440 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.175427 139735039037440 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.175505 139735039037440 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.175864 139735039037440 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.176556 139735039037440 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.179106 139735039037440 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.179731 139735039037440 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.179810 139735039037440 transformer_layer.py:468] tlayer: End windows.
I0123 16:23:20.179848 139735039037440 transformer_layer.py:472] tlayer: final FFN.
I0123 16:23:20.179907 139735039037440 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.180037 139735039037440 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:23:20.180150 139735039037440 nn_components.py:325] mlp: activation = None
I0123 16:23:20.180191 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:23:20.182101 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.182204 139735039037440 nn_components.py:261] mlp: residual
I0123 16:23:20.184595 139735039037440 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.184675 139735039037440 transformer_base.py:443] tbase: final FFN
I0123 16:23:20.184785 139735039037440 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:23:20.187496 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:23:20.189376 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.189474 139735039037440 nn_components.py:261] mlp: residual
I0123 16:23:20.189770 139735039037440 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.189856 139735039037440 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 16:23:20.189970 139735039037440 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:23:20.190010 139735039037440 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:23:20.190041 139735039037440 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:23:20.190105 139735039037440 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.192371 139735039037440 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:23:20.197849 139735039037440 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.198112 139735039037440 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:23:20.200819 139735039037440 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:23:20.213510 139735039037440 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:23:20.213568 139735039037440 attention.py:418] Single window, no scan.
I0123 16:23:20.213605 139735039037440 transformer_layer.py:389] tlayer: self-attention.
I0123 16:23:20.213638 139735039037440 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.213710 139735039037440 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.214271 139735039037440 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.214349 139735039037440 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.214702 139735039037440 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.215397 139735039037440 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.217938 139735039037440 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.218577 139735039037440 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.218657 139735039037440 transformer_layer.py:468] tlayer: End windows.
I0123 16:23:20.218694 139735039037440 transformer_layer.py:472] tlayer: final FFN.
I0123 16:23:20.218754 139735039037440 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.218883 139735039037440 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:23:20.218997 139735039037440 nn_components.py:325] mlp: activation = None
I0123 16:23:20.219039 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:23:20.220935 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.221032 139735039037440 nn_components.py:261] mlp: residual
I0123 16:23:20.223448 139735039037440 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.223531 139735039037440 transformer_base.py:443] tbase: final FFN
I0123 16:23:20.223642 139735039037440 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:23:20.225949 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:23:20.227800 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.227899 139735039037440 nn_components.py:261] mlp: residual
I0123 16:23:20.228187 139735039037440 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.228271 139735039037440 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 16:23:20.228381 139735039037440 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:23:20.228422 139735039037440 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:23:20.228454 139735039037440 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:23:20.228518 139735039037440 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.230757 139735039037440 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:23:20.236167 139735039037440 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.236433 139735039037440 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:23:20.239125 139735039037440 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:23:20.251752 139735039037440 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:23:20.251812 139735039037440 attention.py:418] Single window, no scan.
I0123 16:23:20.251849 139735039037440 transformer_layer.py:389] tlayer: self-attention.
I0123 16:23:20.251882 139735039037440 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.251945 139735039037440 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.252507 139735039037440 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.252585 139735039037440 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.252948 139735039037440 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.253659 139735039037440 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.256185 139735039037440 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.256817 139735039037440 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.256898 139735039037440 transformer_layer.py:468] tlayer: End windows.
I0123 16:23:20.256934 139735039037440 transformer_layer.py:472] tlayer: final FFN.
I0123 16:23:20.256994 139735039037440 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.257125 139735039037440 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:23:20.257238 139735039037440 nn_components.py:325] mlp: activation = None
I0123 16:23:20.257281 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:23:20.259174 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.259273 139735039037440 nn_components.py:261] mlp: residual
I0123 16:23:20.261659 139735039037440 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.261752 139735039037440 transformer_base.py:443] tbase: final FFN
I0123 16:23:20.261867 139735039037440 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:23:20.264163 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:23:20.266040 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.266139 139735039037440 nn_components.py:261] mlp: residual
I0123 16:23:20.266426 139735039037440 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.266511 139735039037440 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 16:23:20.266622 139735039037440 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:23:20.266662 139735039037440 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:23:20.266695 139735039037440 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:23:20.266762 139735039037440 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.268993 139735039037440 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:23:20.274412 139735039037440 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.274677 139735039037440 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:23:20.277370 139735039037440 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:23:20.290058 139735039037440 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:23:20.290118 139735039037440 attention.py:418] Single window, no scan.
I0123 16:23:20.290155 139735039037440 transformer_layer.py:389] tlayer: self-attention.
I0123 16:23:20.290187 139735039037440 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.290251 139735039037440 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.290822 139735039037440 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.290902 139735039037440 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.291270 139735039037440 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.291961 139735039037440 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.294535 139735039037440 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.295161 139735039037440 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.295242 139735039037440 transformer_layer.py:468] tlayer: End windows.
I0123 16:23:20.295278 139735039037440 transformer_layer.py:472] tlayer: final FFN.
I0123 16:23:20.295338 139735039037440 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.295469 139735039037440 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:23:20.295588 139735039037440 nn_components.py:325] mlp: activation = None
I0123 16:23:20.295630 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:23:20.297512 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.297609 139735039037440 nn_components.py:261] mlp: residual
I0123 16:23:20.300040 139735039037440 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.300132 139735039037440 transformer_base.py:443] tbase: final FFN
I0123 16:23:20.300248 139735039037440 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:23:20.302949 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:23:20.304847 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.304945 139735039037440 nn_components.py:261] mlp: residual
I0123 16:23:20.305233 139735039037440 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.305319 139735039037440 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 16:23:20.305432 139735039037440 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:23:20.305472 139735039037440 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:23:20.305505 139735039037440 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:23:20.305569 139735039037440 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.307825 139735039037440 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:23:20.313309 139735039037440 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.313576 139735039037440 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:23:20.316259 139735039037440 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:23:20.328967 139735039037440 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:23:20.329026 139735039037440 attention.py:418] Single window, no scan.
I0123 16:23:20.329063 139735039037440 transformer_layer.py:389] tlayer: self-attention.
I0123 16:23:20.329095 139735039037440 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.329159 139735039037440 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.329735 139735039037440 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.329815 139735039037440 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.330180 139735039037440 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.330867 139735039037440 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.333397 139735039037440 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.334031 139735039037440 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.334113 139735039037440 transformer_layer.py:468] tlayer: End windows.
I0123 16:23:20.334151 139735039037440 transformer_layer.py:472] tlayer: final FFN.
I0123 16:23:20.334209 139735039037440 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.334343 139735039037440 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:23:20.334456 139735039037440 nn_components.py:325] mlp: activation = None
I0123 16:23:20.334497 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:23:20.336842 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.336939 139735039037440 nn_components.py:261] mlp: residual
I0123 16:23:20.339446 139735039037440 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.339529 139735039037440 transformer_base.py:443] tbase: final FFN
I0123 16:23:20.339648 139735039037440 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:23:20.341924 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:23:20.343775 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.343875 139735039037440 nn_components.py:261] mlp: residual
I0123 16:23:20.344163 139735039037440 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.344247 139735039037440 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 16:23:20.344358 139735039037440 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:23:20.344399 139735039037440 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:23:20.344431 139735039037440 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:23:20.344494 139735039037440 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.346742 139735039037440 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:23:20.352182 139735039037440 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.352449 139735039037440 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:23:20.355144 139735039037440 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:23:20.367851 139735039037440 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:23:20.367910 139735039037440 attention.py:418] Single window, no scan.
I0123 16:23:20.367955 139735039037440 transformer_layer.py:389] tlayer: self-attention.
I0123 16:23:20.367989 139735039037440 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.368054 139735039037440 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.368621 139735039037440 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.368699 139735039037440 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.369060 139735039037440 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.369764 139735039037440 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.372311 139735039037440 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.372938 139735039037440 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.373017 139735039037440 transformer_layer.py:468] tlayer: End windows.
I0123 16:23:20.373053 139735039037440 transformer_layer.py:472] tlayer: final FFN.
I0123 16:23:20.373111 139735039037440 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.373241 139735039037440 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:23:20.373354 139735039037440 nn_components.py:325] mlp: activation = None
I0123 16:23:20.373395 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:23:20.375313 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.375410 139735039037440 nn_components.py:261] mlp: residual
I0123 16:23:20.377819 139735039037440 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.377900 139735039037440 transformer_base.py:443] tbase: final FFN
I0123 16:23:20.378010 139735039037440 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:23:20.380308 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:23:20.382165 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.382265 139735039037440 nn_components.py:261] mlp: residual
I0123 16:23:20.382548 139735039037440 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.382632 139735039037440 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 16:23:20.382742 139735039037440 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:23:20.382783 139735039037440 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:23:20.382815 139735039037440 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:23:20.382879 139735039037440 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.385106 139735039037440 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:23:20.390602 139735039037440 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.390867 139735039037440 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:23:20.393564 139735039037440 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:23:20.406246 139735039037440 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:23:20.406306 139735039037440 attention.py:418] Single window, no scan.
I0123 16:23:20.406342 139735039037440 transformer_layer.py:389] tlayer: self-attention.
I0123 16:23:20.406373 139735039037440 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.406436 139735039037440 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.406999 139735039037440 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.407079 139735039037440 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.407433 139735039037440 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.408137 139735039037440 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.410685 139735039037440 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.411312 139735039037440 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.411393 139735039037440 transformer_layer.py:468] tlayer: End windows.
I0123 16:23:20.411429 139735039037440 transformer_layer.py:472] tlayer: final FFN.
I0123 16:23:20.411488 139735039037440 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.411624 139735039037440 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:23:20.411738 139735039037440 nn_components.py:325] mlp: activation = None
I0123 16:23:20.411781 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:23:20.413666 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.413763 139735039037440 nn_components.py:261] mlp: residual
I0123 16:23:20.416166 139735039037440 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.416248 139735039037440 transformer_base.py:443] tbase: final FFN
I0123 16:23:20.416363 139735039037440 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:23:20.419064 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:23:20.420936 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.421034 139735039037440 nn_components.py:261] mlp: residual
I0123 16:23:20.421322 139735039037440 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:20.421412 139735039037440 decoder_stack.py:344] dstack: Final layernorm.
I0123 16:23:20.424256 139735039037440 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:23:24.863369 139735039037440 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
I0123 16:23:25.351855 139735039037440 training_loop.py:409] No working directory specified.
I0123 16:23:25.351982 139735039037440 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0123 16:23:25.352751 139735039037440 checkpoints.py:1062] Restoring legacy Flax checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0123 16:23:28.560470 139735039037440 training_loop.py:447] Only restoring trainable parameters.
I0123 16:23:28.561084 139735039037440 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0123 16:23:28.561165 139735039037440 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0123 16:23:28.561216 139735039037440 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 16:23:28.561261 139735039037440 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 16:23:28.561304 139735039037440 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:23:28.561344 139735039037440 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 16:23:28.561384 139735039037440 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:23:28.561422 139735039037440 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:23:28.561461 139735039037440 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 16:23:28.561501 139735039037440 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 16:23:28.561540 139735039037440 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:23:28.561578 139735039037440 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 16:23:28.561617 139735039037440 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 16:23:28.562168 139735039037440 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 16:23:28.562222 139735039037440 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:23:28.562263 139735039037440 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 16:23:28.562303 139735039037440 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:23:28.562343 139735039037440 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:23:28.562382 139735039037440 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 16:23:28.562421 139735039037440 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 16:23:28.562478 139735039037440 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:23:28.562521 139735039037440 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 16:23:28.562561 139735039037440 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 16:23:28.562601 139735039037440 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 16:23:28.562641 139735039037440 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:23:28.562680 139735039037440 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 16:23:28.562719 139735039037440 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:23:28.562759 139735039037440 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:23:28.562800 139735039037440 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 16:23:28.562839 139735039037440 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 16:23:28.562877 139735039037440 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:23:28.562917 139735039037440 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 16:23:28.562956 139735039037440 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 16:23:28.562994 139735039037440 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 16:23:28.563034 139735039037440 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:23:28.563072 139735039037440 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 16:23:28.563111 139735039037440 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:23:28.563150 139735039037440 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:23:28.563188 139735039037440 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 16:23:28.563225 139735039037440 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 16:23:28.563262 139735039037440 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:23:28.563300 139735039037440 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 16:23:28.563336 139735039037440 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 16:23:28.563372 139735039037440 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 16:23:28.563408 139735039037440 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:23:28.563445 139735039037440 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 16:23:28.563488 139735039037440 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:23:28.563526 139735039037440 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:23:28.563562 139735039037440 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 16:23:28.563598 139735039037440 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 16:23:28.563634 139735039037440 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:23:28.563670 139735039037440 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 16:23:28.563706 139735039037440 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 16:23:28.563743 139735039037440 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 16:23:28.563779 139735039037440 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:23:28.563815 139735039037440 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 16:23:28.563851 139735039037440 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:23:28.563887 139735039037440 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:23:28.563923 139735039037440 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 16:23:28.563959 139735039037440 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 16:23:28.563995 139735039037440 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:23:28.564031 139735039037440 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 16:23:28.564068 139735039037440 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 16:23:28.564105 139735039037440 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 16:23:28.564141 139735039037440 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:23:28.564177 139735039037440 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 16:23:28.564213 139735039037440 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:23:28.564249 139735039037440 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:23:28.564285 139735039037440 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 16:23:28.564322 139735039037440 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 16:23:28.564357 139735039037440 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:23:28.564393 139735039037440 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 16:23:28.564429 139735039037440 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 16:23:28.564472 139735039037440 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 16:23:28.564511 139735039037440 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:23:28.564547 139735039037440 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 16:23:28.564583 139735039037440 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:23:28.564620 139735039037440 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:23:28.564656 139735039037440 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 16:23:28.564692 139735039037440 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 16:23:28.564729 139735039037440 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:23:28.564765 139735039037440 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 16:23:28.564801 139735039037440 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 16:23:28.564837 139735039037440 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 16:23:28.564872 139735039037440 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:23:28.564909 139735039037440 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 16:23:28.564945 139735039037440 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:23:28.564981 139735039037440 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:23:28.565016 139735039037440 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 16:23:28.565052 139735039037440 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 16:23:28.565088 139735039037440 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:23:28.565124 139735039037440 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 16:23:28.565162 139735039037440 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 16:23:28.565199 139735039037440 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 16:23:28.565235 139735039037440 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:23:28.565271 139735039037440 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 16:23:28.565307 139735039037440 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:23:28.565343 139735039037440 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:23:28.565379 139735039037440 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 16:23:28.565415 139735039037440 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 16:23:28.565456 139735039037440 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:23:28.565494 139735039037440 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 16:23:28.565531 139735039037440 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 16:23:28.565568 139735039037440 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 16:23:28.565605 139735039037440 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:23:28.565649 139735039037440 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 16:23:28.565689 139735039037440 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:23:28.565730 139735039037440 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:23:28.565767 139735039037440 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 16:23:28.565803 139735039037440 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 16:23:28.565838 139735039037440 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:23:28.565875 139735039037440 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 16:23:28.565912 139735039037440 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 16:23:28.565949 139735039037440 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 16:23:28.565987 139735039037440 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:23:28.566025 139735039037440 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 16:23:28.566063 139735039037440 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:23:28.566101 139735039037440 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:23:28.566138 139735039037440 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 16:23:28.566175 139735039037440 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 16:23:28.566213 139735039037440 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:23:28.566251 139735039037440 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 16:23:28.566281 139735039037440 training_loop.py:725] Total parameters: 152072288
I0123 16:23:28.566496 139735039037440 training_loop.py:739] Total state size: 0
I0123 16:23:28.590695 139735039037440 training_loop.py:492] Training loop: creating task for mode beam_search
I0123 16:23:28.590910 139735039037440 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0123 16:23:28.591309 139735039037440 training_loop.py:652] Compiling mode beam_search with jit.
I0123 16:23:28.591629 139735039037440 training_loop.py:89] registering functions: dict_keys([])
I0123 16:23:28.607927 139735039037440 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l ? eqratio b d h d m d b d
I0123 16:23:29.703624 139735039037440 ddar.py:60] Depth 1/1000 time = 1.0618305206298828
I0123 16:23:32.370053 139735039037440 ddar.py:60] Depth 2/1000 time = 2.6662449836730957
I0123 16:23:36.594155 139735039037440 ddar.py:60] Depth 3/1000 time = 4.223915100097656
I0123 16:23:41.279998 139735039037440 ddar.py:60] Depth 4/1000 time = 4.685611963272095
I0123 16:23:45.979067 139735039037440 ddar.py:60] Depth 5/1000 time = 4.698800086975098
I0123 16:23:50.678961 139735039037440 ddar.py:60] Depth 6/1000 time = 4.699434041976929
I0123 16:23:56.065299 139735039037440 ddar.py:60] Depth 7/1000 time = 5.378383636474609
I0123 16:24:01.726173 139735039037440 ddar.py:60] Depth 8/1000 time = 5.660651683807373
I0123 16:24:07.809566 139735039037440 ddar.py:60] Depth 9/1000 time = 6.025851726531982
I0123 16:24:14.481403 139735039037440 ddar.py:60] Depth 10/1000 time = 6.671534776687622
I0123 16:24:21.587758 139735039037440 ddar.py:60] Depth 11/1000 time = 7.1060261726379395
I0123 16:24:28.787749 139735039037440 ddar.py:60] Depth 12/1000 time = 7.199741363525391
I0123 16:24:36.277222 139735039037440 ddar.py:60] Depth 13/1000 time = 7.489126682281494
I0123 16:24:43.728906 139735039037440 ddar.py:60] Depth 14/1000 time = 7.410118341445923
I0123 16:24:52.428773 139735039037440 ddar.py:60] Depth 15/1000 time = 8.69959831237793
I0123 16:25:01.334284 139735039037440 ddar.py:60] Depth 16/1000 time = 8.905248641967773
I0123 16:25:10.354573 139735039037440 ddar.py:60] Depth 17/1000 time = 9.020009994506836
I0123 16:25:19.465858 139735039037440 ddar.py:60] Depth 18/1000 time = 9.066384315490723
I0123 16:25:19.466223 139735039037440 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 16:25:19.466323 139735039037440 alphageometry.py:540] Depth 0. There are 1 nodes to expand:
I0123 16:25:19.466361 139735039037440 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b c e 02 T a e b c 03 ; f : C a c f 04 T a c b f 05 ; g : C a b g 06 T a b c g 07 ; h : C a e h 08 C b f h 09 ; i : C f g i 10 T e i f g 11 ; j : C e i j 12 D e i i j 13 ; k : C e g k 14 T e g f k 15 ; l : C f k l 16 D f k k l 17 ; m : C a j m 18 C b l m 19 ? / d b d h d m d b {F1} x00
I0123 16:25:19.466397 139735039037440 alphageometry.py:549] Decoding from {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b c e 02 T a e b c 03 ; f : C a c f 04 T a c b f 05 ; g : C a b g 06 T a b c g 07 ; h : C a e h 08 C b f h 09 ; i : C f g i 10 T e i f g 11 ; j : C e i j 12 D e i i j 13 ; k : C e g k 14 T e g f k 15 ; l : C f k l 16 D f k k l 17 ; m : C a j m 18 C b l m 19 ? / d b d h d m d b {F1} x00
I0123 16:25:19.606032 139735039037440 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.606214 139735039037440 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 16:25:19.606315 139735039037440 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 16:25:19.606390 139735039037440 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 16:25:19.606462 139735039037440 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 16:25:19.606535 139735039037440 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 16:25:19.606604 139735039037440 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 16:25:19.606672 139735039037440 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 16:25:19.606739 139735039037440 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 16:25:19.606806 139735039037440 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 16:25:19.606892 139735039037440 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 16:25:19.606966 139735039037440 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 16:25:19.607037 139735039037440 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 16:25:19.607107 139735039037440 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 16:25:19.607147 139735039037440 attention.py:418] Single window, no scan.
I0123 16:25:19.607194 139735039037440 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 16:25:19.607309 139735039037440 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:25:19.607351 139735039037440 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:25:19.607383 139735039037440 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:25:19.609343 139735039037440 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.611861 139735039037440 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:25:19.617482 139735039037440 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.617768 139735039037440 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:25:19.620382 139735039037440 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:25:19.624210 139735039037440 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:25:19.624271 139735039037440 attention.py:418] Single window, no scan.
I0123 16:25:19.624310 139735039037440 transformer_layer.py:389] tlayer: self-attention.
I0123 16:25:19.624345 139735039037440 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.624411 139735039037440 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.625066 139735039037440 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.625146 139735039037440 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.625508 139735039037440 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.626289 139735039037440 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.628820 139735039037440 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.629444 139735039037440 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.629524 139735039037440 transformer_layer.py:468] tlayer: End windows.
I0123 16:25:19.629560 139735039037440 transformer_layer.py:472] tlayer: final FFN.
I0123 16:25:19.629622 139735039037440 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.629767 139735039037440 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:25:19.630094 139735039037440 nn_components.py:325] mlp: activation = None
I0123 16:25:19.630141 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:25:19.632474 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.632572 139735039037440 nn_components.py:261] mlp: residual
I0123 16:25:19.635035 139735039037440 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.635118 139735039037440 transformer_base.py:443] tbase: final FFN
I0123 16:25:19.635544 139735039037440 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:25:19.637857 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:25:19.639750 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.639845 139735039037440 nn_components.py:261] mlp: residual
I0123 16:25:19.640133 139735039037440 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.640218 139735039037440 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 16:25:19.640326 139735039037440 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:25:19.640366 139735039037440 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:25:19.640399 139735039037440 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:25:19.642255 139735039037440 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.644548 139735039037440 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:25:19.650245 139735039037440 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.650664 139735039037440 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:25:19.653266 139735039037440 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:25:19.656914 139735039037440 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:25:19.656973 139735039037440 attention.py:418] Single window, no scan.
I0123 16:25:19.657010 139735039037440 transformer_layer.py:389] tlayer: self-attention.
I0123 16:25:19.657042 139735039037440 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.657104 139735039037440 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.657658 139735039037440 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.657737 139735039037440 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.658091 139735039037440 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.658852 139735039037440 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.661276 139735039037440 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.661970 139735039037440 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.662053 139735039037440 transformer_layer.py:468] tlayer: End windows.
I0123 16:25:19.662090 139735039037440 transformer_layer.py:472] tlayer: final FFN.
I0123 16:25:19.662150 139735039037440 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.662285 139735039037440 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:25:19.662605 139735039037440 nn_components.py:325] mlp: activation = None
I0123 16:25:19.662651 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:25:19.664547 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.664643 139735039037440 nn_components.py:261] mlp: residual
I0123 16:25:19.667047 139735039037440 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.667129 139735039037440 transformer_base.py:443] tbase: final FFN
I0123 16:25:19.667550 139735039037440 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:25:19.669846 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:25:19.671764 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.671861 139735039037440 nn_components.py:261] mlp: residual
I0123 16:25:19.672145 139735039037440 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.672229 139735039037440 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 16:25:19.672338 139735039037440 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:25:19.672377 139735039037440 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:25:19.672409 139735039037440 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:25:19.674196 139735039037440 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.676482 139735039037440 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:25:19.682145 139735039037440 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.682409 139735039037440 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:25:19.684951 139735039037440 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:25:19.688599 139735039037440 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:25:19.688658 139735039037440 attention.py:418] Single window, no scan.
I0123 16:25:19.688696 139735039037440 transformer_layer.py:389] tlayer: self-attention.
I0123 16:25:19.688729 139735039037440 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.688792 139735039037440 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.689404 139735039037440 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.689485 139735039037440 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.689852 139735039037440 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.690613 139735039037440 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.693051 139735039037440 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.693676 139735039037440 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.693758 139735039037440 transformer_layer.py:468] tlayer: End windows.
I0123 16:25:19.693793 139735039037440 transformer_layer.py:472] tlayer: final FFN.
I0123 16:25:19.693851 139735039037440 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.693979 139735039037440 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:25:19.694297 139735039037440 nn_components.py:325] mlp: activation = None
I0123 16:25:19.694343 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:25:19.696316 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.696412 139735039037440 nn_components.py:261] mlp: residual
I0123 16:25:19.698861 139735039037440 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.698944 139735039037440 transformer_base.py:443] tbase: final FFN
I0123 16:25:19.699370 139735039037440 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:25:19.701764 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:25:19.703649 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.703752 139735039037440 nn_components.py:261] mlp: residual
I0123 16:25:19.704042 139735039037440 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.704313 139735039037440 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 16:25:19.704423 139735039037440 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:25:19.704464 139735039037440 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:25:19.704496 139735039037440 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:25:19.706359 139735039037440 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.708641 139735039037440 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:25:19.714179 139735039037440 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.714438 139735039037440 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:25:19.716956 139735039037440 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:25:19.720692 139735039037440 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:25:19.720752 139735039037440 attention.py:418] Single window, no scan.
I0123 16:25:19.720789 139735039037440 transformer_layer.py:389] tlayer: self-attention.
I0123 16:25:19.720822 139735039037440 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.720885 139735039037440 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.721443 139735039037440 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.721523 139735039037440 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.721888 139735039037440 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.722668 139735039037440 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.725084 139735039037440 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.725712 139735039037440 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.725794 139735039037440 transformer_layer.py:468] tlayer: End windows.
I0123 16:25:19.725830 139735039037440 transformer_layer.py:472] tlayer: final FFN.
I0123 16:25:19.725889 139735039037440 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.726022 139735039037440 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:25:19.726389 139735039037440 nn_components.py:325] mlp: activation = None
I0123 16:25:19.726436 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:25:19.728319 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.728414 139735039037440 nn_components.py:261] mlp: residual
I0123 16:25:19.730817 139735039037440 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.730900 139735039037440 transformer_base.py:443] tbase: final FFN
I0123 16:25:19.731328 139735039037440 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:25:19.733578 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:25:19.735568 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.735666 139735039037440 nn_components.py:261] mlp: residual
I0123 16:25:19.735966 139735039037440 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.736052 139735039037440 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 16:25:19.736162 139735039037440 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:25:19.736203 139735039037440 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:25:19.736235 139735039037440 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:25:19.738023 139735039037440 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.740335 139735039037440 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:25:19.746373 139735039037440 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.746634 139735039037440 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:25:19.749181 139735039037440 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:25:19.752856 139735039037440 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:25:19.752915 139735039037440 attention.py:418] Single window, no scan.
I0123 16:25:19.752953 139735039037440 transformer_layer.py:389] tlayer: self-attention.
I0123 16:25:19.752985 139735039037440 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.753049 139735039037440 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.753672 139735039037440 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.753754 139735039037440 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.754112 139735039037440 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.754889 139735039037440 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.757328 139735039037440 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.757956 139735039037440 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.758038 139735039037440 transformer_layer.py:468] tlayer: End windows.
I0123 16:25:19.758074 139735039037440 transformer_layer.py:472] tlayer: final FFN.
I0123 16:25:19.758135 139735039037440 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.758268 139735039037440 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:25:19.758587 139735039037440 nn_components.py:325] mlp: activation = None
I0123 16:25:19.758634 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:25:19.760596 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.760693 139735039037440 nn_components.py:261] mlp: residual
I0123 16:25:19.763138 139735039037440 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.763221 139735039037440 transformer_base.py:443] tbase: final FFN
I0123 16:25:19.763648 139735039037440 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:25:19.765901 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:25:19.767799 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.767897 139735039037440 nn_components.py:261] mlp: residual
I0123 16:25:19.768185 139735039037440 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.768280 139735039037440 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 16:25:19.768394 139735039037440 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:25:19.768435 139735039037440 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:25:19.768467 139735039037440 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:25:19.770337 139735039037440 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.772627 139735039037440 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:25:19.778181 139735039037440 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.778449 139735039037440 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:25:19.780995 139735039037440 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:25:19.784700 139735039037440 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:25:19.784759 139735039037440 attention.py:418] Single window, no scan.
I0123 16:25:19.784795 139735039037440 transformer_layer.py:389] tlayer: self-attention.
I0123 16:25:19.784828 139735039037440 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.784890 139735039037440 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.785443 139735039037440 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.785522 139735039037440 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.785881 139735039037440 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.786640 139735039037440 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.789071 139735039037440 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.789702 139735039037440 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.789783 139735039037440 transformer_layer.py:468] tlayer: End windows.
I0123 16:25:19.789820 139735039037440 transformer_layer.py:472] tlayer: final FFN.
I0123 16:25:19.789881 139735039037440 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.790032 139735039037440 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:25:19.790406 139735039037440 nn_components.py:325] mlp: activation = None
I0123 16:25:19.790453 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:25:19.792366 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.792462 139735039037440 nn_components.py:261] mlp: residual
I0123 16:25:19.794937 139735039037440 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.795021 139735039037440 transformer_base.py:443] tbase: final FFN
I0123 16:25:19.795449 139735039037440 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:25:19.797700 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:25:19.799678 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.799776 139735039037440 nn_components.py:261] mlp: residual
I0123 16:25:19.800065 139735039037440 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.800159 139735039037440 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 16:25:19.800274 139735039037440 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:25:19.800315 139735039037440 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:25:19.800347 139735039037440 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:25:19.802139 139735039037440 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.804573 139735039037440 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:25:19.810227 139735039037440 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.810490 139735039037440 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:25:19.813027 139735039037440 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:25:19.816676 139735039037440 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:25:19.816734 139735039037440 attention.py:418] Single window, no scan.
I0123 16:25:19.816770 139735039037440 transformer_layer.py:389] tlayer: self-attention.
I0123 16:25:19.816802 139735039037440 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.816865 139735039037440 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.817475 139735039037440 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.817554 139735039037440 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.817915 139735039037440 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.818670 139735039037440 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.821086 139735039037440 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.821714 139735039037440 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.821796 139735039037440 transformer_layer.py:468] tlayer: End windows.
I0123 16:25:19.821833 139735039037440 transformer_layer.py:472] tlayer: final FFN.
I0123 16:25:19.821894 139735039037440 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.822029 139735039037440 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:25:19.822349 139735039037440 nn_components.py:325] mlp: activation = None
I0123 16:25:19.822396 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:25:19.824366 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.824463 139735039037440 nn_components.py:261] mlp: residual
I0123 16:25:19.826909 139735039037440 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.826992 139735039037440 transformer_base.py:443] tbase: final FFN
I0123 16:25:19.827413 139735039037440 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:25:19.829657 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:25:19.831539 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.831635 139735039037440 nn_components.py:261] mlp: residual
I0123 16:25:19.831921 139735039037440 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.832006 139735039037440 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 16:25:19.832123 139735039037440 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:25:19.832165 139735039037440 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:25:19.832196 139735039037440 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:25:19.834064 139735039037440 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.836346 139735039037440 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:25:19.841851 139735039037440 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.842111 139735039037440 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:25:19.844624 139735039037440 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:25:19.848320 139735039037440 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:25:19.848379 139735039037440 attention.py:418] Single window, no scan.
I0123 16:25:19.848414 139735039037440 transformer_layer.py:389] tlayer: self-attention.
I0123 16:25:19.848446 139735039037440 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.848509 139735039037440 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.849071 139735039037440 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.849152 139735039037440 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.849509 139735039037440 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.850273 139735039037440 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.852720 139735039037440 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.853336 139735039037440 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.853417 139735039037440 transformer_layer.py:468] tlayer: End windows.
I0123 16:25:19.853453 139735039037440 transformer_layer.py:472] tlayer: final FFN.
I0123 16:25:19.853511 139735039037440 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.853650 139735039037440 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:25:19.854377 139735039037440 nn_components.py:325] mlp: activation = None
I0123 16:25:19.854426 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:25:19.856336 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.856431 139735039037440 nn_components.py:261] mlp: residual
I0123 16:25:19.858854 139735039037440 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.858937 139735039037440 transformer_base.py:443] tbase: final FFN
I0123 16:25:19.859375 139735039037440 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:25:19.861727 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:25:19.863774 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.863870 139735039037440 nn_components.py:261] mlp: residual
I0123 16:25:19.864158 139735039037440 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.864243 139735039037440 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 16:25:19.864351 139735039037440 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:25:19.864400 139735039037440 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:25:19.864434 139735039037440 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:25:19.866211 139735039037440 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.868558 139735039037440 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:25:19.874176 139735039037440 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.874449 139735039037440 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:25:19.877042 139735039037440 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:25:19.880768 139735039037440 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:25:19.880828 139735039037440 attention.py:418] Single window, no scan.
I0123 16:25:19.880864 139735039037440 transformer_layer.py:389] tlayer: self-attention.
I0123 16:25:19.880896 139735039037440 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.880959 139735039037440 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.881573 139735039037440 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.881663 139735039037440 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.882027 139735039037440 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.882827 139735039037440 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.885332 139735039037440 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.885979 139735039037440 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.886064 139735039037440 transformer_layer.py:468] tlayer: End windows.
I0123 16:25:19.886102 139735039037440 transformer_layer.py:472] tlayer: final FFN.
I0123 16:25:19.886164 139735039037440 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.886455 139735039037440 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:25:19.886793 139735039037440 nn_components.py:325] mlp: activation = None
I0123 16:25:19.886840 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:25:19.888765 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.888860 139735039037440 nn_components.py:261] mlp: residual
I0123 16:25:19.891395 139735039037440 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.891481 139735039037440 transformer_base.py:443] tbase: final FFN
I0123 16:25:19.891921 139735039037440 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:25:19.894148 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:25:19.896093 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.896190 139735039037440 nn_components.py:261] mlp: residual
I0123 16:25:19.896474 139735039037440 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.896559 139735039037440 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 16:25:19.896670 139735039037440 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:25:19.896712 139735039037440 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:25:19.896754 139735039037440 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:25:19.898566 139735039037440 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.901020 139735039037440 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:25:19.906599 139735039037440 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.906870 139735039037440 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:25:19.909417 139735039037440 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:25:19.913102 139735039037440 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:25:19.913159 139735039037440 attention.py:418] Single window, no scan.
I0123 16:25:19.913196 139735039037440 transformer_layer.py:389] tlayer: self-attention.
I0123 16:25:19.913228 139735039037440 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.913290 139735039037440 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.913903 139735039037440 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.913984 139735039037440 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.914345 139735039037440 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.915145 139735039037440 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.917582 139735039037440 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.918202 139735039037440 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.918283 139735039037440 transformer_layer.py:468] tlayer: End windows.
I0123 16:25:19.918320 139735039037440 transformer_layer.py:472] tlayer: final FFN.
I0123 16:25:19.918382 139735039037440 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.918517 139735039037440 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:25:19.918853 139735039037440 nn_components.py:325] mlp: activation = None
I0123 16:25:19.918900 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:25:19.920825 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.920920 139735039037440 nn_components.py:261] mlp: residual
I0123 16:25:19.923460 139735039037440 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.923546 139735039037440 transformer_base.py:443] tbase: final FFN
I0123 16:25:19.924014 139735039037440 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:25:19.926291 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:25:19.928272 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.928370 139735039037440 nn_components.py:261] mlp: residual
I0123 16:25:19.928660 139735039037440 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.928746 139735039037440 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 16:25:19.928856 139735039037440 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:25:19.928896 139735039037440 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:25:19.928928 139735039037440 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:25:19.930736 139735039037440 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.933155 139735039037440 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:25:19.938753 139735039037440 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.939013 139735039037440 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:25:19.941542 139735039037440 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:25:19.945189 139735039037440 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:25:19.945248 139735039037440 attention.py:418] Single window, no scan.
I0123 16:25:19.945285 139735039037440 transformer_layer.py:389] tlayer: self-attention.
I0123 16:25:19.945318 139735039037440 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.945381 139735039037440 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.946006 139735039037440 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.946086 139735039037440 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.946441 139735039037440 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.947205 139735039037440 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.949662 139735039037440 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.950281 139735039037440 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.950362 139735039037440 transformer_layer.py:468] tlayer: End windows.
I0123 16:25:19.950398 139735039037440 transformer_layer.py:472] tlayer: final FFN.
I0123 16:25:19.950458 139735039037440 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.950589 139735039037440 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:25:19.950905 139735039037440 nn_components.py:325] mlp: activation = None
I0123 16:25:19.950951 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:25:19.952832 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.952928 139735039037440 nn_components.py:261] mlp: residual
I0123 16:25:19.955477 139735039037440 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.955563 139735039037440 transformer_base.py:443] tbase: final FFN
I0123 16:25:19.956001 139735039037440 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:25:19.958242 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:25:19.960140 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.960237 139735039037440 nn_components.py:261] mlp: residual
I0123 16:25:19.960529 139735039037440 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.960614 139735039037440 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 16:25:19.960724 139735039037440 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:25:19.960765 139735039037440 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:25:19.960796 139735039037440 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:25:19.962590 139735039037440 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.964989 139735039037440 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:25:19.970541 139735039037440 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.970804 139735039037440 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:25:19.973337 139735039037440 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:25:19.976969 139735039037440 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:25:19.977027 139735039037440 attention.py:418] Single window, no scan.
I0123 16:25:19.977063 139735039037440 transformer_layer.py:389] tlayer: self-attention.
I0123 16:25:19.977094 139735039037440 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.977584 139735039037440 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.978159 139735039037440 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.978240 139735039037440 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.978600 139735039037440 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.979365 139735039037440 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.981837 139735039037440 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.982458 139735039037440 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.982539 139735039037440 transformer_layer.py:468] tlayer: End windows.
I0123 16:25:19.982575 139735039037440 transformer_layer.py:472] tlayer: final FFN.
I0123 16:25:19.982635 139735039037440 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.982767 139735039037440 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:25:19.983086 139735039037440 nn_components.py:325] mlp: activation = None
I0123 16:25:19.983132 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:25:19.985015 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.985111 139735039037440 nn_components.py:261] mlp: residual
I0123 16:25:19.987939 139735039037440 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.988021 139735039037440 transformer_base.py:443] tbase: final FFN
I0123 16:25:19.988449 139735039037440 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:25:19.990672 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:25:19.992576 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.992674 139735039037440 nn_components.py:261] mlp: residual
I0123 16:25:19.992963 139735039037440 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:19.993213 139735039037440 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 16:25:19.993284 139735039037440 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 16:25:19.993342 139735039037440 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 16:25:19.993399 139735039037440 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 16:25:19.993456 139735039037440 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 16:25:19.993522 139735039037440 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 16:25:19.993578 139735039037440 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 16:25:19.993632 139735039037440 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 16:25:19.993693 139735039037440 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 16:25:19.993747 139735039037440 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 16:25:19.993799 139735039037440 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 16:25:19.993851 139735039037440 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 16:25:19.993888 139735039037440 decoder_stack.py:344] dstack: Final layernorm.
I0123 16:25:19.996773 139735039037440 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:25:20.040832 139735039037440 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.040920 139735039037440 decoder_stack.py:333] dstack: autoregressive generator.
I0123 16:25:20.040975 139735039037440 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 16:25:20.041081 139735039037440 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:25:20.041121 139735039037440 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:25:20.041153 139735039037440 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:25:20.041214 139735039037440 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.043874 139735039037440 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:25:20.049216 139735039037440 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.049481 139735039037440 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:25:20.052021 139735039037440 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:25:20.064783 139735039037440 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:25:20.064842 139735039037440 attention.py:418] Single window, no scan.
I0123 16:25:20.064877 139735039037440 transformer_layer.py:389] tlayer: self-attention.
I0123 16:25:20.064908 139735039037440 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.064970 139735039037440 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.065531 139735039037440 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.065609 139735039037440 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.065977 139735039037440 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.066665 139735039037440 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.069141 139735039037440 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.069767 139735039037440 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.069848 139735039037440 transformer_layer.py:468] tlayer: End windows.
I0123 16:25:20.069883 139735039037440 transformer_layer.py:472] tlayer: final FFN.
I0123 16:25:20.069941 139735039037440 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.070082 139735039037440 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:25:20.070196 139735039037440 nn_components.py:325] mlp: activation = None
I0123 16:25:20.070235 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:25:20.072047 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.072142 139735039037440 nn_components.py:261] mlp: residual
I0123 16:25:20.074507 139735039037440 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.074589 139735039037440 transformer_base.py:443] tbase: final FFN
I0123 16:25:20.074702 139735039037440 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:25:20.076929 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:25:20.078764 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.078862 139735039037440 nn_components.py:261] mlp: residual
I0123 16:25:20.079149 139735039037440 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.079232 139735039037440 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 16:25:20.079340 139735039037440 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:25:20.079380 139735039037440 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:25:20.079410 139735039037440 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:25:20.079471 139735039037440 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.081660 139735039037440 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:25:20.086951 139735039037440 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.087396 139735039037440 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:25:20.089996 139735039037440 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:25:20.102378 139735039037440 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:25:20.102436 139735039037440 attention.py:418] Single window, no scan.
I0123 16:25:20.102471 139735039037440 transformer_layer.py:389] tlayer: self-attention.
I0123 16:25:20.102501 139735039037440 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.102562 139735039037440 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.103110 139735039037440 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.103188 139735039037440 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.103533 139735039037440 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.104246 139735039037440 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.106628 139735039037440 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.107236 139735039037440 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.107314 139735039037440 transformer_layer.py:468] tlayer: End windows.
I0123 16:25:20.107348 139735039037440 transformer_layer.py:472] tlayer: final FFN.
I0123 16:25:20.107404 139735039037440 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.107533 139735039037440 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:25:20.107651 139735039037440 nn_components.py:325] mlp: activation = None
I0123 16:25:20.107692 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:25:20.109516 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.109611 139735039037440 nn_components.py:261] mlp: residual
I0123 16:25:20.111952 139735039037440 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.112035 139735039037440 transformer_base.py:443] tbase: final FFN
I0123 16:25:20.112145 139735039037440 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:25:20.114374 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:25:20.116194 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.116292 139735039037440 nn_components.py:261] mlp: residual
I0123 16:25:20.116580 139735039037440 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.116664 139735039037440 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 16:25:20.116774 139735039037440 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:25:20.116813 139735039037440 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:25:20.116843 139735039037440 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:25:20.116904 139735039037440 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.119103 139735039037440 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:25:20.124390 139735039037440 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.124650 139735039037440 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:25:20.127237 139735039037440 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:25:20.139479 139735039037440 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:25:20.139537 139735039037440 attention.py:418] Single window, no scan.
I0123 16:25:20.139573 139735039037440 transformer_layer.py:389] tlayer: self-attention.
I0123 16:25:20.139602 139735039037440 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.139663 139735039037440 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.140212 139735039037440 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.140290 139735039037440 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.140641 139735039037440 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.141357 139735039037440 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.143768 139735039037440 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.144375 139735039037440 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.144454 139735039037440 transformer_layer.py:468] tlayer: End windows.
I0123 16:25:20.144489 139735039037440 transformer_layer.py:472] tlayer: final FFN.
I0123 16:25:20.144546 139735039037440 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.144675 139735039037440 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:25:20.144785 139735039037440 nn_components.py:325] mlp: activation = None
I0123 16:25:20.144833 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:25:20.146653 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.146749 139735039037440 nn_components.py:261] mlp: residual
I0123 16:25:20.149103 139735039037440 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.149184 139735039037440 transformer_base.py:443] tbase: final FFN
I0123 16:25:20.149292 139735039037440 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:25:20.151923 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:25:20.153749 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.153847 139735039037440 nn_components.py:261] mlp: residual
I0123 16:25:20.154135 139735039037440 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.154220 139735039037440 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 16:25:20.154330 139735039037440 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:25:20.154369 139735039037440 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:25:20.154399 139735039037440 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:25:20.154463 139735039037440 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.156644 139735039037440 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:25:20.161909 139735039037440 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.162166 139735039037440 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:25:20.164770 139735039037440 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:25:20.177008 139735039037440 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:25:20.177067 139735039037440 attention.py:418] Single window, no scan.
I0123 16:25:20.177103 139735039037440 transformer_layer.py:389] tlayer: self-attention.
I0123 16:25:20.177134 139735039037440 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.177197 139735039037440 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.177760 139735039037440 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.177840 139735039037440 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.178196 139735039037440 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.178936 139735039037440 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.181361 139735039037440 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.181984 139735039037440 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.182064 139735039037440 transformer_layer.py:468] tlayer: End windows.
I0123 16:25:20.182098 139735039037440 transformer_layer.py:472] tlayer: final FFN.
I0123 16:25:20.182156 139735039037440 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.182290 139735039037440 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:25:20.182406 139735039037440 nn_components.py:325] mlp: activation = None
I0123 16:25:20.182446 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:25:20.184341 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.184436 139735039037440 nn_components.py:261] mlp: residual
I0123 16:25:20.186798 139735039037440 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.186880 139735039037440 transformer_base.py:443] tbase: final FFN
I0123 16:25:20.186993 139735039037440 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:25:20.189221 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:25:20.191055 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.191153 139735039037440 nn_components.py:261] mlp: residual
I0123 16:25:20.191442 139735039037440 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.191526 139735039037440 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 16:25:20.191635 139735039037440 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:25:20.191673 139735039037440 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:25:20.191703 139735039037440 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:25:20.191764 139735039037440 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.194298 139735039037440 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:25:20.199654 139735039037440 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.199920 139735039037440 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:25:20.202572 139735039037440 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:25:20.214892 139735039037440 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:25:20.214951 139735039037440 attention.py:418] Single window, no scan.
I0123 16:25:20.214988 139735039037440 transformer_layer.py:389] tlayer: self-attention.
I0123 16:25:20.215019 139735039037440 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.215082 139735039037440 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.215631 139735039037440 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.215710 139735039037440 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.216062 139735039037440 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.216787 139735039037440 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.219193 139735039037440 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.219805 139735039037440 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.219884 139735039037440 transformer_layer.py:468] tlayer: End windows.
I0123 16:25:20.219919 139735039037440 transformer_layer.py:472] tlayer: final FFN.
I0123 16:25:20.219977 139735039037440 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.220106 139735039037440 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:25:20.220218 139735039037440 nn_components.py:325] mlp: activation = None
I0123 16:25:20.220257 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:25:20.222076 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.222179 139735039037440 nn_components.py:261] mlp: residual
I0123 16:25:20.224536 139735039037440 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.224617 139735039037440 transformer_base.py:443] tbase: final FFN
I0123 16:25:20.224727 139735039037440 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:25:20.226953 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:25:20.228789 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.228885 139735039037440 nn_components.py:261] mlp: residual
I0123 16:25:20.229172 139735039037440 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.229256 139735039037440 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 16:25:20.229365 139735039037440 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:25:20.229405 139735039037440 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:25:20.229435 139735039037440 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:25:20.229497 139735039037440 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.231679 139735039037440 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:25:20.236918 139735039037440 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.237177 139735039037440 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:25:20.239761 139735039037440 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:25:20.251918 139735039037440 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:25:20.251975 139735039037440 attention.py:418] Single window, no scan.
I0123 16:25:20.252011 139735039037440 transformer_layer.py:389] tlayer: self-attention.
I0123 16:25:20.252040 139735039037440 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.252102 139735039037440 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.252647 139735039037440 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.252724 139735039037440 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.253074 139735039037440 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.253803 139735039037440 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.256187 139735039037440 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.256798 139735039037440 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.256877 139735039037440 transformer_layer.py:468] tlayer: End windows.
I0123 16:25:20.256912 139735039037440 transformer_layer.py:472] tlayer: final FFN.
I0123 16:25:20.256970 139735039037440 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.257100 139735039037440 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:25:20.257208 139735039037440 nn_components.py:325] mlp: activation = None
I0123 16:25:20.257248 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:25:20.259042 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.259145 139735039037440 nn_components.py:261] mlp: residual
I0123 16:25:20.261469 139735039037440 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.261549 139735039037440 transformer_base.py:443] tbase: final FFN
I0123 16:25:20.261663 139735039037440 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:25:20.264306 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:25:20.266124 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.266222 139735039037440 nn_components.py:261] mlp: residual
I0123 16:25:20.266509 139735039037440 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.266593 139735039037440 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 16:25:20.266702 139735039037440 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:25:20.266743 139735039037440 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:25:20.266775 139735039037440 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:25:20.266836 139735039037440 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.268996 139735039037440 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:25:20.274329 139735039037440 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.274591 139735039037440 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:25:20.277173 139735039037440 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:25:20.289427 139735039037440 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:25:20.289484 139735039037440 attention.py:418] Single window, no scan.
I0123 16:25:20.289520 139735039037440 transformer_layer.py:389] tlayer: self-attention.
I0123 16:25:20.289552 139735039037440 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.289614 139735039037440 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.290167 139735039037440 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.290245 139735039037440 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.290592 139735039037440 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.291313 139735039037440 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.293705 139735039037440 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.294502 139735039037440 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.294580 139735039037440 transformer_layer.py:468] tlayer: End windows.
I0123 16:25:20.294614 139735039037440 transformer_layer.py:472] tlayer: final FFN.
I0123 16:25:20.294671 139735039037440 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.294801 139735039037440 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:25:20.294910 139735039037440 nn_components.py:325] mlp: activation = None
I0123 16:25:20.294949 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:25:20.296908 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.297003 139735039037440 nn_components.py:261] mlp: residual
I0123 16:25:20.299361 139735039037440 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.299444 139735039037440 transformer_base.py:443] tbase: final FFN
I0123 16:25:20.299554 139735039037440 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:25:20.301775 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:25:20.303591 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.303687 139735039037440 nn_components.py:261] mlp: residual
I0123 16:25:20.303974 139735039037440 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.304057 139735039037440 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 16:25:20.304166 139735039037440 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:25:20.304206 139735039037440 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:25:20.304237 139735039037440 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:25:20.304299 139735039037440 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.306498 139735039037440 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:25:20.311757 139735039037440 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.312021 139735039037440 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:25:20.314614 139735039037440 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:25:20.326747 139735039037440 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:25:20.326805 139735039037440 attention.py:418] Single window, no scan.
I0123 16:25:20.326840 139735039037440 transformer_layer.py:389] tlayer: self-attention.
I0123 16:25:20.326871 139735039037440 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.326932 139735039037440 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.327479 139735039037440 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.327557 139735039037440 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.327905 139735039037440 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.328624 139735039037440 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.331004 139735039037440 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.331612 139735039037440 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.331691 139735039037440 transformer_layer.py:468] tlayer: End windows.
I0123 16:25:20.331726 139735039037440 transformer_layer.py:472] tlayer: final FFN.
I0123 16:25:20.331782 139735039037440 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.331909 139735039037440 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:25:20.332017 139735039037440 nn_components.py:325] mlp: activation = None
I0123 16:25:20.332055 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:25:20.333859 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.333953 139735039037440 nn_components.py:261] mlp: residual
I0123 16:25:20.336286 139735039037440 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.336373 139735039037440 transformer_base.py:443] tbase: final FFN
I0123 16:25:20.336485 139735039037440 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:25:20.338707 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:25:20.340514 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.340609 139735039037440 nn_components.py:261] mlp: residual
I0123 16:25:20.340889 139735039037440 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.340972 139735039037440 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 16:25:20.341078 139735039037440 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:25:20.341117 139735039037440 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:25:20.341146 139735039037440 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:25:20.341207 139735039037440 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.343400 139735039037440 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:25:20.348700 139735039037440 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.348961 139735039037440 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:25:20.351563 139735039037440 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:25:20.363791 139735039037440 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:25:20.363849 139735039037440 attention.py:418] Single window, no scan.
I0123 16:25:20.363885 139735039037440 transformer_layer.py:389] tlayer: self-attention.
I0123 16:25:20.363915 139735039037440 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.363977 139735039037440 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.364529 139735039037440 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.364609 139735039037440 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.364956 139735039037440 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.365631 139735039037440 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.368100 139735039037440 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.368713 139735039037440 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.368793 139735039037440 transformer_layer.py:468] tlayer: End windows.
I0123 16:25:20.368827 139735039037440 transformer_layer.py:472] tlayer: final FFN.
I0123 16:25:20.368884 139735039037440 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.369013 139735039037440 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:25:20.369121 139735039037440 nn_components.py:325] mlp: activation = None
I0123 16:25:20.369160 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:25:20.370982 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.371077 139735039037440 nn_components.py:261] mlp: residual
I0123 16:25:20.373407 139735039037440 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.373497 139735039037440 transformer_base.py:443] tbase: final FFN
I0123 16:25:20.373611 139735039037440 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:25:20.376256 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:25:20.378081 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.378179 139735039037440 nn_components.py:261] mlp: residual
I0123 16:25:20.378462 139735039037440 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.378545 139735039037440 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 16:25:20.378654 139735039037440 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:25:20.378693 139735039037440 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:25:20.378723 139735039037440 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:25:20.378785 139735039037440 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.380962 139735039037440 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:25:20.386199 139735039037440 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.386457 139735039037440 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:25:20.389061 139735039037440 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:25:20.401429 139735039037440 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:25:20.401487 139735039037440 attention.py:418] Single window, no scan.
I0123 16:25:20.401522 139735039037440 transformer_layer.py:389] tlayer: self-attention.
I0123 16:25:20.401553 139735039037440 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.401615 139735039037440 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.402196 139735039037440 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.402275 139735039037440 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.402624 139735039037440 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.403295 139735039037440 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.405755 139735039037440 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.406366 139735039037440 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.406445 139735039037440 transformer_layer.py:468] tlayer: End windows.
I0123 16:25:20.406479 139735039037440 transformer_layer.py:472] tlayer: final FFN.
I0123 16:25:20.406535 139735039037440 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.406664 139735039037440 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:25:20.406776 139735039037440 nn_components.py:325] mlp: activation = None
I0123 16:25:20.406817 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:25:20.408640 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.408735 139735039037440 nn_components.py:261] mlp: residual
I0123 16:25:20.411078 139735039037440 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.411169 139735039037440 transformer_base.py:443] tbase: final FFN
I0123 16:25:20.411285 139735039037440 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:25:20.413734 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:25:20.415549 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.415645 139735039037440 nn_components.py:261] mlp: residual
I0123 16:25:20.415931 139735039037440 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.416013 139735039037440 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 16:25:20.416120 139735039037440 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:25:20.416158 139735039037440 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:25:20.416188 139735039037440 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:25:20.416248 139735039037440 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.418416 139735039037440 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:25:20.423668 139735039037440 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.423928 139735039037440 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:25:20.426513 139735039037440 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:25:20.438791 139735039037440 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:25:20.438849 139735039037440 attention.py:418] Single window, no scan.
I0123 16:25:20.438885 139735039037440 transformer_layer.py:389] tlayer: self-attention.
I0123 16:25:20.438917 139735039037440 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.438983 139735039037440 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.439534 139735039037440 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.439613 139735039037440 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.439962 139735039037440 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.440642 139735039037440 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.443108 139735039037440 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.443722 139735039037440 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.443801 139735039037440 transformer_layer.py:468] tlayer: End windows.
I0123 16:25:20.443836 139735039037440 transformer_layer.py:472] tlayer: final FFN.
I0123 16:25:20.443892 139735039037440 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.444020 139735039037440 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:25:20.444128 139735039037440 nn_components.py:325] mlp: activation = None
I0123 16:25:20.444166 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:25:20.445995 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.446091 139735039037440 nn_components.py:261] mlp: residual
I0123 16:25:20.448426 139735039037440 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.448507 139735039037440 transformer_base.py:443] tbase: final FFN
I0123 16:25:20.448625 139735039037440 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:25:20.450865 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:25:20.452686 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.452783 139735039037440 nn_components.py:261] mlp: residual
I0123 16:25:20.453071 139735039037440 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.453154 139735039037440 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 16:25:20.453266 139735039037440 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:25:20.453306 139735039037440 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:25:20.453337 139735039037440 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:25:20.453398 139735039037440 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.455577 139735039037440 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:25:20.460859 139735039037440 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.461115 139735039037440 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:25:20.463727 139735039037440 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:25:20.476330 139735039037440 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:25:20.476389 139735039037440 attention.py:418] Single window, no scan.
I0123 16:25:20.476424 139735039037440 transformer_layer.py:389] tlayer: self-attention.
I0123 16:25:20.476454 139735039037440 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.476515 139735039037440 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.477062 139735039037440 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.477140 139735039037440 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.477491 139735039037440 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.478176 139735039037440 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.480632 139735039037440 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.481247 139735039037440 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.481327 139735039037440 transformer_layer.py:468] tlayer: End windows.
I0123 16:25:20.481362 139735039037440 transformer_layer.py:472] tlayer: final FFN.
I0123 16:25:20.481420 139735039037440 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.481549 139735039037440 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:25:20.481664 139735039037440 nn_components.py:325] mlp: activation = None
I0123 16:25:20.481706 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:25:20.483532 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.483628 139735039037440 nn_components.py:261] mlp: residual
I0123 16:25:20.485980 139735039037440 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.486062 139735039037440 transformer_base.py:443] tbase: final FFN
I0123 16:25:20.486172 139735039037440 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:25:20.488795 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:25:20.490618 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.490716 139735039037440 nn_components.py:261] mlp: residual
I0123 16:25:20.491003 139735039037440 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.491092 139735039037440 decoder_stack.py:344] dstack: Final layernorm.
I0123 16:25:20.493850 139735039037440 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:25:20.722625 139735039037440 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.722807 139735039037440 decoder_stack.py:333] dstack: autoregressive generator.
I0123 16:25:20.722874 139735039037440 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 16:25:20.722986 139735039037440 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:25:20.723028 139735039037440 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:25:20.723061 139735039037440 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:25:20.723130 139735039037440 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.725566 139735039037440 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:25:20.731086 139735039037440 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.731355 139735039037440 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:25:20.734011 139735039037440 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:25:20.747130 139735039037440 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:25:20.747193 139735039037440 attention.py:418] Single window, no scan.
I0123 16:25:20.747232 139735039037440 transformer_layer.py:389] tlayer: self-attention.
I0123 16:25:20.747266 139735039037440 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.747330 139735039037440 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.747928 139735039037440 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.748008 139735039037440 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.748368 139735039037440 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.749055 139735039037440 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.751698 139735039037440 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.752383 139735039037440 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.752465 139735039037440 transformer_layer.py:468] tlayer: End windows.
I0123 16:25:20.752502 139735039037440 transformer_layer.py:472] tlayer: final FFN.
I0123 16:25:20.752564 139735039037440 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.752698 139735039037440 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:25:20.752816 139735039037440 nn_components.py:325] mlp: activation = None
I0123 16:25:20.752858 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:25:20.754753 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.754872 139735039037440 nn_components.py:261] mlp: residual
I0123 16:25:20.757347 139735039037440 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.757430 139735039037440 transformer_base.py:443] tbase: final FFN
I0123 16:25:20.757546 139735039037440 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:25:20.759827 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:25:20.761822 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.761921 139735039037440 nn_components.py:261] mlp: residual
I0123 16:25:20.762220 139735039037440 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.762309 139735039037440 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 16:25:20.762425 139735039037440 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:25:20.762469 139735039037440 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:25:20.762503 139735039037440 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:25:20.762572 139735039037440 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.764850 139735039037440 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:25:20.770212 139735039037440 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.770478 139735039037440 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:25:20.773091 139735039037440 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:25:20.785607 139735039037440 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:25:20.785672 139735039037440 attention.py:418] Single window, no scan.
I0123 16:25:20.785710 139735039037440 transformer_layer.py:389] tlayer: self-attention.
I0123 16:25:20.785742 139735039037440 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.785807 139735039037440 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.786382 139735039037440 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.786465 139735039037440 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.786835 139735039037440 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.787546 139735039037440 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.790160 139735039037440 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.790784 139735039037440 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.790864 139735039037440 transformer_layer.py:468] tlayer: End windows.
I0123 16:25:20.790899 139735039037440 transformer_layer.py:472] tlayer: final FFN.
I0123 16:25:20.790959 139735039037440 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.791090 139735039037440 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:25:20.791204 139735039037440 nn_components.py:325] mlp: activation = None
I0123 16:25:20.791244 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:25:20.793150 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.793247 139735039037440 nn_components.py:261] mlp: residual
I0123 16:25:20.795655 139735039037440 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.795738 139735039037440 transformer_base.py:443] tbase: final FFN
I0123 16:25:20.795849 139735039037440 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:25:20.798054 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:25:20.799973 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.800071 139735039037440 nn_components.py:261] mlp: residual
I0123 16:25:20.800362 139735039037440 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.800448 139735039037440 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 16:25:20.800559 139735039037440 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:25:20.800598 139735039037440 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:25:20.800631 139735039037440 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:25:20.800693 139735039037440 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.802906 139735039037440 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:25:20.808275 139735039037440 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.808543 139735039037440 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:25:20.811182 139735039037440 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:25:20.823714 139735039037440 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:25:20.823774 139735039037440 attention.py:418] Single window, no scan.
I0123 16:25:20.823811 139735039037440 transformer_layer.py:389] tlayer: self-attention.
I0123 16:25:20.823843 139735039037440 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.823905 139735039037440 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.824464 139735039037440 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.824544 139735039037440 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.824898 139735039037440 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.825573 139735039037440 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.828001 139735039037440 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.828623 139735039037440 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.828704 139735039037440 transformer_layer.py:468] tlayer: End windows.
I0123 16:25:20.828741 139735039037440 transformer_layer.py:472] tlayer: final FFN.
I0123 16:25:20.828799 139735039037440 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.828931 139735039037440 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:25:20.829044 139735039037440 nn_components.py:325] mlp: activation = None
I0123 16:25:20.829084 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:25:20.831357 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.831456 139735039037440 nn_components.py:261] mlp: residual
I0123 16:25:20.834024 139735039037440 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.834115 139735039037440 transformer_base.py:443] tbase: final FFN
I0123 16:25:20.834230 139735039037440 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:25:20.836386 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:25:20.838294 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.838392 139735039037440 nn_components.py:261] mlp: residual
I0123 16:25:20.838682 139735039037440 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.838768 139735039037440 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 16:25:20.838880 139735039037440 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:25:20.838921 139735039037440 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:25:20.838954 139735039037440 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:25:20.839018 139735039037440 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.841238 139735039037440 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:25:20.846643 139735039037440 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.846909 139735039037440 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:25:20.849551 139735039037440 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:25:20.861986 139735039037440 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:25:20.862047 139735039037440 attention.py:418] Single window, no scan.
I0123 16:25:20.862085 139735039037440 transformer_layer.py:389] tlayer: self-attention.
I0123 16:25:20.862118 139735039037440 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.862184 139735039037440 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.862754 139735039037440 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.862835 139735039037440 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.863198 139735039037440 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.863885 139735039037440 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.866282 139735039037440 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.866918 139735039037440 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.867000 139735039037440 transformer_layer.py:468] tlayer: End windows.
I0123 16:25:20.867036 139735039037440 transformer_layer.py:472] tlayer: final FFN.
I0123 16:25:20.867096 139735039037440 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.867229 139735039037440 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:25:20.867344 139735039037440 nn_components.py:325] mlp: activation = None
I0123 16:25:20.867385 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:25:20.869257 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.869352 139735039037440 nn_components.py:261] mlp: residual
I0123 16:25:20.871732 139735039037440 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.871825 139735039037440 transformer_base.py:443] tbase: final FFN
I0123 16:25:20.871939 139735039037440 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:25:20.874118 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:25:20.876058 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.876155 139735039037440 nn_components.py:261] mlp: residual
I0123 16:25:20.876442 139735039037440 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.876526 139735039037440 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 16:25:20.876636 139735039037440 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:25:20.876675 139735039037440 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:25:20.876706 139735039037440 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:25:20.876769 139735039037440 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.878974 139735039037440 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:25:20.884439 139735039037440 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.884715 139735039037440 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:25:20.887424 139735039037440 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:25:20.899955 139735039037440 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:25:20.900014 139735039037440 attention.py:418] Single window, no scan.
I0123 16:25:20.900049 139735039037440 transformer_layer.py:389] tlayer: self-attention.
I0123 16:25:20.900080 139735039037440 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.900142 139735039037440 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.900692 139735039037440 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.900771 139735039037440 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.901125 139735039037440 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.901798 139735039037440 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.904242 139735039037440 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.904851 139735039037440 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.904930 139735039037440 transformer_layer.py:468] tlayer: End windows.
I0123 16:25:20.904964 139735039037440 transformer_layer.py:472] tlayer: final FFN.
I0123 16:25:20.905021 139735039037440 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.905148 139735039037440 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:25:20.905258 139735039037440 nn_components.py:325] mlp: activation = None
I0123 16:25:20.905297 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:25:20.907197 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.907296 139735039037440 nn_components.py:261] mlp: residual
I0123 16:25:20.909652 139735039037440 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.909733 139735039037440 transformer_base.py:443] tbase: final FFN
I0123 16:25:20.909852 139735039037440 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:25:20.912073 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:25:20.913957 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.914057 139735039037440 nn_components.py:261] mlp: residual
I0123 16:25:20.914354 139735039037440 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.914443 139735039037440 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 16:25:20.914558 139735039037440 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:25:20.914600 139735039037440 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:25:20.914633 139735039037440 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:25:20.914700 139735039037440 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.916957 139735039037440 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:25:20.922345 139735039037440 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.922616 139735039037440 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:25:20.925561 139735039037440 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:25:20.938615 139735039037440 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:25:20.938676 139735039037440 attention.py:418] Single window, no scan.
I0123 16:25:20.938713 139735039037440 transformer_layer.py:389] tlayer: self-attention.
I0123 16:25:20.938746 139735039037440 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.938811 139735039037440 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.939385 139735039037440 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.939465 139735039037440 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.939835 139735039037440 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.940505 139735039037440 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.942962 139735039037440 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.943597 139735039037440 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.943680 139735039037440 transformer_layer.py:468] tlayer: End windows.
I0123 16:25:20.943717 139735039037440 transformer_layer.py:472] tlayer: final FFN.
I0123 16:25:20.943778 139735039037440 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.943910 139735039037440 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:25:20.944020 139735039037440 nn_components.py:325] mlp: activation = None
I0123 16:25:20.944060 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:25:20.945933 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.946029 139735039037440 nn_components.py:261] mlp: residual
I0123 16:25:20.948459 139735039037440 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.948539 139735039037440 transformer_base.py:443] tbase: final FFN
I0123 16:25:20.948649 139735039037440 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:25:20.950876 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:25:20.952814 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.952911 139735039037440 nn_components.py:261] mlp: residual
I0123 16:25:20.953203 139735039037440 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.953287 139735039037440 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 16:25:20.953397 139735039037440 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:25:20.953437 139735039037440 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:25:20.953469 139735039037440 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:25:20.953534 139735039037440 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.955817 139735039037440 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:25:20.961225 139735039037440 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.961492 139735039037440 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:25:20.964139 139735039037440 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:25:20.976674 139735039037440 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:25:20.976732 139735039037440 attention.py:418] Single window, no scan.
I0123 16:25:20.976768 139735039037440 transformer_layer.py:389] tlayer: self-attention.
I0123 16:25:20.976800 139735039037440 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.976862 139735039037440 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.977415 139735039037440 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.977493 139735039037440 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.977850 139735039037440 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.978520 139735039037440 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.980911 139735039037440 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.981527 139735039037440 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.981607 139735039037440 transformer_layer.py:468] tlayer: End windows.
I0123 16:25:20.981647 139735039037440 transformer_layer.py:472] tlayer: final FFN.
I0123 16:25:20.981707 139735039037440 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.981839 139735039037440 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:25:20.981949 139735039037440 nn_components.py:325] mlp: activation = None
I0123 16:25:20.981988 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:25:20.983863 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.983961 139735039037440 nn_components.py:261] mlp: residual
I0123 16:25:20.986339 139735039037440 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.986422 139735039037440 transformer_base.py:443] tbase: final FFN
I0123 16:25:20.986535 139735039037440 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:25:20.988723 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:25:20.990568 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.990667 139735039037440 nn_components.py:261] mlp: residual
I0123 16:25:20.990960 139735039037440 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.991045 139735039037440 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 16:25:20.991157 139735039037440 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:25:20.991198 139735039037440 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:25:20.991230 139735039037440 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:25:20.991294 139735039037440 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.993572 139735039037440 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:25:20.998932 139735039037440 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:20.999194 139735039037440 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:25:21.001755 139735039037440 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:25:21.014184 139735039037440 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:25:21.014244 139735039037440 attention.py:418] Single window, no scan.
I0123 16:25:21.014281 139735039037440 transformer_layer.py:389] tlayer: self-attention.
I0123 16:25:21.014313 139735039037440 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.014376 139735039037440 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.014934 139735039037440 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.015013 139735039037440 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.015371 139735039037440 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.016053 139735039037440 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.018898 139735039037440 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.019516 139735039037440 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.019596 139735039037440 transformer_layer.py:468] tlayer: End windows.
I0123 16:25:21.019632 139735039037440 transformer_layer.py:472] tlayer: final FFN.
I0123 16:25:21.019692 139735039037440 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.019826 139735039037440 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:25:21.019938 139735039037440 nn_components.py:325] mlp: activation = None
I0123 16:25:21.019978 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:25:21.021815 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.021912 139735039037440 nn_components.py:261] mlp: residual
I0123 16:25:21.024277 139735039037440 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.024357 139735039037440 transformer_base.py:443] tbase: final FFN
I0123 16:25:21.024471 139735039037440 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:25:21.026726 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:25:21.028588 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.028684 139735039037440 nn_components.py:261] mlp: residual
I0123 16:25:21.028973 139735039037440 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.029057 139735039037440 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 16:25:21.029167 139735039037440 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:25:21.029209 139735039037440 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:25:21.029243 139735039037440 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:25:21.029308 139735039037440 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.031534 139735039037440 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:25:21.036916 139735039037440 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.037368 139735039037440 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:25:21.040015 139735039037440 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:25:21.052620 139735039037440 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:25:21.052680 139735039037440 attention.py:418] Single window, no scan.
I0123 16:25:21.052716 139735039037440 transformer_layer.py:389] tlayer: self-attention.
I0123 16:25:21.052747 139735039037440 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.052808 139735039037440 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.053415 139735039037440 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.053493 139735039037440 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.053854 139735039037440 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.054536 139735039037440 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.056981 139735039037440 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.057599 139735039037440 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.057685 139735039037440 transformer_layer.py:468] tlayer: End windows.
I0123 16:25:21.057721 139735039037440 transformer_layer.py:472] tlayer: final FFN.
I0123 16:25:21.057781 139735039037440 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.057914 139735039037440 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:25:21.058027 139735039037440 nn_components.py:325] mlp: activation = None
I0123 16:25:21.058066 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:25:21.059906 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.060002 139735039037440 nn_components.py:261] mlp: residual
I0123 16:25:21.062464 139735039037440 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.062561 139735039037440 transformer_base.py:443] tbase: final FFN
I0123 16:25:21.062685 139735039037440 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:25:21.064955 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:25:21.066813 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.066921 139735039037440 nn_components.py:261] mlp: residual
I0123 16:25:21.067213 139735039037440 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.067297 139735039037440 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 16:25:21.067407 139735039037440 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:25:21.067447 139735039037440 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:25:21.067479 139735039037440 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:25:21.067543 139735039037440 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.069768 139735039037440 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:25:21.075245 139735039037440 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.075510 139735039037440 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:25:21.078076 139735039037440 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:25:21.090555 139735039037440 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:25:21.090614 139735039037440 attention.py:418] Single window, no scan.
I0123 16:25:21.090652 139735039037440 transformer_layer.py:389] tlayer: self-attention.
I0123 16:25:21.090685 139735039037440 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.090746 139735039037440 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.091305 139735039037440 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.091383 139735039037440 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.091736 139735039037440 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.092415 139735039037440 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.094874 139735039037440 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.095548 139735039037440 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.095628 139735039037440 transformer_layer.py:468] tlayer: End windows.
I0123 16:25:21.095664 139735039037440 transformer_layer.py:472] tlayer: final FFN.
I0123 16:25:21.095726 139735039037440 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.095858 139735039037440 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:25:21.095971 139735039037440 nn_components.py:325] mlp: activation = None
I0123 16:25:21.096011 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:25:21.097866 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.097961 139735039037440 nn_components.py:261] mlp: residual
I0123 16:25:21.100364 139735039037440 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.100445 139735039037440 transformer_base.py:443] tbase: final FFN
I0123 16:25:21.100558 139735039037440 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:25:21.102773 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:25:21.104692 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.104797 139735039037440 nn_components.py:261] mlp: residual
I0123 16:25:21.105092 139735039037440 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.105175 139735039037440 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 16:25:21.105287 139735039037440 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:25:21.105328 139735039037440 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:25:21.105362 139735039037440 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:25:21.105426 139735039037440 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.107683 139735039037440 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:25:21.113082 139735039037440 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.113348 139735039037440 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:25:21.115959 139735039037440 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:25:21.128809 139735039037440 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:25:21.128870 139735039037440 attention.py:418] Single window, no scan.
I0123 16:25:21.128908 139735039037440 transformer_layer.py:389] tlayer: self-attention.
I0123 16:25:21.128941 139735039037440 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.129002 139735039037440 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.129557 139735039037440 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.129634 139735039037440 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.129999 139735039037440 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.130682 139735039037440 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.133409 139735039037440 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.134037 139735039037440 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.134117 139735039037440 transformer_layer.py:468] tlayer: End windows.
I0123 16:25:21.134153 139735039037440 transformer_layer.py:472] tlayer: final FFN.
I0123 16:25:21.134211 139735039037440 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.134342 139735039037440 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:25:21.134456 139735039037440 nn_components.py:325] mlp: activation = None
I0123 16:25:21.134496 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:25:21.136359 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.136454 139735039037440 nn_components.py:261] mlp: residual
I0123 16:25:21.138879 139735039037440 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.138961 139735039037440 transformer_base.py:443] tbase: final FFN
I0123 16:25:21.139075 139735039037440 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:25:21.141367 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:25:21.143231 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.143328 139735039037440 nn_components.py:261] mlp: residual
I0123 16:25:21.143631 139735039037440 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.143716 139735039037440 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 16:25:21.143826 139735039037440 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:25:21.143867 139735039037440 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:25:21.143898 139735039037440 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:25:21.143962 139735039037440 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.146209 139735039037440 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:25:21.151635 139735039037440 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.151895 139735039037440 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:25:21.154545 139735039037440 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:25:21.167199 139735039037440 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:25:21.167258 139735039037440 attention.py:418] Single window, no scan.
I0123 16:25:21.167296 139735039037440 transformer_layer.py:389] tlayer: self-attention.
I0123 16:25:21.167330 139735039037440 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.167394 139735039037440 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.168011 139735039037440 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.168091 139735039037440 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.168457 139735039037440 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.169150 139735039037440 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.171619 139735039037440 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.172242 139735039037440 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.172321 139735039037440 transformer_layer.py:468] tlayer: End windows.
I0123 16:25:21.172356 139735039037440 transformer_layer.py:472] tlayer: final FFN.
I0123 16:25:21.172415 139735039037440 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.172545 139735039037440 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:25:21.172658 139735039037440 nn_components.py:325] mlp: activation = None
I0123 16:25:21.172697 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:25:21.174569 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.174665 139735039037440 nn_components.py:261] mlp: residual
I0123 16:25:21.177105 139735039037440 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.177185 139735039037440 transformer_base.py:443] tbase: final FFN
I0123 16:25:21.177295 139735039037440 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:25:21.179502 139735039037440 nn_components.py:329] mlp: final activation = None
I0123 16:25:21.181351 139735039037440 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.181447 139735039037440 nn_components.py:261] mlp: residual
I0123 16:25:21.181749 139735039037440 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:21.181848 139735039037440 decoder_stack.py:344] dstack: Final layernorm.
I0123 16:25:21.184724 139735039037440 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:25:32.490154 139735039037440 alphageometry.py:566] LM output (score=-1.364601): "n : T c d c n 20 ;"
I0123 16:25:32.490449 139735039037440 alphageometry.py:567] Translation: "n = on_tline n c c d"

I0123 16:25:32.490510 139735039037440 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = on_tline n c c d ? eqratio b d h d m d b d"
I0123 16:25:32.490671 139735039037440 graph.py:498] 
I0123 16:25:32.490737 139735039037440 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = on_tline n c c d ? eqratio b d h d m d b d
I0123 16:25:33.945117 139735039037440 ddar.py:60] Depth 1/1000 time = 1.4168407917022705
I0123 16:25:36.920464 139735039037440 ddar.py:60] Depth 2/1000 time = 2.9751806259155273
I0123 16:25:44.639501 139735039037440 ddar.py:60] Depth 3/1000 time = 7.718843221664429
I0123 16:25:51.199510 139735039037440 ddar.py:60] Depth 4/1000 time = 6.559797763824463
I0123 16:25:57.934691 139735039037440 ddar.py:60] Depth 5/1000 time = 6.734977722167969
I0123 16:26:04.511226 139735039037440 ddar.py:60] Depth 6/1000 time = 6.575884103775024
I0123 16:26:11.501952 139735039037440 ddar.py:60] Depth 7/1000 time = 6.923858642578125
I0123 16:26:19.018840 139735039037440 ddar.py:60] Depth 8/1000 time = 7.516646862030029
I0123 16:26:27.084587 139735039037440 ddar.py:60] Depth 9/1000 time = 8.065492153167725
I0123 16:26:35.160556 139735039037440 ddar.py:60] Depth 10/1000 time = 8.075676441192627
I0123 16:26:43.274317 139735039037440 ddar.py:60] Depth 11/1000 time = 8.113519430160522
I0123 16:26:51.667183 139735039037440 ddar.py:60] Depth 12/1000 time = 8.35104775428772
I0123 16:27:00.934718 139735039037440 ddar.py:60] Depth 13/1000 time = 9.267279863357544
I0123 16:27:10.782750 139735039037440 ddar.py:60] Depth 14/1000 time = 9.847686290740967
I0123 16:27:20.812812 139735039037440 ddar.py:60] Depth 15/1000 time = 10.029649257659912
I0123 16:27:31.005651 139735039037440 ddar.py:60] Depth 16/1000 time = 10.1487455368042
I0123 16:27:31.006248 139735039037440 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 16:27:31.006409 139735039037440 alphageometry.py:566] LM output (score=-2.048148): "n : D b c b n 20 D b c c n 21 ;"
I0123 16:27:31.006449 139735039037440 alphageometry.py:567] Translation: "n = on_circle n b c, on_circle n c b"

I0123 16:27:31.006506 139735039037440 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = on_circle n b c, on_circle n c b ? eqratio b d h d m d b d"
I0123 16:27:31.006699 139735039037440 graph.py:498] 
I0123 16:27:31.006762 139735039037440 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = on_circle n b c, on_circle n c b ? eqratio b d h d m d b d
I0123 16:27:32.454620 139735039037440 ddar.py:60] Depth 1/1000 time = 1.4041786193847656
I0123 16:27:35.772382 139735039037440 ddar.py:60] Depth 2/1000 time = 3.317589521408081
I0123 16:27:40.746299 139735039037440 ddar.py:60] Depth 3/1000 time = 4.973747491836548
I0123 16:27:46.346289 139735039037440 ddar.py:60] Depth 4/1000 time = 5.599805593490601
I0123 16:27:51.699798 139735039037440 ddar.py:60] Depth 5/1000 time = 5.353283166885376
I0123 16:27:57.259877 139735039037440 ddar.py:60] Depth 6/1000 time = 5.559622526168823
I0123 16:28:02.859831 139735039037440 ddar.py:60] Depth 7/1000 time = 5.5985496044158936
I0123 16:28:09.304141 139735039037440 ddar.py:60] Depth 8/1000 time = 6.43538761138916
I0123 16:28:16.249188 139735039037440 ddar.py:60] Depth 9/1000 time = 6.944830417633057
I0123 16:28:23.435006 139735039037440 ddar.py:60] Depth 10/1000 time = 7.106222152709961
I0123 16:28:31.540676 139735039037440 ddar.py:60] Depth 11/1000 time = 8.105393886566162
I0123 16:28:40.028140 139735039037440 ddar.py:60] Depth 12/1000 time = 8.487128496170044
I0123 16:28:48.603266 139735039037440 ddar.py:60] Depth 13/1000 time = 8.574897766113281
I0123 16:28:57.550038 139735039037440 ddar.py:60] Depth 14/1000 time = 8.946499347686768
I0123 16:29:06.532388 139735039037440 ddar.py:60] Depth 15/1000 time = 8.940800428390503
I0123 16:29:16.653275 139735039037440 ddar.py:60] Depth 16/1000 time = 10.120571374893188
I0123 16:29:27.174185 139735039037440 ddar.py:60] Depth 17/1000 time = 10.520519018173218
I0123 16:29:37.776236 139735039037440 ddar.py:60] Depth 18/1000 time = 10.601715326309204
I0123 16:29:48.731968 139735039037440 ddar.py:60] Depth 19/1000 time = 10.91202187538147
I0123 16:29:48.732337 139735039037440 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 16:29:48.732445 139735039037440 alphageometry.py:566] LM output (score=-2.212945): "n : D a c a n 20 D a c c n 21 ;"
I0123 16:29:48.732484 139735039037440 alphageometry.py:567] Translation: "n = on_circle n a c, on_circle n c a"

I0123 16:29:48.732526 139735039037440 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = on_circle n a c, on_circle n c a ? eqratio b d h d m d b d"
I0123 16:29:48.732694 139735039037440 graph.py:498] 
I0123 16:29:48.732759 139735039037440 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = on_circle n a c, on_circle n c a ? eqratio b d h d m d b d
I0123 16:29:50.128435 139735039037440 ddar.py:60] Depth 1/1000 time = 1.3523364067077637
I0123 16:29:53.559173 139735039037440 ddar.py:60] Depth 2/1000 time = 3.430553913116455
I0123 16:29:58.730207 139735039037440 ddar.py:60] Depth 3/1000 time = 5.1708385944366455
I0123 16:30:04.481157 139735039037440 ddar.py:60] Depth 4/1000 time = 5.750734329223633
I0123 16:30:10.319476 139735039037440 ddar.py:60] Depth 5/1000 time = 5.8380982875823975
I0123 16:30:15.911262 139735039037440 ddar.py:60] Depth 6/1000 time = 5.591360807418823
I0123 16:30:21.849318 139735039037440 ddar.py:60] Depth 7/1000 time = 5.936579942703247
I0123 16:30:28.399841 139735039037440 ddar.py:60] Depth 8/1000 time = 6.541353940963745
I0123 16:30:35.817911 139735039037440 ddar.py:60] Depth 9/1000 time = 7.417890548706055
I0123 16:30:43.050386 139735039037440 ddar.py:60] Depth 10/1000 time = 7.154905557632446
I0123 16:30:51.471719 139735039037440 ddar.py:60] Depth 11/1000 time = 8.421009540557861
I0123 16:31:00.133353 139735039037440 ddar.py:60] Depth 12/1000 time = 8.661415576934814
I0123 16:31:09.370338 139735039037440 ddar.py:60] Depth 13/1000 time = 9.236703634262085
I0123 16:31:18.365859 139735039037440 ddar.py:60] Depth 14/1000 time = 8.995243072509766
I0123 16:31:27.746568 139735039037440 ddar.py:60] Depth 15/1000 time = 9.329680442810059
I0123 16:31:38.272829 139735039037440 ddar.py:60] Depth 16/1000 time = 10.525984764099121
I0123 16:31:49.188837 139735039037440 ddar.py:60] Depth 17/1000 time = 10.915693759918213
I0123 16:32:00.265176 139735039037440 ddar.py:60] Depth 18/1000 time = 11.075960636138916
I0123 16:32:11.848929 139735039037440 ddar.py:60] Depth 19/1000 time = 11.5379056930542
I0123 16:32:11.849257 139735039037440 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 16:32:11.849354 139735039037440 alphageometry.py:566] LM output (score=-2.228589): "n : D c f e n 20 P c f e n 21 ;"
I0123 16:32:11.849391 139735039037440 alphageometry.py:567] Translation: "n = eqdistance n e c f, on_pline n e c f"

I0123 16:32:11.849434 139735039037440 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = eqdistance n e c f, on_pline n e c f ? eqratio b d h d m d b d"
I0123 16:32:11.849600 139735039037440 graph.py:498] 
I0123 16:32:11.849668 139735039037440 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = eqdistance n e c f, on_pline n e c f ? eqratio b d h d m d b d
I0123 16:32:12.969263 139735039037440 ddar.py:60] Depth 1/1000 time = 1.0744976997375488
I0123 16:32:16.155686 139735039037440 ddar.py:60] Depth 2/1000 time = 3.1862597465515137
I0123 16:32:21.353613 139735039037440 ddar.py:60] Depth 3/1000 time = 5.197763681411743
I0123 16:32:27.750769 139735039037440 ddar.py:60] Depth 4/1000 time = 6.396929979324341
I0123 16:32:35.535268 139735039037440 ddar.py:60] Depth 5/1000 time = 7.784191370010376
I0123 16:32:42.950012 139735039037440 ddar.py:60] Depth 6/1000 time = 7.41455864906311
I0123 16:32:50.352433 139735039037440 ddar.py:60] Depth 7/1000 time = 7.401787281036377
I0123 16:32:58.066722 139735039037440 ddar.py:60] Depth 8/1000 time = 7.640618085861206
I0123 16:33:06.580994 139735039037440 ddar.py:60] Depth 9/1000 time = 8.514064073562622
I0123 16:33:15.205431 139735039037440 ddar.py:60] Depth 10/1000 time = 8.624180793762207
I0123 16:33:24.104444 139735039037440 ddar.py:60] Depth 11/1000 time = 8.898735761642456
I0123 16:33:33.090257 139735039037440 ddar.py:60] Depth 12/1000 time = 8.98549771308899
I0123 16:33:42.318047 139735039037440 ddar.py:60] Depth 13/1000 time = 9.172569751739502
I0123 16:33:52.929999 139735039037440 ddar.py:60] Depth 14/1000 time = 10.611626386642456
I0123 16:34:03.595649 139735039037440 ddar.py:60] Depth 15/1000 time = 10.664722442626953
I0123 16:34:14.442417 139735039037440 ddar.py:60] Depth 16/1000 time = 10.846441745758057
I0123 16:34:25.750850 139735039037440 ddar.py:60] Depth 17/1000 time = 11.262442827224731
I0123 16:34:25.751142 139735039037440 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 16:34:25.751253 139735039037440 alphageometry.py:566] LM output (score=-2.350113): "n : D c d d n 20 ;"
I0123 16:34:25.751291 139735039037440 alphageometry.py:567] Translation: "n = on_circle n d c"

I0123 16:34:25.751328 139735039037440 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = on_circle n d c ? eqratio b d h d m d b d"
I0123 16:34:25.751493 139735039037440 graph.py:498] 
I0123 16:34:25.751557 139735039037440 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = on_circle n d c ? eqratio b d h d m d b d
I0123 16:34:26.969273 139735039037440 ddar.py:60] Depth 1/1000 time = 1.1816928386688232
I0123 16:34:30.461542 139735039037440 ddar.py:60] Depth 2/1000 time = 3.49210524559021
I0123 16:34:35.854420 139735039037440 ddar.py:60] Depth 3/1000 time = 5.392650604248047
I0123 16:34:41.468819 139735039037440 ddar.py:60] Depth 4/1000 time = 5.614118576049805
I0123 16:34:47.166225 139735039037440 ddar.py:60] Depth 5/1000 time = 5.697024583816528
I0123 16:34:52.879412 139735039037440 ddar.py:60] Depth 6/1000 time = 5.712791681289673
I0123 16:34:59.888545 139735039037440 ddar.py:60] Depth 7/1000 time = 6.99947452545166
I0123 16:35:06.843760 139735039037440 ddar.py:60] Depth 8/1000 time = 6.954982042312622
I0123 16:35:14.090158 139735039037440 ddar.py:60] Depth 9/1000 time = 7.169189453125
I0123 16:35:22.743709 139735039037440 ddar.py:60] Depth 10/1000 time = 8.653303146362305
I0123 16:35:31.261583 139735039037440 ddar.py:60] Depth 11/1000 time = 8.517627477645874
I0123 16:35:40.447382 139735039037440 ddar.py:60] Depth 12/1000 time = 9.185500383377075
I0123 16:35:49.433770 139735039037440 ddar.py:60] Depth 13/1000 time = 8.986008167266846
I0123 16:35:58.869018 139735039037440 ddar.py:60] Depth 14/1000 time = 9.381428241729736
I0123 16:36:09.729784 139735039037440 ddar.py:60] Depth 15/1000 time = 10.860490560531616
I0123 16:36:20.615787 139735039037440 ddar.py:60] Depth 16/1000 time = 10.885714769363403
I0123 16:36:32.008817 139735039037440 ddar.py:60] Depth 17/1000 time = 11.392722368240356
I0123 16:36:43.526706 139735039037440 ddar.py:60] Depth 18/1000 time = 11.471726179122925
I0123 16:36:43.527153 139735039037440 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 16:36:43.527283 139735039037440 alphageometry.py:566] LM output (score=-2.458623): "n : D c i j n 20 D c n i j 21 ;"
I0123 16:36:43.527318 139735039037440 alphageometry.py:567] Translation: "n = eqdistance n j c i, eqdistance n c i j"

I0123 16:36:43.527368 139735039037440 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = eqdistance n j c i, eqdistance n c i j ? eqratio b d h d m d b d"
I0123 16:36:43.527559 139735039037440 graph.py:498] 
I0123 16:36:43.527619 139735039037440 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = eqdistance n j c i, eqdistance n c i j ? eqratio b d h d m d b d
I0123 16:36:44.655599 139735039037440 ddar.py:60] Depth 1/1000 time = 1.0841693878173828
I0123 16:36:47.988615 139735039037440 ddar.py:60] Depth 2/1000 time = 3.3328258991241455
I0123 16:36:53.213386 139735039037440 ddar.py:60] Depth 3/1000 time = 5.224597215652466
I0123 16:36:58.177487 139735039037440 ddar.py:60] Depth 4/1000 time = 4.963925123214722
I0123 16:37:04.235530 139735039037440 ddar.py:60] Depth 5/1000 time = 6.057790279388428
I0123 16:37:09.930429 139735039037440 ddar.py:60] Depth 6/1000 time = 5.694267511367798
I0123 16:37:17.317121 139735039037440 ddar.py:60] Depth 7/1000 time = 7.376255989074707
I0123 16:37:24.752589 139735039037440 ddar.py:60] Depth 8/1000 time = 7.435254812240601
I0123 16:37:32.142038 139735039037440 ddar.py:60] Depth 9/1000 time = 7.322528123855591
I0123 16:37:40.747931 139735039037440 ddar.py:60] Depth 10/1000 time = 8.605653524398804
I0123 16:37:49.665651 139735039037440 ddar.py:60] Depth 11/1000 time = 8.91738772392273
I0123 16:37:58.609344 139735039037440 ddar.py:60] Depth 12/1000 time = 8.943322658538818
I0123 16:38:07.525011 139735039037440 ddar.py:60] Depth 13/1000 time = 8.915412664413452
I0123 16:38:17.095609 139735039037440 ddar.py:60] Depth 14/1000 time = 9.520640134811401
I0123 16:38:27.476602 139735039037440 ddar.py:60] Depth 15/1000 time = 10.380733251571655
I0123 16:38:38.726229 139735039037440 ddar.py:60] Depth 16/1000 time = 11.249244451522827
I0123 16:38:49.947057 139735039037440 ddar.py:60] Depth 17/1000 time = 11.220561981201172
I0123 16:39:01.341696 139735039037440 ddar.py:60] Depth 18/1000 time = 11.348222017288208
I0123 16:39:01.342024 139735039037440 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 16:39:01.342130 139735039037440 alphageometry.py:566] LM output (score=-2.496935): "n : T a c c n 20 ;"
I0123 16:39:01.342168 139735039037440 alphageometry.py:567] Translation: "n = on_tline n c a c"

I0123 16:39:01.342209 139735039037440 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = on_tline n c a c ? eqratio b d h d m d b d"
I0123 16:39:01.342380 139735039037440 graph.py:498] 
I0123 16:39:01.342443 139735039037440 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = on_tline n c a c ? eqratio b d h d m d b d
I0123 16:39:02.673005 139735039037440 ddar.py:60] Depth 1/1000 time = 1.2924823760986328
I0123 16:39:05.771292 139735039037440 ddar.py:60] Depth 2/1000 time = 3.0981173515319824
I0123 16:39:10.373248 139735039037440 ddar.py:60] Depth 3/1000 time = 4.601779222488403
I0123 16:39:15.575189 139735039037440 ddar.py:60] Depth 4/1000 time = 5.2017292976379395
I0123 16:39:20.951945 139735039037440 ddar.py:60] Depth 5/1000 time = 5.376523733139038
I0123 16:39:26.280989 139735039037440 ddar.py:60] Depth 6/1000 time = 5.328546047210693
I0123 16:39:32.806985 139735039037440 ddar.py:60] Depth 7/1000 time = 6.5170910358428955
I0123 16:39:39.160812 139735039037440 ddar.py:60] Depth 8/1000 time = 6.353621006011963
I0123 16:39:46.046660 139735039037440 ddar.py:60] Depth 9/1000 time = 6.821469306945801
I0123 16:39:53.478929 139735039037440 ddar.py:60] Depth 10/1000 time = 7.432055234909058
I0123 16:40:01.127723 139735039037440 ddar.py:60] Depth 11/1000 time = 7.648364543914795
I0123 16:40:09.435690 139735039037440 ddar.py:60] Depth 12/1000 time = 8.307716369628906
I0123 16:40:17.514903 139735039037440 ddar.py:60] Depth 13/1000 time = 8.078945875167847
I0123 16:40:25.644134 139735039037440 ddar.py:60] Depth 14/1000 time = 8.08077597618103
I0123 16:40:35.199654 139735039037440 ddar.py:60] Depth 15/1000 time = 9.555298328399658
I0123 16:40:45.163098 139735039037440 ddar.py:60] Depth 16/1000 time = 9.963158369064331
I0123 16:40:55.142972 139735039037440 ddar.py:60] Depth 17/1000 time = 9.979613542556763
I0123 16:41:05.346645 139735039037440 ddar.py:60] Depth 18/1000 time = 10.16246247291565
I0123 16:41:05.346963 139735039037440 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 16:41:05.347056 139735039037440 alphageometry.py:566] LM output (score=-2.511258): "n : D a n c n 20 ;"
I0123 16:41:05.347092 139735039037440 alphageometry.py:567] Translation: "n = on_bline n c a"

I0123 16:41:05.347131 139735039037440 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = on_bline n c a ? eqratio b d h d m d b d"
I0123 16:41:05.347295 139735039037440 graph.py:498] 
I0123 16:41:05.347355 139735039037440 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = on_bline n c a ? eqratio b d h d m d b d
I0123 16:41:06.720590 139735039037440 ddar.py:60] Depth 1/1000 time = 1.333167314529419
I0123 16:41:10.007831 139735039037440 ddar.py:60] Depth 2/1000 time = 3.2870731353759766
I0123 16:41:14.715352 139735039037440 ddar.py:60] Depth 3/1000 time = 4.707336187362671
I0123 16:41:19.474427 139735039037440 ddar.py:60] Depth 4/1000 time = 4.758889198303223
I0123 16:41:25.686316 139735039037440 ddar.py:60] Depth 5/1000 time = 6.211679935455322
I0123 16:41:31.609037 139735039037440 ddar.py:60] Depth 6/1000 time = 5.922214984893799
I0123 16:41:38.547561 139735039037440 ddar.py:60] Depth 7/1000 time = 6.9294726848602295
I0123 16:41:45.991848 139735039037440 ddar.py:60] Depth 8/1000 time = 7.444043874740601
I0123 16:41:53.697824 139735039037440 ddar.py:60] Depth 9/1000 time = 7.637349367141724
I0123 16:42:02.324980 139735039037440 ddar.py:60] Depth 10/1000 time = 8.626933574676514
I0123 16:42:11.513133 139735039037440 ddar.py:60] Depth 11/1000 time = 9.187874555587769
I0123 16:42:20.633032 139735039037440 ddar.py:60] Depth 12/1000 time = 9.119656085968018
I0123 16:42:30.183151 139735039037440 ddar.py:60] Depth 13/1000 time = 9.549794673919678
I0123 16:42:39.944484 139735039037440 ddar.py:60] Depth 14/1000 time = 9.703622341156006
I0123 16:42:51.009654 139735039037440 ddar.py:60] Depth 15/1000 time = 11.064871072769165
I0123 16:43:02.028495 139735039037440 ddar.py:60] Depth 16/1000 time = 11.018463134765625
I0123 16:43:13.511039 139735039037440 ddar.py:60] Depth 17/1000 time = 11.482242345809937
I0123 16:43:25.262019 139735039037440 ddar.py:60] Depth 18/1000 time = 11.709286451339722
I0123 16:43:25.262336 139735039037440 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 16:43:25.262441 139735039037440 alphageometry.py:566] LM output (score=-2.548882): "n : T b c c n 20 ;"
I0123 16:43:25.262477 139735039037440 alphageometry.py:567] Translation: "n = on_tline n c b c"

I0123 16:43:25.262516 139735039037440 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = on_tline n c b c ? eqratio b d h d m d b d"
I0123 16:43:25.262683 139735039037440 graph.py:498] 
I0123 16:43:25.262747 139735039037440 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = on_tline n c b c ? eqratio b d h d m d b d
I0123 16:43:26.299399 139735039037440 ddar.py:60] Depth 1/1000 time = 0.996793270111084
I0123 16:43:29.409394 139735039037440 ddar.py:60] Depth 2/1000 time = 3.109795570373535
I0123 16:43:34.106600 139735039037440 ddar.py:60] Depth 3/1000 time = 4.696935176849365
I0123 16:43:39.279546 139735039037440 ddar.py:60] Depth 4/1000 time = 5.172778844833374
I0123 16:43:44.764683 139735039037440 ddar.py:60] Depth 5/1000 time = 5.484959125518799
I0123 16:43:50.293230 139735039037440 ddar.py:60] Depth 6/1000 time = 5.528029918670654
I0123 16:43:56.682941 139735039037440 ddar.py:60] Depth 7/1000 time = 6.380460262298584
I0123 16:44:03.338917 139735039037440 ddar.py:60] Depth 8/1000 time = 6.6557652950286865
I0123 16:44:10.233153 139735039037440 ddar.py:60] Depth 9/1000 time = 6.823968887329102
I0123 16:44:17.956927 139735039037440 ddar.py:60] Depth 10/1000 time = 7.723554372787476
I0123 16:44:26.180322 139735039037440 ddar.py:60] Depth 11/1000 time = 8.223159313201904
I0123 16:44:34.380975 139735039037440 ddar.py:60] Depth 12/1000 time = 8.200359344482422
I0123 16:44:42.971691 139735039037440 ddar.py:60] Depth 13/1000 time = 8.590323448181152
I0123 16:44:51.405123 139735039037440 ddar.py:60] Depth 14/1000 time = 8.384527921676636
I0123 16:45:00.991003 139735039037440 ddar.py:60] Depth 15/1000 time = 9.58558177947998
I0123 16:45:11.303150 139735039037440 ddar.py:60] Depth 16/1000 time = 10.31183671951294
I0123 16:45:21.406890 139735039037440 ddar.py:60] Depth 17/1000 time = 10.103408575057983
I0123 16:45:32.029542 139735039037440 ddar.py:60] Depth 18/1000 time = 10.578940153121948
I0123 16:45:32.029844 139735039037440 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 16:45:32.029961 139735039037440 alphageometry.py:566] LM output (score=-2.657543): "n : T a d a n 20 ;"
I0123 16:45:32.029999 139735039037440 alphageometry.py:567] Translation: "n = on_tline n a a d"

I0123 16:45:32.030038 139735039037440 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = on_tline n a a d ? eqratio b d h d m d b d"
I0123 16:45:32.030219 139735039037440 graph.py:498] 
I0123 16:45:32.030285 139735039037440 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = on_tline n a a d ? eqratio b d h d m d b d
I0123 16:45:32.997274 139735039037440 ddar.py:60] Depth 1/1000 time = 0.9267346858978271
I0123 16:45:36.427057 139735039037440 ddar.py:60] Depth 2/1000 time = 3.429621696472168
I0123 16:45:44.537618 139735039037440 ddar.py:60] Depth 3/1000 time = 8.11035704612732
I0123 16:45:52.000451 139735039037440 ddar.py:60] Depth 4/1000 time = 7.462602853775024
I0123 16:45:59.453909 139735039037440 ddar.py:60] Depth 5/1000 time = 7.45318341255188
I0123 16:46:07.427555 139735039037440 ddar.py:60] Depth 6/1000 time = 7.972818851470947
I0123 16:46:15.082769 139735039037440 ddar.py:60] Depth 7/1000 time = 7.5865092277526855
I0123 16:46:23.630330 139735039037440 ddar.py:60] Depth 8/1000 time = 8.547236680984497
I0123 16:46:32.842027 139735039037440 ddar.py:60] Depth 9/1000 time = 9.211302042007446
I0123 16:46:41.822306 139735039037440 ddar.py:60] Depth 10/1000 time = 8.979938983917236
I0123 16:46:50.878337 139735039037440 ddar.py:60] Depth 11/1000 time = 9.055603981018066
I0123 16:47:00.484210 139735039037440 ddar.py:60] Depth 12/1000 time = 9.556341886520386
I0123 16:47:10.926106 139735039037440 ddar.py:60] Depth 13/1000 time = 10.441583633422852
I0123 16:47:22.173339 139735039037440 ddar.py:60] Depth 14/1000 time = 11.24694561958313
I0123 16:47:33.234091 139735039037440 ddar.py:60] Depth 15/1000 time = 11.060474395751953
I0123 16:47:44.298996 139735039037440 ddar.py:60] Depth 16/1000 time = 11.020189762115479
I0123 16:47:44.299357 139735039037440 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 16:47:44.299461 139735039037440 alphageometry.py:566] LM output (score=-2.775326): "n : D b c b n 20 D c d d n 21 ;"
I0123 16:47:44.299497 139735039037440 alphageometry.py:567] Translation: "n = on_circle n b c, on_circle n d c"

I0123 16:47:44.299538 139735039037440 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = on_circle n b c, on_circle n d c ? eqratio b d h d m d b d"
I0123 16:47:44.299709 139735039037440 graph.py:498] 
I0123 16:47:44.299771 139735039037440 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = on_circle n b c, on_circle n d c ? eqratio b d h d m d b d
I0123 16:47:45.664287 139735039037440 ddar.py:60] Depth 1/1000 time = 1.3203279972076416
I0123 16:47:49.538952 139735039037440 ddar.py:60] Depth 2/1000 time = 3.8744657039642334
I0123 16:48:01.858139 139735039037440 ddar.py:60] Depth 3/1000 time = 12.318956851959229
I0123 16:48:12.326429 139735039037440 ddar.py:60] Depth 4/1000 time = 10.468067169189453
I0123 16:48:22.666329 139735039037440 ddar.py:60] Depth 5/1000 time = 10.339664936065674
I0123 16:48:33.048415 139735039037440 ddar.py:60] Depth 6/1000 time = 10.381284475326538
I0123 16:48:43.906898 139735039037440 ddar.py:60] Depth 7/1000 time = 10.731712341308594
I0123 16:48:55.464717 139735039037440 ddar.py:60] Depth 8/1000 time = 11.557490825653076
I0123 16:49:07.707059 139735039037440 ddar.py:60] Depth 9/1000 time = 12.24200701713562
I0123 16:49:20.295311 139735039037440 ddar.py:60] Depth 10/1000 time = 12.587992668151855
I0123 16:49:32.959392 139735039037440 ddar.py:60] Depth 11/1000 time = 12.663775205612183
I0123 16:49:45.476963 139735039037440 ddar.py:60] Depth 12/1000 time = 12.468762874603271
I0123 16:49:59.776268 139735039037440 ddar.py:60] Depth 13/1000 time = 14.298909902572632
I0123 16:50:14.653439 139735039037440 ddar.py:60] Depth 14/1000 time = 14.876734733581543
I0123 16:50:29.176830 139735039037440 ddar.py:60] Depth 15/1000 time = 14.523080825805664
I0123 16:50:44.110334 139735039037440 ddar.py:60] Depth 16/1000 time = 14.872267484664917
I0123 16:50:44.110823 139735039037440 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 16:50:44.110960 139735039037440 alphageometry.py:566] LM output (score=-2.796025): "n : T f g g n 20 ;"
I0123 16:50:44.110996 139735039037440 alphageometry.py:567] Translation: "n = on_tline n g f g"

I0123 16:50:44.111050 139735039037440 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = on_tline n g f g ? eqratio b d h d m d b d"
I0123 16:50:44.111241 139735039037440 graph.py:498] 
I0123 16:50:44.111300 139735039037440 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = on_tline n g f g ? eqratio b d h d m d b d
I0123 16:50:45.571799 139735039037440 ddar.py:60] Depth 1/1000 time = 1.421980857849121
I0123 16:50:48.899905 139735039037440 ddar.py:60] Depth 2/1000 time = 3.3279268741607666
I0123 16:50:53.601980 139735039037440 ddar.py:60] Depth 3/1000 time = 4.701890230178833
I0123 16:50:58.380498 139735039037440 ddar.py:60] Depth 4/1000 time = 4.778334379196167
I0123 16:51:03.978467 139735039037440 ddar.py:60] Depth 5/1000 time = 5.597757339477539
I0123 16:51:09.965807 139735039037440 ddar.py:60] Depth 6/1000 time = 5.986674785614014
I0123 16:51:16.460022 139735039037440 ddar.py:60] Depth 7/1000 time = 6.485412120819092
I0123 16:51:24.162714 139735039037440 ddar.py:60] Depth 8/1000 time = 7.702472925186157
I0123 16:51:31.604244 139735039037440 ddar.py:60] Depth 9/1000 time = 7.379719495773315
I0123 16:51:39.888499 139735039037440 ddar.py:60] Depth 10/1000 time = 8.284042358398438
I0123 16:51:47.967519 139735039037440 ddar.py:60] Depth 11/1000 time = 8.078732967376709
I0123 16:51:57.096491 139735039037440 ddar.py:60] Depth 12/1000 time = 9.128663778305054
I0123 16:52:05.924775 139735039037440 ddar.py:60] Depth 13/1000 time = 8.827950239181519
I0123 16:52:15.053546 139735039037440 ddar.py:60] Depth 14/1000 time = 9.077193975448608
I0123 16:52:25.248102 139735039037440 ddar.py:60] Depth 15/1000 time = 10.194281578063965
I0123 16:52:35.851255 139735039037440 ddar.py:60] Depth 16/1000 time = 10.60285234451294
I0123 16:52:46.510389 139735039037440 ddar.py:60] Depth 17/1000 time = 10.658840894699097
I0123 16:52:57.784975 139735039037440 ddar.py:60] Depth 18/1000 time = 11.231614351272583
I0123 16:52:57.785308 139735039037440 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 16:52:57.785401 139735039037440 alphageometry.py:566] LM output (score=-2.796925): "n : T b f f n 20 ;"
I0123 16:52:57.785438 139735039037440 alphageometry.py:567] Translation: "n = on_tline n f b f"

I0123 16:52:57.785477 139735039037440 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = on_tline n f b f ? eqratio b d h d m d b d"
I0123 16:52:57.785650 139735039037440 graph.py:498] 
I0123 16:52:57.785714 139735039037440 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = on_tline n f b f ? eqratio b d h d m d b d
I0123 16:52:59.485957 139735039037440 ddar.py:60] Depth 1/1000 time = 1.6681361198425293
I0123 16:53:02.954838 139735039037440 ddar.py:60] Depth 2/1000 time = 3.4687044620513916
I0123 16:53:08.404588 139735039037440 ddar.py:60] Depth 3/1000 time = 5.449569463729858
I0123 16:53:14.676871 139735039037440 ddar.py:60] Depth 4/1000 time = 6.272082805633545
I0123 16:53:20.831093 139735039037440 ddar.py:60] Depth 5/1000 time = 6.1540138721466064
I0123 16:53:26.992880 139735039037440 ddar.py:60] Depth 6/1000 time = 6.161360025405884
I0123 16:53:34.763964 139735039037440 ddar.py:60] Depth 7/1000 time = 7.76072096824646
I0123 16:53:42.048061 139735039037440 ddar.py:60] Depth 8/1000 time = 7.283865213394165
I0123 16:53:50.094124 139735039037440 ddar.py:60] Depth 9/1000 time = 7.979114294052124
I0123 16:53:58.971013 139735039037440 ddar.py:60] Depth 10/1000 time = 8.876654624938965
I0123 16:54:08.033773 139735039037440 ddar.py:60] Depth 11/1000 time = 9.062527179718018
I0123 16:54:17.390663 139735039037440 ddar.py:60] Depth 12/1000 time = 9.356616735458374
I0123 16:54:26.861637 139735039037440 ddar.py:60] Depth 13/1000 time = 9.470683097839355
I0123 16:54:36.115686 139735039037440 ddar.py:60] Depth 14/1000 time = 9.201310157775879
I0123 16:54:47.046593 139735039037440 ddar.py:60] Depth 15/1000 time = 10.930513858795166
I0123 16:54:57.790237 139735039037440 ddar.py:60] Depth 16/1000 time = 10.74332332611084
I0123 16:55:09.121994 139735039037440 ddar.py:60] Depth 17/1000 time = 11.33137059211731
I0123 16:55:20.606957 139735039037440 ddar.py:60] Depth 18/1000 time = 11.439232110977173
I0123 16:55:20.607356 139735039037440 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 16:55:20.607492 139735039037440 alphageometry.py:566] LM output (score=-2.864491): "n : D c f e n 20 D c n e f 21 ;"
I0123 16:55:20.607529 139735039037440 alphageometry.py:567] Translation: "n = eqdistance n e c f, eqdistance n c e f"

I0123 16:55:20.607577 139735039037440 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = eqdistance n e c f, eqdistance n c e f ? eqratio b d h d m d b d"
I0123 16:55:20.607763 139735039037440 graph.py:498] 
I0123 16:55:20.607823 139735039037440 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = eqdistance n e c f, eqdistance n c e f ? eqratio b d h d m d b d
I0123 16:55:22.320324 139735039037440 ddar.py:60] Depth 1/1000 time = 1.6626286506652832
I0123 16:55:25.475045 139735039037440 ddar.py:60] Depth 2/1000 time = 3.1545443534851074
I0123 16:55:31.323741 139735039037440 ddar.py:60] Depth 3/1000 time = 5.848513603210449
I0123 16:55:39.389711 139735039037440 ddar.py:60] Depth 4/1000 time = 8.065776824951172
I0123 16:55:47.338346 139735039037440 ddar.py:60] Depth 5/1000 time = 7.948419094085693
I0123 16:55:54.628518 139735039037440 ddar.py:60] Depth 6/1000 time = 7.289637327194214
I0123 16:56:03.676687 139735039037440 ddar.py:60] Depth 7/1000 time = 9.038313150405884
I0123 16:56:13.588725 139735039037440 ddar.py:60] Depth 8/1000 time = 9.911702871322632
I0123 16:56:23.168403 139735039037440 ddar.py:60] Depth 9/1000 time = 9.48476791381836
I0123 16:56:33.736950 139735039037440 ddar.py:60] Depth 10/1000 time = 10.568266153335571
I0123 16:56:45.055958 139735039037440 ddar.py:60] Depth 11/1000 time = 11.31873369216919
I0123 16:56:56.256510 139735039037440 ddar.py:60] Depth 12/1000 time = 11.200252294540405
I0123 16:57:07.434582 139735039037440 ddar.py:60] Depth 13/1000 time = 11.177710771560669
I0123 16:57:19.364980 139735039037440 ddar.py:60] Depth 14/1000 time = 11.872101545333862
I0123 16:57:32.684794 139735039037440 ddar.py:60] Depth 15/1000 time = 13.3195059299469
I0123 16:57:46.461367 139735039037440 ddar.py:60] Depth 16/1000 time = 13.776275157928467
I0123 16:57:59.924615 139735039037440 ddar.py:60] Depth 17/1000 time = 13.46293830871582
I0123 16:58:13.374376 139735039037440 ddar.py:60] Depth 18/1000 time = 13.397158861160278
I0123 16:58:13.374814 139735039037440 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 16:58:13.374958 139735039037440 alphageometry.py:566] LM output (score=-2.876322): "n : T b g g n 20 ;"
I0123 16:58:13.374998 139735039037440 alphageometry.py:567] Translation: "n = on_tline n g b g"

I0123 16:58:13.375048 139735039037440 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = on_tline n g b g ? eqratio b d h d m d b d"
I0123 16:58:13.375237 139735039037440 graph.py:498] 
I0123 16:58:13.375296 139735039037440 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = on_tline n g b g ? eqratio b d h d m d b d
I0123 16:58:14.454040 139735039037440 ddar.py:60] Depth 1/1000 time = 1.0462262630462646
I0123 16:58:17.846369 139735039037440 ddar.py:60] Depth 2/1000 time = 3.392151355743408
I0123 16:58:22.798877 139735039037440 ddar.py:60] Depth 3/1000 time = 4.952329874038696
I0123 16:58:29.117400 139735039037440 ddar.py:60] Depth 4/1000 time = 6.318353891372681
I0123 16:58:35.408411 139735039037440 ddar.py:60] Depth 5/1000 time = 6.290801763534546
I0123 16:58:41.580825 139735039037440 ddar.py:60] Depth 6/1000 time = 6.172008037567139
I0123 16:58:48.590985 139735039037440 ddar.py:60] Depth 7/1000 time = 7.00240159034729
I0123 16:58:56.575901 139735039037440 ddar.py:60] Depth 8/1000 time = 7.98469090461731
I0123 16:59:04.184201 139735039037440 ddar.py:60] Depth 9/1000 time = 7.546095848083496
I0123 16:59:13.069666 139735039037440 ddar.py:60] Depth 10/1000 time = 8.885191202163696
I0123 16:59:22.272937 139735039037440 ddar.py:60] Depth 11/1000 time = 9.20292067527771
I0123 16:59:31.159896 139735039037440 ddar.py:60] Depth 12/1000 time = 8.886726379394531
I0123 16:59:41.070342 139735039037440 ddar.py:60] Depth 13/1000 time = 9.910162925720215
I0123 16:59:50.836799 139735039037440 ddar.py:60] Depth 14/1000 time = 9.720573663711548
I0123 17:00:01.218125 139735039037440 ddar.py:60] Depth 15/1000 time = 10.38106083869934
I0123 17:00:12.495491 139735039037440 ddar.py:60] Depth 16/1000 time = 11.277090311050415
I0123 17:00:23.397336 139735039037440 ddar.py:60] Depth 17/1000 time = 10.901510000228882
I0123 17:00:34.839000 139735039037440 ddar.py:60] Depth 18/1000 time = 11.397087812423706
I0123 17:00:34.839290 139735039037440 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 17:00:34.839403 139735039037440 alphageometry.py:566] LM output (score=-2.890132): "n : D c i c n 20 D g i g n 21 ;"
I0123 17:00:34.839439 139735039037440 alphageometry.py:567] Translation: "n = on_circle n c i, on_circle n g i"

I0123 17:00:34.839478 139735039037440 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = on_circle n c i, on_circle n g i ? eqratio b d h d m d b d"
I0123 17:00:34.839656 139735039037440 graph.py:498] 
I0123 17:00:34.839718 139735039037440 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = on_circle n c i, on_circle n g i ? eqratio b d h d m d b d
I0123 17:00:36.022841 139735039037440 ddar.py:60] Depth 1/1000 time = 1.1305618286132812
I0123 17:00:40.022718 139735039037440 ddar.py:60] Depth 2/1000 time = 3.9996073246002197
I0123 17:00:45.422494 139735039037440 ddar.py:60] Depth 3/1000 time = 5.399610757827759
I0123 17:00:53.212913 139735039037440 ddar.py:60] Depth 4/1000 time = 7.790232181549072
I0123 17:01:01.442517 139735039037440 ddar.py:60] Depth 5/1000 time = 8.2294020652771
I0123 17:01:09.660571 139735039037440 ddar.py:60] Depth 6/1000 time = 8.217823028564453
I0123 17:01:17.932667 139735039037440 ddar.py:60] Depth 7/1000 time = 8.271608829498291
I0123 17:01:27.345537 139735039037440 ddar.py:60] Depth 8/1000 time = 9.403128147125244
I0123 17:01:36.956289 139735039037440 ddar.py:60] Depth 9/1000 time = 9.610536336898804
I0123 17:01:47.167141 139735039037440 ddar.py:60] Depth 10/1000 time = 10.175700902938843
I0123 17:01:57.679945 139735039037440 ddar.py:60] Depth 11/1000 time = 10.453089714050293
I0123 17:02:09.551608 139735039037440 ddar.py:60] Depth 12/1000 time = 11.871419429779053
I0123 17:02:21.108024 139735039037440 ddar.py:60] Depth 13/1000 time = 11.556081295013428
I0123 17:02:33.709317 139735039037440 ddar.py:60] Depth 14/1000 time = 12.600905895233154
I0123 17:02:45.779230 139735039037440 ddar.py:60] Depth 15/1000 time = 12.06957745552063
I0123 17:02:58.547167 139735039037440 ddar.py:60] Depth 16/1000 time = 12.739243268966675
I0123 17:03:11.113687 139735039037440 ddar.py:60] Depth 17/1000 time = 12.527460813522339
I0123 17:03:24.938019 139735039037440 ddar.py:60] Depth 18/1000 time = 13.823974370956421
I0123 17:03:39.818006 139735039037440 ddar.py:60] Depth 19/1000 time = 14.879598379135132
I0123 17:03:55.358828 139735039037440 ddar.py:60] Depth 20/1000 time = 15.540543794631958
I0123 17:04:10.664310 139735039037440 ddar.py:60] Depth 21/1000 time = 15.254459619522095
I0123 17:04:10.664790 139735039037440 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 17:04:10.664942 139735039037440 alphageometry.py:566] LM output (score=-2.891202): "n : P b c e n 20 ;"
I0123 17:04:10.664983 139735039037440 alphageometry.py:567] Translation: "ERROR: Traceback (most recent call last):
  File "/home/chi/alphageometry-test/alphageometry.py", line 438, in try_translate_constrained_to_construct
    g.copy().add_clause(clause, 0, DEFINITIONS)
  File "/home/chi/alphageometry-test/graph.py", line 2570, in add_clause
    raise DepCheckFailError(
graph.DepCheckFailError: ncoll e b c
"

I0123 17:04:10.665036 139735039037440 alphageometry.py:566] LM output (score=-2.945563): "n : T a c a n 20 ;"
I0123 17:04:10.665065 139735039037440 alphageometry.py:567] Translation: "n = on_tline n a a c"

I0123 17:04:10.665098 139735039037440 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = on_tline n a a c ? eqratio b d h d m d b d"
I0123 17:04:10.665284 139735039037440 graph.py:498] 
I0123 17:04:10.665346 139735039037440 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = on_tline n a a c ? eqratio b d h d m d b d
I0123 17:04:11.708339 139735039037440 ddar.py:60] Depth 1/1000 time = 1.0010757446289062
I0123 17:04:14.611837 139735039037440 ddar.py:60] Depth 2/1000 time = 2.9033305644989014
I0123 17:04:19.420842 139735039037440 ddar.py:60] Depth 3/1000 time = 4.808847665786743
I0123 17:04:24.162257 139735039037440 ddar.py:60] Depth 4/1000 time = 4.741177082061768
I0123 17:04:29.970846 139735039037440 ddar.py:60] Depth 5/1000 time = 5.8082733154296875
I0123 17:04:35.257405 139735039037440 ddar.py:60] Depth 6/1000 time = 5.286091327667236
I0123 17:04:41.298672 139735039037440 ddar.py:60] Depth 7/1000 time = 6.033440351486206
I0123 17:04:47.635496 139735039037440 ddar.py:60] Depth 8/1000 time = 6.336554050445557
I0123 17:04:54.576506 139735039037440 ddar.py:60] Depth 9/1000 time = 6.874135255813599
I0123 17:05:01.726641 139735039037440 ddar.py:60] Depth 10/1000 time = 7.149918556213379
I0123 17:05:09.775320 139735039037440 ddar.py:60] Depth 11/1000 time = 8.048380613327026
I0123 17:05:17.486474 139735039037440 ddar.py:60] Depth 12/1000 time = 7.710859775543213
I0123 17:05:25.788359 139735039037440 ddar.py:60] Depth 13/1000 time = 8.301647424697876
I0123 17:05:34.454599 139735039037440 ddar.py:60] Depth 14/1000 time = 8.625178337097168
I0123 17:05:43.523978 139735039037440 ddar.py:60] Depth 15/1000 time = 9.069114685058594
I0123 17:05:52.905712 139735039037440 ddar.py:60] Depth 16/1000 time = 9.381417512893677
I0123 17:06:02.854690 139735039037440 ddar.py:60] Depth 17/1000 time = 9.94866394996643
I0123 17:06:13.056797 139735039037440 ddar.py:60] Depth 18/1000 time = 10.15929365158081
I0123 17:06:13.057233 139735039037440 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 17:06:13.057375 139735039037440 alphageometry.py:566] LM output (score=-2.970116): "n : D f i j n 20 D f n i j 21 ;"
I0123 17:06:13.057415 139735039037440 alphageometry.py:567] Translation: "n = eqdistance n j f i, eqdistance n f i j"

I0123 17:06:13.057468 139735039037440 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = eqdistance n j f i, eqdistance n f i j ? eqratio b d h d m d b d"
I0123 17:06:13.057668 139735039037440 graph.py:498] 
I0123 17:06:13.057733 139735039037440 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = eqdistance n j f i, eqdistance n f i j ? eqratio b d h d m d b d
I0123 17:06:14.765009 139735039037440 ddar.py:60] Depth 1/1000 time = 1.6630280017852783
I0123 17:06:18.181950 139735039037440 ddar.py:60] Depth 2/1000 time = 3.4167635440826416
I0123 17:06:24.216167 139735039037440 ddar.py:60] Depth 3/1000 time = 6.034029960632324
I0123 17:06:30.988467 139735039037440 ddar.py:60] Depth 4/1000 time = 6.772085189819336
I0123 17:06:38.050869 139735039037440 ddar.py:60] Depth 5/1000 time = 7.062186241149902
I0123 17:06:45.646747 139735039037440 ddar.py:60] Depth 6/1000 time = 7.595332860946655
I0123 17:06:54.174874 139735039037440 ddar.py:60] Depth 7/1000 time = 8.517343282699585
I0123 17:07:03.376590 139735039037440 ddar.py:60] Depth 8/1000 time = 9.201363801956177
I0123 17:07:12.886836 139735039037440 ddar.py:60] Depth 9/1000 time = 9.417815208435059
I0123 17:07:23.352445 139735039037440 ddar.py:60] Depth 10/1000 time = 10.465238094329834
I0123 17:07:34.119809 139735039037440 ddar.py:60] Depth 11/1000 time = 10.767052412033081
I0123 17:07:45.222320 139735039037440 ddar.py:60] Depth 12/1000 time = 11.10210371017456
I0123 17:07:55.712056 139735039037440 ddar.py:60] Depth 13/1000 time = 10.48936152458191
I0123 17:08:07.463814 139735039037440 ddar.py:60] Depth 14/1000 time = 11.69941234588623
I0123 17:08:19.998280 139735039037440 ddar.py:60] Depth 15/1000 time = 12.534144163131714
I0123 17:08:32.935316 139735039037440 ddar.py:60] Depth 16/1000 time = 12.936687707901001
I0123 17:08:46.101134 139735039037440 ddar.py:60] Depth 17/1000 time = 13.165534257888794
I0123 17:08:59.391881 139735039037440 ddar.py:60] Depth 18/1000 time = 13.242991209030151
I0123 17:08:59.392383 139735039037440 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 17:08:59.392518 139735039037440 alphageometry.py:566] LM output (score=-2.983292): "n : D b c c n 20 D b d d n 21 ;"
I0123 17:08:59.392555 139735039037440 alphageometry.py:567] Translation: "n = on_circle n c b, on_circle n d b"

I0123 17:08:59.392608 139735039037440 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = on_circle n c b, on_circle n d b ? eqratio b d h d m d b d"
I0123 17:08:59.392820 139735039037440 graph.py:498] 
I0123 17:08:59.392879 139735039037440 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = on_circle n c b, on_circle n d b ? eqratio b d h d m d b d
I0123 17:09:01.229264 139735039037440 ddar.py:60] Depth 1/1000 time = 1.7924773693084717
I0123 17:09:04.950297 139735039037440 ddar.py:60] Depth 2/1000 time = 3.7208564281463623
I0123 17:09:17.291282 139735039037440 ddar.py:60] Depth 3/1000 time = 12.34072470664978
I0123 17:09:26.884820 139735039037440 ddar.py:60] Depth 4/1000 time = 9.593149662017822
I0123 17:09:36.303912 139735039037440 ddar.py:60] Depth 5/1000 time = 9.418776988983154
I0123 17:09:45.232890 139735039037440 ddar.py:60] Depth 6/1000 time = 8.928111791610718
I0123 17:09:54.728525 139735039037440 ddar.py:60] Depth 7/1000 time = 9.383683681488037
I0123 17:10:05.157351 139735039037440 ddar.py:60] Depth 8/1000 time = 10.428508043289185
I0123 17:10:15.804610 139735039037440 ddar.py:60] Depth 9/1000 time = 10.646969556808472
I0123 17:10:26.785070 139735039037440 ddar.py:60] Depth 10/1000 time = 10.980146408081055
I0123 17:10:37.798378 139735039037440 ddar.py:60] Depth 11/1000 time = 11.013038635253906
I0123 17:10:49.075514 139735039037440 ddar.py:60] Depth 12/1000 time = 11.23040246963501
I0123 17:11:01.592976 139735039037440 ddar.py:60] Depth 13/1000 time = 12.517120838165283
I0123 17:11:14.725861 139735039037440 ddar.py:60] Depth 14/1000 time = 13.13255500793457
I0123 17:11:27.348129 139735039037440 ddar.py:60] Depth 15/1000 time = 12.621896028518677
I0123 17:11:27.402006 139735039037440 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 17:11:27.402143 139735039037440 alphageometry.py:566] LM output (score=-3.115156): "n : D a i a n 20 D c i c n 21 ;"
I0123 17:11:27.402181 139735039037440 alphageometry.py:567] Translation: "n = on_circle n a i, on_circle n c i"

I0123 17:11:27.402233 139735039037440 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = on_circle n a i, on_circle n c i ? eqratio b d h d m d b d"
I0123 17:11:27.402420 139735039037440 graph.py:498] 
I0123 17:11:27.402479 139735039037440 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = on_circle n a i, on_circle n c i ? eqratio b d h d m d b d
I0123 17:11:29.202859 139735039037440 ddar.py:60] Depth 1/1000 time = 1.7528371810913086
I0123 17:11:32.726806 139735039037440 ddar.py:60] Depth 2/1000 time = 3.5237843990325928
I0123 17:11:38.881234 139735039037440 ddar.py:60] Depth 3/1000 time = 6.154237508773804
I0123 17:11:45.430031 139735039037440 ddar.py:60] Depth 4/1000 time = 6.54856276512146
I0123 17:11:52.100401 139735039037440 ddar.py:60] Depth 5/1000 time = 6.670126438140869
I0123 17:11:58.741394 139735039037440 ddar.py:60] Depth 6/1000 time = 6.640533447265625
I0123 17:12:06.924000 139735039037440 ddar.py:60] Depth 7/1000 time = 8.173799991607666
I0123 17:12:15.470211 139735039037440 ddar.py:60] Depth 8/1000 time = 8.54596495628357
I0123 17:12:23.332825 139735039037440 ddar.py:60] Depth 9/1000 time = 7.862158536911011
I0123 17:12:31.929740 139735039037440 ddar.py:60] Depth 10/1000 time = 8.563708543777466
I0123 17:12:40.854761 139735039037440 ddar.py:60] Depth 11/1000 time = 8.862346410751343
I0123 17:12:50.213900 139735039037440 ddar.py:60] Depth 12/1000 time = 9.358881711959839
I0123 17:13:00.347033 139735039037440 ddar.py:60] Depth 13/1000 time = 10.13285493850708
I0123 17:13:10.939440 139735039037440 ddar.py:60] Depth 14/1000 time = 10.592105150222778
I0123 17:13:20.861608 139735039037440 ddar.py:60] Depth 15/1000 time = 9.921855926513672
I0123 17:13:31.730052 139735039037440 ddar.py:60] Depth 16/1000 time = 10.818050384521484
I0123 17:13:43.834017 139735039037440 ddar.py:60] Depth 17/1000 time = 12.103556394577026
I0123 17:13:56.433088 139735039037440 ddar.py:60] Depth 18/1000 time = 12.598737716674805
I0123 17:14:09.186103 139735039037440 ddar.py:60] Depth 19/1000 time = 12.75263500213623
I0123 17:14:22.102500 139735039037440 ddar.py:60] Depth 20/1000 time = 12.868651628494263
I0123 17:14:22.102795 139735039037440 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 17:14:22.102887 139735039037440 alphageometry.py:566] LM output (score=-3.123308): "n : T b f b n 20 ;"
I0123 17:14:22.102923 139735039037440 alphageometry.py:567] Translation: "n = on_tline n b b f"

I0123 17:14:22.102961 139735039037440 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = on_tline n b b f ? eqratio b d h d m d b d"
I0123 17:14:22.103126 139735039037440 graph.py:498] 
I0123 17:14:22.103188 139735039037440 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = on_tline n b b f ? eqratio b d h d m d b d
I0123 17:14:23.759903 139735039037440 ddar.py:60] Depth 1/1000 time = 1.618408441543579
I0123 17:14:26.895313 139735039037440 ddar.py:60] Depth 2/1000 time = 3.1352407932281494
I0123 17:14:31.486683 139735039037440 ddar.py:60] Depth 3/1000 time = 4.591164827346802
I0123 17:14:37.011271 139735039037440 ddar.py:60] Depth 4/1000 time = 5.524372100830078
I0123 17:14:42.499699 139735039037440 ddar.py:60] Depth 5/1000 time = 5.488234519958496
I0123 17:14:47.347976 139735039037440 ddar.py:60] Depth 6/1000 time = 4.84783148765564
I0123 17:14:54.210184 139735039037440 ddar.py:60] Depth 7/1000 time = 6.85438346862793
I0123 17:15:00.611007 139735039037440 ddar.py:60] Depth 8/1000 time = 6.40062403678894
I0123 17:15:07.278096 139735039037440 ddar.py:60] Depth 9/1000 time = 6.6002113819122314
I0123 17:15:14.768021 139735039037440 ddar.py:60] Depth 10/1000 time = 7.4896955490112305
I0123 17:15:22.973445 139735039037440 ddar.py:60] Depth 11/1000 time = 8.205179929733276
I0123 17:15:30.923575 139735039037440 ddar.py:60] Depth 12/1000 time = 7.949842929840088
I0123 17:15:38.906685 139735039037440 ddar.py:60] Depth 13/1000 time = 7.982740640640259
I0123 17:15:47.739222 139735039037440 ddar.py:60] Depth 14/1000 time = 8.791874885559082
I0123 17:15:57.075409 139735039037440 ddar.py:60] Depth 15/1000 time = 9.335848569869995
I0123 17:16:06.682167 139735039037440 ddar.py:60] Depth 16/1000 time = 9.606420755386353
I0123 17:16:17.101582 139735039037440 ddar.py:60] Depth 17/1000 time = 10.419054985046387
I0123 17:16:26.924065 139735039037440 ddar.py:60] Depth 18/1000 time = 9.781299352645874
I0123 17:16:26.924330 139735039037440 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 17:16:26.924427 139735039037440 alphageometry.py:566] LM output (score=-3.128717): "n : D c i j n 20 P c i j n 21 ;"
I0123 17:16:26.924463 139735039037440 alphageometry.py:567] Translation: "n = eqdistance n j c i, on_pline n j c i"

I0123 17:16:26.924501 139735039037440 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = eqdistance n j c i, on_pline n j c i ? eqratio b d h d m d b d"
I0123 17:16:26.924683 139735039037440 graph.py:498] 
I0123 17:16:26.924751 139735039037440 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = eqdistance n j c i, on_pline n j c i ? eqratio b d h d m d b d
I0123 17:16:28.711775 139735039037440 ddar.py:60] Depth 1/1000 time = 1.7390022277832031
I0123 17:16:32.205557 139735039037440 ddar.py:60] Depth 2/1000 time = 3.4935998916625977
I0123 17:16:38.088672 139735039037440 ddar.py:60] Depth 3/1000 time = 5.8829026222229
I0123 17:16:44.563863 139735039037440 ddar.py:60] Depth 4/1000 time = 6.474984884262085
I0123 17:16:50.269039 139735039037440 ddar.py:60] Depth 5/1000 time = 5.70497727394104
I0123 17:16:57.407887 139735039037440 ddar.py:60] Depth 6/1000 time = 7.138378381729126
I0123 17:17:04.801928 139735039037440 ddar.py:60] Depth 7/1000 time = 7.384215593338013
I0123 17:17:12.455141 139735039037440 ddar.py:60] Depth 8/1000 time = 7.653008460998535
I0123 17:17:20.968166 139735039037440 ddar.py:60] Depth 9/1000 time = 8.427647590637207
I0123 17:17:29.626323 139735039037440 ddar.py:60] Depth 10/1000 time = 8.657913208007812
I0123 17:17:39.232011 139735039037440 ddar.py:60] Depth 11/1000 time = 9.605367183685303
I0123 17:17:48.488771 139735039037440 ddar.py:60] Depth 12/1000 time = 9.2563796043396
I0123 17:17:58.439281 139735039037440 ddar.py:60] Depth 13/1000 time = 9.95018219947815
I0123 17:18:08.548221 139735039037440 ddar.py:60] Depth 14/1000 time = 10.058382987976074
I0123 17:18:19.972629 139735039037440 ddar.py:60] Depth 15/1000 time = 11.424118757247925
I0123 17:18:30.985420 139735039037440 ddar.py:60] Depth 16/1000 time = 11.012518167495728
I0123 17:18:42.877269 139735039037440 ddar.py:60] Depth 17/1000 time = 11.891484260559082
I0123 17:18:55.006515 139735039037440 ddar.py:60] Depth 18/1000 time = 12.084597826004028
I0123 17:18:55.006837 139735039037440 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 17:18:55.006938 139735039037440 alphageometry.py:566] LM output (score=-3.134105): "n : D b n c n 20 ;"
I0123 17:18:55.006973 139735039037440 alphageometry.py:567] Translation: "n = on_bline n c b"

I0123 17:18:55.007013 139735039037440 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = on_bline n c b ? eqratio b d h d m d b d"
I0123 17:18:55.007186 139735039037440 graph.py:498] 
I0123 17:18:55.007248 139735039037440 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = on_bline n c b ? eqratio b d h d m d b d
I0123 17:18:56.090140 139735039037440 ddar.py:60] Depth 1/1000 time = 1.0425190925598145
I0123 17:18:59.341259 139735039037440 ddar.py:60] Depth 2/1000 time = 3.2508955001831055
I0123 17:19:04.778460 139735039037440 ddar.py:60] Depth 3/1000 time = 5.436923503875732
I0123 17:19:10.562516 139735039037440 ddar.py:60] Depth 4/1000 time = 5.783862113952637
I0123 17:19:16.370883 139735039037440 ddar.py:60] Depth 5/1000 time = 5.808170318603516
I0123 17:19:22.267240 139735039037440 ddar.py:60] Depth 6/1000 time = 5.89589262008667
I0123 17:19:29.016621 139735039037440 ddar.py:60] Depth 7/1000 time = 6.740566730499268
I0123 17:19:36.163523 139735039037440 ddar.py:60] Depth 8/1000 time = 7.146720886230469
I0123 17:19:43.637160 139735039037440 ddar.py:60] Depth 9/1000 time = 7.403486490249634
I0123 17:19:51.907974 139735039037440 ddar.py:60] Depth 10/1000 time = 8.270468950271606
I0123 17:20:01.161152 139735039037440 ddar.py:60] Depth 11/1000 time = 9.252946376800537
I0123 17:20:10.100598 139735039037440 ddar.py:60] Depth 12/1000 time = 8.9391930103302
I0123 17:20:19.008200 139735039037440 ddar.py:60] Depth 13/1000 time = 8.907328367233276
I0123 17:20:28.198678 139735039037440 ddar.py:60] Depth 14/1000 time = 9.140839099884033
I0123 17:20:38.523319 139735039037440 ddar.py:60] Depth 15/1000 time = 10.324323654174805
I0123 17:20:49.129222 139735039037440 ddar.py:60] Depth 16/1000 time = 10.605570554733276
I0123 17:21:00.511935 139735039037440 ddar.py:60] Depth 17/1000 time = 11.382306575775146
I0123 17:21:11.417905 139735039037440 ddar.py:60] Depth 18/1000 time = 10.861358642578125
I0123 17:21:11.418258 139735039037440 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 17:21:11.418356 139735039037440 alphageometry.py:566] LM output (score=-3.134387): "n : D c n d f 20 P c n d f 21 ;"
I0123 17:21:11.418392 139735039037440 alphageometry.py:567] Translation: "n = eqdistance n c d f, on_pline n c d f"

I0123 17:21:11.418435 139735039037440 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = eqdistance n c d f, on_pline n c d f ? eqratio b d h d m d b d"
I0123 17:21:11.418612 139735039037440 graph.py:498] 
I0123 17:21:11.418673 139735039037440 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = eqdistance n c d f, on_pline n c d f ? eqratio b d h d m d b d
I0123 17:21:12.485204 139735039037440 ddar.py:60] Depth 1/1000 time = 1.0207912921905518
I0123 17:21:15.720927 139735039037440 ddar.py:60] Depth 2/1000 time = 3.235541343688965
I0123 17:21:21.282773 139735039037440 ddar.py:60] Depth 3/1000 time = 5.561673641204834
I0123 17:21:27.351104 139735039037440 ddar.py:60] Depth 4/1000 time = 6.068082809448242
I0123 17:21:32.705278 139735039037440 ddar.py:60] Depth 5/1000 time = 5.353847026824951
I0123 17:21:38.646962 139735039037440 ddar.py:60] Depth 6/1000 time = 5.941213846206665
I0123 17:21:46.204869 139735039037440 ddar.py:60] Depth 7/1000 time = 7.549911737442017
I0123 17:21:53.398661 139735039037440 ddar.py:60] Depth 8/1000 time = 7.193453311920166
I0123 17:22:00.581650 139735039037440 ddar.py:60] Depth 9/1000 time = 7.119091033935547
I0123 17:22:08.773884 139735039037440 ddar.py:60] Depth 10/1000 time = 8.191978454589844
I0123 17:22:17.221330 139735039037440 ddar.py:60] Depth 11/1000 time = 8.447092533111572
I0123 17:22:25.789718 139735039037440 ddar.py:60] Depth 12/1000 time = 8.568147659301758
I0123 17:22:35.267822 139735039037440 ddar.py:60] Depth 13/1000 time = 9.47782564163208
I0123 17:22:44.207967 139735039037440 ddar.py:60] Depth 14/1000 time = 8.898075580596924
I0123 17:22:54.311213 139735039037440 ddar.py:60] Depth 15/1000 time = 10.10291051864624
I0123 17:23:04.746087 139735039037440 ddar.py:60] Depth 16/1000 time = 10.434451580047607
I0123 17:23:15.974436 139735039037440 ddar.py:60] Depth 17/1000 time = 11.228010416030884
I0123 17:23:26.600716 139735039037440 ddar.py:60] Depth 18/1000 time = 10.581215620040894
I0123 17:23:26.601101 139735039037440 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 17:23:26.601203 139735039037440 alphageometry.py:566] LM output (score=-3.136593): "n : D f i j n 20 P f i j n 21 ;"
I0123 17:23:26.601240 139735039037440 alphageometry.py:567] Translation: "n = eqdistance n j f i, on_pline n j f i"

I0123 17:23:26.601280 139735039037440 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = eqdistance n j f i, on_pline n j f i ? eqratio b d h d m d b d"
I0123 17:23:26.601462 139735039037440 graph.py:498] 
I0123 17:23:26.601536 139735039037440 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = eqdistance n j f i, on_pline n j f i ? eqratio b d h d m d b d
I0123 17:23:28.420959 139735039037440 ddar.py:60] Depth 1/1000 time = 1.7747702598571777
I0123 17:23:32.400495 139735039037440 ddar.py:60] Depth 2/1000 time = 3.97928786277771
I0123 17:23:38.092539 139735039037440 ddar.py:60] Depth 3/1000 time = 5.691736698150635
I0123 17:23:44.856104 139735039037440 ddar.py:60] Depth 4/1000 time = 6.763376235961914
I0123 17:23:52.480405 139735039037440 ddar.py:60] Depth 5/1000 time = 7.624063730239868
I0123 17:23:58.655203 139735039037440 ddar.py:60] Depth 6/1000 time = 6.174135684967041
I0123 17:24:07.136310 139735039037440 ddar.py:60] Depth 7/1000 time = 8.47281789779663
I0123 17:24:15.261888 139735039037440 ddar.py:60] Depth 8/1000 time = 8.125333309173584
I0123 17:24:23.694439 139735039037440 ddar.py:60] Depth 9/1000 time = 8.363067150115967
I0123 17:24:32.908113 139735039037440 ddar.py:60] Depth 10/1000 time = 9.213480472564697
I0123 17:24:43.104484 139735039037440 ddar.py:60] Depth 11/1000 time = 10.196142435073853
I0123 17:24:52.890011 139735039037440 ddar.py:60] Depth 12/1000 time = 9.785301685333252
I0123 17:25:02.659049 139735039037440 ddar.py:60] Depth 13/1000 time = 9.76880693435669
I0123 17:25:13.295674 139735039037440 ddar.py:60] Depth 14/1000 time = 10.58871579170227
I0123 17:25:24.444293 139735039037440 ddar.py:60] Depth 15/1000 time = 11.14835810661316
I0123 17:25:36.670939 139735039037440 ddar.py:60] Depth 16/1000 time = 12.226314783096313
I0123 17:25:48.313667 139735039037440 ddar.py:60] Depth 17/1000 time = 11.64233136177063
I0123 17:26:00.878180 139735039037440 ddar.py:60] Depth 18/1000 time = 12.521456241607666
I0123 17:26:00.878687 139735039037440 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 17:26:00.878823 139735039037440 alphageometry.py:566] LM output (score=-3.139597): "n : T c g g n 20 ;"
I0123 17:26:00.878860 139735039037440 alphageometry.py:567] Translation: "n = on_tline n g c g"

I0123 17:26:00.878912 139735039037440 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = on_tline n g c g ? eqratio b d h d m d b d"
I0123 17:26:00.879104 139735039037440 graph.py:498] 
I0123 17:26:00.879165 139735039037440 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = on_tline n g c g ? eqratio b d h d m d b d
I0123 17:26:02.042494 139735039037440 ddar.py:60] Depth 1/1000 time = 1.1305875778198242
I0123 17:26:06.054308 139735039037440 ddar.py:60] Depth 2/1000 time = 4.011624336242676
I0123 17:26:11.678782 139735039037440 ddar.py:60] Depth 3/1000 time = 5.624294996261597
I0123 17:26:17.504492 139735039037440 ddar.py:60] Depth 4/1000 time = 5.8255274295806885
I0123 17:26:24.313074 139735039037440 ddar.py:60] Depth 5/1000 time = 6.808371543884277
I0123 17:26:31.758292 139735039037440 ddar.py:60] Depth 6/1000 time = 7.444645166397095
I0123 17:26:40.432119 139735039037440 ddar.py:60] Depth 7/1000 time = 8.663988828659058
I0123 17:26:48.504304 139735039037440 ddar.py:60] Depth 8/1000 time = 8.071827411651611
I0123 17:26:58.116391 139735039037440 ddar.py:60] Depth 9/1000 time = 9.551069021224976
I0123 17:27:08.081620 139735039037440 ddar.py:60] Depth 10/1000 time = 9.964953660964966
I0123 17:27:17.390945 139735039037440 ddar.py:60] Depth 11/1000 time = 9.309041976928711
I0123 17:27:28.586450 139735039037440 ddar.py:60] Depth 12/1000 time = 11.195215702056885
I0123 17:27:38.285973 139735039037440 ddar.py:60] Depth 13/1000 time = 9.699239730834961
I0123 17:27:49.798568 139735039037440 ddar.py:60] Depth 14/1000 time = 11.458077430725098
I0123 17:28:01.549369 139735039037440 ddar.py:60] Depth 15/1000 time = 11.750539779663086
I0123 17:28:13.832659 139735039037440 ddar.py:60] Depth 16/1000 time = 12.282980680465698
I0123 17:28:26.185700 139735039037440 ddar.py:60] Depth 17/1000 time = 12.352638006210327
I0123 17:28:38.630548 139735039037440 ddar.py:60] Depth 18/1000 time = 12.40250563621521
I0123 17:28:38.630855 139735039037440 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 17:28:38.630958 139735039037440 alphageometry.py:566] LM output (score=-3.145992): "n : T b j j n 20 ;"
I0123 17:28:38.630994 139735039037440 alphageometry.py:567] Translation: "n = on_tline n j b j"

I0123 17:28:38.631033 139735039037440 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = on_tline n j b j ? eqratio b d h d m d b d"
I0123 17:28:38.631203 139735039037440 graph.py:498] 
I0123 17:28:38.631264 139735039037440 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = on_tline n j b j ? eqratio b d h d m d b d
I0123 17:28:40.298371 139735039037440 ddar.py:60] Depth 1/1000 time = 1.6286239624023438
I0123 17:28:43.626094 139735039037440 ddar.py:60] Depth 2/1000 time = 3.327547073364258
I0123 17:28:48.342795 139735039037440 ddar.py:60] Depth 3/1000 time = 4.716516733169556
I0123 17:28:53.739111 139735039037440 ddar.py:60] Depth 4/1000 time = 5.3961145877838135
I0123 17:28:59.297157 139735039037440 ddar.py:60] Depth 5/1000 time = 5.557866334915161
I0123 17:29:05.585468 139735039037440 ddar.py:60] Depth 6/1000 time = 6.287822008132935
I0123 17:29:12.958803 139735039037440 ddar.py:60] Depth 7/1000 time = 7.363695383071899
I0123 17:29:20.327678 139735039037440 ddar.py:60] Depth 8/1000 time = 7.368680715560913
I0123 17:29:28.117880 139735039037440 ddar.py:60] Depth 9/1000 time = 7.719745635986328
I0123 17:29:36.817193 139735039037440 ddar.py:60] Depth 10/1000 time = 8.699034452438354
I0123 17:29:45.773768 139735039037440 ddar.py:60] Depth 11/1000 time = 8.956339120864868
I0123 17:29:55.030842 139735039037440 ddar.py:60] Depth 12/1000 time = 9.256801128387451
I0123 17:30:04.342349 139735039037440 ddar.py:60] Depth 13/1000 time = 9.311251401901245
I0123 17:30:13.873139 139735039037440 ddar.py:60] Depth 14/1000 time = 9.468215227127075
I0123 17:30:24.691744 139735039037440 ddar.py:60] Depth 15/1000 time = 10.818320512771606
I0123 17:30:36.009967 139735039037440 ddar.py:60] Depth 16/1000 time = 11.317909479141235
I0123 17:30:47.394023 139735039037440 ddar.py:60] Depth 17/1000 time = 11.383734226226807
I0123 17:30:58.920709 139735039037440 ddar.py:60] Depth 18/1000 time = 11.482195138931274
I0123 17:30:58.921030 139735039037440 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 17:30:58.921122 139735039037440 alphageometry.py:566] LM output (score=-3.190692): "n : T c d i n 20 ;"
I0123 17:30:58.921158 139735039037440 alphageometry.py:567] Translation: "n = on_tline n i c d"

I0123 17:30:58.921197 139735039037440 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = on_tline n i c d ? eqratio b d h d m d b d"
I0123 17:30:58.921362 139735039037440 graph.py:498] 
I0123 17:30:58.921424 139735039037440 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = on_tline n i c d ? eqratio b d h d m d b d
I0123 17:31:00.612350 139735039037440 ddar.py:60] Depth 1/1000 time = 1.6521048545837402
I0123 17:31:03.051724 139735039037440 ddar.py:60] Depth 2/1000 time = 2.4392082691192627
I0123 17:31:08.320763 139735039037440 ddar.py:60] Depth 3/1000 time = 5.268868446350098
I0123 17:31:13.362171 139735039037440 ddar.py:60] Depth 4/1000 time = 5.041218042373657
I0123 17:31:19.162607 139735039037440 ddar.py:60] Depth 5/1000 time = 5.800217866897583
I0123 17:31:24.168887 139735039037440 ddar.py:60] Depth 6/1000 time = 5.005787134170532
I0123 17:31:30.761445 139735039037440 ddar.py:60] Depth 7/1000 time = 6.582272529602051
I0123 17:31:37.793336 139735039037440 ddar.py:60] Depth 8/1000 time = 7.0315773487091064
I0123 17:31:44.184188 139735039037440 ddar.py:60] Depth 9/1000 time = 6.328363418579102
I0123 17:31:52.184889 139735039037440 ddar.py:60] Depth 10/1000 time = 8.000422716140747
I0123 17:32:01.036356 139735039037440 ddar.py:60] Depth 11/1000 time = 8.851114988327026
I0123 17:32:09.627885 139735039037440 ddar.py:60] Depth 12/1000 time = 8.59128713607788
I0123 17:32:18.337707 139735039037440 ddar.py:60] Depth 13/1000 time = 8.709547281265259
I0123 17:32:26.366620 139735039037440 ddar.py:60] Depth 14/1000 time = 7.980109453201294
I0123 17:32:37.102648 139735039037440 ddar.py:60] Depth 15/1000 time = 10.73570704460144
I0123 17:32:47.533704 139735039037440 ddar.py:60] Depth 16/1000 time = 10.430647611618042
I0123 17:32:58.025835 139735039037440 ddar.py:60] Depth 17/1000 time = 10.491847276687622
I0123 17:33:08.615245 139735039037440 ddar.py:60] Depth 18/1000 time = 10.545861721038818
I0123 17:33:08.615542 139735039037440 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 17:33:08.615644 139735039037440 alphageometry.py:566] LM output (score=-3.198460): "n : T b c d n 20 ;"
I0123 17:33:08.615682 139735039037440 alphageometry.py:567] Translation: "n = on_tline n d b c"

I0123 17:33:08.615722 139735039037440 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = on_tline n d b c ? eqratio b d h d m d b d"
I0123 17:33:08.615890 139735039037440 graph.py:498] 
I0123 17:33:08.615957 139735039037440 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = on_tline n d b c ? eqratio b d h d m d b d
I0123 17:33:09.637896 139735039037440 ddar.py:60] Depth 1/1000 time = 0.9830851554870605
I0123 17:33:12.732321 139735039037440 ddar.py:60] Depth 2/1000 time = 3.094252824783325
I0123 17:33:17.460096 139735039037440 ddar.py:60] Depth 3/1000 time = 4.7275636196136475
I0123 17:33:22.897720 139735039037440 ddar.py:60] Depth 4/1000 time = 5.437384605407715
I0123 17:33:28.438064 139735039037440 ddar.py:60] Depth 5/1000 time = 5.54015326499939
I0123 17:33:34.736493 139735039037440 ddar.py:60] Depth 6/1000 time = 6.297832012176514
I0123 17:33:42.116703 139735039037440 ddar.py:60] Depth 7/1000 time = 7.371782064437866
I0123 17:33:49.706690 139735039037440 ddar.py:60] Depth 8/1000 time = 7.589773178100586
I0123 17:33:57.543288 139735039037440 ddar.py:60] Depth 9/1000 time = 7.836333751678467
I0123 17:34:05.254779 139735039037440 ddar.py:60] Depth 10/1000 time = 7.711148262023926
I0123 17:34:13.275300 139735039037440 ddar.py:60] Depth 11/1000 time = 7.951626777648926
I0123 17:34:22.357168 139735039037440 ddar.py:60] Depth 12/1000 time = 9.081616640090942
I0123 17:34:31.388208 139735039037440 ddar.py:60] Depth 13/1000 time = 9.030749082565308
I0123 17:34:40.880776 139735039037440 ddar.py:60] Depth 14/1000 time = 9.492230892181396
I0123 17:34:50.406972 139735039037440 ddar.py:60] Depth 15/1000 time = 9.525803804397583
I0123 17:34:59.951337 139735039037440 ddar.py:60] Depth 16/1000 time = 9.526241302490234
I0123 17:35:09.788187 139735039037440 ddar.py:60] Depth 17/1000 time = 9.800049781799316
I0123 17:35:20.815471 139735039037440 ddar.py:60] Depth 18/1000 time = 11.026966094970703
I0123 17:35:32.189217 139735039037440 ddar.py:60] Depth 19/1000 time = 11.37333607673645
I0123 17:35:44.627052 139735039037440 ddar.py:60] Depth 20/1000 time = 12.437501907348633
I0123 17:35:56.466019 139735039037440 ddar.py:60] Depth 21/1000 time = 11.793727159500122
I0123 17:35:56.466333 139735039037440 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 17:35:56.466435 139735039037440 alphageometry.py:566] LM output (score=-3.199681): "n : T c d j n 20 ;"
I0123 17:35:56.466470 139735039037440 alphageometry.py:567] Translation: "n = on_tline n j c d"

I0123 17:35:56.466510 139735039037440 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = on_tline n j c d ? eqratio b d h d m d b d"
I0123 17:35:56.466677 139735039037440 graph.py:498] 
I0123 17:35:56.466738 139735039037440 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = on_tline n j c d ? eqratio b d h d m d b d
I0123 17:35:57.538460 139735039037440 ddar.py:60] Depth 1/1000 time = 1.0284631252288818
I0123 17:36:00.963773 139735039037440 ddar.py:60] Depth 2/1000 time = 3.425091028213501
I0123 17:36:05.881401 139735039037440 ddar.py:60] Depth 3/1000 time = 4.917343854904175
I0123 17:36:11.365228 139735039037440 ddar.py:60] Depth 4/1000 time = 5.483628034591675
I0123 17:36:17.478103 139735039037440 ddar.py:60] Depth 5/1000 time = 6.11267876625061
I0123 17:36:22.834399 139735039037440 ddar.py:60] Depth 6/1000 time = 5.355828046798706
I0123 17:36:30.160295 139735039037440 ddar.py:60] Depth 7/1000 time = 7.314911127090454
I0123 17:36:38.454249 139735039037440 ddar.py:60] Depth 8/1000 time = 8.293741703033447
I0123 17:36:46.955930 139735039037440 ddar.py:60] Depth 9/1000 time = 8.426018953323364
I0123 17:36:56.064700 139735039037440 ddar.py:60] Depth 10/1000 time = 9.108465671539307
I0123 17:37:05.536148 139735039037440 ddar.py:60] Depth 11/1000 time = 9.471090316772461
I0123 17:37:15.422448 139735039037440 ddar.py:60] Depth 12/1000 time = 9.885875940322876
I0123 17:37:25.263900 139735039037440 ddar.py:60] Depth 13/1000 time = 9.841099500656128
I0123 17:37:35.429252 139735039037440 ddar.py:60] Depth 14/1000 time = 10.116770029067993
I0123 17:37:46.740665 139735039037440 ddar.py:60] Depth 15/1000 time = 11.311061143875122
I0123 17:37:59.183513 139735039037440 ddar.py:60] Depth 16/1000 time = 12.442458868026733
I0123 17:38:11.010664 139735039037440 ddar.py:60] Depth 17/1000 time = 11.826799631118774
I0123 17:38:22.994389 139735039037440 ddar.py:60] Depth 18/1000 time = 11.940764904022217
I0123 17:38:22.994713 139735039037440 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 17:38:22.994842 139735039037440 alphageometry.py:566] LM output (score=-3.200585): "n : T b c g n 20 ;"
I0123 17:38:22.994885 139735039037440 alphageometry.py:567] Translation: "n = on_tline n g b c"

I0123 17:38:22.994927 139735039037440 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = on_tline n g b c ? eqratio b d h d m d b d"
I0123 17:38:22.995106 139735039037440 graph.py:498] 
I0123 17:38:22.995172 139735039037440 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = on_tline n g b c ? eqratio b d h d m d b d
I0123 17:38:24.791199 139735039037440 ddar.py:60] Depth 1/1000 time = 1.7569894790649414
I0123 17:38:27.121802 139735039037440 ddar.py:60] Depth 2/1000 time = 2.330434560775757
I0123 17:38:32.619475 139735039037440 ddar.py:60] Depth 3/1000 time = 5.497483730316162
I0123 17:38:37.587699 139735039037440 ddar.py:60] Depth 4/1000 time = 4.968023061752319
I0123 17:38:43.278452 139735039037440 ddar.py:60] Depth 5/1000 time = 5.690558910369873
I0123 17:38:48.216704 139735039037440 ddar.py:60] Depth 6/1000 time = 4.937675476074219
I0123 17:38:54.702741 139735039037440 ddar.py:60] Depth 7/1000 time = 6.477503538131714
I0123 17:39:01.441209 139735039037440 ddar.py:60] Depth 8/1000 time = 6.7382683753967285
I0123 17:39:07.565562 139735039037440 ddar.py:60] Depth 9/1000 time = 6.0617289543151855
I0123 17:39:15.270415 139735039037440 ddar.py:60] Depth 10/1000 time = 7.70464015007019
I0123 17:39:23.146343 139735039037440 ddar.py:60] Depth 11/1000 time = 7.875691652297974
I0123 17:39:31.396619 139735039037440 ddar.py:60] Depth 12/1000 time = 8.24999713897705
I0123 17:39:38.825033 139735039037440 ddar.py:60] Depth 13/1000 time = 7.428069829940796
I0123 17:39:47.144829 139735039037440 ddar.py:60] Depth 14/1000 time = 8.272845268249512
I0123 17:39:57.508165 139735039037440 ddar.py:60] Depth 15/1000 time = 10.363027334213257
I0123 17:40:06.490385 139735039037440 ddar.py:60] Depth 16/1000 time = 8.98193883895874
I0123 17:40:16.461458 139735039037440 ddar.py:60] Depth 17/1000 time = 9.970725059509277
I0123 17:40:27.249384 139735039037440 ddar.py:60] Depth 18/1000 time = 10.744181632995605
I0123 17:40:27.249694 139735039037440 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 17:40:27.249811 139735039037440 alphageometry.py:540] Depth 1. There are 31 nodes to expand:
I0123 17:40:27.249850 139735039037440 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b c e 02 T a e b c 03 ; f : C a c f 04 T a c b f 05 ; g : C a b g 06 T a b c g 07 ; h : C a e h 08 C b f h 09 ; i : C f g i 10 T e i f g 11 ; j : C e i j 12 D e i i j 13 ; k : C e g k 14 T e g f k 15 ; l : C f k l 16 D f k k l 17 ; m : C a j m 18 C b l m 19 ? / d b d h d m d b {F1} x00 n : T c d c n 20 ; x00
I0123 17:40:27.249882 139735039037440 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b c e 02 T a e b c 03 ; f : C a c f 04 T a c b f 05 ; g : C a b g 06 T a b c g 07 ; h : C a e h 08 C b f h 09 ; i : C f g i 10 T e i f g 11 ; j : C e i j 12 D e i i j 13 ; k : C e g k 14 T e g f k 15 ; l : C f k l 16 D f k k l 17 ; m : C a j m 18 C b l m 19 ? / d b d h d m d b {F1} x00 n : D b c b n 20 D b c c n 21 ; x00
I0123 17:40:27.249909 139735039037440 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b c e 02 T a e b c 03 ; f : C a c f 04 T a c b f 05 ; g : C a b g 06 T a b c g 07 ; h : C a e h 08 C b f h 09 ; i : C f g i 10 T e i f g 11 ; j : C e i j 12 D e i i j 13 ; k : C e g k 14 T e g f k 15 ; l : C f k l 16 D f k k l 17 ; m : C a j m 18 C b l m 19 ? / d b d h d m d b {F1} x00 n : D a c a n 20 D a c c n 21 ; x00
I0123 17:40:27.249933 139735039037440 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b c e 02 T a e b c 03 ; f : C a c f 04 T a c b f 05 ; g : C a b g 06 T a b c g 07 ; h : C a e h 08 C b f h 09 ; i : C f g i 10 T e i f g 11 ; j : C e i j 12 D e i i j 13 ; k : C e g k 14 T e g f k 15 ; l : C f k l 16 D f k k l 17 ; m : C a j m 18 C b l m 19 ? / d b d h d m d b {F1} x00 n : D c f e n 20 P c f e n 21 ; x00
I0123 17:40:27.249956 139735039037440 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b c e 02 T a e b c 03 ; f : C a c f 04 T a c b f 05 ; g : C a b g 06 T a b c g 07 ; h : C a e h 08 C b f h 09 ; i : C f g i 10 T e i f g 11 ; j : C e i j 12 D e i i j 13 ; k : C e g k 14 T e g f k 15 ; l : C f k l 16 D f k k l 17 ; m : C a j m 18 C b l m 19 ? / d b d h d m d b {F1} x00 n : D c d d n 20 ; x00
I0123 17:40:27.249991 139735039037440 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b c e 02 T a e b c 03 ; f : C a c f 04 T a c b f 05 ; g : C a b g 06 T a b c g 07 ; h : C a e h 08 C b f h 09 ; i : C f g i 10 T e i f g 11 ; j : C e i j 12 D e i i j 13 ; k : C e g k 14 T e g f k 15 ; l : C f k l 16 D f k k l 17 ; m : C a j m 18 C b l m 19 ? / d b d h d m d b {F1} x00 n : D c i j n 20 D c n i j 21 ; x00
I0123 17:40:27.250017 139735039037440 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b c e 02 T a e b c 03 ; f : C a c f 04 T a c b f 05 ; g : C a b g 06 T a b c g 07 ; h : C a e h 08 C b f h 09 ; i : C f g i 10 T e i f g 11 ; j : C e i j 12 D e i i j 13 ; k : C e g k 14 T e g f k 15 ; l : C f k l 16 D f k k l 17 ; m : C a j m 18 C b l m 19 ? / d b d h d m d b {F1} x00 n : T a c c n 20 ; x00
I0123 17:40:27.250041 139735039037440 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b c e 02 T a e b c 03 ; f : C a c f 04 T a c b f 05 ; g : C a b g 06 T a b c g 07 ; h : C a e h 08 C b f h 09 ; i : C f g i 10 T e i f g 11 ; j : C e i j 12 D e i i j 13 ; k : C e g k 14 T e g f k 15 ; l : C f k l 16 D f k k l 17 ; m : C a j m 18 C b l m 19 ? / d b d h d m d b {F1} x00 n : D a n c n 20 ; x00
I0123 17:40:27.250064 139735039037440 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b c e 02 T a e b c 03 ; f : C a c f 04 T a c b f 05 ; g : C a b g 06 T a b c g 07 ; h : C a e h 08 C b f h 09 ; i : C f g i 10 T e i f g 11 ; j : C e i j 12 D e i i j 13 ; k : C e g k 14 T e g f k 15 ; l : C f k l 16 D f k k l 17 ; m : C a j m 18 C b l m 19 ? / d b d h d m d b {F1} x00 n : T b c c n 20 ; x00
I0123 17:40:27.250088 139735039037440 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b c e 02 T a e b c 03 ; f : C a c f 04 T a c b f 05 ; g : C a b g 06 T a b c g 07 ; h : C a e h 08 C b f h 09 ; i : C f g i 10 T e i f g 11 ; j : C e i j 12 D e i i j 13 ; k : C e g k 14 T e g f k 15 ; l : C f k l 16 D f k k l 17 ; m : C a j m 18 C b l m 19 ? / d b d h d m d b {F1} x00 n : T a d a n 20 ; x00
I0123 17:40:27.250111 139735039037440 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b c e 02 T a e b c 03 ; f : C a c f 04 T a c b f 05 ; g : C a b g 06 T a b c g 07 ; h : C a e h 08 C b f h 09 ; i : C f g i 10 T e i f g 11 ; j : C e i j 12 D e i i j 13 ; k : C e g k 14 T e g f k 15 ; l : C f k l 16 D f k k l 17 ; m : C a j m 18 C b l m 19 ? / d b d h d m d b {F1} x00 n : D b c b n 20 D c d d n 21 ; x00
I0123 17:40:27.250134 139735039037440 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b c e 02 T a e b c 03 ; f : C a c f 04 T a c b f 05 ; g : C a b g 06 T a b c g 07 ; h : C a e h 08 C b f h 09 ; i : C f g i 10 T e i f g 11 ; j : C e i j 12 D e i i j 13 ; k : C e g k 14 T e g f k 15 ; l : C f k l 16 D f k k l 17 ; m : C a j m 18 C b l m 19 ? / d b d h d m d b {F1} x00 n : T f g g n 20 ; x00
I0123 17:40:27.250155 139735039037440 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b c e 02 T a e b c 03 ; f : C a c f 04 T a c b f 05 ; g : C a b g 06 T a b c g 07 ; h : C a e h 08 C b f h 09 ; i : C f g i 10 T e i f g 11 ; j : C e i j 12 D e i i j 13 ; k : C e g k 14 T e g f k 15 ; l : C f k l 16 D f k k l 17 ; m : C a j m 18 C b l m 19 ? / d b d h d m d b {F1} x00 n : T b f f n 20 ; x00
I0123 17:40:27.250178 139735039037440 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b c e 02 T a e b c 03 ; f : C a c f 04 T a c b f 05 ; g : C a b g 06 T a b c g 07 ; h : C a e h 08 C b f h 09 ; i : C f g i 10 T e i f g 11 ; j : C e i j 12 D e i i j 13 ; k : C e g k 14 T e g f k 15 ; l : C f k l 16 D f k k l 17 ; m : C a j m 18 C b l m 19 ? / d b d h d m d b {F1} x00 n : D c f e n 20 D c n e f 21 ; x00
I0123 17:40:27.250200 139735039037440 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b c e 02 T a e b c 03 ; f : C a c f 04 T a c b f 05 ; g : C a b g 06 T a b c g 07 ; h : C a e h 08 C b f h 09 ; i : C f g i 10 T e i f g 11 ; j : C e i j 12 D e i i j 13 ; k : C e g k 14 T e g f k 15 ; l : C f k l 16 D f k k l 17 ; m : C a j m 18 C b l m 19 ? / d b d h d m d b {F1} x00 n : T b g g n 20 ; x00
I0123 17:40:27.250225 139735039037440 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b c e 02 T a e b c 03 ; f : C a c f 04 T a c b f 05 ; g : C a b g 06 T a b c g 07 ; h : C a e h 08 C b f h 09 ; i : C f g i 10 T e i f g 11 ; j : C e i j 12 D e i i j 13 ; k : C e g k 14 T e g f k 15 ; l : C f k l 16 D f k k l 17 ; m : C a j m 18 C b l m 19 ? / d b d h d m d b {F1} x00 n : D c i c n 20 D g i g n 21 ; x00
I0123 17:40:27.250248 139735039037440 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b c e 02 T a e b c 03 ; f : C a c f 04 T a c b f 05 ; g : C a b g 06 T a b c g 07 ; h : C a e h 08 C b f h 09 ; i : C f g i 10 T e i f g 11 ; j : C e i j 12 D e i i j 13 ; k : C e g k 14 T e g f k 15 ; l : C f k l 16 D f k k l 17 ; m : C a j m 18 C b l m 19 ? / d b d h d m d b {F1} x00 n : T a c a n 20 ; x00
I0123 17:40:27.250271 139735039037440 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b c e 02 T a e b c 03 ; f : C a c f 04 T a c b f 05 ; g : C a b g 06 T a b c g 07 ; h : C a e h 08 C b f h 09 ; i : C f g i 10 T e i f g 11 ; j : C e i j 12 D e i i j 13 ; k : C e g k 14 T e g f k 15 ; l : C f k l 16 D f k k l 17 ; m : C a j m 18 C b l m 19 ? / d b d h d m d b {F1} x00 n : D f i j n 20 D f n i j 21 ; x00
I0123 17:40:27.250293 139735039037440 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b c e 02 T a e b c 03 ; f : C a c f 04 T a c b f 05 ; g : C a b g 06 T a b c g 07 ; h : C a e h 08 C b f h 09 ; i : C f g i 10 T e i f g 11 ; j : C e i j 12 D e i i j 13 ; k : C e g k 14 T e g f k 15 ; l : C f k l 16 D f k k l 17 ; m : C a j m 18 C b l m 19 ? / d b d h d m d b {F1} x00 n : D b c c n 20 D b d d n 21 ; x00
I0123 17:40:27.250315 139735039037440 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b c e 02 T a e b c 03 ; f : C a c f 04 T a c b f 05 ; g : C a b g 06 T a b c g 07 ; h : C a e h 08 C b f h 09 ; i : C f g i 10 T e i f g 11 ; j : C e i j 12 D e i i j 13 ; k : C e g k 14 T e g f k 15 ; l : C f k l 16 D f k k l 17 ; m : C a j m 18 C b l m 19 ? / d b d h d m d b {F1} x00 n : D a i a n 20 D c i c n 21 ; x00
I0123 17:40:27.250337 139735039037440 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b c e 02 T a e b c 03 ; f : C a c f 04 T a c b f 05 ; g : C a b g 06 T a b c g 07 ; h : C a e h 08 C b f h 09 ; i : C f g i 10 T e i f g 11 ; j : C e i j 12 D e i i j 13 ; k : C e g k 14 T e g f k 15 ; l : C f k l 16 D f k k l 17 ; m : C a j m 18 C b l m 19 ? / d b d h d m d b {F1} x00 n : T b f b n 20 ; x00
I0123 17:40:27.250359 139735039037440 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b c e 02 T a e b c 03 ; f : C a c f 04 T a c b f 05 ; g : C a b g 06 T a b c g 07 ; h : C a e h 08 C b f h 09 ; i : C f g i 10 T e i f g 11 ; j : C e i j 12 D e i i j 13 ; k : C e g k 14 T e g f k 15 ; l : C f k l 16 D f k k l 17 ; m : C a j m 18 C b l m 19 ? / d b d h d m d b {F1} x00 n : D c i j n 20 P c i j n 21 ; x00
I0123 17:40:27.250381 139735039037440 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b c e 02 T a e b c 03 ; f : C a c f 04 T a c b f 05 ; g : C a b g 06 T a b c g 07 ; h : C a e h 08 C b f h 09 ; i : C f g i 10 T e i f g 11 ; j : C e i j 12 D e i i j 13 ; k : C e g k 14 T e g f k 15 ; l : C f k l 16 D f k k l 17 ; m : C a j m 18 C b l m 19 ? / d b d h d m d b {F1} x00 n : D b n c n 20 ; x00
I0123 17:40:27.250403 139735039037440 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b c e 02 T a e b c 03 ; f : C a c f 04 T a c b f 05 ; g : C a b g 06 T a b c g 07 ; h : C a e h 08 C b f h 09 ; i : C f g i 10 T e i f g 11 ; j : C e i j 12 D e i i j 13 ; k : C e g k 14 T e g f k 15 ; l : C f k l 16 D f k k l 17 ; m : C a j m 18 C b l m 19 ? / d b d h d m d b {F1} x00 n : D c n d f 20 P c n d f 21 ; x00
I0123 17:40:27.250428 139735039037440 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b c e 02 T a e b c 03 ; f : C a c f 04 T a c b f 05 ; g : C a b g 06 T a b c g 07 ; h : C a e h 08 C b f h 09 ; i : C f g i 10 T e i f g 11 ; j : C e i j 12 D e i i j 13 ; k : C e g k 14 T e g f k 15 ; l : C f k l 16 D f k k l 17 ; m : C a j m 18 C b l m 19 ? / d b d h d m d b {F1} x00 n : D f i j n 20 P f i j n 21 ; x00
I0123 17:40:27.250451 139735039037440 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b c e 02 T a e b c 03 ; f : C a c f 04 T a c b f 05 ; g : C a b g 06 T a b c g 07 ; h : C a e h 08 C b f h 09 ; i : C f g i 10 T e i f g 11 ; j : C e i j 12 D e i i j 13 ; k : C e g k 14 T e g f k 15 ; l : C f k l 16 D f k k l 17 ; m : C a j m 18 C b l m 19 ? / d b d h d m d b {F1} x00 n : T c g g n 20 ; x00
I0123 17:40:27.250473 139735039037440 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b c e 02 T a e b c 03 ; f : C a c f 04 T a c b f 05 ; g : C a b g 06 T a b c g 07 ; h : C a e h 08 C b f h 09 ; i : C f g i 10 T e i f g 11 ; j : C e i j 12 D e i i j 13 ; k : C e g k 14 T e g f k 15 ; l : C f k l 16 D f k k l 17 ; m : C a j m 18 C b l m 19 ? / d b d h d m d b {F1} x00 n : T b j j n 20 ; x00
I0123 17:40:27.250494 139735039037440 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b c e 02 T a e b c 03 ; f : C a c f 04 T a c b f 05 ; g : C a b g 06 T a b c g 07 ; h : C a e h 08 C b f h 09 ; i : C f g i 10 T e i f g 11 ; j : C e i j 12 D e i i j 13 ; k : C e g k 14 T e g f k 15 ; l : C f k l 16 D f k k l 17 ; m : C a j m 18 C b l m 19 ? / d b d h d m d b {F1} x00 n : T c d i n 20 ; x00
I0123 17:40:27.250516 139735039037440 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b c e 02 T a e b c 03 ; f : C a c f 04 T a c b f 05 ; g : C a b g 06 T a b c g 07 ; h : C a e h 08 C b f h 09 ; i : C f g i 10 T e i f g 11 ; j : C e i j 12 D e i i j 13 ; k : C e g k 14 T e g f k 15 ; l : C f k l 16 D f k k l 17 ; m : C a j m 18 C b l m 19 ? / d b d h d m d b {F1} x00 n : T b c d n 20 ; x00
I0123 17:40:27.250539 139735039037440 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b c e 02 T a e b c 03 ; f : C a c f 04 T a c b f 05 ; g : C a b g 06 T a b c g 07 ; h : C a e h 08 C b f h 09 ; i : C f g i 10 T e i f g 11 ; j : C e i j 12 D e i i j 13 ; k : C e g k 14 T e g f k 15 ; l : C f k l 16 D f k k l 17 ; m : C a j m 18 C b l m 19 ? / d b d h d m d b {F1} x00 n : T c d j n 20 ; x00
I0123 17:40:27.250561 139735039037440 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b c e 02 T a e b c 03 ; f : C a c f 04 T a c b f 05 ; g : C a b g 06 T a b c g 07 ; h : C a e h 08 C b f h 09 ; i : C f g i 10 T e i f g 11 ; j : C e i j 12 D e i i j 13 ; k : C e g k 14 T e g f k 15 ; l : C f k l 16 D f k k l 17 ; m : C a j m 18 C b l m 19 ? / d b d h d m d b {F1} x00 n : T b c g n 20 ; x00
I0123 17:40:27.250586 139735039037440 alphageometry.py:549] Decoding from {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b c e 02 T a e b c 03 ; f : C a c f 04 T a c b f 05 ; g : C a b g 06 T a b c g 07 ; h : C a e h 08 C b f h 09 ; i : C f g i 10 T e i f g 11 ; j : C e i j 12 D e i i j 13 ; k : C e g k 14 T e g f k 15 ; l : C f k l 16 D f k k l 17 ; m : C a j m 18 C b l m 19 ? / d b d h d m d b {F1} x00 n : T c d c n 20 ; x00
I0123 17:40:32.654132 139735039037440 alphageometry.py:566] LM output (score=-1.261602): "o : P b c e o 21 ;"
I0123 17:40:32.654424 139735039037440 alphageometry.py:567] Translation: "ERROR: Traceback (most recent call last):
  File "/home/chi/alphageometry-test/alphageometry.py", line 438, in try_translate_constrained_to_construct
    g.copy().add_clause(clause, 0, DEFINITIONS)
  File "/home/chi/alphageometry-test/graph.py", line 2570, in add_clause
    raise DepCheckFailError(
graph.DepCheckFailError: ncoll e b c
"

I0123 17:40:32.654491 139735039037440 alphageometry.py:566] LM output (score=-1.377028): "o : P b c f o 21 ;"
I0123 17:40:32.654525 139735039037440 alphageometry.py:567] Translation: "o = on_pline o f b c"

I0123 17:40:32.654570 139735039037440 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = on_tline n c c d; o = on_pline o f b c ? eqratio b d h d m d b d"
I0123 17:40:32.654768 139735039037440 graph.py:498] 
I0123 17:40:32.654831 139735039037440 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = on_tline n c c d; o = on_pline o f b c ? eqratio b d h d m d b d
I0123 17:40:33.807604 139735039037440 ddar.py:60] Depth 1/1000 time = 1.107224702835083
I0123 17:40:37.659979 139735039037440 ddar.py:60] Depth 2/1000 time = 3.852181911468506
I0123 17:40:45.726679 139735039037440 ddar.py:60] Depth 3/1000 time = 8.066490650177002
I0123 17:40:52.535005 139735039037440 ddar.py:60] Depth 4/1000 time = 6.8081278800964355
I0123 17:41:00.970839 139735039037440 ddar.py:60] Depth 5/1000 time = 8.435635805130005
I0123 17:41:07.723998 139735039037440 ddar.py:60] Depth 6/1000 time = 6.752557992935181
I0123 17:41:16.346419 139735039037440 ddar.py:60] Depth 7/1000 time = 8.550886392593384
I0123 17:41:24.935415 139735039037440 ddar.py:60] Depth 8/1000 time = 8.588750123977661
I0123 17:41:33.670960 139735039037440 ddar.py:60] Depth 9/1000 time = 8.735255002975464
I0123 17:41:42.794213 139735039037440 ddar.py:60] Depth 10/1000 time = 9.122891902923584
I0123 17:41:51.943403 139735039037440 ddar.py:60] Depth 11/1000 time = 9.148869037628174
I0123 17:42:01.264126 139735039037440 ddar.py:60] Depth 12/1000 time = 9.279810667037964
I0123 17:42:11.668696 139735039037440 ddar.py:60] Depth 13/1000 time = 10.40424656867981
I0123 17:42:22.527567 139735039037440 ddar.py:60] Depth 14/1000 time = 10.858469247817993
I0123 17:42:33.489709 139735039037440 ddar.py:60] Depth 15/1000 time = 10.961785793304443
I0123 17:42:44.545091 139735039037440 ddar.py:60] Depth 16/1000 time = 11.012989044189453
I0123 17:42:44.545424 139735039037440 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 17:42:44.545490 139735039037440 alphageometry.py:566] LM output (score=-1.728340): "o : P b c c o 21 ;"
I0123 17:42:44.545525 139735039037440 alphageometry.py:567] Translation: "ERROR: Invalid predicate P b c c o"

I0123 17:42:44.545562 139735039037440 alphageometry.py:566] LM output (score=-1.829660): "o : P b i j o 21 P b o i j 22 ;"
I0123 17:42:44.545588 139735039037440 alphageometry.py:567] Translation: "o = on_pline o j b i, on_pline o b i j"

I0123 17:42:44.545619 139735039037440 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = on_tline n c c d; o = on_pline o j b i, on_pline o b i j ? eqratio b d h d m d b d"
I0123 17:42:44.545816 139735039037440 graph.py:498] 
I0123 17:42:44.545884 139735039037440 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = on_tline n c c d; o = on_pline o j b i, on_pline o b i j ? eqratio b d h d m d b d
I0123 17:42:45.787214 139735039037440 ddar.py:60] Depth 1/1000 time = 1.1853513717651367
I0123 17:42:49.764856 139735039037440 ddar.py:60] Depth 2/1000 time = 3.9774014949798584
I0123 17:42:58.724686 139735039037440 ddar.py:60] Depth 3/1000 time = 8.959527969360352
I0123 17:43:07.273165 139735039037440 ddar.py:60] Depth 4/1000 time = 8.548239707946777
I0123 17:43:15.809590 139735039037440 ddar.py:60] Depth 5/1000 time = 8.53607439994812
I0123 17:43:25.219069 139735039037440 ddar.py:60] Depth 6/1000 time = 9.408782482147217
I0123 17:43:33.891783 139735039037440 ddar.py:60] Depth 7/1000 time = 8.581570386886597
I0123 17:43:43.369759 139735039037440 ddar.py:60] Depth 8/1000 time = 9.477722883224487
I0123 17:43:53.078440 139735039037440 ddar.py:60] Depth 9/1000 time = 9.708280563354492
I0123 17:44:03.074793 139735039037440 ddar.py:60] Depth 10/1000 time = 9.996038913726807
I0123 17:44:13.105971 139735039037440 ddar.py:60] Depth 11/1000 time = 10.030799627304077
I0123 17:44:24.219357 139735039037440 ddar.py:60] Depth 12/1000 time = 11.065258026123047
I0123 17:44:35.587756 139735039037440 ddar.py:60] Depth 13/1000 time = 11.368114471435547
I0123 17:44:47.343498 139735039037440 ddar.py:60] Depth 14/1000 time = 11.755439758300781
I0123 17:45:00.133795 139735039037440 ddar.py:60] Depth 15/1000 time = 12.789934158325195
I0123 17:45:12.240076 139735039037440 ddar.py:60] Depth 16/1000 time = 12.060745000839233
I0123 17:45:12.240538 139735039037440 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 17:45:12.240623 139735039037440 alphageometry.py:566] LM output (score=-2.021170): "o : P b c h o 21 ;"
I0123 17:45:12.240661 139735039037440 alphageometry.py:567] Translation: "o = on_pline o h b c"

I0123 17:45:12.240700 139735039037440 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = on_tline n c c d; o = on_pline o h b c ? eqratio b d h d m d b d"
I0123 17:45:12.240884 139735039037440 graph.py:498] 
I0123 17:45:12.240951 139735039037440 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = on_tline n c c d; o = on_pline o h b c ? eqratio b d h d m d b d
I0123 17:45:13.348762 139735039037440 ddar.py:60] Depth 1/1000 time = 1.0618512630462646
I0123 17:45:17.068559 139735039037440 ddar.py:60] Depth 2/1000 time = 3.719571113586426
I0123 17:45:24.392944 139735039037440 ddar.py:60] Depth 3/1000 time = 7.324091911315918
I0123 17:45:31.981206 139735039037440 ddar.py:60] Depth 4/1000 time = 7.588050842285156
I0123 17:45:39.553813 139735039037440 ddar.py:60] Depth 5/1000 time = 7.5723700523376465
I0123 17:45:47.045669 139735039037440 ddar.py:60] Depth 6/1000 time = 7.491173267364502
I0123 17:45:54.868756 139735039037440 ddar.py:60] Depth 7/1000 time = 7.753771781921387
I0123 17:46:02.368155 139735039037440 ddar.py:60] Depth 8/1000 time = 7.499056100845337
I0123 17:46:10.846962 139735039037440 ddar.py:60] Depth 9/1000 time = 8.478569507598877
I0123 17:46:20.474290 139735039037440 ddar.py:60] Depth 10/1000 time = 9.627038478851318
I0123 17:46:29.431395 139735039037440 ddar.py:60] Depth 11/1000 time = 8.956835269927979
I0123 17:46:37.755418 139735039037440 ddar.py:60] Depth 12/1000 time = 8.277836084365845
I0123 17:46:48.769838 139735039037440 ddar.py:60] Depth 13/1000 time = 11.014073848724365
I0123 17:46:59.209817 139735039037440 ddar.py:60] Depth 14/1000 time = 10.439537525177002
I0123 17:47:09.902086 139735039037440 ddar.py:60] Depth 15/1000 time = 10.691925525665283
I0123 17:47:20.702016 139735039037440 ddar.py:60] Depth 16/1000 time = 10.756783723831177
I0123 17:47:20.702370 139735039037440 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 17:47:20.702429 139735039037440 alphageometry.py:566] LM output (score=-2.158411): "o : P b c g o 21 ;"
I0123 17:47:20.702464 139735039037440 alphageometry.py:567] Translation: "o = on_pline o g b c"

I0123 17:47:20.702503 139735039037440 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = on_tline n c c d; o = on_pline o g b c ? eqratio b d h d m d b d"
I0123 17:47:20.702696 139735039037440 graph.py:498] 
I0123 17:47:20.702763 139735039037440 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = on_tline n c c d; o = on_pline o g b c ? eqratio b d h d m d b d
I0123 17:47:21.797836 139735039037440 ddar.py:60] Depth 1/1000 time = 1.0488264560699463
I0123 17:47:25.502068 139735039037440 ddar.py:60] Depth 2/1000 time = 3.704052686691284
I0123 17:47:32.783223 139735039037440 ddar.py:60] Depth 3/1000 time = 7.280954360961914
I0123 17:47:40.115614 139735039037440 ddar.py:60] Depth 4/1000 time = 7.332167387008667
I0123 17:47:49.131263 139735039037440 ddar.py:60] Depth 5/1000 time = 9.01537561416626
I0123 17:47:58.048331 139735039037440 ddar.py:60] Depth 6/1000 time = 8.916145086288452
I0123 17:48:07.199626 139735039037440 ddar.py:60] Depth 7/1000 time = 9.082076072692871
I0123 17:48:17.041167 139735039037440 ddar.py:60] Depth 8/1000 time = 9.841261386871338
I0123 17:48:26.985319 139735039037440 ddar.py:60] Depth 9/1000 time = 9.943881034851074
I0123 17:48:37.274303 139735039037440 ddar.py:60] Depth 10/1000 time = 10.288710832595825
I0123 17:48:47.710902 139735039037440 ddar.py:60] Depth 11/1000 time = 10.436306476593018
I0123 17:48:58.365697 139735039037440 ddar.py:60] Depth 12/1000 time = 10.602093696594238
I0123 17:49:10.179737 139735039037440 ddar.py:60] Depth 13/1000 time = 11.81361174583435
I0123 17:49:22.200271 139735039037440 ddar.py:60] Depth 14/1000 time = 12.020203590393066
I0123 17:49:34.363276 139735039037440 ddar.py:60] Depth 15/1000 time = 12.162696599960327
I0123 17:49:46.648233 139735039037440 ddar.py:60] Depth 16/1000 time = 12.24234652519226
I0123 17:49:46.648573 139735039037440 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 17:49:46.648632 139735039037440 alphageometry.py:566] LM output (score=-2.295054): "o : P b g c o 21 ;"
I0123 17:49:46.648667 139735039037440 alphageometry.py:567] Translation: "o = on_pline o c b g"

I0123 17:49:46.648706 139735039037440 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = on_tline n c c d; o = on_pline o c b g ? eqratio b d h d m d b d"
I0123 17:49:46.648885 139735039037440 graph.py:498] 
I0123 17:49:46.648948 139735039037440 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = on_tline n c c d; o = on_pline o c b g ? eqratio b d h d m d b d
I0123 17:49:48.590079 139735039037440 ddar.py:60] Depth 1/1000 time = 1.8956124782562256
I0123 17:49:51.434691 139735039037440 ddar.py:60] Depth 2/1000 time = 2.8444395065307617
I0123 17:50:00.327069 139735039037440 ddar.py:60] Depth 3/1000 time = 8.892168521881104
I0123 17:50:09.088711 139735039037440 ddar.py:60] Depth 4/1000 time = 8.761375904083252
I0123 17:50:17.624931 139735039037440 ddar.py:60] Depth 5/1000 time = 8.535939931869507
I0123 17:50:25.074418 139735039037440 ddar.py:60] Depth 6/1000 time = 7.448646545410156
I0123 17:50:34.497355 139735039037440 ddar.py:60] Depth 7/1000 time = 9.350721836090088
I0123 17:50:43.945258 139735039037440 ddar.py:60] Depth 8/1000 time = 9.447702407836914
I0123 17:50:53.549512 139735039037440 ddar.py:60] Depth 9/1000 time = 9.604013919830322
I0123 17:51:03.479708 139735039037440 ddar.py:60] Depth 10/1000 time = 9.929945230484009
I0123 17:51:13.610670 139735039037440 ddar.py:60] Depth 11/1000 time = 10.130648851394653
I0123 17:51:22.868281 139735039037440 ddar.py:60] Depth 12/1000 time = 9.201509714126587
I0123 17:51:35.054731 139735039037440 ddar.py:60] Depth 13/1000 time = 12.186156749725342
I0123 17:51:46.639055 139735039037440 ddar.py:60] Depth 14/1000 time = 11.583954572677612
I0123 17:51:58.419866 139735039037440 ddar.py:60] Depth 15/1000 time = 11.780397891998291
I0123 17:52:10.450160 139735039037440 ddar.py:60] Depth 16/1000 time = 11.986552715301514
I0123 17:52:10.450594 139735039037440 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 17:52:10.450667 139735039037440 alphageometry.py:566] LM output (score=-2.302974): "o : P f i h o 21 ;"
I0123 17:52:10.450702 139735039037440 alphageometry.py:567] Translation: "o = on_pline o h f i"

I0123 17:52:10.450747 139735039037440 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = on_tline n c c d; o = on_pline o h f i ? eqratio b d h d m d b d"
I0123 17:52:10.450946 139735039037440 graph.py:498] 
I0123 17:52:10.451007 139735039037440 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = foot e a b c; f = foot f b a c; g = foot g c a b; h = on_line h a e, on_line h b f; i = foot i e f g; j = mirror j e i; k = foot k f e g; l = mirror l f k; m = on_line m a j, on_line m b l; n = on_tline n c c d; o = on_pline o h f i ? eqratio b d h d m d b d
I0123 17:52:11.567029 139735039037440 ddar.py:60] Depth 1/1000 time = 1.0697007179260254
I0123 17:52:15.355810 139735039037440 ddar.py:60] Depth 2/1000 time = 3.7886006832122803
I0123 17:52:23.685833 139735039037440 ddar.py:60] Depth 3/1000 time = 8.329793453216553
I0123 17:52:31.169900 139735039037440 ddar.py:60] Depth 4/1000 time = 7.483755111694336
I0123 17:52:40.320015 139735039037440 ddar.py:60] Depth 5/1000 time = 9.149874925613403
I0123 17:52:49.479575 139735039037440 ddar.py:60] Depth 6/1000 time = 9.158652782440186
I0123 17:52:58.431831 139735039037440 ddar.py:60] Depth 7/1000 time = 8.880150556564331
I0123 17:53:08.255405 139735039037440 ddar.py:60] Depth 8/1000 time = 9.823278665542603
I0123 17:53:18.191311 139735039037440 ddar.py:60] Depth 9/1000 time = 9.935553550720215
I0123 17:53:28.499783 139735039037440 ddar.py:60] Depth 10/1000 time = 10.308050155639648
I0123 17:53:39.733049 139735039037440 ddar.py:60] Depth 11/1000 time = 11.232938766479492
I0123 17:53:39.783211 139735039037440 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 17:53:39.783262 139735039037440 alphageometry.py:585] Timeout.
