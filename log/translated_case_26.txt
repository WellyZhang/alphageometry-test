I0123 15:52:28.112120 139811328716800 inference_utils.py:69] Parsing gin configuration.
I0123 15:52:28.112231 139811328716800 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0123 15:52:28.112442 139811328716800 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0123 15:52:28.112475 139811328716800 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0123 15:52:28.112505 139811328716800 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0123 15:52:28.112531 139811328716800 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0123 15:52:28.112558 139811328716800 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0123 15:52:28.112585 139811328716800 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0123 15:52:28.112613 139811328716800 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0123 15:52:28.112639 139811328716800 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=32
I0123 15:52:28.112665 139811328716800 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0123 15:52:28.112692 139811328716800 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0123 15:52:28.112745 139811328716800 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0123 15:52:28.112933 139811328716800 resource_reader.py:55] Path not found: base_htrans.gin
I0123 15:52:28.113173 139811328716800 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0123 15:52:28.113271 139811328716800 resource_reader.py:55] Path not found: trainer_configuration.gin
I0123 15:52:28.119710 139811328716800 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0123 15:52:28.119831 139811328716800 resource_reader.py:55] Path not found: size/medium_150M.gin
I0123 15:52:28.120150 139811328716800 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0123 15:52:28.120253 139811328716800 resource_reader.py:55] Path not found: options/positions_t5.gin
I0123 15:52:28.120533 139811328716800 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0123 15:52:28.120634 139811328716800 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0123 15:52:28.121039 139811328716800 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0123 15:52:28.121140 139811328716800 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0123 15:52:28.124813 139811328716800 training_loop.py:334] ==== Training loop: initializing model ====
I0123 15:52:28.225645 139811328716800 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0123 15:52:28.226400 139811328716800 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0123 15:52:28.232657 139811328716800 training_loop.py:335] Process 0 of 1
I0123 15:52:28.232710 139811328716800 training_loop.py:336] Local device count = 1
I0123 15:52:28.232750 139811328716800 training_loop.py:337] Number of replicas = 1
I0123 15:52:28.232781 139811328716800 training_loop.py:339] Using random number seed 42
I0123 15:52:28.709171 139811328716800 training_loop.py:359] Initializing the model.
I0123 15:52:29.128620 139811328716800 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.128877 139811328716800 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 15:52:29.128979 139811328716800 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 15:52:29.129058 139811328716800 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 15:52:29.129135 139811328716800 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 15:52:29.129217 139811328716800 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 15:52:29.129290 139811328716800 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 15:52:29.129359 139811328716800 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 15:52:29.129427 139811328716800 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 15:52:29.129495 139811328716800 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 15:52:29.129564 139811328716800 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 15:52:29.129632 139811328716800 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 15:52:29.129714 139811328716800 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 15:52:29.129786 139811328716800 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 15:52:29.129827 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:29.129872 139811328716800 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 15:52:29.129986 139811328716800 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:52:29.130024 139811328716800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:52:29.130055 139811328716800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:52:29.132035 139811328716800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.137343 139811328716800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:52:29.147993 139811328716800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.148280 139811328716800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:52:29.152673 139811328716800 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:52:29.164811 139811328716800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:52:29.165093 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:29.165133 139811328716800 transformer_layer.py:389] tlayer: self-attention.
I0123 15:52:29.165166 139811328716800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.165268 139811328716800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.166645 139811328716800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.166723 139811328716800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.167451 139811328716800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.169999 139811328716800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.176225 139811328716800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.177517 139811328716800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.177599 139811328716800 transformer_layer.py:468] tlayer: End windows.
I0123 15:52:29.177635 139811328716800 transformer_layer.py:472] tlayer: final FFN.
I0123 15:52:29.177706 139811328716800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.177837 139811328716800 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:52:29.178236 139811328716800 nn_components.py:325] mlp: activation = None
I0123 15:52:29.178285 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:29.180251 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.180349 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:29.183210 139811328716800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.183296 139811328716800 transformer_base.py:443] tbase: final FFN
I0123 15:52:29.183782 139811328716800 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:52:29.194039 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:29.202992 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.203093 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:29.203390 139811328716800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.203471 139811328716800 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 15:52:29.203581 139811328716800 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:52:29.203621 139811328716800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:52:29.203652 139811328716800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:52:29.205546 139811328716800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.208060 139811328716800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:52:29.213682 139811328716800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.213945 139811328716800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:52:29.216596 139811328716800 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:52:29.220468 139811328716800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:52:29.220527 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:29.220564 139811328716800 transformer_layer.py:389] tlayer: self-attention.
I0123 15:52:29.220596 139811328716800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.220658 139811328716800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.221232 139811328716800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.221309 139811328716800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.221680 139811328716800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.222488 139811328716800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.225018 139811328716800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.225651 139811328716800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.225729 139811328716800 transformer_layer.py:468] tlayer: End windows.
I0123 15:52:29.225765 139811328716800 transformer_layer.py:472] tlayer: final FFN.
I0123 15:52:29.225826 139811328716800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.225953 139811328716800 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:52:29.226279 139811328716800 nn_components.py:325] mlp: activation = None
I0123 15:52:29.226323 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:29.228263 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.228357 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:29.230902 139811328716800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.230983 139811328716800 transformer_base.py:443] tbase: final FFN
I0123 15:52:29.231413 139811328716800 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:52:29.233758 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:29.235659 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.235758 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:29.236060 139811328716800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.236140 139811328716800 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 15:52:29.236250 139811328716800 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:52:29.236289 139811328716800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:52:29.236322 139811328716800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:52:29.238588 139811328716800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.241113 139811328716800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:52:29.246790 139811328716800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.247054 139811328716800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:52:29.249726 139811328716800 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:52:29.253569 139811328716800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:52:29.253624 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:29.253668 139811328716800 transformer_layer.py:389] tlayer: self-attention.
I0123 15:52:29.253700 139811328716800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.253762 139811328716800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.254324 139811328716800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.254400 139811328716800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.254760 139811328716800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.255520 139811328716800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.258002 139811328716800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.258667 139811328716800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.258744 139811328716800 transformer_layer.py:468] tlayer: End windows.
I0123 15:52:29.258780 139811328716800 transformer_layer.py:472] tlayer: final FFN.
I0123 15:52:29.258839 139811328716800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.258969 139811328716800 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:52:29.259293 139811328716800 nn_components.py:325] mlp: activation = None
I0123 15:52:29.259337 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:29.261228 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.261321 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:29.263846 139811328716800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.263933 139811328716800 transformer_base.py:443] tbase: final FFN
I0123 15:52:29.264415 139811328716800 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:52:29.266717 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:29.268632 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.268728 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:29.269024 139811328716800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.269105 139811328716800 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 15:52:29.269215 139811328716800 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:52:29.269255 139811328716800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:52:29.269287 139811328716800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:52:29.271204 139811328716800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.273615 139811328716800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:52:29.279255 139811328716800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.279518 139811328716800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:52:29.282170 139811328716800 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:52:29.285960 139811328716800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:52:29.286018 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:29.286055 139811328716800 transformer_layer.py:389] tlayer: self-attention.
I0123 15:52:29.286087 139811328716800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.286153 139811328716800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.286714 139811328716800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.286790 139811328716800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.287150 139811328716800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.287929 139811328716800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.290506 139811328716800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.291134 139811328716800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.291210 139811328716800 transformer_layer.py:468] tlayer: End windows.
I0123 15:52:29.291247 139811328716800 transformer_layer.py:472] tlayer: final FFN.
I0123 15:52:29.291308 139811328716800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.291440 139811328716800 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:52:29.291766 139811328716800 nn_components.py:325] mlp: activation = None
I0123 15:52:29.291810 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:29.293717 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.293813 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:29.296406 139811328716800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.296493 139811328716800 transformer_base.py:443] tbase: final FFN
I0123 15:52:29.296922 139811328716800 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:52:29.299193 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:29.301109 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.301206 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:29.301501 139811328716800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.301582 139811328716800 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 15:52:29.301699 139811328716800 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:52:29.301739 139811328716800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:52:29.301772 139811328716800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:52:29.303659 139811328716800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.306077 139811328716800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:52:29.311687 139811328716800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.311951 139811328716800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:52:29.314973 139811328716800 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:52:29.318721 139811328716800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:52:29.318777 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:29.318813 139811328716800 transformer_layer.py:389] tlayer: self-attention.
I0123 15:52:29.318844 139811328716800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.318907 139811328716800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.319474 139811328716800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.319548 139811328716800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.319909 139811328716800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.320675 139811328716800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.323244 139811328716800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.323864 139811328716800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.323941 139811328716800 transformer_layer.py:468] tlayer: End windows.
I0123 15:52:29.323976 139811328716800 transformer_layer.py:472] tlayer: final FFN.
I0123 15:52:29.324035 139811328716800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.324168 139811328716800 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:52:29.324494 139811328716800 nn_components.py:325] mlp: activation = None
I0123 15:52:29.324539 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:29.326456 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.326555 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:29.329124 139811328716800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.329204 139811328716800 transformer_base.py:443] tbase: final FFN
I0123 15:52:29.329646 139811328716800 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:52:29.331919 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:29.333933 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.334033 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:29.334340 139811328716800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.334420 139811328716800 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 15:52:29.334530 139811328716800 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:52:29.334569 139811328716800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:52:29.334601 139811328716800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:52:29.336449 139811328716800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.338845 139811328716800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:52:29.344560 139811328716800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.344817 139811328716800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:52:29.347522 139811328716800 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:52:29.351246 139811328716800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:52:29.351304 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:29.351341 139811328716800 transformer_layer.py:389] tlayer: self-attention.
I0123 15:52:29.351372 139811328716800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.351433 139811328716800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.352041 139811328716800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.352118 139811328716800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.352482 139811328716800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.353276 139811328716800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.355764 139811328716800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.356380 139811328716800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.356458 139811328716800 transformer_layer.py:468] tlayer: End windows.
I0123 15:52:29.356495 139811328716800 transformer_layer.py:472] tlayer: final FFN.
I0123 15:52:29.356554 139811328716800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.356683 139811328716800 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:52:29.357009 139811328716800 nn_components.py:325] mlp: activation = None
I0123 15:52:29.357054 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:29.358963 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.359062 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:29.361636 139811328716800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.361724 139811328716800 transformer_base.py:443] tbase: final FFN
I0123 15:52:29.362149 139811328716800 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:52:29.364470 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:29.366404 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.366502 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:29.366800 139811328716800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.366880 139811328716800 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 15:52:29.366989 139811328716800 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:52:29.367028 139811328716800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:52:29.367059 139811328716800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:52:29.368916 139811328716800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.371373 139811328716800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:52:29.377027 139811328716800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.377292 139811328716800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:52:29.379926 139811328716800 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:52:29.383711 139811328716800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:52:29.383767 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:29.383803 139811328716800 transformer_layer.py:389] tlayer: self-attention.
I0123 15:52:29.383835 139811328716800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.383897 139811328716800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.384464 139811328716800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.384541 139811328716800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.384899 139811328716800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.385684 139811328716800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.388385 139811328716800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.389021 139811328716800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.389104 139811328716800 transformer_layer.py:468] tlayer: End windows.
I0123 15:52:29.389139 139811328716800 transformer_layer.py:472] tlayer: final FFN.
I0123 15:52:29.389200 139811328716800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.389328 139811328716800 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:52:29.389665 139811328716800 nn_components.py:325] mlp: activation = None
I0123 15:52:29.389712 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:29.392013 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.392108 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:29.394641 139811328716800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.394721 139811328716800 transformer_base.py:443] tbase: final FFN
I0123 15:52:29.395151 139811328716800 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:52:29.533963 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:29.536127 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.536278 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:29.536594 139811328716800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.536686 139811328716800 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 15:52:29.536802 139811328716800 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:52:29.536842 139811328716800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:52:29.536874 139811328716800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:52:29.538904 139811328716800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.541394 139811328716800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:52:29.547281 139811328716800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.547554 139811328716800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:52:29.550239 139811328716800 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:52:29.554189 139811328716800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:52:29.554247 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:29.554285 139811328716800 transformer_layer.py:389] tlayer: self-attention.
I0123 15:52:29.554318 139811328716800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.554385 139811328716800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.554993 139811328716800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.555073 139811328716800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.555439 139811328716800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.556220 139811328716800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.558805 139811328716800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.559442 139811328716800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.559521 139811328716800 transformer_layer.py:468] tlayer: End windows.
I0123 15:52:29.559557 139811328716800 transformer_layer.py:472] tlayer: final FFN.
I0123 15:52:29.559617 139811328716800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.559745 139811328716800 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:52:29.560074 139811328716800 nn_components.py:325] mlp: activation = None
I0123 15:52:29.560119 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:29.562028 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.562123 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:29.564649 139811328716800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.564730 139811328716800 transformer_base.py:443] tbase: final FFN
I0123 15:52:29.565216 139811328716800 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:52:29.567549 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:29.569470 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.569573 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:29.569883 139811328716800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.569966 139811328716800 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 15:52:29.570076 139811328716800 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:52:29.570115 139811328716800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:52:29.570147 139811328716800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:52:29.572056 139811328716800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.574447 139811328716800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:52:29.580186 139811328716800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.580454 139811328716800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:52:29.583186 139811328716800 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:52:29.586986 139811328716800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:52:29.587042 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:29.587079 139811328716800 transformer_layer.py:389] tlayer: self-attention.
I0123 15:52:29.587111 139811328716800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.587173 139811328716800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.587778 139811328716800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.587854 139811328716800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.588219 139811328716800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.589000 139811328716800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.591582 139811328716800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.592209 139811328716800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.592287 139811328716800 transformer_layer.py:468] tlayer: End windows.
I0123 15:52:29.592322 139811328716800 transformer_layer.py:472] tlayer: final FFN.
I0123 15:52:29.592382 139811328716800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.592509 139811328716800 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:52:29.592834 139811328716800 nn_components.py:325] mlp: activation = None
I0123 15:52:29.592879 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:29.594801 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.594900 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:29.597467 139811328716800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.597547 139811328716800 transformer_base.py:443] tbase: final FFN
I0123 15:52:29.597991 139811328716800 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:52:29.600307 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:29.602241 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.602337 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:29.602635 139811328716800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.602723 139811328716800 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 15:52:29.602836 139811328716800 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:52:29.602876 139811328716800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:52:29.602909 139811328716800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:52:29.604826 139811328716800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.607238 139811328716800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:52:29.613236 139811328716800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.613508 139811328716800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:52:29.616213 139811328716800 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:52:29.620023 139811328716800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:52:29.620078 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:29.620115 139811328716800 transformer_layer.py:389] tlayer: self-attention.
I0123 15:52:29.620146 139811328716800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.620207 139811328716800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.620778 139811328716800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.620854 139811328716800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.621214 139811328716800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.622044 139811328716800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.624547 139811328716800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.625167 139811328716800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.625244 139811328716800 transformer_layer.py:468] tlayer: End windows.
I0123 15:52:29.625280 139811328716800 transformer_layer.py:472] tlayer: final FFN.
I0123 15:52:29.625339 139811328716800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.625468 139811328716800 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:52:29.625805 139811328716800 nn_components.py:325] mlp: activation = None
I0123 15:52:29.625850 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:29.627764 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.627858 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:29.630410 139811328716800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.630491 139811328716800 transformer_base.py:443] tbase: final FFN
I0123 15:52:29.630934 139811328716800 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:52:29.633224 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:29.635183 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.635279 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:29.635575 139811328716800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.635662 139811328716800 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 15:52:29.635773 139811328716800 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:52:29.635812 139811328716800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:52:29.635844 139811328716800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:52:29.637686 139811328716800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.640126 139811328716800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:52:29.645699 139811328716800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.645960 139811328716800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:52:29.648750 139811328716800 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:52:29.652800 139811328716800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:52:29.652855 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:29.652891 139811328716800 transformer_layer.py:389] tlayer: self-attention.
I0123 15:52:29.652923 139811328716800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.653026 139811328716800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.653590 139811328716800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.653693 139811328716800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.654054 139811328716800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.654829 139811328716800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.657286 139811328716800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.657915 139811328716800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.657993 139811328716800 transformer_layer.py:468] tlayer: End windows.
I0123 15:52:29.658029 139811328716800 transformer_layer.py:472] tlayer: final FFN.
I0123 15:52:29.658092 139811328716800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.658219 139811328716800 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:52:29.658538 139811328716800 nn_components.py:325] mlp: activation = None
I0123 15:52:29.658582 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:29.660523 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.660617 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:29.663379 139811328716800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.663459 139811328716800 transformer_base.py:443] tbase: final FFN
I0123 15:52:29.663889 139811328716800 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:52:29.666198 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:29.668076 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.668173 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:29.668465 139811328716800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.668545 139811328716800 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 15:52:29.668658 139811328716800 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:52:29.668698 139811328716800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:52:29.668729 139811328716800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:52:29.670558 139811328716800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.673000 139811328716800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:52:29.678590 139811328716800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.678850 139811328716800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:52:29.681483 139811328716800 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:52:29.685582 139811328716800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:52:29.685638 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:29.685686 139811328716800 transformer_layer.py:389] tlayer: self-attention.
I0123 15:52:29.685718 139811328716800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.685780 139811328716800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.686350 139811328716800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.686427 139811328716800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.686786 139811328716800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.687565 139811328716800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.690063 139811328716800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.690689 139811328716800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.690767 139811328716800 transformer_layer.py:468] tlayer: End windows.
I0123 15:52:29.690802 139811328716800 transformer_layer.py:472] tlayer: final FFN.
I0123 15:52:29.690863 139811328716800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.690992 139811328716800 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:52:29.691312 139811328716800 nn_components.py:325] mlp: activation = None
I0123 15:52:29.691357 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:29.693349 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.693443 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:29.695958 139811328716800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.696038 139811328716800 transformer_base.py:443] tbase: final FFN
I0123 15:52:29.696469 139811328716800 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:52:29.698777 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:29.700692 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.700788 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:29.701078 139811328716800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.701376 139811328716800 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 15:52:29.701452 139811328716800 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 15:52:29.701519 139811328716800 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 15:52:29.701579 139811328716800 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 15:52:29.701636 139811328716800 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 15:52:29.701700 139811328716800 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 15:52:29.701754 139811328716800 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 15:52:29.701808 139811328716800 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 15:52:29.701862 139811328716800 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 15:52:29.701915 139811328716800 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 15:52:29.701967 139811328716800 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 15:52:29.702019 139811328716800 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 15:52:29.702056 139811328716800 decoder_stack.py:344] dstack: Final layernorm.
I0123 15:52:29.705605 139811328716800 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:29.753468 139811328716800 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.753555 139811328716800 decoder_stack.py:333] dstack: autoregressive generator.
I0123 15:52:29.753610 139811328716800 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 15:52:29.753723 139811328716800 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:52:29.753764 139811328716800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:52:29.753796 139811328716800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:52:29.753862 139811328716800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.756340 139811328716800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:52:29.761870 139811328716800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.762134 139811328716800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:52:29.764785 139811328716800 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:52:29.781416 139811328716800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:52:29.781472 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:29.781509 139811328716800 transformer_layer.py:389] tlayer: self-attention.
I0123 15:52:29.781542 139811328716800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.781605 139811328716800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.782748 139811328716800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.782826 139811328716800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.783557 139811328716800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.785561 139811328716800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.790303 139811328716800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.791613 139811328716800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.791700 139811328716800 transformer_layer.py:468] tlayer: End windows.
I0123 15:52:29.791738 139811328716800 transformer_layer.py:472] tlayer: final FFN.
I0123 15:52:29.791800 139811328716800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.791932 139811328716800 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:52:29.792046 139811328716800 nn_components.py:325] mlp: activation = None
I0123 15:52:29.792086 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:29.794006 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.794102 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:29.796518 139811328716800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.796602 139811328716800 transformer_base.py:443] tbase: final FFN
I0123 15:52:29.796712 139811328716800 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:52:29.798961 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:29.800913 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.801012 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:29.801307 139811328716800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.801389 139811328716800 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 15:52:29.801499 139811328716800 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:52:29.801540 139811328716800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:52:29.801572 139811328716800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:52:29.801636 139811328716800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.803910 139811328716800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:52:29.809408 139811328716800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.809682 139811328716800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:52:29.812396 139811328716800 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:52:29.830397 139811328716800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:52:29.830482 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:29.830521 139811328716800 transformer_layer.py:389] tlayer: self-attention.
I0123 15:52:29.830555 139811328716800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.830629 139811328716800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.831240 139811328716800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.831317 139811328716800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.831688 139811328716800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.832394 139811328716800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.834996 139811328716800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.835627 139811328716800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.835705 139811328716800 transformer_layer.py:468] tlayer: End windows.
I0123 15:52:29.835747 139811328716800 transformer_layer.py:472] tlayer: final FFN.
I0123 15:52:29.835814 139811328716800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.835943 139811328716800 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:52:29.836057 139811328716800 nn_components.py:325] mlp: activation = None
I0123 15:52:29.836097 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:29.838136 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.838232 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:29.840721 139811328716800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.840803 139811328716800 transformer_base.py:443] tbase: final FFN
I0123 15:52:29.840914 139811328716800 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:52:29.843184 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:29.845131 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.845229 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:29.845519 139811328716800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.845601 139811328716800 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 15:52:29.845720 139811328716800 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:52:29.845764 139811328716800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:52:29.845797 139811328716800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:52:29.845866 139811328716800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.848155 139811328716800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:52:29.853624 139811328716800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.853890 139811328716800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:52:29.856636 139811328716800 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:52:29.869615 139811328716800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:52:29.869679 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:29.869717 139811328716800 transformer_layer.py:389] tlayer: self-attention.
I0123 15:52:29.869749 139811328716800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.869810 139811328716800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.870365 139811328716800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.870443 139811328716800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.870802 139811328716800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.871496 139811328716800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.873961 139811328716800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.874574 139811328716800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.874649 139811328716800 transformer_layer.py:468] tlayer: End windows.
I0123 15:52:29.874684 139811328716800 transformer_layer.py:472] tlayer: final FFN.
I0123 15:52:29.874748 139811328716800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.874880 139811328716800 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:52:29.874989 139811328716800 nn_components.py:325] mlp: activation = None
I0123 15:52:29.875028 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:29.876932 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.877025 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:29.879472 139811328716800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.879551 139811328716800 transformer_base.py:443] tbase: final FFN
I0123 15:52:29.879658 139811328716800 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:52:29.881908 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:29.883821 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.883917 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:29.884200 139811328716800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.884280 139811328716800 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 15:52:29.884389 139811328716800 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:52:29.884428 139811328716800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:52:29.884460 139811328716800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:52:29.884524 139811328716800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.886756 139811328716800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:52:29.892254 139811328716800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.892514 139811328716800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:52:29.895205 139811328716800 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:52:29.907868 139811328716800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:52:29.907923 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:29.907960 139811328716800 transformer_layer.py:389] tlayer: self-attention.
I0123 15:52:29.907991 139811328716800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.908052 139811328716800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.908607 139811328716800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.908684 139811328716800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.909043 139811328716800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.909739 139811328716800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.912228 139811328716800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.912848 139811328716800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.912925 139811328716800 transformer_layer.py:468] tlayer: End windows.
I0123 15:52:29.912960 139811328716800 transformer_layer.py:472] tlayer: final FFN.
I0123 15:52:29.913019 139811328716800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.913149 139811328716800 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:52:29.913258 139811328716800 nn_components.py:325] mlp: activation = None
I0123 15:52:29.913298 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:29.915543 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.915638 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:29.918056 139811328716800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.918136 139811328716800 transformer_base.py:443] tbase: final FFN
I0123 15:52:29.918250 139811328716800 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:52:29.920444 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:29.922323 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.922419 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:29.922704 139811328716800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.922784 139811328716800 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 15:52:29.922893 139811328716800 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:52:29.922933 139811328716800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:52:29.922965 139811328716800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:52:29.923028 139811328716800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.925344 139811328716800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:52:29.930835 139811328716800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.931100 139811328716800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:52:29.933731 139811328716800 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:52:29.946446 139811328716800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:52:29.946501 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:29.946538 139811328716800 transformer_layer.py:389] tlayer: self-attention.
I0123 15:52:29.946570 139811328716800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.946632 139811328716800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.947190 139811328716800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.947266 139811328716800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.947619 139811328716800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.948315 139811328716800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.950871 139811328716800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.951495 139811328716800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.951573 139811328716800 transformer_layer.py:468] tlayer: End windows.
I0123 15:52:29.951609 139811328716800 transformer_layer.py:472] tlayer: final FFN.
I0123 15:52:29.951668 139811328716800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.951800 139811328716800 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:52:29.951908 139811328716800 nn_components.py:325] mlp: activation = None
I0123 15:52:29.951946 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:29.953836 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.953931 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:29.956364 139811328716800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.956445 139811328716800 transformer_base.py:443] tbase: final FFN
I0123 15:52:29.956554 139811328716800 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:52:29.958843 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:29.960701 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.960796 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:29.961081 139811328716800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.961161 139811328716800 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 15:52:29.961269 139811328716800 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:52:29.961308 139811328716800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:52:29.961340 139811328716800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:52:29.961404 139811328716800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.963850 139811328716800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:52:29.969343 139811328716800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.969605 139811328716800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:52:29.972298 139811328716800 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:52:29.984893 139811328716800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:52:29.984948 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:29.984985 139811328716800 transformer_layer.py:389] tlayer: self-attention.
I0123 15:52:29.985015 139811328716800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.985076 139811328716800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.985624 139811328716800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.985710 139811328716800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.986065 139811328716800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.986768 139811328716800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.989243 139811328716800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.989858 139811328716800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.989936 139811328716800 transformer_layer.py:468] tlayer: End windows.
I0123 15:52:29.989970 139811328716800 transformer_layer.py:472] tlayer: final FFN.
I0123 15:52:29.990029 139811328716800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.990159 139811328716800 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:52:29.990273 139811328716800 nn_components.py:325] mlp: activation = None
I0123 15:52:29.990313 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:29.992253 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.992348 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:29.994761 139811328716800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.994842 139811328716800 transformer_base.py:443] tbase: final FFN
I0123 15:52:29.994951 139811328716800 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:52:29.997169 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:29.999023 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.999119 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:29.999404 139811328716800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:29.999485 139811328716800 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 15:52:29.999593 139811328716800 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:52:29.999631 139811328716800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:52:29.999663 139811328716800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:52:29.999726 139811328716800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.002162 139811328716800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:52:30.007659 139811328716800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.007915 139811328716800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:52:30.010510 139811328716800 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:52:30.023675 139811328716800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:52:30.023730 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:30.023766 139811328716800 transformer_layer.py:389] tlayer: self-attention.
I0123 15:52:30.023797 139811328716800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.023857 139811328716800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.024409 139811328716800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.024488 139811328716800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.025000 139811328716800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.025694 139811328716800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.028161 139811328716800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.028817 139811328716800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.028894 139811328716800 transformer_layer.py:468] tlayer: End windows.
I0123 15:52:30.028930 139811328716800 transformer_layer.py:472] tlayer: final FFN.
I0123 15:52:30.028993 139811328716800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.029126 139811328716800 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:52:30.029237 139811328716800 nn_components.py:325] mlp: activation = None
I0123 15:52:30.029283 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:30.031198 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.031293 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:30.033694 139811328716800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.033774 139811328716800 transformer_base.py:443] tbase: final FFN
I0123 15:52:30.033882 139811328716800 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:52:30.036092 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:30.038014 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.038111 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:30.038397 139811328716800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.038477 139811328716800 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 15:52:30.038586 139811328716800 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:52:30.038625 139811328716800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:52:30.038657 139811328716800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:52:30.038721 139811328716800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.041253 139811328716800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:52:30.046718 139811328716800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.046988 139811328716800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:52:30.049693 139811328716800 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:52:30.062309 139811328716800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:52:30.062364 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:30.062401 139811328716800 transformer_layer.py:389] tlayer: self-attention.
I0123 15:52:30.062433 139811328716800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.062498 139811328716800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.063097 139811328716800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.063174 139811328716800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.063526 139811328716800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.064212 139811328716800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.066693 139811328716800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.067313 139811328716800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.067390 139811328716800 transformer_layer.py:468] tlayer: End windows.
I0123 15:52:30.067425 139811328716800 transformer_layer.py:472] tlayer: final FFN.
I0123 15:52:30.067485 139811328716800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.067615 139811328716800 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:52:30.067724 139811328716800 nn_components.py:325] mlp: activation = None
I0123 15:52:30.067768 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:30.069636 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.069738 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:30.072200 139811328716800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.072280 139811328716800 transformer_base.py:443] tbase: final FFN
I0123 15:52:30.072389 139811328716800 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:52:30.074599 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:30.076482 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.076578 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:30.076866 139811328716800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.076947 139811328716800 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 15:52:30.077054 139811328716800 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:52:30.077094 139811328716800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:52:30.077126 139811328716800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:52:30.077189 139811328716800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.079621 139811328716800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:52:30.085119 139811328716800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.085377 139811328716800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:52:30.088237 139811328716800 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:52:30.100837 139811328716800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:52:30.100894 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:30.100930 139811328716800 transformer_layer.py:389] tlayer: self-attention.
I0123 15:52:30.100961 139811328716800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.101022 139811328716800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.101584 139811328716800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.101668 139811328716800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.102023 139811328716800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.102707 139811328716800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.105173 139811328716800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.105846 139811328716800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.105925 139811328716800 transformer_layer.py:468] tlayer: End windows.
I0123 15:52:30.105961 139811328716800 transformer_layer.py:472] tlayer: final FFN.
I0123 15:52:30.106021 139811328716800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.106317 139811328716800 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:52:30.106426 139811328716800 nn_components.py:325] mlp: activation = None
I0123 15:52:30.106465 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:30.108342 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.108435 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:30.110815 139811328716800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.110898 139811328716800 transformer_base.py:443] tbase: final FFN
I0123 15:52:30.111006 139811328716800 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:52:30.113206 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:30.115144 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.115241 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:30.115529 139811328716800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.115610 139811328716800 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 15:52:30.115719 139811328716800 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:52:30.115758 139811328716800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:52:30.115789 139811328716800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:52:30.115853 139811328716800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.118095 139811328716800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:52:30.123504 139811328716800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.123761 139811328716800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:52:30.126737 139811328716800 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:52:30.139292 139811328716800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:52:30.139348 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:30.139383 139811328716800 transformer_layer.py:389] tlayer: self-attention.
I0123 15:52:30.139414 139811328716800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.139482 139811328716800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.140076 139811328716800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.140151 139811328716800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.140507 139811328716800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.141188 139811328716800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.143643 139811328716800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.144255 139811328716800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.144332 139811328716800 transformer_layer.py:468] tlayer: End windows.
I0123 15:52:30.144367 139811328716800 transformer_layer.py:472] tlayer: final FFN.
I0123 15:52:30.144427 139811328716800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.144556 139811328716800 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:52:30.144666 139811328716800 nn_components.py:325] mlp: activation = None
I0123 15:52:30.144706 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:30.146573 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.146673 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:30.149121 139811328716800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.149201 139811328716800 transformer_base.py:443] tbase: final FFN
I0123 15:52:30.149309 139811328716800 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:52:30.151530 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:30.153377 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.153473 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:30.153767 139811328716800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.153849 139811328716800 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 15:52:30.153957 139811328716800 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:52:30.153997 139811328716800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:52:30.154029 139811328716800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:52:30.154094 139811328716800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.156324 139811328716800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:52:30.161824 139811328716800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.162083 139811328716800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:52:30.164700 139811328716800 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:52:30.177325 139811328716800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:52:30.177381 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:30.177417 139811328716800 transformer_layer.py:389] tlayer: self-attention.
I0123 15:52:30.177448 139811328716800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.177510 139811328716800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.178075 139811328716800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.178153 139811328716800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.178507 139811328716800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.179192 139811328716800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.181676 139811328716800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.182334 139811328716800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.182411 139811328716800 transformer_layer.py:468] tlayer: End windows.
I0123 15:52:30.182446 139811328716800 transformer_layer.py:472] tlayer: final FFN.
I0123 15:52:30.182504 139811328716800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.182628 139811328716800 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:52:30.182749 139811328716800 nn_components.py:325] mlp: activation = None
I0123 15:52:30.182789 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:30.184652 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.184752 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:30.187176 139811328716800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.187256 139811328716800 transformer_base.py:443] tbase: final FFN
I0123 15:52:30.187363 139811328716800 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:52:30.189561 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:30.191495 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.191593 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:30.191879 139811328716800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.191961 139811328716800 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 15:52:30.192069 139811328716800 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:52:30.192108 139811328716800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:52:30.192141 139811328716800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:52:30.192205 139811328716800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.194416 139811328716800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:52:30.199854 139811328716800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.200110 139811328716800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:52:30.202739 139811328716800 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:52:30.215440 139811328716800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:52:30.215496 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:30.215533 139811328716800 transformer_layer.py:389] tlayer: self-attention.
I0123 15:52:30.215564 139811328716800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.215626 139811328716800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.216181 139811328716800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.216257 139811328716800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.216612 139811328716800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.217306 139811328716800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.219867 139811328716800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.220486 139811328716800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.220565 139811328716800 transformer_layer.py:468] tlayer: End windows.
I0123 15:52:30.220600 139811328716800 transformer_layer.py:472] tlayer: final FFN.
I0123 15:52:30.220660 139811328716800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.220793 139811328716800 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:52:30.220905 139811328716800 nn_components.py:325] mlp: activation = None
I0123 15:52:30.220945 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:30.222831 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.222927 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:30.225410 139811328716800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.225489 139811328716800 transformer_base.py:443] tbase: final FFN
I0123 15:52:30.225595 139811328716800 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:52:30.228195 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:30.230066 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.230162 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:30.230448 139811328716800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.230537 139811328716800 decoder_stack.py:344] dstack: Final layernorm.
I0123 15:52:30.233456 139811328716800 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:30.288695 139811328716800 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.288781 139811328716800 decoder_stack.py:333] dstack: autoregressive generator.
I0123 15:52:30.288836 139811328716800 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 15:52:30.288940 139811328716800 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:52:30.288979 139811328716800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:52:30.289010 139811328716800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:52:30.289073 139811328716800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.291420 139811328716800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:52:30.296828 139811328716800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.297087 139811328716800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:52:30.299688 139811328716800 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:52:30.311965 139811328716800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:52:30.312021 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:30.312058 139811328716800 transformer_layer.py:389] tlayer: self-attention.
I0123 15:52:30.312089 139811328716800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.312150 139811328716800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.312705 139811328716800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.312781 139811328716800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.313138 139811328716800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.313812 139811328716800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.316299 139811328716800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.316911 139811328716800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.316987 139811328716800 transformer_layer.py:468] tlayer: End windows.
I0123 15:52:30.317023 139811328716800 transformer_layer.py:472] tlayer: final FFN.
I0123 15:52:30.317083 139811328716800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.317210 139811328716800 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:52:30.317326 139811328716800 nn_components.py:325] mlp: activation = None
I0123 15:52:30.317366 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:30.319197 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.319292 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:30.321661 139811328716800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.321740 139811328716800 transformer_base.py:443] tbase: final FFN
I0123 15:52:30.321850 139811328716800 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:52:30.324085 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:30.325928 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.326025 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:30.326310 139811328716800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.326390 139811328716800 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 15:52:30.326496 139811328716800 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:52:30.326535 139811328716800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:52:30.326566 139811328716800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:52:30.326630 139811328716800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.328834 139811328716800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:52:30.334166 139811328716800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.334424 139811328716800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:52:30.337048 139811328716800 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:52:30.349175 139811328716800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:52:30.349231 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:30.349268 139811328716800 transformer_layer.py:389] tlayer: self-attention.
I0123 15:52:30.349300 139811328716800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.349360 139811328716800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.349914 139811328716800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.349992 139811328716800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.350345 139811328716800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.351022 139811328716800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.353493 139811328716800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.354104 139811328716800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.354181 139811328716800 transformer_layer.py:468] tlayer: End windows.
I0123 15:52:30.354217 139811328716800 transformer_layer.py:472] tlayer: final FFN.
I0123 15:52:30.354276 139811328716800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.354402 139811328716800 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:52:30.354510 139811328716800 nn_components.py:325] mlp: activation = None
I0123 15:52:30.354555 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:30.356376 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.356471 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:30.358832 139811328716800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.358911 139811328716800 transformer_base.py:443] tbase: final FFN
I0123 15:52:30.359020 139811328716800 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:52:30.361259 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:30.363092 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.363187 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:30.363471 139811328716800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.363550 139811328716800 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 15:52:30.363659 139811328716800 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:52:30.363698 139811328716800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:52:30.363729 139811328716800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:52:30.363793 139811328716800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.366000 139811328716800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:52:30.371297 139811328716800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.371556 139811328716800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:52:30.374202 139811328716800 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:52:30.386348 139811328716800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:52:30.386404 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:30.386440 139811328716800 transformer_layer.py:389] tlayer: self-attention.
I0123 15:52:30.386471 139811328716800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.386533 139811328716800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.387081 139811328716800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.387157 139811328716800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.387511 139811328716800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.388185 139811328716800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.391108 139811328716800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.391716 139811328716800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.391794 139811328716800 transformer_layer.py:468] tlayer: End windows.
I0123 15:52:30.391830 139811328716800 transformer_layer.py:472] tlayer: final FFN.
I0123 15:52:30.391890 139811328716800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.392017 139811328716800 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:52:30.392126 139811328716800 nn_components.py:325] mlp: activation = None
I0123 15:52:30.392165 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:30.394002 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.394097 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:30.396453 139811328716800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.396532 139811328716800 transformer_base.py:443] tbase: final FFN
I0123 15:52:30.396640 139811328716800 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:52:30.398874 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:30.400691 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.400785 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:30.401070 139811328716800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.401152 139811328716800 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 15:52:30.401259 139811328716800 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:52:30.401299 139811328716800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:52:30.401331 139811328716800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:52:30.401394 139811328716800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.403601 139811328716800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:52:30.408919 139811328716800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.409179 139811328716800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:52:30.411831 139811328716800 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:52:30.424131 139811328716800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:52:30.424187 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:30.424224 139811328716800 transformer_layer.py:389] tlayer: self-attention.
I0123 15:52:30.424266 139811328716800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.424329 139811328716800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.424881 139811328716800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.424956 139811328716800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.425311 139811328716800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.426002 139811328716800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.428496 139811328716800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.429115 139811328716800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.429191 139811328716800 transformer_layer.py:468] tlayer: End windows.
I0123 15:52:30.429224 139811328716800 transformer_layer.py:472] tlayer: final FFN.
I0123 15:52:30.429283 139811328716800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.429406 139811328716800 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:52:30.429512 139811328716800 nn_components.py:325] mlp: activation = None
I0123 15:52:30.429552 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:30.431418 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.431511 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:30.433893 139811328716800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.433971 139811328716800 transformer_base.py:443] tbase: final FFN
I0123 15:52:30.434079 139811328716800 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:52:30.436342 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:30.438184 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.438277 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:30.438559 139811328716800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.438638 139811328716800 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 15:52:30.438744 139811328716800 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:52:30.438781 139811328716800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:52:30.438811 139811328716800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:52:30.438874 139811328716800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.441067 139811328716800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:52:30.446419 139811328716800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.446669 139811328716800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:52:30.449312 139811328716800 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:52:30.461655 139811328716800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:52:30.461708 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:30.461742 139811328716800 transformer_layer.py:389] tlayer: self-attention.
I0123 15:52:30.461772 139811328716800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.461833 139811328716800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.462388 139811328716800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.462461 139811328716800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.462818 139811328716800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.463490 139811328716800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.466004 139811328716800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.466613 139811328716800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.466691 139811328716800 transformer_layer.py:468] tlayer: End windows.
I0123 15:52:30.466725 139811328716800 transformer_layer.py:472] tlayer: final FFN.
I0123 15:52:30.466783 139811328716800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.466907 139811328716800 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:52:30.467017 139811328716800 nn_components.py:325] mlp: activation = None
I0123 15:52:30.467055 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:30.468896 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.468994 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:30.471378 139811328716800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.471459 139811328716800 transformer_base.py:443] tbase: final FFN
I0123 15:52:30.471565 139811328716800 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:52:30.473919 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:30.475760 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.475853 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:30.476135 139811328716800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.476214 139811328716800 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 15:52:30.476321 139811328716800 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:52:30.476359 139811328716800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:52:30.476389 139811328716800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:52:30.476452 139811328716800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.478672 139811328716800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:52:30.484036 139811328716800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.484293 139811328716800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:52:30.486937 139811328716800 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:52:30.499239 139811328716800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:52:30.499292 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:30.499326 139811328716800 transformer_layer.py:389] tlayer: self-attention.
I0123 15:52:30.499356 139811328716800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.499421 139811328716800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.499969 139811328716800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.500042 139811328716800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.500393 139811328716800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.501069 139811328716800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.503981 139811328716800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.504599 139811328716800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.504675 139811328716800 transformer_layer.py:468] tlayer: End windows.
I0123 15:52:30.504709 139811328716800 transformer_layer.py:472] tlayer: final FFN.
I0123 15:52:30.504767 139811328716800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.504892 139811328716800 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:52:30.504999 139811328716800 nn_components.py:325] mlp: activation = None
I0123 15:52:30.505036 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:30.506904 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.507011 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:30.509388 139811328716800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.509466 139811328716800 transformer_base.py:443] tbase: final FFN
I0123 15:52:30.509572 139811328716800 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:52:30.511840 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:30.513688 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.513782 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:30.514064 139811328716800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.514143 139811328716800 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 15:52:30.514248 139811328716800 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:52:30.514286 139811328716800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:52:30.514316 139811328716800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:52:30.514379 139811328716800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.516598 139811328716800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:52:30.521966 139811328716800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.522222 139811328716800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:52:30.524861 139811328716800 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:52:30.537214 139811328716800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:52:30.537268 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:30.537304 139811328716800 transformer_layer.py:389] tlayer: self-attention.
I0123 15:52:30.537335 139811328716800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.537395 139811328716800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.537950 139811328716800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.538025 139811328716800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.538378 139811328716800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.539050 139811328716800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.541556 139811328716800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.542180 139811328716800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.542257 139811328716800 transformer_layer.py:468] tlayer: End windows.
I0123 15:52:30.542292 139811328716800 transformer_layer.py:472] tlayer: final FFN.
I0123 15:52:30.542351 139811328716800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.542476 139811328716800 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:52:30.542582 139811328716800 nn_components.py:325] mlp: activation = None
I0123 15:52:30.542619 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:30.544475 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.544567 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:30.547024 139811328716800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.547105 139811328716800 transformer_base.py:443] tbase: final FFN
I0123 15:52:30.547217 139811328716800 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:52:30.549471 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:30.551325 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.551419 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:30.551700 139811328716800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.551779 139811328716800 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 15:52:30.551883 139811328716800 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:52:30.551920 139811328716800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:52:30.551950 139811328716800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:52:30.552013 139811328716800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.554219 139811328716800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:52:30.559593 139811328716800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.559848 139811328716800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:52:30.562512 139811328716800 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:52:30.575064 139811328716800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:52:30.575116 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:30.575151 139811328716800 transformer_layer.py:389] tlayer: self-attention.
I0123 15:52:30.575180 139811328716800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.575240 139811328716800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.575795 139811328716800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.576016 139811328716800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.576370 139811328716800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.577055 139811328716800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.579567 139811328716800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.580182 139811328716800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.580257 139811328716800 transformer_layer.py:468] tlayer: End windows.
I0123 15:52:30.580291 139811328716800 transformer_layer.py:472] tlayer: final FFN.
I0123 15:52:30.580349 139811328716800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.580473 139811328716800 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:52:30.580578 139811328716800 nn_components.py:325] mlp: activation = None
I0123 15:52:30.580615 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:30.582466 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.582558 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:30.584921 139811328716800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.585004 139811328716800 transformer_base.py:443] tbase: final FFN
I0123 15:52:30.585112 139811328716800 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:52:30.587380 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:30.589213 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.589306 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:30.589587 139811328716800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.589673 139811328716800 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 15:52:30.589778 139811328716800 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:52:30.589816 139811328716800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:52:30.589846 139811328716800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:52:30.589907 139811328716800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.592125 139811328716800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:52:30.597456 139811328716800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.597719 139811328716800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:52:30.600369 139811328716800 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:52:30.612791 139811328716800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:52:30.612845 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:30.612879 139811328716800 transformer_layer.py:389] tlayer: self-attention.
I0123 15:52:30.612910 139811328716800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.612970 139811328716800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.613520 139811328716800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.613595 139811328716800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.613965 139811328716800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.614639 139811328716800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.617510 139811328716800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.618128 139811328716800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.618205 139811328716800 transformer_layer.py:468] tlayer: End windows.
I0123 15:52:30.618240 139811328716800 transformer_layer.py:472] tlayer: final FFN.
I0123 15:52:30.618298 139811328716800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.618423 139811328716800 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:52:30.618533 139811328716800 nn_components.py:325] mlp: activation = None
I0123 15:52:30.618571 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:30.620426 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.620517 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:30.622907 139811328716800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.622994 139811328716800 transformer_base.py:443] tbase: final FFN
I0123 15:52:30.623102 139811328716800 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:52:30.625380 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:30.627246 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.627340 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:30.627625 139811328716800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.627704 139811328716800 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 15:52:30.627809 139811328716800 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:52:30.627847 139811328716800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:52:30.627878 139811328716800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:52:30.627940 139811328716800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.630170 139811328716800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:52:30.635526 139811328716800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.635782 139811328716800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:52:30.638461 139811328716800 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:52:30.650877 139811328716800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:52:30.650932 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:30.650967 139811328716800 transformer_layer.py:389] tlayer: self-attention.
I0123 15:52:30.650997 139811328716800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.651057 139811328716800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.651611 139811328716800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.651686 139811328716800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.652038 139811328716800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.652704 139811328716800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.655209 139811328716800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.655821 139811328716800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.655897 139811328716800 transformer_layer.py:468] tlayer: End windows.
I0123 15:52:30.655931 139811328716800 transformer_layer.py:472] tlayer: final FFN.
I0123 15:52:30.655987 139811328716800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.656112 139811328716800 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:52:30.656217 139811328716800 nn_components.py:325] mlp: activation = None
I0123 15:52:30.656255 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:30.658623 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.658717 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:30.661104 139811328716800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.661181 139811328716800 transformer_base.py:443] tbase: final FFN
I0123 15:52:30.661295 139811328716800 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:52:30.663530 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:30.665345 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.665438 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:30.665727 139811328716800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.665807 139811328716800 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 15:52:30.665913 139811328716800 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:52:30.665950 139811328716800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:52:30.665980 139811328716800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:52:30.666042 139811328716800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.668235 139811328716800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:52:30.673571 139811328716800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.673835 139811328716800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:52:30.676575 139811328716800 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:52:30.689050 139811328716800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:52:30.689104 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:30.689140 139811328716800 transformer_layer.py:389] tlayer: self-attention.
I0123 15:52:30.689169 139811328716800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.689228 139811328716800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.689793 139811328716800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.689869 139811328716800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.690222 139811328716800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.690905 139811328716800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.693426 139811328716800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.694052 139811328716800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.694128 139811328716800 transformer_layer.py:468] tlayer: End windows.
I0123 15:52:30.694163 139811328716800 transformer_layer.py:472] tlayer: final FFN.
I0123 15:52:30.694220 139811328716800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.694344 139811328716800 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:52:30.694450 139811328716800 nn_components.py:325] mlp: activation = None
I0123 15:52:30.694487 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:30.696350 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.696440 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:30.698820 139811328716800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.698897 139811328716800 transformer_base.py:443] tbase: final FFN
I0123 15:52:30.699001 139811328716800 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:52:30.701261 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:30.703099 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.703192 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:30.703474 139811328716800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.703552 139811328716800 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 15:52:30.703657 139811328716800 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:52:30.703694 139811328716800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:52:30.703724 139811328716800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:52:30.703784 139811328716800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.706001 139811328716800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:52:30.711325 139811328716800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.711581 139811328716800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:52:30.714235 139811328716800 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:52:30.726465 139811328716800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:52:30.726519 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:30.726554 139811328716800 transformer_layer.py:389] tlayer: self-attention.
I0123 15:52:30.726585 139811328716800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.726645 139811328716800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.727191 139811328716800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.727265 139811328716800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.727617 139811328716800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.728293 139811328716800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.731148 139811328716800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.731764 139811328716800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.731838 139811328716800 transformer_layer.py:468] tlayer: End windows.
I0123 15:52:30.731872 139811328716800 transformer_layer.py:472] tlayer: final FFN.
I0123 15:52:30.731929 139811328716800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.732051 139811328716800 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:52:30.732156 139811328716800 nn_components.py:325] mlp: activation = None
I0123 15:52:30.732193 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:30.734048 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.734140 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:30.736685 139811328716800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.736761 139811328716800 transformer_base.py:443] tbase: final FFN
I0123 15:52:30.736865 139811328716800 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:52:30.739118 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:30.740954 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.741047 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:30.741328 139811328716800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:30.741410 139811328716800 decoder_stack.py:344] dstack: Final layernorm.
I0123 15:52:30.744186 139811328716800 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:35.148115 139811328716800 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
I0123 15:52:35.697909 139811328716800 training_loop.py:409] No working directory specified.
I0123 15:52:35.698029 139811328716800 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0123 15:52:35.698798 139811328716800 checkpoints.py:1062] Restoring legacy Flax checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0123 15:52:38.750563 139811328716800 training_loop.py:447] Only restoring trainable parameters.
I0123 15:52:38.751315 139811328716800 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0123 15:52:38.751374 139811328716800 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0123 15:52:38.751421 139811328716800 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 15:52:38.751462 139811328716800 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 15:52:38.751500 139811328716800 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:52:38.751540 139811328716800 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 15:52:38.751579 139811328716800 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:52:38.751618 139811328716800 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:52:38.751653 139811328716800 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 15:52:38.751689 139811328716800 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 15:52:38.751727 139811328716800 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:52:38.751765 139811328716800 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 15:52:38.751802 139811328716800 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 15:52:38.751838 139811328716800 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 15:52:38.751873 139811328716800 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:52:38.751910 139811328716800 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 15:52:38.751946 139811328716800 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:52:38.751982 139811328716800 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:52:38.752017 139811328716800 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 15:52:38.752053 139811328716800 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 15:52:38.752104 139811328716800 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:52:38.752143 139811328716800 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 15:52:38.752180 139811328716800 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 15:52:38.752216 139811328716800 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 15:52:38.752251 139811328716800 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:52:38.752286 139811328716800 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 15:52:38.752321 139811328716800 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:52:38.752357 139811328716800 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:52:38.752393 139811328716800 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 15:52:38.752428 139811328716800 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 15:52:38.752463 139811328716800 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:52:38.752498 139811328716800 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 15:52:38.752533 139811328716800 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 15:52:38.752568 139811328716800 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 15:52:38.752602 139811328716800 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:52:38.752637 139811328716800 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 15:52:38.752671 139811328716800 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:52:38.752706 139811328716800 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:52:38.752741 139811328716800 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 15:52:38.752776 139811328716800 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 15:52:38.752810 139811328716800 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:52:38.752845 139811328716800 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 15:52:38.752880 139811328716800 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 15:52:38.752915 139811328716800 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 15:52:38.752949 139811328716800 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:52:38.752984 139811328716800 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 15:52:38.753025 139811328716800 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:52:38.753062 139811328716800 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:52:38.753098 139811328716800 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 15:52:38.753132 139811328716800 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 15:52:38.753166 139811328716800 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:52:38.753201 139811328716800 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 15:52:38.753235 139811328716800 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 15:52:38.753269 139811328716800 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 15:52:38.753304 139811328716800 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:52:38.753339 139811328716800 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 15:52:38.753373 139811328716800 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:52:38.753407 139811328716800 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:52:38.753443 139811328716800 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 15:52:38.753477 139811328716800 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 15:52:38.753511 139811328716800 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:52:38.753546 139811328716800 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 15:52:38.753581 139811328716800 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 15:52:38.753615 139811328716800 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 15:52:38.753658 139811328716800 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:52:38.753695 139811328716800 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 15:52:38.753731 139811328716800 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:52:38.753765 139811328716800 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:52:38.753800 139811328716800 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 15:52:38.753835 139811328716800 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 15:52:38.753869 139811328716800 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:52:38.753904 139811328716800 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 15:52:38.753938 139811328716800 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 15:52:38.753978 139811328716800 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 15:52:38.754015 139811328716800 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:52:38.754050 139811328716800 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 15:52:38.754084 139811328716800 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:52:38.754120 139811328716800 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:52:38.754154 139811328716800 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 15:52:38.754189 139811328716800 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 15:52:38.754223 139811328716800 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:52:38.754257 139811328716800 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 15:52:38.754292 139811328716800 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 15:52:38.754327 139811328716800 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 15:52:38.754362 139811328716800 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:52:38.754397 139811328716800 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 15:52:38.754432 139811328716800 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:52:38.754467 139811328716800 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:52:38.754502 139811328716800 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 15:52:38.754536 139811328716800 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 15:52:38.754570 139811328716800 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:52:38.754604 139811328716800 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 15:52:38.754638 139811328716800 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 15:52:38.754673 139811328716800 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 15:52:38.754708 139811328716800 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:52:38.754743 139811328716800 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 15:52:38.754776 139811328716800 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:52:38.754811 139811328716800 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:52:38.754845 139811328716800 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 15:52:38.754879 139811328716800 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 15:52:38.754918 139811328716800 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:52:38.754955 139811328716800 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 15:52:38.754990 139811328716800 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 15:52:38.755025 139811328716800 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 15:52:38.755060 139811328716800 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:52:38.755095 139811328716800 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 15:52:38.755129 139811328716800 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:52:38.755163 139811328716800 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:52:38.755197 139811328716800 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 15:52:38.755231 139811328716800 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 15:52:38.755264 139811328716800 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:52:38.755298 139811328716800 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 15:52:38.755333 139811328716800 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 15:52:38.755368 139811328716800 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 15:52:38.755403 139811328716800 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:52:38.755437 139811328716800 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 15:52:38.755471 139811328716800 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:52:38.755505 139811328716800 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:52:38.755539 139811328716800 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 15:52:38.755575 139811328716800 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 15:52:38.755609 139811328716800 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 15:52:38.755644 139811328716800 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 15:52:38.755672 139811328716800 training_loop.py:725] Total parameters: 152072288
I0123 15:52:38.755903 139811328716800 training_loop.py:739] Total state size: 0
I0123 15:52:38.777777 139811328716800 training_loop.py:492] Training loop: creating task for mode beam_search
I0123 15:52:38.778073 139811328716800 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0123 15:52:38.778428 139811328716800 training_loop.py:652] Compiling mode beam_search with jit.
I0123 15:52:38.778764 139811328716800 training_loop.py:89] registering functions: dict_keys([])
I0123 15:52:38.795370 139811328716800 graph.py:499] a b c = triangle a b c; d = incenter d b a c; e = on_circle e a c, on_line e b a; f = on_circle f a c, on_line f b a; g = on_circle g c a, on_line g b c; h = on_circle h c a, on_line h b c; i = circle i b a c; j = on_line j e g, on_line j i d ? perp g j j i
I0123 15:52:39.461745 139811328716800 ddar.py:60] Depth 1/1000 time = 0.6291928291320801
I0123 15:52:40.509022 139811328716800 ddar.py:60] Depth 2/1000 time = 1.0470688343048096
I0123 15:52:41.584036 139811328716800 ddar.py:60] Depth 3/1000 time = 1.074819803237915
I0123 15:52:42.657776 139811328716800 ddar.py:60] Depth 4/1000 time = 1.073380470275879
I0123 15:52:43.793739 139811328716800 ddar.py:60] Depth 5/1000 time = 1.130836009979248
I0123 15:52:44.967602 139811328716800 ddar.py:60] Depth 6/1000 time = 1.1736812591552734
I0123 15:52:46.292652 139811328716800 ddar.py:60] Depth 7/1000 time = 1.3248834609985352
I0123 15:52:47.806242 139811328716800 ddar.py:60] Depth 8/1000 time = 1.5134153366088867
I0123 15:52:49.336024 139811328716800 ddar.py:60] Depth 9/1000 time = 1.5295791625976562
I0123 15:52:50.864722 139811328716800 ddar.py:60] Depth 10/1000 time = 1.5284600257873535
I0123 15:52:50.870455 139811328716800 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 15:52:50.870545 139811328716800 alphageometry.py:540] Depth 0. There are 1 nodes to expand:
I0123 15:52:50.870583 139811328716800 alphageometry.py:544] {S} a : ; b : ; c : ; d : ^ b a b d b d b c 00 ^ c a c d c d c b 01 ; e : C a b e 02 D a c a e 03 ; f : C a b f 04 D a c a f 05 ; g : C b c g 06 D a c c g 07 ; h : C b c h 08 D a c c h 09 ; i : D a i b i 10 D a i c i 11 ; j : C d i j 12 C e g j 13 ? T g j j i {F1} x00
I0123 15:52:50.870615 139811328716800 alphageometry.py:549] Decoding from {S} a : ; b : ; c : ; d : ^ b a b d b d b c 00 ^ c a c d c d c b 01 ; e : C a b e 02 D a c a e 03 ; f : C a b f 04 D a c a f 05 ; g : C b c g 06 D a c c g 07 ; h : C b c h 08 D a c c h 09 ; i : D a i b i 10 D a i c i 11 ; j : C d i j 12 C e g j 13 ? T g j j i {F1} x00
I0123 15:52:50.991206 139811328716800 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:50.991414 139811328716800 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 15:52:50.991515 139811328716800 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 15:52:50.991589 139811328716800 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 15:52:50.991658 139811328716800 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 15:52:50.991725 139811328716800 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 15:52:50.991791 139811328716800 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 15:52:50.991858 139811328716800 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 15:52:50.991925 139811328716800 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 15:52:50.991993 139811328716800 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 15:52:50.992060 139811328716800 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 15:52:50.992128 139811328716800 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 15:52:50.992194 139811328716800 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 15:52:50.992259 139811328716800 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 15:52:50.992296 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:50.992339 139811328716800 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 15:52:50.992446 139811328716800 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:52:50.992485 139811328716800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:52:50.992524 139811328716800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:52:50.994397 139811328716800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:50.996902 139811328716800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:52:51.002676 139811328716800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.002951 139811328716800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:52:51.005562 139811328716800 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:52:51.009917 139811328716800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:52:51.009973 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:51.010009 139811328716800 transformer_layer.py:389] tlayer: self-attention.
I0123 15:52:51.010044 139811328716800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.010107 139811328716800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.010727 139811328716800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.010803 139811328716800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.011307 139811328716800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.012079 139811328716800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.014628 139811328716800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.015253 139811328716800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.015328 139811328716800 transformer_layer.py:468] tlayer: End windows.
I0123 15:52:51.015362 139811328716800 transformer_layer.py:472] tlayer: final FFN.
I0123 15:52:51.015421 139811328716800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.015548 139811328716800 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:52:51.015874 139811328716800 nn_components.py:325] mlp: activation = None
I0123 15:52:51.015917 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:51.017876 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.017970 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:51.020402 139811328716800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.020481 139811328716800 transformer_base.py:443] tbase: final FFN
I0123 15:52:51.020898 139811328716800 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:52:51.023212 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:51.025113 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.025207 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:51.025496 139811328716800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.025575 139811328716800 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 15:52:51.025688 139811328716800 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:52:51.025728 139811328716800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:52:51.025759 139811328716800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:52:51.027605 139811328716800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.029928 139811328716800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:52:51.035472 139811328716800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.035729 139811328716800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:52:51.038341 139811328716800 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:52:51.041949 139811328716800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:52:51.042002 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:51.042037 139811328716800 transformer_layer.py:389] tlayer: self-attention.
I0123 15:52:51.042067 139811328716800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.042129 139811328716800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.042683 139811328716800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.042760 139811328716800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.043116 139811328716800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.043875 139811328716800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.046303 139811328716800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.046969 139811328716800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.047046 139811328716800 transformer_layer.py:468] tlayer: End windows.
I0123 15:52:51.047079 139811328716800 transformer_layer.py:472] tlayer: final FFN.
I0123 15:52:51.047137 139811328716800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.047261 139811328716800 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:52:51.047577 139811328716800 nn_components.py:325] mlp: activation = None
I0123 15:52:51.047619 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:51.049514 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.049605 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:51.052058 139811328716800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.052136 139811328716800 transformer_base.py:443] tbase: final FFN
I0123 15:52:51.052559 139811328716800 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:52:51.054908 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:51.056808 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.056900 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:51.057189 139811328716800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.057269 139811328716800 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 15:52:51.057374 139811328716800 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:52:51.057411 139811328716800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:52:51.057441 139811328716800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:52:51.059226 139811328716800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.061537 139811328716800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:52:51.067207 139811328716800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.067465 139811328716800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:52:51.070027 139811328716800 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:52:51.073638 139811328716800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:52:51.073700 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:51.073735 139811328716800 transformer_layer.py:389] tlayer: self-attention.
I0123 15:52:51.073766 139811328716800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.073831 139811328716800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.074444 139811328716800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.074521 139811328716800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.074875 139811328716800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.075637 139811328716800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.078274 139811328716800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.078888 139811328716800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.078963 139811328716800 transformer_layer.py:468] tlayer: End windows.
I0123 15:52:51.078997 139811328716800 transformer_layer.py:472] tlayer: final FFN.
I0123 15:52:51.079054 139811328716800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.079180 139811328716800 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:52:51.079492 139811328716800 nn_components.py:325] mlp: activation = None
I0123 15:52:51.079534 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:51.081495 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.081587 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:51.084030 139811328716800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.084109 139811328716800 transformer_base.py:443] tbase: final FFN
I0123 15:52:51.084529 139811328716800 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:52:51.086768 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:51.088656 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.088749 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:51.089036 139811328716800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.089116 139811328716800 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 15:52:51.089221 139811328716800 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:52:51.089259 139811328716800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:52:51.089289 139811328716800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:52:51.091235 139811328716800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.093559 139811328716800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:52:51.099136 139811328716800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.099391 139811328716800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:52:51.102008 139811328716800 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:52:51.105603 139811328716800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:52:51.105662 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:51.105699 139811328716800 transformer_layer.py:389] tlayer: self-attention.
I0123 15:52:51.105730 139811328716800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.105793 139811328716800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.106344 139811328716800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.106418 139811328716800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.106770 139811328716800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.107528 139811328716800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.109968 139811328716800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.110635 139811328716800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.110712 139811328716800 transformer_layer.py:468] tlayer: End windows.
I0123 15:52:51.110747 139811328716800 transformer_layer.py:472] tlayer: final FFN.
I0123 15:52:51.110805 139811328716800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.110934 139811328716800 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:52:51.111245 139811328716800 nn_components.py:325] mlp: activation = None
I0123 15:52:51.111290 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:51.113193 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.113286 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:51.115741 139811328716800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.115821 139811328716800 transformer_base.py:443] tbase: final FFN
I0123 15:52:51.116245 139811328716800 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:52:51.118897 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:51.120815 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.120909 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:51.121197 139811328716800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.121277 139811328716800 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 15:52:51.121383 139811328716800 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:52:51.121421 139811328716800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:52:51.121452 139811328716800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:52:51.123229 139811328716800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.125532 139811328716800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:52:51.131189 139811328716800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.131454 139811328716800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:52:51.134025 139811328716800 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:52:51.137654 139811328716800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:52:51.137708 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:51.137743 139811328716800 transformer_layer.py:389] tlayer: self-attention.
I0123 15:52:51.137773 139811328716800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.137883 139811328716800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.138442 139811328716800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.138519 139811328716800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.138878 139811328716800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.139647 139811328716800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.142100 139811328716800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.142710 139811328716800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.142786 139811328716800 transformer_layer.py:468] tlayer: End windows.
I0123 15:52:51.142821 139811328716800 transformer_layer.py:472] tlayer: final FFN.
I0123 15:52:51.142878 139811328716800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.143002 139811328716800 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:52:51.143313 139811328716800 nn_components.py:325] mlp: activation = None
I0123 15:52:51.143355 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:51.145303 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.145394 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:51.147833 139811328716800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.147912 139811328716800 transformer_base.py:443] tbase: final FFN
I0123 15:52:51.148328 139811328716800 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:52:51.150582 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:51.152472 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.152565 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:51.152851 139811328716800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.152930 139811328716800 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 15:52:51.153036 139811328716800 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:52:51.153074 139811328716800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:52:51.153105 139811328716800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:52:51.154975 139811328716800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.157268 139811328716800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:52:51.162790 139811328716800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.163049 139811328716800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:52:51.165661 139811328716800 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:52:51.169277 139811328716800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:52:51.169330 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:51.169364 139811328716800 transformer_layer.py:389] tlayer: self-attention.
I0123 15:52:51.169395 139811328716800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.169455 139811328716800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.170012 139811328716800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.170089 139811328716800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.170441 139811328716800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.171203 139811328716800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.173657 139811328716800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.174328 139811328716800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.174404 139811328716800 transformer_layer.py:468] tlayer: End windows.
I0123 15:52:51.174439 139811328716800 transformer_layer.py:472] tlayer: final FFN.
I0123 15:52:51.174498 139811328716800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.174644 139811328716800 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:52:51.174960 139811328716800 nn_components.py:325] mlp: activation = None
I0123 15:52:51.175003 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:51.177108 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.177199 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:51.179809 139811328716800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.179887 139811328716800 transformer_base.py:443] tbase: final FFN
I0123 15:52:51.180305 139811328716800 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:52:51.182635 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:51.184534 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.184627 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:51.184916 139811328716800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.184995 139811328716800 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 15:52:51.185101 139811328716800 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:52:51.185138 139811328716800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:52:51.185168 139811328716800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:52:51.186954 139811328716800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.189252 139811328716800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:52:51.194882 139811328716800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.195136 139811328716800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:52:51.197674 139811328716800 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:52:51.201248 139811328716800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:52:51.201302 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:51.201337 139811328716800 transformer_layer.py:389] tlayer: self-attention.
I0123 15:52:51.201368 139811328716800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.201478 139811328716800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.202039 139811328716800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.202114 139811328716800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.202466 139811328716800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.203226 139811328716800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.205676 139811328716800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.206294 139811328716800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.206371 139811328716800 transformer_layer.py:468] tlayer: End windows.
I0123 15:52:51.206405 139811328716800 transformer_layer.py:472] tlayer: final FFN.
I0123 15:52:51.206462 139811328716800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.206589 139811328716800 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:52:51.206902 139811328716800 nn_components.py:325] mlp: activation = None
I0123 15:52:51.206943 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:51.208886 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.208978 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:51.211419 139811328716800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.211498 139811328716800 transformer_base.py:443] tbase: final FFN
I0123 15:52:51.211921 139811328716800 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:52:51.214169 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:51.216059 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.216152 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:51.216439 139811328716800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.216519 139811328716800 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 15:52:51.216625 139811328716800 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:52:51.216663 139811328716800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:52:51.216694 139811328716800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:52:51.218542 139811328716800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.220857 139811328716800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:52:51.226424 139811328716800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.226682 139811328716800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:52:51.229645 139811328716800 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:52:51.233278 139811328716800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:52:51.233331 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:51.233366 139811328716800 transformer_layer.py:389] tlayer: self-attention.
I0123 15:52:51.233396 139811328716800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.233459 139811328716800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.234021 139811328716800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.234098 139811328716800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.234453 139811328716800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.235212 139811328716800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.237683 139811328716800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.238354 139811328716800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.238431 139811328716800 transformer_layer.py:468] tlayer: End windows.
I0123 15:52:51.238466 139811328716800 transformer_layer.py:472] tlayer: final FFN.
I0123 15:52:51.238525 139811328716800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.238653 139811328716800 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:52:51.238973 139811328716800 nn_components.py:325] mlp: activation = None
I0123 15:52:51.239015 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:51.240911 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.241003 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:51.243436 139811328716800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.243515 139811328716800 transformer_base.py:443] tbase: final FFN
I0123 15:52:51.243930 139811328716800 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:52:51.246236 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:51.248128 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.248222 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:51.248510 139811328716800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.248589 139811328716800 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 15:52:51.248697 139811328716800 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:52:51.248735 139811328716800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:52:51.248767 139811328716800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:52:51.250560 139811328716800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.252874 139811328716800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:52:51.258500 139811328716800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.258756 139811328716800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:52:51.261290 139811328716800 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:52:51.264895 139811328716800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:52:51.264954 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:51.264989 139811328716800 transformer_layer.py:389] tlayer: self-attention.
I0123 15:52:51.265020 139811328716800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.265083 139811328716800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.265704 139811328716800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.265784 139811328716800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.266141 139811328716800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.266896 139811328716800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.269366 139811328716800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.269993 139811328716800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.270070 139811328716800 transformer_layer.py:468] tlayer: End windows.
I0123 15:52:51.270104 139811328716800 transformer_layer.py:472] tlayer: final FFN.
I0123 15:52:51.270162 139811328716800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.270291 139811328716800 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:52:51.270606 139811328716800 nn_components.py:325] mlp: activation = None
I0123 15:52:51.270649 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:51.272547 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.272639 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:51.275162 139811328716800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.275241 139811328716800 transformer_base.py:443] tbase: final FFN
I0123 15:52:51.275660 139811328716800 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:52:51.278072 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:51.280131 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.280224 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:51.280510 139811328716800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.280590 139811328716800 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 15:52:51.280696 139811328716800 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:52:51.280733 139811328716800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:52:51.280764 139811328716800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:52:51.282540 139811328716800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.284930 139811328716800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:52:51.290470 139811328716800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.290725 139811328716800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:52:51.293305 139811328716800 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:52:51.296927 139811328716800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:52:51.296980 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:51.297015 139811328716800 transformer_layer.py:389] tlayer: self-attention.
I0123 15:52:51.297055 139811328716800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.297174 139811328716800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.297744 139811328716800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.297820 139811328716800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.298176 139811328716800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.298931 139811328716800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.301395 139811328716800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.302011 139811328716800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.302088 139811328716800 transformer_layer.py:468] tlayer: End windows.
I0123 15:52:51.302123 139811328716800 transformer_layer.py:472] tlayer: final FFN.
I0123 15:52:51.302181 139811328716800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.302308 139811328716800 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:52:51.302617 139811328716800 nn_components.py:325] mlp: activation = None
I0123 15:52:51.302658 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:51.304598 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.304689 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:51.307120 139811328716800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.307198 139811328716800 transformer_base.py:443] tbase: final FFN
I0123 15:52:51.307615 139811328716800 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:52:51.309867 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:51.311774 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.311867 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:51.312156 139811328716800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.312234 139811328716800 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 15:52:51.312340 139811328716800 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:52:51.312378 139811328716800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:52:51.312408 139811328716800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:52:51.314270 139811328716800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.316564 139811328716800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:52:51.322083 139811328716800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.322340 139811328716800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:52:51.324879 139811328716800 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:52:51.328533 139811328716800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:52:51.328587 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:51.328622 139811328716800 transformer_layer.py:389] tlayer: self-attention.
I0123 15:52:51.328653 139811328716800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.328720 139811328716800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.329277 139811328716800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.329352 139811328716800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.329712 139811328716800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.330473 139811328716800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.332917 139811328716800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.333528 139811328716800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.333603 139811328716800 transformer_layer.py:468] tlayer: End windows.
I0123 15:52:51.333637 139811328716800 transformer_layer.py:472] tlayer: final FFN.
I0123 15:52:51.333705 139811328716800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.333832 139811328716800 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:52:51.334143 139811328716800 nn_components.py:325] mlp: activation = None
I0123 15:52:51.334184 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:51.336138 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.336230 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:51.338676 139811328716800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.338755 139811328716800 transformer_base.py:443] tbase: final FFN
I0123 15:52:51.339179 139811328716800 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:52:51.341406 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:51.343290 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.343384 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:51.343674 139811328716800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.343753 139811328716800 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 15:52:51.343860 139811328716800 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:52:51.343897 139811328716800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:52:51.343928 139811328716800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:52:51.346198 139811328716800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.348520 139811328716800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:52:51.354084 139811328716800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.354338 139811328716800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:52:51.356921 139811328716800 transformer_layer.py:213] tlayer: windowed attention.
I0123 15:52:51.360603 139811328716800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:52:51.360657 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:51.360691 139811328716800 transformer_layer.py:389] tlayer: self-attention.
I0123 15:52:51.360721 139811328716800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.360788 139811328716800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.361341 139811328716800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.361418 139811328716800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.361788 139811328716800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.362559 139811328716800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.365007 139811328716800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.365621 139811328716800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.365710 139811328716800 transformer_layer.py:468] tlayer: End windows.
I0123 15:52:51.365746 139811328716800 transformer_layer.py:472] tlayer: final FFN.
I0123 15:52:51.365805 139811328716800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.365934 139811328716800 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:52:51.366250 139811328716800 nn_components.py:325] mlp: activation = None
I0123 15:52:51.366291 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:51.368259 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.368351 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:51.370804 139811328716800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.370883 139811328716800 transformer_base.py:443] tbase: final FFN
I0123 15:52:51.371306 139811328716800 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:52:51.373573 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:51.375484 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.375578 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:51.375867 139811328716800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.376111 139811328716800 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 15:52:51.376177 139811328716800 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 15:52:51.376232 139811328716800 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 15:52:51.376285 139811328716800 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 15:52:51.376338 139811328716800 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 15:52:51.376391 139811328716800 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 15:52:51.376442 139811328716800 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 15:52:51.376494 139811328716800 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 15:52:51.376545 139811328716800 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 15:52:51.376596 139811328716800 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 15:52:51.376647 139811328716800 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 15:52:51.376697 139811328716800 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 15:52:51.376732 139811328716800 decoder_stack.py:344] dstack: Final layernorm.
I0123 15:52:51.379814 139811328716800 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 15:52:51.424809 139811328716800 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.424892 139811328716800 decoder_stack.py:333] dstack: autoregressive generator.
I0123 15:52:51.424944 139811328716800 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 15:52:51.425046 139811328716800 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:52:51.425084 139811328716800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:52:51.425114 139811328716800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:52:51.425176 139811328716800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.427536 139811328716800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:52:51.432915 139811328716800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.433173 139811328716800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:52:51.435743 139811328716800 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:52:51.448549 139811328716800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:52:51.448603 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:51.448638 139811328716800 transformer_layer.py:389] tlayer: self-attention.
I0123 15:52:51.448668 139811328716800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.448729 139811328716800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.449289 139811328716800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.449363 139811328716800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.449734 139811328716800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.450422 139811328716800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.452948 139811328716800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.453560 139811328716800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.453636 139811328716800 transformer_layer.py:468] tlayer: End windows.
I0123 15:52:51.453678 139811328716800 transformer_layer.py:472] tlayer: final FFN.
I0123 15:52:51.453738 139811328716800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.453864 139811328716800 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:52:51.453971 139811328716800 nn_components.py:325] mlp: activation = None
I0123 15:52:51.454009 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:51.455854 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.455945 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:51.458348 139811328716800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.458426 139811328716800 transformer_base.py:443] tbase: final FFN
I0123 15:52:51.458532 139811328716800 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:52:51.460776 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:51.462639 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.462733 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:51.463021 139811328716800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.463101 139811328716800 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 15:52:51.463208 139811328716800 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:52:51.463245 139811328716800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:52:51.463276 139811328716800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:52:51.463340 139811328716800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.465576 139811328716800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:52:51.470940 139811328716800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.471201 139811328716800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:52:51.473873 139811328716800 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:52:51.486362 139811328716800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:52:51.486416 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:51.486450 139811328716800 transformer_layer.py:389] tlayer: self-attention.
I0123 15:52:51.486480 139811328716800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.486541 139811328716800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.487088 139811328716800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.487161 139811328716800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.487518 139811328716800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.488251 139811328716800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.490708 139811328716800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.491318 139811328716800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.491394 139811328716800 transformer_layer.py:468] tlayer: End windows.
I0123 15:52:51.491427 139811328716800 transformer_layer.py:472] tlayer: final FFN.
I0123 15:52:51.491485 139811328716800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.491612 139811328716800 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:52:51.491721 139811328716800 nn_components.py:325] mlp: activation = None
I0123 15:52:51.491760 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:51.493599 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.493699 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:51.496116 139811328716800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.496194 139811328716800 transformer_base.py:443] tbase: final FFN
I0123 15:52:51.496301 139811328716800 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:52:51.498551 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:51.500396 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.500496 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:51.500788 139811328716800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.500868 139811328716800 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 15:52:51.500975 139811328716800 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:52:51.501013 139811328716800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:52:51.501044 139811328716800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:52:51.501107 139811328716800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.503356 139811328716800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:52:51.508730 139811328716800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.508991 139811328716800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:52:51.511664 139811328716800 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:52:51.524262 139811328716800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:52:51.524317 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:51.524352 139811328716800 transformer_layer.py:389] tlayer: self-attention.
I0123 15:52:51.524382 139811328716800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.524445 139811328716800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.525008 139811328716800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.525084 139811328716800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.525443 139811328716800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.526564 139811328716800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.529044 139811328716800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.529668 139811328716800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.529745 139811328716800 transformer_layer.py:468] tlayer: End windows.
I0123 15:52:51.529780 139811328716800 transformer_layer.py:472] tlayer: final FFN.
I0123 15:52:51.529837 139811328716800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.529965 139811328716800 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:52:51.530074 139811328716800 nn_components.py:325] mlp: activation = None
I0123 15:52:51.530113 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:51.531974 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.532066 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:51.534489 139811328716800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.534568 139811328716800 transformer_base.py:443] tbase: final FFN
I0123 15:52:51.534676 139811328716800 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:52:51.536936 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:51.538798 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.538900 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:51.539190 139811328716800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.539269 139811328716800 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 15:52:51.539375 139811328716800 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:52:51.539414 139811328716800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:52:51.539444 139811328716800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:52:51.539507 139811328716800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.541743 139811328716800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:52:51.547143 139811328716800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.547404 139811328716800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:52:51.550077 139811328716800 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:52:51.562457 139811328716800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:52:51.562511 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:51.562546 139811328716800 transformer_layer.py:389] tlayer: self-attention.
I0123 15:52:51.562577 139811328716800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.562638 139811328716800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.563188 139811328716800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.563263 139811328716800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.563616 139811328716800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.564347 139811328716800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.566816 139811328716800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.567427 139811328716800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.567504 139811328716800 transformer_layer.py:468] tlayer: End windows.
I0123 15:52:51.567538 139811328716800 transformer_layer.py:472] tlayer: final FFN.
I0123 15:52:51.567597 139811328716800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.567724 139811328716800 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:52:51.567830 139811328716800 nn_components.py:325] mlp: activation = None
I0123 15:52:51.567867 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:51.569720 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.569814 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:51.572203 139811328716800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.572280 139811328716800 transformer_base.py:443] tbase: final FFN
I0123 15:52:51.572385 139811328716800 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:52:51.574635 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:51.576478 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.576572 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:51.576868 139811328716800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.576948 139811328716800 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 15:52:51.577056 139811328716800 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:52:51.577094 139811328716800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:52:51.577125 139811328716800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:52:51.577187 139811328716800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.579417 139811328716800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:52:51.584790 139811328716800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.585057 139811328716800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:52:51.587706 139811328716800 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:52:51.600005 139811328716800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:52:51.600058 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:51.600093 139811328716800 transformer_layer.py:389] tlayer: self-attention.
I0123 15:52:51.600124 139811328716800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.600187 139811328716800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.600740 139811328716800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.600815 139811328716800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.601167 139811328716800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.601907 139811328716800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.604357 139811328716800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.604972 139811328716800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.605048 139811328716800 transformer_layer.py:468] tlayer: End windows.
I0123 15:52:51.605082 139811328716800 transformer_layer.py:472] tlayer: final FFN.
I0123 15:52:51.605140 139811328716800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.605268 139811328716800 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:52:51.605375 139811328716800 nn_components.py:325] mlp: activation = None
I0123 15:52:51.605413 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:51.607262 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.607356 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:51.609747 139811328716800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.609827 139811328716800 transformer_base.py:443] tbase: final FFN
I0123 15:52:51.609934 139811328716800 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:52:51.612179 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:51.614024 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.614119 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:51.614408 139811328716800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.614493 139811328716800 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 15:52:51.614603 139811328716800 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:52:51.614640 139811328716800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:52:51.614671 139811328716800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:52:51.614733 139811328716800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.616964 139811328716800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:52:51.622498 139811328716800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.622757 139811328716800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:52:51.625425 139811328716800 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:52:51.637758 139811328716800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:52:51.637813 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:51.637848 139811328716800 transformer_layer.py:389] tlayer: self-attention.
I0123 15:52:51.637879 139811328716800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.637940 139811328716800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.638492 139811328716800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.638567 139811328716800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.638925 139811328716800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.640046 139811328716800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.642512 139811328716800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.643125 139811328716800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.643201 139811328716800 transformer_layer.py:468] tlayer: End windows.
I0123 15:52:51.643236 139811328716800 transformer_layer.py:472] tlayer: final FFN.
I0123 15:52:51.643294 139811328716800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.643419 139811328716800 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:52:51.643530 139811328716800 nn_components.py:325] mlp: activation = None
I0123 15:52:51.643568 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:51.645410 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.645502 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:51.647912 139811328716800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.647990 139811328716800 transformer_base.py:443] tbase: final FFN
I0123 15:52:51.648098 139811328716800 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:52:51.650349 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:51.652191 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.652285 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:51.652573 139811328716800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.652660 139811328716800 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 15:52:51.652768 139811328716800 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:52:51.652806 139811328716800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:52:51.652836 139811328716800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:52:51.652899 139811328716800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.655133 139811328716800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:52:51.660574 139811328716800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.660835 139811328716800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:52:51.663506 139811328716800 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:52:51.675817 139811328716800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:52:51.675871 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:51.675906 139811328716800 transformer_layer.py:389] tlayer: self-attention.
I0123 15:52:51.675936 139811328716800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.675998 139811328716800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.676550 139811328716800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.676625 139811328716800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.676984 139811328716800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.677728 139811328716800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.680186 139811328716800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.680793 139811328716800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.680868 139811328716800 transformer_layer.py:468] tlayer: End windows.
I0123 15:52:51.680902 139811328716800 transformer_layer.py:472] tlayer: final FFN.
I0123 15:52:51.680959 139811328716800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.681086 139811328716800 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:52:51.681191 139811328716800 nn_components.py:325] mlp: activation = None
I0123 15:52:51.681229 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:51.683074 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.683166 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:51.685564 139811328716800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.685648 139811328716800 transformer_base.py:443] tbase: final FFN
I0123 15:52:51.685759 139811328716800 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:52:51.688016 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:51.689864 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.689960 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:51.690251 139811328716800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.690332 139811328716800 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 15:52:51.690445 139811328716800 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:52:51.690484 139811328716800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:52:51.690515 139811328716800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:52:51.690578 139811328716800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.692799 139811328716800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:52:51.698151 139811328716800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.698418 139811328716800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:52:51.701066 139811328716800 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:52:51.713380 139811328716800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:52:51.713435 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:51.713469 139811328716800 transformer_layer.py:389] tlayer: self-attention.
I0123 15:52:51.713499 139811328716800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.713560 139811328716800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.714118 139811328716800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.714194 139811328716800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.714547 139811328716800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.715267 139811328716800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.717700 139811328716800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.718309 139811328716800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.718384 139811328716800 transformer_layer.py:468] tlayer: End windows.
I0123 15:52:51.718419 139811328716800 transformer_layer.py:472] tlayer: final FFN.
I0123 15:52:51.718476 139811328716800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.718601 139811328716800 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:52:51.718707 139811328716800 nn_components.py:325] mlp: activation = None
I0123 15:52:51.718745 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:51.720590 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.720683 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:51.723092 139811328716800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.723170 139811328716800 transformer_base.py:443] tbase: final FFN
I0123 15:52:51.723276 139811328716800 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:52:51.725520 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:51.727370 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.727465 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:51.727757 139811328716800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.727836 139811328716800 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 15:52:51.727942 139811328716800 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:52:51.727986 139811328716800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:52:51.728018 139811328716800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:52:51.728081 139811328716800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.730319 139811328716800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:52:51.735704 139811328716800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.736112 139811328716800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:52:51.738780 139811328716800 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:52:51.751256 139811328716800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:52:51.751311 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:51.751347 139811328716800 transformer_layer.py:389] tlayer: self-attention.
I0123 15:52:51.751378 139811328716800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.751439 139811328716800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.751988 139811328716800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.752065 139811328716800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.752424 139811328716800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.753106 139811328716800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.756007 139811328716800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.756618 139811328716800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.756696 139811328716800 transformer_layer.py:468] tlayer: End windows.
I0123 15:52:51.756731 139811328716800 transformer_layer.py:472] tlayer: final FFN.
I0123 15:52:51.756789 139811328716800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.756918 139811328716800 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:52:51.757026 139811328716800 nn_components.py:325] mlp: activation = None
I0123 15:52:51.757064 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:51.758919 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.759012 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:51.761416 139811328716800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.761494 139811328716800 transformer_base.py:443] tbase: final FFN
I0123 15:52:51.761602 139811328716800 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:52:51.763882 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:51.765725 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.765821 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:51.766116 139811328716800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.766197 139811328716800 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 15:52:51.766304 139811328716800 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:52:51.766350 139811328716800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:52:51.766381 139811328716800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:52:51.766445 139811328716800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.768671 139811328716800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:52:51.774035 139811328716800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.774295 139811328716800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:52:51.776948 139811328716800 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:52:51.789339 139811328716800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:52:51.789394 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:51.789428 139811328716800 transformer_layer.py:389] tlayer: self-attention.
I0123 15:52:51.789459 139811328716800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.789523 139811328716800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.790085 139811328716800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.790163 139811328716800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.790521 139811328716800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.791204 139811328716800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.793719 139811328716800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.794329 139811328716800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.794405 139811328716800 transformer_layer.py:468] tlayer: End windows.
I0123 15:52:51.794439 139811328716800 transformer_layer.py:472] tlayer: final FFN.
I0123 15:52:51.794496 139811328716800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.794623 139811328716800 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:52:51.794729 139811328716800 nn_components.py:325] mlp: activation = None
I0123 15:52:51.794766 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:51.796596 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.796688 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:51.799089 139811328716800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.799168 139811328716800 transformer_base.py:443] tbase: final FFN
I0123 15:52:51.799274 139811328716800 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:52:51.801508 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:51.803352 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.803446 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:51.803734 139811328716800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.803813 139811328716800 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 15:52:51.803920 139811328716800 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:52:51.803958 139811328716800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:52:51.803997 139811328716800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:52:51.804063 139811328716800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.806303 139811328716800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:52:51.811703 139811328716800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.811966 139811328716800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:52:51.814630 139811328716800 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:52:51.826976 139811328716800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:52:51.827031 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:51.827066 139811328716800 transformer_layer.py:389] tlayer: self-attention.
I0123 15:52:51.827097 139811328716800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.827159 139811328716800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.827711 139811328716800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.827785 139811328716800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.828140 139811328716800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.828820 139811328716800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.831357 139811328716800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.831969 139811328716800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.832044 139811328716800 transformer_layer.py:468] tlayer: End windows.
I0123 15:52:51.832079 139811328716800 transformer_layer.py:472] tlayer: final FFN.
I0123 15:52:51.832136 139811328716800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.832264 139811328716800 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:52:51.832373 139811328716800 nn_components.py:325] mlp: activation = None
I0123 15:52:51.832410 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:51.834267 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.834360 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:51.836778 139811328716800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.836856 139811328716800 transformer_base.py:443] tbase: final FFN
I0123 15:52:51.836963 139811328716800 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:52:51.839236 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:51.841251 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.841346 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:51.841638 139811328716800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.841726 139811328716800 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 15:52:51.841834 139811328716800 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:52:51.841872 139811328716800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:52:51.841903 139811328716800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:52:51.841974 139811328716800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.844225 139811328716800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:52:51.849723 139811328716800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.849985 139811328716800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:52:51.852657 139811328716800 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:52:51.865205 139811328716800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:52:51.865259 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:51.865293 139811328716800 transformer_layer.py:389] tlayer: self-attention.
I0123 15:52:51.865324 139811328716800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.865385 139811328716800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.865947 139811328716800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.866027 139811328716800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.866402 139811328716800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.867118 139811328716800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.870061 139811328716800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.870704 139811328716800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.870784 139811328716800 transformer_layer.py:468] tlayer: End windows.
I0123 15:52:51.870821 139811328716800 transformer_layer.py:472] tlayer: final FFN.
I0123 15:52:51.870881 139811328716800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.871016 139811328716800 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:52:51.871127 139811328716800 nn_components.py:325] mlp: activation = None
I0123 15:52:51.871167 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:51.873032 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.873124 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:51.875626 139811328716800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.875705 139811328716800 transformer_base.py:443] tbase: final FFN
I0123 15:52:51.875814 139811328716800 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:52:51.878089 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:51.879938 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.880032 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:51.880326 139811328716800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.880410 139811328716800 decoder_stack.py:344] dstack: Final layernorm.
I0123 15:52:51.883255 139811328716800 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 15:52:51.933933 139811328716800 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:51.934026 139811328716800 decoder_stack.py:333] dstack: autoregressive generator.
I0123 15:52:51.934080 139811328716800 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 15:52:51.934182 139811328716800 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:52:51.934220 139811328716800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:52:51.934249 139811328716800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:52:51.934311 139811328716800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:51.936616 139811328716800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:52:51.942317 139811328716800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:51.942581 139811328716800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:52:51.945160 139811328716800 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:52:51.957742 139811328716800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:52:51.957798 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:51.957833 139811328716800 transformer_layer.py:389] tlayer: self-attention.
I0123 15:52:51.957864 139811328716800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:51.957924 139811328716800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:51.958478 139811328716800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:51.958553 139811328716800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:51.958910 139811328716800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:51.959587 139811328716800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:51.962205 139811328716800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:51.962815 139811328716800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:51.963042 139811328716800 transformer_layer.py:468] tlayer: End windows.
I0123 15:52:51.963075 139811328716800 transformer_layer.py:472] tlayer: final FFN.
I0123 15:52:51.963133 139811328716800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:51.963260 139811328716800 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:52:51.963367 139811328716800 nn_components.py:325] mlp: activation = None
I0123 15:52:51.963404 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:51.965318 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:51.965410 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:51.967804 139811328716800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:51.967881 139811328716800 transformer_base.py:443] tbase: final FFN
I0123 15:52:51.967987 139811328716800 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:52:51.970356 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:51.972199 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:51.972293 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:51.972585 139811328716800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:51.972673 139811328716800 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 15:52:51.972781 139811328716800 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:52:51.972819 139811328716800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:52:51.972850 139811328716800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:52:51.972912 139811328716800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:51.975169 139811328716800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:52:51.980638 139811328716800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:51.980899 139811328716800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:52:51.983482 139811328716800 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:52:51.995803 139811328716800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:52:51.995858 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:51.995893 139811328716800 transformer_layer.py:389] tlayer: self-attention.
I0123 15:52:51.995925 139811328716800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:51.995986 139811328716800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:51.996538 139811328716800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:51.996613 139811328716800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:51.996971 139811328716800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:51.997654 139811328716800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.000077 139811328716800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.000690 139811328716800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.000767 139811328716800 transformer_layer.py:468] tlayer: End windows.
I0123 15:52:52.000802 139811328716800 transformer_layer.py:472] tlayer: final FFN.
I0123 15:52:52.000861 139811328716800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.000990 139811328716800 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:52:52.001097 139811328716800 nn_components.py:325] mlp: activation = None
I0123 15:52:52.001135 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:52.003057 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.003151 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:52.005558 139811328716800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.005636 139811328716800 transformer_base.py:443] tbase: final FFN
I0123 15:52:52.005752 139811328716800 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:52:52.007931 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:52.009774 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.009870 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:52.010161 139811328716800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.010240 139811328716800 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 15:52:52.010353 139811328716800 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:52:52.010391 139811328716800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:52:52.010422 139811328716800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:52:52.010485 139811328716800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.012697 139811328716800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:52:52.018156 139811328716800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.018416 139811328716800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:52:52.020995 139811328716800 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:52:52.033312 139811328716800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:52:52.033366 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:52.033401 139811328716800 transformer_layer.py:389] tlayer: self-attention.
I0123 15:52:52.033432 139811328716800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.033493 139811328716800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.034050 139811328716800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.034126 139811328716800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.034481 139811328716800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.035150 139811328716800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.037563 139811328716800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.038180 139811328716800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.038257 139811328716800 transformer_layer.py:468] tlayer: End windows.
I0123 15:52:52.038291 139811328716800 transformer_layer.py:472] tlayer: final FFN.
I0123 15:52:52.038350 139811328716800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.038477 139811328716800 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:52:52.038585 139811328716800 nn_components.py:325] mlp: activation = None
I0123 15:52:52.038623 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:52.040526 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.040618 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:52.043014 139811328716800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.043092 139811328716800 transformer_base.py:443] tbase: final FFN
I0123 15:52:52.043199 139811328716800 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:52:52.045379 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:52.047230 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.047324 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:52.047614 139811328716800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.047693 139811328716800 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 15:52:52.047799 139811328716800 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:52:52.047843 139811328716800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:52:52.047875 139811328716800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:52:52.047937 139811328716800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.050146 139811328716800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:52:52.056006 139811328716800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.056268 139811328716800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:52:52.058835 139811328716800 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:52:52.071429 139811328716800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:52:52.071483 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:52.071518 139811328716800 transformer_layer.py:389] tlayer: self-attention.
I0123 15:52:52.071549 139811328716800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.071611 139811328716800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.072163 139811328716800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.072237 139811328716800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.072591 139811328716800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.073269 139811328716800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.075722 139811328716800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.076329 139811328716800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.076406 139811328716800 transformer_layer.py:468] tlayer: End windows.
I0123 15:52:52.076441 139811328716800 transformer_layer.py:472] tlayer: final FFN.
I0123 15:52:52.076499 139811328716800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.076627 139811328716800 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:52:52.076734 139811328716800 nn_components.py:325] mlp: activation = None
I0123 15:52:52.076772 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:52.078698 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.078793 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:52.081183 139811328716800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.081261 139811328716800 transformer_base.py:443] tbase: final FFN
I0123 15:52:52.081368 139811328716800 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:52:52.083556 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:52.085389 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.085483 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:52.085795 139811328716800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.085875 139811328716800 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 15:52:52.085982 139811328716800 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:52:52.086023 139811328716800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:52:52.086061 139811328716800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:52:52.086128 139811328716800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.088417 139811328716800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:52:52.093902 139811328716800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.094162 139811328716800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:52:52.096715 139811328716800 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:52:52.109039 139811328716800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:52:52.109094 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:52.109129 139811328716800 transformer_layer.py:389] tlayer: self-attention.
I0123 15:52:52.109160 139811328716800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.109220 139811328716800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.109780 139811328716800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.109855 139811328716800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.110212 139811328716800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.110881 139811328716800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.113301 139811328716800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.113918 139811328716800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.113996 139811328716800 transformer_layer.py:468] tlayer: End windows.
I0123 15:52:52.114029 139811328716800 transformer_layer.py:472] tlayer: final FFN.
I0123 15:52:52.114086 139811328716800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.114212 139811328716800 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:52:52.114319 139811328716800 nn_components.py:325] mlp: activation = None
I0123 15:52:52.114356 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:52.116262 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.116355 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:52.118738 139811328716800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.118817 139811328716800 transformer_base.py:443] tbase: final FFN
I0123 15:52:52.118925 139811328716800 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:52:52.121085 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:52.122945 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.123039 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:52.123329 139811328716800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.123409 139811328716800 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 15:52:52.123516 139811328716800 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:52:52.123553 139811328716800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:52:52.123588 139811328716800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:52:52.123653 139811328716800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.125873 139811328716800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:52:52.131334 139811328716800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.131595 139811328716800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:52:52.134186 139811328716800 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:52:52.146582 139811328716800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:52:52.146636 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:52.146672 139811328716800 transformer_layer.py:389] tlayer: self-attention.
I0123 15:52:52.146703 139811328716800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.146765 139811328716800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.147319 139811328716800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.147393 139811328716800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.147749 139811328716800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.148427 139811328716800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.150903 139811328716800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.151514 139811328716800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.151591 139811328716800 transformer_layer.py:468] tlayer: End windows.
I0123 15:52:52.151625 139811328716800 transformer_layer.py:472] tlayer: final FFN.
I0123 15:52:52.151682 139811328716800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.151809 139811328716800 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:52:52.151916 139811328716800 nn_components.py:325] mlp: activation = None
I0123 15:52:52.151954 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:52.153875 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.153968 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:52.156373 139811328716800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.156452 139811328716800 transformer_base.py:443] tbase: final FFN
I0123 15:52:52.156560 139811328716800 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:52:52.158749 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:52.160590 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.160682 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:52.160971 139811328716800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.161051 139811328716800 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 15:52:52.161158 139811328716800 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:52:52.161195 139811328716800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:52:52.161226 139811328716800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:52:52.161295 139811328716800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.163535 139811328716800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:52:52.169399 139811328716800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.169668 139811328716800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:52:52.172554 139811328716800 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:52:52.184852 139811328716800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:52:52.184906 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:52.184940 139811328716800 transformer_layer.py:389] tlayer: self-attention.
I0123 15:52:52.184971 139811328716800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.185032 139811328716800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.185586 139811328716800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.185671 139811328716800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.186039 139811328716800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.186718 139811328716800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.189154 139811328716800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.189771 139811328716800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.189849 139811328716800 transformer_layer.py:468] tlayer: End windows.
I0123 15:52:52.189882 139811328716800 transformer_layer.py:472] tlayer: final FFN.
I0123 15:52:52.189941 139811328716800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.190067 139811328716800 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:52:52.190174 139811328716800 nn_components.py:325] mlp: activation = None
I0123 15:52:52.190211 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:52.192110 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.192203 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:52.194583 139811328716800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.194663 139811328716800 transformer_base.py:443] tbase: final FFN
I0123 15:52:52.194769 139811328716800 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:52:52.196930 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:52.198769 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.198863 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:52.199150 139811328716800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.199230 139811328716800 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 15:52:52.199337 139811328716800 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:52:52.199375 139811328716800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:52:52.199405 139811328716800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:52:52.199466 139811328716800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.201696 139811328716800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:52:52.207132 139811328716800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.207392 139811328716800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:52:52.209968 139811328716800 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:52:52.222247 139811328716800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:52:52.222301 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:52.222335 139811328716800 transformer_layer.py:389] tlayer: self-attention.
I0123 15:52:52.222366 139811328716800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.222427 139811328716800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.222981 139811328716800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.223057 139811328716800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.223415 139811328716800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.224089 139811328716800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.226537 139811328716800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.227146 139811328716800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.227221 139811328716800 transformer_layer.py:468] tlayer: End windows.
I0123 15:52:52.227255 139811328716800 transformer_layer.py:472] tlayer: final FFN.
I0123 15:52:52.227312 139811328716800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.227438 139811328716800 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:52:52.227546 139811328716800 nn_components.py:325] mlp: activation = None
I0123 15:52:52.227583 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:52.229498 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.229589 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:52.231989 139811328716800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.232067 139811328716800 transformer_base.py:443] tbase: final FFN
I0123 15:52:52.232173 139811328716800 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:52:52.234377 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:52.236231 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.236325 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:52.236617 139811328716800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.236695 139811328716800 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 15:52:52.236802 139811328716800 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:52:52.236840 139811328716800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:52:52.236871 139811328716800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:52:52.236934 139811328716800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.239166 139811328716800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:52:52.244635 139811328716800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.244897 139811328716800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:52:52.247485 139811328716800 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:52:52.259860 139811328716800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:52:52.259914 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:52.259949 139811328716800 transformer_layer.py:389] tlayer: self-attention.
I0123 15:52:52.259980 139811328716800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.260041 139811328716800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.260594 139811328716800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.260667 139811328716800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.261022 139811328716800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.261705 139811328716800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.264128 139811328716800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.264730 139811328716800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.264804 139811328716800 transformer_layer.py:468] tlayer: End windows.
I0123 15:52:52.264838 139811328716800 transformer_layer.py:472] tlayer: final FFN.
I0123 15:52:52.264894 139811328716800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.265019 139811328716800 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:52:52.265124 139811328716800 nn_components.py:325] mlp: activation = None
I0123 15:52:52.265162 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:52.267068 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.267160 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:52.269542 139811328716800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.269618 139811328716800 transformer_base.py:443] tbase: final FFN
I0123 15:52:52.269732 139811328716800 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:52:52.272002 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:52.274033 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.274128 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:52.274420 139811328716800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.274502 139811328716800 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 15:52:52.274607 139811328716800 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:52:52.274644 139811328716800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:52:52.274674 139811328716800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:52:52.274735 139811328716800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.276936 139811328716800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:52:52.282712 139811328716800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.282970 139811328716800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:52:52.285795 139811328716800 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:52:52.298066 139811328716800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:52:52.298120 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:52.298155 139811328716800 transformer_layer.py:389] tlayer: self-attention.
I0123 15:52:52.298186 139811328716800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.298247 139811328716800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.298794 139811328716800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.298868 139811328716800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.299223 139811328716800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.299903 139811328716800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.302341 139811328716800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.302946 139811328716800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.303020 139811328716800 transformer_layer.py:468] tlayer: End windows.
I0123 15:52:52.303055 139811328716800 transformer_layer.py:472] tlayer: final FFN.
I0123 15:52:52.303111 139811328716800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.303237 139811328716800 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:52:52.303344 139811328716800 nn_components.py:325] mlp: activation = None
I0123 15:52:52.303382 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:52.305284 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.305375 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:52.308039 139811328716800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.308116 139811328716800 transformer_base.py:443] tbase: final FFN
I0123 15:52:52.308222 139811328716800 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:52:52.310598 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:52.312612 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.312705 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:52.312997 139811328716800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.313076 139811328716800 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 15:52:52.313182 139811328716800 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:52:52.313219 139811328716800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:52:52.313250 139811328716800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:52:52.313313 139811328716800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.315535 139811328716800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:52:52.320977 139811328716800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.321235 139811328716800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:52:52.323826 139811328716800 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:52:52.336129 139811328716800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:52:52.336184 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:52.336219 139811328716800 transformer_layer.py:389] tlayer: self-attention.
I0123 15:52:52.336250 139811328716800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.336310 139811328716800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.336854 139811328716800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.336927 139811328716800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.337277 139811328716800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.337962 139811328716800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.340387 139811328716800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.340994 139811328716800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.341069 139811328716800 transformer_layer.py:468] tlayer: End windows.
I0123 15:52:52.341102 139811328716800 transformer_layer.py:472] tlayer: final FFN.
I0123 15:52:52.341159 139811328716800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.341285 139811328716800 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:52:52.341390 139811328716800 nn_components.py:325] mlp: activation = None
I0123 15:52:52.341428 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:52.343349 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.343441 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:52.345848 139811328716800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.345925 139811328716800 transformer_base.py:443] tbase: final FFN
I0123 15:52:52.346033 139811328716800 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:52:52.348209 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:52.350040 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.350133 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:52.350424 139811328716800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.350502 139811328716800 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 15:52:52.350606 139811328716800 transformer_layer.py:154] tlayer: recurrent = False
I0123 15:52:52.350644 139811328716800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 15:52:52.350673 139811328716800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 15:52:52.350735 139811328716800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.352946 139811328716800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 15:52:52.358405 139811328716800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.358671 139811328716800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 15:52:52.361242 139811328716800 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 15:52:52.373577 139811328716800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 15:52:52.373632 139811328716800 attention.py:418] Single window, no scan.
I0123 15:52:52.373675 139811328716800 transformer_layer.py:389] tlayer: self-attention.
I0123 15:52:52.373707 139811328716800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.373768 139811328716800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.374323 139811328716800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.374398 139811328716800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.374753 139811328716800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.375426 139811328716800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.377855 139811328716800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.378461 139811328716800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.378537 139811328716800 transformer_layer.py:468] tlayer: End windows.
I0123 15:52:52.378570 139811328716800 transformer_layer.py:472] tlayer: final FFN.
I0123 15:52:52.378626 139811328716800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.378749 139811328716800 transformer_base.py:410] tbase: post-attention MLP.
I0123 15:52:52.378855 139811328716800 nn_components.py:325] mlp: activation = None
I0123 15:52:52.378892 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:52.380798 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.380888 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:52.383295 139811328716800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.383373 139811328716800 transformer_base.py:443] tbase: final FFN
I0123 15:52:52.383480 139811328716800 nn_components.py:320] mlp: hidden 4096, relu
I0123 15:52:52.385664 139811328716800 nn_components.py:329] mlp: final activation = None
I0123 15:52:52.387510 139811328716800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.387603 139811328716800 nn_components.py:261] mlp: residual
I0123 15:52:52.387894 139811328716800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:52:52.387978 139811328716800 decoder_stack.py:344] dstack: Final layernorm.
I0123 15:52:52.391144 139811328716800 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 15:53:03.691280 139811328716800 alphageometry.py:566] LM output (score=-1.113810): "k : C a b k 14 D a k b k 15 ;"
I0123 15:53:03.691454 139811328716800 alphageometry.py:567] Translation: "k = on_line k a b, on_bline k b a"

I0123 15:53:03.691501 139811328716800 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d b a c; e = on_circle e a c, on_line e b a; f = on_circle f a c, on_line f b a; g = on_circle g c a, on_line g b c; h = on_circle h c a, on_line h b c; i = circle i b a c; j = on_line j e g, on_line j i d; k = on_line k a b, on_bline k b a ? perp g j j i"
I0123 15:53:03.691653 139811328716800 graph.py:498] 
I0123 15:53:03.691707 139811328716800 graph.py:499] a b c = triangle a b c; d = incenter d b a c; e = on_circle e a c, on_line e b a; f = on_circle f a c, on_line f b a; g = on_circle g c a, on_line g b c; h = on_circle h c a, on_line h b c; i = circle i b a c; j = on_line j e g, on_line j i d; k = on_line k a b, on_bline k b a ? perp g j j i
I0123 15:53:05.114420 139811328716800 ddar.py:60] Depth 1/1000 time = 1.3546414375305176
I0123 15:53:07.047298 139811328716800 ddar.py:60] Depth 2/1000 time = 1.9326982498168945
I0123 15:53:09.121955 139811328716800 ddar.py:60] Depth 3/1000 time = 2.0744709968566895
I0123 15:53:11.000727 139811328716800 ddar.py:60] Depth 4/1000 time = 1.8783891201019287
I0123 15:53:13.126032 139811328716800 ddar.py:60] Depth 5/1000 time = 2.1157777309417725
I0123 15:53:15.071444 139811328716800 ddar.py:60] Depth 6/1000 time = 1.9452431201934814
I0123 15:53:17.440017 139811328716800 ddar.py:60] Depth 7/1000 time = 2.3684067726135254
I0123 15:53:20.091225 139811328716800 ddar.py:60] Depth 8/1000 time = 2.65103816986084
I0123 15:53:22.738654 139811328716800 ddar.py:60] Depth 9/1000 time = 2.647254467010498
I0123 15:53:22.746595 139811328716800 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 15:53:22.746699 139811328716800 alphageometry.py:566] LM output (score=-1.769187): "k : C a c k 14 D a k c k 15 ;"
I0123 15:53:22.746740 139811328716800 alphageometry.py:567] Translation: "k = on_line k a c, on_bline k c a"

I0123 15:53:22.746779 139811328716800 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d b a c; e = on_circle e a c, on_line e b a; f = on_circle f a c, on_line f b a; g = on_circle g c a, on_line g b c; h = on_circle h c a, on_line h b c; i = circle i b a c; j = on_line j e g, on_line j i d; k = on_line k a c, on_bline k c a ? perp g j j i"
I0123 15:53:22.746932 139811328716800 graph.py:498] 
I0123 15:53:22.746989 139811328716800 graph.py:499] a b c = triangle a b c; d = incenter d b a c; e = on_circle e a c, on_line e b a; f = on_circle f a c, on_line f b a; g = on_circle g c a, on_line g b c; h = on_circle h c a, on_line h b c; i = circle i b a c; j = on_line j e g, on_line j i d; k = on_line k a c, on_bline k c a ? perp g j j i
I0123 15:53:23.581220 139811328716800 ddar.py:60] Depth 1/1000 time = 0.7947003841400146
I0123 15:53:24.959258 139811328716800 ddar.py:60] Depth 2/1000 time = 1.3778724670410156
I0123 15:53:26.405354 139811328716800 ddar.py:60] Depth 3/1000 time = 1.4459137916564941
I0123 15:53:27.857836 139811328716800 ddar.py:60] Depth 4/1000 time = 1.452023983001709
I0123 15:53:29.430936 139811328716800 ddar.py:60] Depth 5/1000 time = 1.5539023876190186
I0123 15:53:31.007647 139811328716800 ddar.py:60] Depth 6/1000 time = 1.5765252113342285
I0123 15:53:32.838971 139811328716800 ddar.py:60] Depth 7/1000 time = 1.8311467170715332
I0123 15:53:34.932575 139811328716800 ddar.py:60] Depth 8/1000 time = 2.0934042930603027
I0123 15:53:37.161553 139811328716800 ddar.py:60] Depth 9/1000 time = 2.228731155395508
I0123 15:53:37.168768 139811328716800 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 15:53:37.168853 139811328716800 alphageometry.py:566] LM output (score=-1.785052): "k : C a c k 14 T a c k d 15 ;"
I0123 15:53:37.168889 139811328716800 alphageometry.py:567] Translation: "k = on_line k a c, on_tline k d a c"

I0123 15:53:37.168927 139811328716800 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d b a c; e = on_circle e a c, on_line e b a; f = on_circle f a c, on_line f b a; g = on_circle g c a, on_line g b c; h = on_circle h c a, on_line h b c; i = circle i b a c; j = on_line j e g, on_line j i d; k = on_line k a c, on_tline k d a c ? perp g j j i"
I0123 15:53:37.169076 139811328716800 graph.py:498] 
I0123 15:53:37.169129 139811328716800 graph.py:499] a b c = triangle a b c; d = incenter d b a c; e = on_circle e a c, on_line e b a; f = on_circle f a c, on_line f b a; g = on_circle g c a, on_line g b c; h = on_circle h c a, on_line h b c; i = circle i b a c; j = on_line j e g, on_line j i d; k = on_line k a c, on_tline k d a c ? perp g j j i
I0123 15:53:37.956918 139811328716800 ddar.py:60] Depth 1/1000 time = 0.7521495819091797
I0123 15:53:39.200298 139811328716800 ddar.py:60] Depth 2/1000 time = 1.2432196140289307
I0123 15:53:40.740898 139811328716800 ddar.py:60] Depth 3/1000 time = 1.5404248237609863
I0123 15:53:41.905845 139811328716800 ddar.py:60] Depth 4/1000 time = 1.1647703647613525
I0123 15:53:43.246106 139811328716800 ddar.py:60] Depth 5/1000 time = 1.3400866985321045
I0123 15:53:44.575132 139811328716800 ddar.py:60] Depth 6/1000 time = 1.3286912441253662
I0123 15:53:46.038924 139811328716800 ddar.py:60] Depth 7/1000 time = 1.4556167125701904
I0123 15:53:47.744707 139811328716800 ddar.py:60] Depth 8/1000 time = 1.7055985927581787
I0123 15:53:49.563682 139811328716800 ddar.py:60] Depth 9/1000 time = 1.8187916278839111
I0123 15:53:51.675544 139811328716800 ddar.py:60] Depth 10/1000 time = 2.111670732498169
I0123 15:53:54.263491 139811328716800 ddar.py:60] Depth 11/1000 time = 2.587374687194824
I0123 15:53:54.275377 139811328716800 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 15:53:54.275716 139811328716800 alphageometry.py:566] LM output (score=-1.860945): "k : D b i i k 14 ;"
I0123 15:53:54.275757 139811328716800 alphageometry.py:567] Translation: "k = on_circle k i b"

I0123 15:53:54.275848 139811328716800 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d b a c; e = on_circle e a c, on_line e b a; f = on_circle f a c, on_line f b a; g = on_circle g c a, on_line g b c; h = on_circle h c a, on_line h b c; i = circle i b a c; j = on_line j e g, on_line j i d; k = on_circle k i b ? perp g j j i"
I0123 15:53:54.276110 139811328716800 graph.py:498] 
I0123 15:53:54.276194 139811328716800 graph.py:499] a b c = triangle a b c; d = incenter d b a c; e = on_circle e a c, on_line e b a; f = on_circle f a c, on_line f b a; g = on_circle g c a, on_line g b c; h = on_circle h c a, on_line h b c; i = circle i b a c; j = on_line j e g, on_line j i d; k = on_circle k i b ? perp g j j i
I0123 15:53:55.391815 139811328716800 ddar.py:60] Depth 1/1000 time = 1.076507329940796
I0123 15:53:56.924555 139811328716800 ddar.py:60] Depth 2/1000 time = 1.53226900100708
I0123 15:53:58.501226 139811328716800 ddar.py:60] Depth 3/1000 time = 1.5764107704162598
I0123 15:54:00.046086 139811328716800 ddar.py:60] Depth 4/1000 time = 1.544405460357666
I0123 15:54:01.497147 139811328716800 ddar.py:60] Depth 5/1000 time = 1.4440937042236328
I0123 15:54:03.132156 139811328716800 ddar.py:60] Depth 6/1000 time = 1.6346912384033203
I0123 15:54:04.981704 139811328716800 ddar.py:60] Depth 7/1000 time = 1.8491880893707275
I0123 15:54:07.238029 139811328716800 ddar.py:60] Depth 8/1000 time = 2.256053924560547
I0123 15:54:09.348025 139811328716800 ddar.py:60] Depth 9/1000 time = 2.1096608638763428
I0123 15:54:11.209669 139811328716800 ddar.py:60] Depth 10/1000 time = 1.8613338470458984
I0123 15:54:11.217681 139811328716800 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 15:54:11.217837 139811328716800 alphageometry.py:566] LM output (score=-1.959365): "k : T b d d k 14 ;"
I0123 15:54:11.217875 139811328716800 alphageometry.py:567] Translation: "k = on_tline k d b d"

I0123 15:54:11.217930 139811328716800 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d b a c; e = on_circle e a c, on_line e b a; f = on_circle f a c, on_line f b a; g = on_circle g c a, on_line g b c; h = on_circle h c a, on_line h b c; i = circle i b a c; j = on_line j e g, on_line j i d; k = on_tline k d b d ? perp g j j i"
I0123 15:54:11.218112 139811328716800 graph.py:498] 
I0123 15:54:11.218171 139811328716800 graph.py:499] a b c = triangle a b c; d = incenter d b a c; e = on_circle e a c, on_line e b a; f = on_circle f a c, on_line f b a; g = on_circle g c a, on_line g b c; h = on_circle h c a, on_line h b c; i = circle i b a c; j = on_line j e g, on_line j i d; k = on_tline k d b d ? perp g j j i
I0123 15:54:12.214028 139811328716800 ddar.py:60] Depth 1/1000 time = 0.946300745010376
I0123 15:54:13.217273 139811328716800 ddar.py:60] Depth 2/1000 time = 1.003005027770996
I0123 15:54:14.453966 139811328716800 ddar.py:60] Depth 3/1000 time = 1.2364380359649658
I0123 15:54:15.704357 139811328716800 ddar.py:60] Depth 4/1000 time = 1.249985694885254
I0123 15:54:16.784030 139811328716800 ddar.py:60] Depth 5/1000 time = 1.0785579681396484
I0123 15:54:18.149686 139811328716800 ddar.py:60] Depth 6/1000 time = 1.3588635921478271
I0123 15:54:19.530645 139811328716800 ddar.py:60] Depth 7/1000 time = 1.3807127475738525
I0123 15:54:21.110983 139811328716800 ddar.py:60] Depth 8/1000 time = 1.5801265239715576
I0123 15:54:23.005403 139811328716800 ddar.py:60] Depth 9/1000 time = 1.8941760063171387
I0123 15:54:24.628075 139811328716800 ddar.py:60] Depth 10/1000 time = 1.622344970703125
I0123 15:54:26.422800 139811328716800 ddar.py:60] Depth 11/1000 time = 1.794440507888794
I0123 15:54:26.430125 139811328716800 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 15:54:26.430452 139811328716800 alphageometry.py:566] LM output (score=-1.961267): "k : C a c k 14 T a c d k 15 ;"
I0123 15:54:26.430491 139811328716800 alphageometry.py:567] Translation: "k = on_line k a c, on_tline k d a c"

I0123 15:54:26.430535 139811328716800 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d b a c; e = on_circle e a c, on_line e b a; f = on_circle f a c, on_line f b a; g = on_circle g c a, on_line g b c; h = on_circle h c a, on_line h b c; i = circle i b a c; j = on_line j e g, on_line j i d; k = on_line k a c, on_tline k d a c ? perp g j j i"
I0123 15:54:26.430705 139811328716800 graph.py:498] 
I0123 15:54:26.430762 139811328716800 graph.py:499] a b c = triangle a b c; d = incenter d b a c; e = on_circle e a c, on_line e b a; f = on_circle f a c, on_line f b a; g = on_circle g c a, on_line g b c; h = on_circle h c a, on_line h b c; i = circle i b a c; j = on_line j e g, on_line j i d; k = on_line k a c, on_tline k d a c ? perp g j j i
I0123 15:54:27.453610 139811328716800 ddar.py:60] Depth 1/1000 time = 0.9717581272125244
I0123 15:54:28.778077 139811328716800 ddar.py:60] Depth 2/1000 time = 1.3242380619049072
I0123 15:54:30.212351 139811328716800 ddar.py:60] Depth 3/1000 time = 1.434021234512329
I0123 15:54:31.402683 139811328716800 ddar.py:60] Depth 4/1000 time = 1.190101146697998
I0123 15:54:32.792819 139811328716800 ddar.py:60] Depth 5/1000 time = 1.3897032737731934
I0123 15:54:34.297230 139811328716800 ddar.py:60] Depth 6/1000 time = 1.4959356784820557
I0123 15:54:35.827720 139811328716800 ddar.py:60] Depth 7/1000 time = 1.5302412509918213
I0123 15:54:37.582406 139811328716800 ddar.py:60] Depth 8/1000 time = 1.7544808387756348
I0123 15:54:39.819891 139811328716800 ddar.py:60] Depth 9/1000 time = 2.2372820377349854
I0123 15:54:41.879040 139811328716800 ddar.py:60] Depth 10/1000 time = 2.058910608291626
I0123 15:54:41.888217 139811328716800 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 15:54:41.888345 139811328716800 alphageometry.py:566] LM output (score=-1.965212): "k : C c f k 14 D c k f k 15 ;"
I0123 15:54:41.888383 139811328716800 alphageometry.py:567] Translation: "k = on_line k c f, on_bline k f c"

I0123 15:54:41.888423 139811328716800 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d b a c; e = on_circle e a c, on_line e b a; f = on_circle f a c, on_line f b a; g = on_circle g c a, on_line g b c; h = on_circle h c a, on_line h b c; i = circle i b a c; j = on_line j e g, on_line j i d; k = on_line k c f, on_bline k f c ? perp g j j i"
I0123 15:54:41.888584 139811328716800 graph.py:498] 
I0123 15:54:41.888641 139811328716800 graph.py:499] a b c = triangle a b c; d = incenter d b a c; e = on_circle e a c, on_line e b a; f = on_circle f a c, on_line f b a; g = on_circle g c a, on_line g b c; h = on_circle h c a, on_line h b c; i = circle i b a c; j = on_line j e g, on_line j i d; k = on_line k c f, on_bline k f c ? perp g j j i
I0123 15:54:42.761968 139811328716800 ddar.py:60] Depth 1/1000 time = 0.8411769866943359
I0123 15:54:44.172370 139811328716800 ddar.py:60] Depth 2/1000 time = 1.4100594520568848
I0123 15:54:45.609039 139811328716800 ddar.py:60] Depth 3/1000 time = 1.436408519744873
I0123 15:54:47.023051 139811328716800 ddar.py:60] Depth 4/1000 time = 1.4136006832122803
I0123 15:54:48.545192 139811328716800 ddar.py:60] Depth 5/1000 time = 1.5158390998840332
I0123 15:54:50.343787 139811328716800 ddar.py:60] Depth 6/1000 time = 1.798346757888794
I0123 15:54:51.896709 139811328716800 ddar.py:60] Depth 7/1000 time = 1.5527143478393555
I0123 15:54:54.163424 139811328716800 ddar.py:60] Depth 8/1000 time = 2.266491413116455
I0123 15:54:56.217798 139811328716800 ddar.py:60] Depth 9/1000 time = 2.054082155227661
I0123 15:54:58.525535 139811328716800 ddar.py:60] Depth 10/1000 time = 2.307353973388672
I0123 15:54:58.531991 139811328716800 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 15:54:58.532140 139811328716800 alphageometry.py:566] LM output (score=-2.089019): "k : D b c c k 14 D b d d k 15 ;"
I0123 15:54:58.532180 139811328716800 alphageometry.py:567] Translation: "k = on_circle k c b, on_circle k d b"

I0123 15:54:58.532219 139811328716800 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d b a c; e = on_circle e a c, on_line e b a; f = on_circle f a c, on_line f b a; g = on_circle g c a, on_line g b c; h = on_circle h c a, on_line h b c; i = circle i b a c; j = on_line j e g, on_line j i d; k = on_circle k c b, on_circle k d b ? perp g j j i"
I0123 15:54:58.532385 139811328716800 graph.py:498] 
I0123 15:54:58.532442 139811328716800 graph.py:499] a b c = triangle a b c; d = incenter d b a c; e = on_circle e a c, on_line e b a; f = on_circle f a c, on_line f b a; g = on_circle g c a, on_line g b c; h = on_circle h c a, on_line h b c; i = circle i b a c; j = on_line j e g, on_line j i d; k = on_circle k c b, on_circle k d b ? perp g j j i
I0123 15:54:59.430026 139811328716800 ddar.py:60] Depth 1/1000 time = 0.8549404144287109
I0123 15:55:00.623580 139811328716800 ddar.py:60] Depth 2/1000 time = 1.193333625793457
I0123 15:55:02.211836 139811328716800 ddar.py:60] Depth 3/1000 time = 1.5880088806152344
I0123 15:55:04.106693 139811328716800 ddar.py:60] Depth 4/1000 time = 1.8946483135223389
I0123 15:55:06.635438 139811328716800 ddar.py:60] Depth 5/1000 time = 2.5285298824310303
I0123 15:55:09.651259 139811328716800 ddar.py:60] Depth 6/1000 time = 3.0155892372131348
I0123 15:55:12.713189 139811328716800 ddar.py:60] Depth 7/1000 time = 3.0616707801818848
I0123 15:55:15.526540 139811328716800 ddar.py:60] Depth 8/1000 time = 2.812897205352783
I0123 15:55:18.348237 139811328716800 ddar.py:60] Depth 9/1000 time = 2.8201282024383545
I0123 15:55:21.481750 139811328716800 ddar.py:60] Depth 10/1000 time = 3.133270502090454
I0123 15:55:24.655145 139811328716800 ddar.py:60] Depth 11/1000 time = 3.1731948852539062
I0123 15:55:27.634266 139811328716800 ddar.py:60] Depth 12/1000 time = 2.9591634273529053
I0123 15:55:27.645121 139811328716800 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 15:55:27.645268 139811328716800 alphageometry.py:566] LM output (score=-2.207513): "k : C b c k 14 D b k c k 15 ;"
I0123 15:55:27.645307 139811328716800 alphageometry.py:567] Translation: "k = on_line k b c, on_bline k c b"

I0123 15:55:27.645347 139811328716800 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d b a c; e = on_circle e a c, on_line e b a; f = on_circle f a c, on_line f b a; g = on_circle g c a, on_line g b c; h = on_circle h c a, on_line h b c; i = circle i b a c; j = on_line j e g, on_line j i d; k = on_line k b c, on_bline k c b ? perp g j j i"
I0123 15:55:27.645514 139811328716800 graph.py:498] 
I0123 15:55:27.645570 139811328716800 graph.py:499] a b c = triangle a b c; d = incenter d b a c; e = on_circle e a c, on_line e b a; f = on_circle f a c, on_line f b a; g = on_circle g c a, on_line g b c; h = on_circle h c a, on_line h b c; i = circle i b a c; j = on_line j e g, on_line j i d; k = on_line k b c, on_bline k c b ? perp g j j i
I0123 15:55:29.178759 139811328716800 ddar.py:60] Depth 1/1000 time = 1.494584083557129
I0123 15:55:31.125254 139811328716800 ddar.py:60] Depth 2/1000 time = 1.9461350440979004
I0123 15:55:33.367144 139811328716800 ddar.py:60] Depth 3/1000 time = 2.2415995597839355
I0123 15:55:35.389151 139811328716800 ddar.py:60] Depth 4/1000 time = 2.021547794342041
I0123 15:55:37.488950 139811328716800 ddar.py:60] Depth 5/1000 time = 2.09187388420105
I0123 15:55:39.649040 139811328716800 ddar.py:60] Depth 6/1000 time = 2.1598329544067383
I0123 15:55:42.242301 139811328716800 ddar.py:60] Depth 7/1000 time = 2.5930466651916504
I0123 15:55:44.856339 139811328716800 ddar.py:60] Depth 8/1000 time = 2.613816738128662
I0123 15:55:47.691089 139811328716800 ddar.py:60] Depth 9/1000 time = 2.834510326385498
I0123 15:55:50.568162 139811328716800 ddar.py:60] Depth 10/1000 time = 2.876769781112671
I0123 15:55:50.576272 139811328716800 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 15:55:50.576436 139811328716800 alphageometry.py:566] LM output (score=-2.278089): "k : C h j k 14 D h k j k 15 ;"
I0123 15:55:50.576477 139811328716800 alphageometry.py:567] Translation: "k = on_line k h j, on_bline k j h"

I0123 15:55:50.576528 139811328716800 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d b a c; e = on_circle e a c, on_line e b a; f = on_circle f a c, on_line f b a; g = on_circle g c a, on_line g b c; h = on_circle h c a, on_line h b c; i = circle i b a c; j = on_line j e g, on_line j i d; k = on_line k h j, on_bline k j h ? perp g j j i"
I0123 15:55:50.576714 139811328716800 graph.py:498] 
I0123 15:55:50.576773 139811328716800 graph.py:499] a b c = triangle a b c; d = incenter d b a c; e = on_circle e a c, on_line e b a; f = on_circle f a c, on_line f b a; g = on_circle g c a, on_line g b c; h = on_circle h c a, on_line h b c; i = circle i b a c; j = on_line j e g, on_line j i d; k = on_line k h j, on_bline k j h ? perp g j j i
I0123 15:55:51.487138 139811328716800 ddar.py:60] Depth 1/1000 time = 0.8702421188354492
I0123 15:55:52.942230 139811328716800 ddar.py:60] Depth 2/1000 time = 1.4548664093017578
I0123 15:55:54.446026 139811328716800 ddar.py:60] Depth 3/1000 time = 1.503556489944458
I0123 15:55:55.964570 139811328716800 ddar.py:60] Depth 4/1000 time = 1.5181305408477783
I0123 15:55:57.548634 139811328716800 ddar.py:60] Depth 5/1000 time = 1.5773346424102783
I0123 15:55:59.175758 139811328716800 ddar.py:60] Depth 6/1000 time = 1.6268830299377441
I0123 15:56:00.729109 139811328716800 ddar.py:60] Depth 7/1000 time = 1.5531444549560547
I0123 15:56:02.947489 139811328716800 ddar.py:60] Depth 8/1000 time = 2.218174934387207
I0123 15:56:04.961416 139811328716800 ddar.py:60] Depth 9/1000 time = 2.0137031078338623
I0123 15:56:06.975357 139811328716800 ddar.py:60] Depth 10/1000 time = 2.013705015182495
I0123 15:56:06.981731 139811328716800 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 15:56:06.981853 139811328716800 alphageometry.py:566] LM output (score=-2.301833): "k : C h i k 14 D h i i k 15 ;"
I0123 15:56:06.981892 139811328716800 alphageometry.py:567] Translation: "k = on_line k h i, on_circle k i h"

I0123 15:56:06.981931 139811328716800 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d b a c; e = on_circle e a c, on_line e b a; f = on_circle f a c, on_line f b a; g = on_circle g c a, on_line g b c; h = on_circle h c a, on_line h b c; i = circle i b a c; j = on_line j e g, on_line j i d; k = on_line k h i, on_circle k i h ? perp g j j i"
I0123 15:56:06.982087 139811328716800 graph.py:498] 
I0123 15:56:06.982141 139811328716800 graph.py:499] a b c = triangle a b c; d = incenter d b a c; e = on_circle e a c, on_line e b a; f = on_circle f a c, on_line f b a; g = on_circle g c a, on_line g b c; h = on_circle h c a, on_line h b c; i = circle i b a c; j = on_line j e g, on_line j i d; k = on_line k h i, on_circle k i h ? perp g j j i
I0123 15:56:07.869429 139811328716800 ddar.py:60] Depth 1/1000 time = 0.8555431365966797
I0123 15:56:09.300195 139811328716800 ddar.py:60] Depth 2/1000 time = 1.4305570125579834
I0123 15:56:10.565686 139811328716800 ddar.py:60] Depth 3/1000 time = 1.265225887298584
I0123 15:56:12.060940 139811328716800 ddar.py:60] Depth 4/1000 time = 1.4947736263275146
I0123 15:56:13.660672 139811328716800 ddar.py:60] Depth 5/1000 time = 1.5925610065460205
I0123 15:56:15.301308 139811328716800 ddar.py:60] Depth 6/1000 time = 1.6403827667236328
I0123 15:56:16.834081 139811328716800 ddar.py:60] Depth 7/1000 time = 1.532562255859375
I0123 15:56:19.027463 139811328716800 ddar.py:60] Depth 8/1000 time = 2.1931748390197754
I0123 15:56:20.751166 139811328716800 ddar.py:60] Depth 9/1000 time = 1.7234737873077393
I0123 15:56:22.964131 139811328716800 ddar.py:60] Depth 10/1000 time = 2.212714433670044
I0123 15:56:22.970409 139811328716800 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 15:56:22.970543 139811328716800 alphageometry.py:566] LM output (score=-2.310989): "k : D b d d k 14 T b d d k 15 ;"
I0123 15:56:22.970584 139811328716800 alphageometry.py:567] Translation: "k = on_circle k d b, on_tline k d b d"

I0123 15:56:22.970622 139811328716800 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d b a c; e = on_circle e a c, on_line e b a; f = on_circle f a c, on_line f b a; g = on_circle g c a, on_line g b c; h = on_circle h c a, on_line h b c; i = circle i b a c; j = on_line j e g, on_line j i d; k = on_circle k d b, on_tline k d b d ? perp g j j i"
I0123 15:56:22.970786 139811328716800 graph.py:498] 
I0123 15:56:22.970850 139811328716800 graph.py:499] a b c = triangle a b c; d = incenter d b a c; e = on_circle e a c, on_line e b a; f = on_circle f a c, on_line f b a; g = on_circle g c a, on_line g b c; h = on_circle h c a, on_line h b c; i = circle i b a c; j = on_line j e g, on_line j i d; k = on_circle k d b, on_tline k d b d ? perp g j j i
I0123 15:56:23.801904 139811328716800 ddar.py:60] Depth 1/1000 time = 0.7975306510925293
I0123 15:56:24.923390 139811328716800 ddar.py:60] Depth 2/1000 time = 1.1212844848632812
I0123 15:56:26.346563 139811328716800 ddar.py:60] Depth 3/1000 time = 1.4229295253753662
I0123 15:56:27.532227 139811328716800 ddar.py:60] Depth 4/1000 time = 1.1852707862854004
I0123 15:56:28.965333 139811328716800 ddar.py:60] Depth 5/1000 time = 1.4325635433197021
I0123 15:56:30.164042 139811328716800 ddar.py:60] Depth 6/1000 time = 1.1973421573638916
I0123 15:56:31.454996 139811328716800 ddar.py:60] Depth 7/1000 time = 1.2834053039550781
I0123 15:56:32.997108 139811328716800 ddar.py:60] Depth 8/1000 time = 1.5418641567230225
I0123 15:56:34.793823 139811328716800 ddar.py:60] Depth 9/1000 time = 1.796497106552124
I0123 15:56:36.833189 139811328716800 ddar.py:60] Depth 10/1000 time = 2.0391228199005127
I0123 15:56:38.597444 139811328716800 ddar.py:60] Depth 11/1000 time = 1.7640323638916016
I0123 15:56:40.579304 139811328716800 ddar.py:60] Depth 12/1000 time = 1.9816126823425293
I0123 15:56:40.587160 139811328716800 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 15:56:40.587255 139811328716800 alphageometry.py:566] LM output (score=-2.316975): "k : D b c b k 14 D c d d k 15 ;"
I0123 15:56:40.587294 139811328716800 alphageometry.py:567] Translation: "k = on_circle k b c, on_circle k d c"

I0123 15:56:40.587331 139811328716800 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d b a c; e = on_circle e a c, on_line e b a; f = on_circle f a c, on_line f b a; g = on_circle g c a, on_line g b c; h = on_circle h c a, on_line h b c; i = circle i b a c; j = on_line j e g, on_line j i d; k = on_circle k b c, on_circle k d c ? perp g j j i"
I0123 15:56:40.587486 139811328716800 graph.py:498] 
I0123 15:56:40.587543 139811328716800 graph.py:499] a b c = triangle a b c; d = incenter d b a c; e = on_circle e a c, on_line e b a; f = on_circle f a c, on_line f b a; g = on_circle g c a, on_line g b c; h = on_circle h c a, on_line h b c; i = circle i b a c; j = on_line j e g, on_line j i d; k = on_circle k b c, on_circle k d c ? perp g j j i
I0123 15:56:41.723031 139811328716800 ddar.py:60] Depth 1/1000 time = 1.088235855102539
I0123 15:56:42.941480 139811328716800 ddar.py:60] Depth 2/1000 time = 1.2182281017303467
I0123 15:56:44.550035 139811328716800 ddar.py:60] Depth 3/1000 time = 1.6082723140716553
I0123 15:56:46.834841 139811328716800 ddar.py:60] Depth 4/1000 time = 2.2846007347106934
I0123 15:56:49.902465 139811328716800 ddar.py:60] Depth 5/1000 time = 3.0674171447753906
I0123 15:56:53.375489 139811328716800 ddar.py:60] Depth 6/1000 time = 3.4728152751922607
I0123 15:56:56.643573 139811328716800 ddar.py:60] Depth 7/1000 time = 3.267838954925537
I0123 15:57:00.111677 139811328716800 ddar.py:60] Depth 8/1000 time = 3.467815399169922
I0123 15:57:03.299613 139811328716800 ddar.py:60] Depth 9/1000 time = 3.1873905658721924
I0123 15:57:06.854516 139811328716800 ddar.py:60] Depth 10/1000 time = 3.539930582046509
I0123 15:57:10.398592 139811328716800 ddar.py:60] Depth 11/1000 time = 3.543821334838867
I0123 15:57:10.411129 139811328716800 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 15:57:10.411288 139811328716800 alphageometry.py:566] LM output (score=-2.430957): "k : D b c d k 14 D b k c d 15 ;"
I0123 15:57:10.411329 139811328716800 alphageometry.py:567] Translation: "k = eqdistance k d b c, eqdistance k b c d"

I0123 15:57:10.411367 139811328716800 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d b a c; e = on_circle e a c, on_line e b a; f = on_circle f a c, on_line f b a; g = on_circle g c a, on_line g b c; h = on_circle h c a, on_line h b c; i = circle i b a c; j = on_line j e g, on_line j i d; k = eqdistance k d b c, eqdistance k b c d ? perp g j j i"
I0123 15:57:10.411535 139811328716800 graph.py:498] 
I0123 15:57:10.411593 139811328716800 graph.py:499] a b c = triangle a b c; d = incenter d b a c; e = on_circle e a c, on_line e b a; f = on_circle f a c, on_line f b a; g = on_circle g c a, on_line g b c; h = on_circle h c a, on_line h b c; i = circle i b a c; j = on_line j e g, on_line j i d; k = eqdistance k d b c, eqdistance k b c d ? perp g j j i
I0123 15:57:11.332106 139811328716800 ddar.py:60] Depth 1/1000 time = 0.864098310470581
I0123 15:57:12.582975 139811328716800 ddar.py:60] Depth 2/1000 time = 1.250657081604004
I0123 15:57:14.162920 139811328716800 ddar.py:60] Depth 3/1000 time = 1.5796830654144287
I0123 15:57:15.504266 139811328716800 ddar.py:60] Depth 4/1000 time = 1.3409373760223389
I0123 15:57:17.197449 139811328716800 ddar.py:60] Depth 5/1000 time = 1.6840803623199463
I0123 15:57:18.927856 139811328716800 ddar.py:60] Depth 6/1000 time = 1.7300875186920166
I0123 15:57:20.973406 139811328716800 ddar.py:60] Depth 7/1000 time = 2.0453004837036133
I0123 15:57:23.202486 139811328716800 ddar.py:60] Depth 8/1000 time = 2.2288591861724854
I0123 15:57:25.399567 139811328716800 ddar.py:60] Depth 9/1000 time = 2.1968564987182617
I0123 15:57:27.600703 139811328716800 ddar.py:60] Depth 10/1000 time = 2.2008697986602783
I0123 15:57:27.606625 139811328716800 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 15:57:27.606743 139811328716800 alphageometry.py:566] LM output (score=-2.440475): "k : D b c d k 14 D b d c k 15 ;"
I0123 15:57:27.606779 139811328716800 alphageometry.py:567] Translation: "k = eqdistance k d b c, eqdistance k c b d"

I0123 15:57:27.606817 139811328716800 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d b a c; e = on_circle e a c, on_line e b a; f = on_circle f a c, on_line f b a; g = on_circle g c a, on_line g b c; h = on_circle h c a, on_line h b c; i = circle i b a c; j = on_line j e g, on_line j i d; k = eqdistance k d b c, eqdistance k c b d ? perp g j j i"
I0123 15:57:27.606986 139811328716800 graph.py:498] 
I0123 15:57:27.607046 139811328716800 graph.py:499] a b c = triangle a b c; d = incenter d b a c; e = on_circle e a c, on_line e b a; f = on_circle f a c, on_line f b a; g = on_circle g c a, on_line g b c; h = on_circle h c a, on_line h b c; i = circle i b a c; j = on_line j e g, on_line j i d; k = eqdistance k d b c, eqdistance k c b d ? perp g j j i
I0123 15:57:28.497983 139811328716800 ddar.py:60] Depth 1/1000 time = 0.8457627296447754
I0123 15:57:30.009220 139811328716800 ddar.py:60] Depth 2/1000 time = 1.511000633239746
I0123 15:57:31.583191 139811328716800 ddar.py:60] Depth 3/1000 time = 1.5737085342407227
I0123 15:57:32.901298 139811328716800 ddar.py:60] Depth 4/1000 time = 1.3177032470703125
I0123 15:57:34.574086 139811328716800 ddar.py:60] Depth 5/1000 time = 1.6625759601593018
I0123 15:57:36.271752 139811328716800 ddar.py:60] Depth 6/1000 time = 1.6974186897277832
I0123 15:57:38.204037 139811328716800 ddar.py:60] Depth 7/1000 time = 1.932018518447876
I0123 15:57:40.366383 139811328716800 ddar.py:60] Depth 8/1000 time = 2.1620068550109863
I0123 15:57:42.517525 139811328716800 ddar.py:60] Depth 9/1000 time = 2.150890827178955
I0123 15:57:44.654742 139811328716800 ddar.py:60] Depth 10/1000 time = 2.136948585510254
I0123 15:57:44.661622 139811328716800 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 15:57:44.661757 139811328716800 alphageometry.py:566] LM output (score=-2.449766): "k : D b k d k 14 D c k d k 15 ;"
I0123 15:57:44.661796 139811328716800 alphageometry.py:567] Translation: "k = on_bline k d b, on_bline k d c"

I0123 15:57:44.661834 139811328716800 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d b a c; e = on_circle e a c, on_line e b a; f = on_circle f a c, on_line f b a; g = on_circle g c a, on_line g b c; h = on_circle h c a, on_line h b c; i = circle i b a c; j = on_line j e g, on_line j i d; k = on_bline k d b, on_bline k d c ? perp g j j i"
I0123 15:57:44.661995 139811328716800 graph.py:498] 
I0123 15:57:44.662055 139811328716800 graph.py:499] a b c = triangle a b c; d = incenter d b a c; e = on_circle e a c, on_line e b a; f = on_circle f a c, on_line f b a; g = on_circle g c a, on_line g b c; h = on_circle h c a, on_line h b c; i = circle i b a c; j = on_line j e g, on_line j i d; k = on_bline k d b, on_bline k d c ? perp g j j i
I0123 15:57:45.918178 139811328716800 ddar.py:60] Depth 1/1000 time = 1.2056806087493896
I0123 15:57:47.219945 139811328716800 ddar.py:60] Depth 2/1000 time = 1.30155348777771
I0123 15:57:48.828488 139811328716800 ddar.py:60] Depth 3/1000 time = 1.6082849502563477
I0123 15:57:50.456136 139811328716800 ddar.py:60] Depth 4/1000 time = 1.6271991729736328
I0123 15:57:51.869444 139811328716800 ddar.py:60] Depth 5/1000 time = 1.411670207977295
I0123 15:57:53.532949 139811328716800 ddar.py:60] Depth 6/1000 time = 1.663238525390625
I0123 15:57:55.316349 139811328716800 ddar.py:60] Depth 7/1000 time = 1.7806503772735596
I0123 15:57:56.944283 139811328716800 ddar.py:60] Depth 8/1000 time = 1.6277275085449219
I0123 15:57:59.065273 139811328716800 ddar.py:60] Depth 9/1000 time = 2.1207685470581055
I0123 15:58:01.721120 139811328716800 ddar.py:60] Depth 10/1000 time = 2.6556127071380615
I0123 15:58:06.050300 139811328716800 ddar.py:60] Depth 11/1000 time = 4.328930854797363
I0123 15:58:09.569738 139811328716800 ddar.py:60] Depth 12/1000 time = 3.5192055702209473
I0123 15:58:13.556253 139811328716800 ddar.py:60] Depth 13/1000 time = 3.9685657024383545
I0123 15:58:17.766939 139811328716800 ddar.py:60] Depth 14/1000 time = 4.210467100143433
I0123 15:58:17.880652 139811328716800 alphageometry.py:191] 
==========================
 * From theorem premises:
A B C D E G I J : Points
ABD = DBC [00]
BCD = DCA [01]
AE = AC [02]
A,E,B are collinear [03]
CG = CA [04]
B,C,G are collinear [05]
IA = IC [06]
IB = IA [07]
J,E,G are collinear [08]
I,D,J are collinear [09]

 * Auxiliary Constructions:
F K : Points
A,F,B are collinear [10]
AF = AC [11]
KD = KB [12]
KD = KC [13]
KDB = DBK [14]
KDC = DCK [15]

 * Proof steps:
001. AE = AC [02]   ACE = CEA [16]
002. ACE = CEA [16] & A,E,B are collinear [03]   ACE = (CE-AB) [17]
003. A,F,B are collinear [10] & (CE-AB) = ACE [17]   (EC-AF) = ACE [18]
004. AF = AC [11] & AE = AC [02]   A is the circumcenter of \Delta FCE [19]
005. A,E,B are collinear [03] & A,F,B are collinear [10]   A,F,E are collinear [20]
006. A is the circumcenter of \Delta FCE [19] & A,F,E are collinear [20]   CF  CE [21]
007. IA = IC [06] & IB = IA [07]   IB = IC [22]
008. KD = KB [12] & KD = KC [13]   KC = KB [23]
009. IB = IC [22] & KC = KB [23]   BC  IK [24]
010. CF  CE [21] & BC  IK [24]   (CE-IK) = FCB [25]
011. CF  CE [21] & BC  IK [24]   BCE = (IK-CF) [26]
012. CF  CE [21] & BC  IK [24]   FCE = (BC-IK) [27]
013. ABD = DBC [00] & KDB = DBK [14] (Angle chase)  (BC-DK) = KBA [28]
014. KC = KB [23]   KCB = CBK [29]
015. ABD = DBC [00] & BCD = DCA [01] & KDB = DBK [14] & KDC = DCK [15] & ACE = (CE-AB) [17] & KCB = CBK [29] (Angle chase)  CE  DK [30]
016. A,F,B are collinear [10] & (CE-IK) = FCB [25] & (BC-DK) = KBA [28] & CE  DK [30] & CF  CE [21]   (AF-BK) = (EC-IK) [31]
017. (EC-AF) = ACE [18] & (AF-BK) = (EC-IK) [31]   ECA = BKI [32]
018. IB = IA [07]   IBA = BAI [33]
019. IB = IC [22]   IBC = BCI [34]
020. IA = IC [06]   IAC = ACI [35]
021. ABD = DBC [00] & BCD = DCA [01] & KDB = DBK [14] & KDC = DCK [15] & ACE = (CE-AB) [17] & IBA = BAI [33] & IBC = BCI [34] & IAC = ACI [35] & KCB = CBK [29] (Angle chase)  ECA = IBK [36]
022. A,E,B are collinear [03] & ECA = IBK [36] & ECA = (AB-CE) [17]   AEC = IBK [37]
023. ECA = BKI [32] & AEC = IBK [37] (Similar Triangles)  CA:CE = KI:KB [38]
024. AF = AC [11]   ACF = CFA [39]
025. A,F,B are collinear [10] & (BC-DK) = KBA [28] & CE  DK [30] & CF  CE [21]   (AF-BK) = FCB [40]
026. ACF = CFA [39] & (AF-BK) = FCB [40]   (CF-BK) = ACB [41]
027. ABD = DBC [00] & BCD = DCA [01] & KDB = DBK [14] & KDC = DCK [15] (Angle chase)  KBA = KCA [42]
028. KBA = KCA [42]   A,K,B,C are concyclic [43]
029. A,K,B,C are concyclic [43]   AKB = ACB [44]
030. (CF-BK) = ACB [41] & CE  DK [30] & CF  CE [21] & AKB = ACB [44]   AKB = (FC-BK) [45]
031. AKB = (FC-BK) [45]   AK  FC [46]
032. AK  CF [46] & CE  DK [30] & CF  CE [21]   KD  KA [47]
033. KD  KA [47]   A,D,K are collinear [48]
034. ABD = DBC [00] & BCD = DCA [01] & KDB = DBK [14] & KDC = DCK [15] & IBA = BAI [33] & IBC = BCI [34] & IAC = ACI [35] & KCB = CBK [29] & BC  IK [24] (Angle chase)  (DK-AI) = IKD [49]
035. A,D,K are collinear [48] & (DK-AI) = IKD [49]   IAK = AKI [50]
036. IAK = AKI [50]   IA = IK [51]
037. CA:CE = KI:KB [38] & IA = IK [51] & CK = BK [23] & CG = AC [04] & DK = CK [13]   CG:CE = KI:KD [52]
038. B,C,G are collinear [05] & D,A,K are collinear [48] & BCE = (IK-CF) [26] & CE  DK [30] & CF  CE [21]   GCE = IKD [53]
039. CG:CE = KI:KD [52] & GCE = IKD [53] (Similar Triangles)  GEC = IDK [54]
040. CG:CE = KI:KD [52] & GCE = IKD [53] (Similar Triangles)  CGE = KID [55]
041. J,E,G are collinear [08] & I,J,D are collinear [09] & GEC = IDK [54] & A,D,K are collinear [48] & CE  DK [30] & CF  CE [21] & FCE = (BC-IK) [27] & CGE = KID [55] & B,C,G are collinear [05]   GJ  JI
==========================

I0123 15:58:17.880764 139811328716800 alphageometry.py:582] Solved.
