I0123 12:48:58.122123 140169433182208 inference_utils.py:69] Parsing gin configuration.
I0123 12:48:58.122244 140169433182208 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0123 12:48:58.122445 140169433182208 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0123 12:48:58.122478 140169433182208 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0123 12:48:58.122507 140169433182208 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0123 12:48:58.122533 140169433182208 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0123 12:48:58.122559 140169433182208 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0123 12:48:58.122585 140169433182208 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0123 12:48:58.122613 140169433182208 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0123 12:48:58.122639 140169433182208 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=32
I0123 12:48:58.122664 140169433182208 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0123 12:48:58.122689 140169433182208 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0123 12:48:58.122734 140169433182208 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0123 12:48:58.122877 140169433182208 resource_reader.py:55] Path not found: base_htrans.gin
I0123 12:48:58.123133 140169433182208 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0123 12:48:58.123238 140169433182208 resource_reader.py:55] Path not found: trainer_configuration.gin
I0123 12:48:58.129655 140169433182208 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0123 12:48:58.129780 140169433182208 resource_reader.py:55] Path not found: size/medium_150M.gin
I0123 12:48:58.130106 140169433182208 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0123 12:48:58.130209 140169433182208 resource_reader.py:55] Path not found: options/positions_t5.gin
I0123 12:48:58.130486 140169433182208 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0123 12:48:58.130584 140169433182208 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0123 12:48:58.130990 140169433182208 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0123 12:48:58.131086 140169433182208 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0123 12:48:58.134746 140169433182208 training_loop.py:334] ==== Training loop: initializing model ====
I0123 12:48:58.241075 140169433182208 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0123 12:48:58.241864 140169433182208 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0123 12:48:58.248233 140169433182208 training_loop.py:335] Process 0 of 1
I0123 12:48:58.248287 140169433182208 training_loop.py:336] Local device count = 1
I0123 12:48:58.248327 140169433182208 training_loop.py:337] Number of replicas = 1
I0123 12:48:58.248359 140169433182208 training_loop.py:339] Using random number seed 42
I0123 12:48:58.753631 140169433182208 training_loop.py:359] Initializing the model.
I0123 12:48:59.158016 140169433182208 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.158323 140169433182208 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 12:48:59.158433 140169433182208 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:48:59.158514 140169433182208 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:48:59.158592 140169433182208 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:48:59.158679 140169433182208 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:48:59.158754 140169433182208 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:48:59.158827 140169433182208 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:48:59.158897 140169433182208 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:48:59.158969 140169433182208 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:48:59.159041 140169433182208 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:48:59.159112 140169433182208 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:48:59.159183 140169433182208 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:48:59.159254 140169433182208 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:48:59.159294 140169433182208 attention.py:418] Single window, no scan.
I0123 12:48:59.159341 140169433182208 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 12:48:59.159462 140169433182208 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:48:59.159503 140169433182208 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:48:59.159536 140169433182208 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:48:59.161558 140169433182208 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.166904 140169433182208 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:48:59.177529 140169433182208 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.177812 140169433182208 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:48:59.182159 140169433182208 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:48:59.192752 140169433182208 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:48:59.192811 140169433182208 attention.py:418] Single window, no scan.
I0123 12:48:59.192848 140169433182208 transformer_layer.py:389] tlayer: self-attention.
I0123 12:48:59.192880 140169433182208 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.192941 140169433182208 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.194133 140169433182208 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.194212 140169433182208 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.194920 140169433182208 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.197386 140169433182208 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.204302 140169433182208 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.205730 140169433182208 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.205817 140169433182208 transformer_layer.py:468] tlayer: End windows.
I0123 12:48:59.205853 140169433182208 transformer_layer.py:472] tlayer: final FFN.
I0123 12:48:59.205917 140169433182208 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.206051 140169433182208 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:48:59.206407 140169433182208 nn_components.py:325] mlp: activation = None
I0123 12:48:59.206455 140169433182208 nn_components.py:329] mlp: final activation = None
I0123 12:48:59.208396 140169433182208 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.208496 140169433182208 nn_components.py:261] mlp: residual
I0123 12:48:59.211372 140169433182208 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.211454 140169433182208 transformer_base.py:443] tbase: final FFN
I0123 12:48:59.211939 140169433182208 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:48:59.222137 140169433182208 nn_components.py:329] mlp: final activation = None
I0123 12:48:59.230923 140169433182208 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.231021 140169433182208 nn_components.py:261] mlp: residual
I0123 12:48:59.231317 140169433182208 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.231397 140169433182208 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 12:48:59.231506 140169433182208 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:48:59.231544 140169433182208 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:48:59.231575 140169433182208 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:48:59.233419 140169433182208 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.235903 140169433182208 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:48:59.241499 140169433182208 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.241762 140169433182208 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:48:59.244388 140169433182208 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:48:59.248217 140169433182208 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:48:59.248276 140169433182208 attention.py:418] Single window, no scan.
I0123 12:48:59.248311 140169433182208 transformer_layer.py:389] tlayer: self-attention.
I0123 12:48:59.248342 140169433182208 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.248402 140169433182208 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.248975 140169433182208 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.249229 140169433182208 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.249589 140169433182208 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.250508 140169433182208 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.252978 140169433182208 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.253598 140169433182208 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.253680 140169433182208 transformer_layer.py:468] tlayer: End windows.
I0123 12:48:59.253714 140169433182208 transformer_layer.py:472] tlayer: final FFN.
I0123 12:48:59.253772 140169433182208 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.253900 140169433182208 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:48:59.254222 140169433182208 nn_components.py:325] mlp: activation = None
I0123 12:48:59.254265 140169433182208 nn_components.py:329] mlp: final activation = None
I0123 12:48:59.256206 140169433182208 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.256299 140169433182208 nn_components.py:261] mlp: residual
I0123 12:48:59.258801 140169433182208 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.258880 140169433182208 transformer_base.py:443] tbase: final FFN
I0123 12:48:59.259301 140169433182208 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:48:59.261602 140169433182208 nn_components.py:329] mlp: final activation = None
I0123 12:48:59.263484 140169433182208 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.263577 140169433182208 nn_components.py:261] mlp: residual
I0123 12:48:59.263869 140169433182208 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.263948 140169433182208 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 12:48:59.264055 140169433182208 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:48:59.264094 140169433182208 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:48:59.264125 140169433182208 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:48:59.266355 140169433182208 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.268742 140169433182208 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:48:59.274358 140169433182208 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.274622 140169433182208 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:48:59.277336 140169433182208 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:48:59.281203 140169433182208 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:48:59.281258 140169433182208 attention.py:418] Single window, no scan.
I0123 12:48:59.281294 140169433182208 transformer_layer.py:389] tlayer: self-attention.
I0123 12:48:59.281325 140169433182208 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.281389 140169433182208 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.281970 140169433182208 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.282046 140169433182208 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.282408 140169433182208 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.283186 140169433182208 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.285714 140169433182208 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.286386 140169433182208 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.286462 140169433182208 transformer_layer.py:468] tlayer: End windows.
I0123 12:48:59.286498 140169433182208 transformer_layer.py:472] tlayer: final FFN.
I0123 12:48:59.286557 140169433182208 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.286686 140169433182208 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:48:59.287015 140169433182208 nn_components.py:325] mlp: activation = None
I0123 12:48:59.287059 140169433182208 nn_components.py:329] mlp: final activation = None
I0123 12:48:59.288994 140169433182208 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.289086 140169433182208 nn_components.py:261] mlp: residual
I0123 12:48:59.291635 140169433182208 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.291720 140169433182208 transformer_base.py:443] tbase: final FFN
I0123 12:48:59.292206 140169433182208 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:48:59.294505 140169433182208 nn_components.py:329] mlp: final activation = None
I0123 12:48:59.296435 140169433182208 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.296530 140169433182208 nn_components.py:261] mlp: residual
I0123 12:48:59.296826 140169433182208 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.296908 140169433182208 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 12:48:59.297018 140169433182208 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:48:59.297056 140169433182208 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:48:59.297087 140169433182208 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:48:59.299019 140169433182208 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.301463 140169433182208 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:48:59.307196 140169433182208 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.307459 140169433182208 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:48:59.310160 140169433182208 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:48:59.314032 140169433182208 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:48:59.314089 140169433182208 attention.py:418] Single window, no scan.
I0123 12:48:59.314125 140169433182208 transformer_layer.py:389] tlayer: self-attention.
I0123 12:48:59.314157 140169433182208 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.314219 140169433182208 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.314785 140169433182208 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.314861 140169433182208 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.315228 140169433182208 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.316005 140169433182208 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.318590 140169433182208 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.319218 140169433182208 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.319295 140169433182208 transformer_layer.py:468] tlayer: End windows.
I0123 12:48:59.319331 140169433182208 transformer_layer.py:472] tlayer: final FFN.
I0123 12:48:59.319394 140169433182208 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.319523 140169433182208 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:48:59.319844 140169433182208 nn_components.py:325] mlp: activation = None
I0123 12:48:59.319886 140169433182208 nn_components.py:329] mlp: final activation = None
I0123 12:48:59.321827 140169433182208 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.321923 140169433182208 nn_components.py:261] mlp: residual
I0123 12:48:59.324753 140169433182208 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.324837 140169433182208 transformer_base.py:443] tbase: final FFN
I0123 12:48:59.325275 140169433182208 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:48:59.327560 140169433182208 nn_components.py:329] mlp: final activation = None
I0123 12:48:59.329501 140169433182208 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.329594 140169433182208 nn_components.py:261] mlp: residual
I0123 12:48:59.329896 140169433182208 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.329978 140169433182208 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 12:48:59.330088 140169433182208 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:48:59.330126 140169433182208 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:48:59.330157 140169433182208 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:48:59.332086 140169433182208 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.334514 140169433182208 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:48:59.340219 140169433182208 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.340475 140169433182208 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:48:59.343545 140169433182208 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:48:59.347320 140169433182208 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:48:59.347376 140169433182208 attention.py:418] Single window, no scan.
I0123 12:48:59.347412 140169433182208 transformer_layer.py:389] tlayer: self-attention.
I0123 12:48:59.347443 140169433182208 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.347510 140169433182208 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.348079 140169433182208 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.348155 140169433182208 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.348519 140169433182208 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.349287 140169433182208 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.351864 140169433182208 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.352493 140169433182208 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.352569 140169433182208 transformer_layer.py:468] tlayer: End windows.
I0123 12:48:59.352604 140169433182208 transformer_layer.py:472] tlayer: final FFN.
I0123 12:48:59.352663 140169433182208 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.352793 140169433182208 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:48:59.353114 140169433182208 nn_components.py:325] mlp: activation = None
I0123 12:48:59.353157 140169433182208 nn_components.py:329] mlp: final activation = None
I0123 12:48:59.355084 140169433182208 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.355178 140169433182208 nn_components.py:261] mlp: residual
I0123 12:48:59.357766 140169433182208 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.357849 140169433182208 transformer_base.py:443] tbase: final FFN
I0123 12:48:59.358276 140169433182208 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:48:59.360551 140169433182208 nn_components.py:329] mlp: final activation = None
I0123 12:48:59.362520 140169433182208 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.362616 140169433182208 nn_components.py:261] mlp: residual
I0123 12:48:59.362912 140169433182208 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.362993 140169433182208 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 12:48:59.363102 140169433182208 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:48:59.363141 140169433182208 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:48:59.363172 140169433182208 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:48:59.365017 140169433182208 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.367431 140169433182208 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:48:59.373096 140169433182208 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.373360 140169433182208 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:48:59.376078 140169433182208 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:48:59.379832 140169433182208 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:48:59.379890 140169433182208 attention.py:418] Single window, no scan.
I0123 12:48:59.379927 140169433182208 transformer_layer.py:389] tlayer: self-attention.
I0123 12:48:59.379958 140169433182208 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.380019 140169433182208 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.380627 140169433182208 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.380707 140169433182208 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.381068 140169433182208 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.381865 140169433182208 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.384381 140169433182208 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.385007 140169433182208 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.385083 140169433182208 transformer_layer.py:468] tlayer: End windows.
I0123 12:48:59.385118 140169433182208 transformer_layer.py:472] tlayer: final FFN.
I0123 12:48:59.385177 140169433182208 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.385301 140169433182208 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:48:59.385630 140169433182208 nn_components.py:325] mlp: activation = None
I0123 12:48:59.385681 140169433182208 nn_components.py:329] mlp: final activation = None
I0123 12:48:59.387606 140169433182208 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.387699 140169433182208 nn_components.py:261] mlp: residual
I0123 12:48:59.390277 140169433182208 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.390357 140169433182208 transformer_base.py:443] tbase: final FFN
I0123 12:48:59.390788 140169433182208 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:48:59.393136 140169433182208 nn_components.py:329] mlp: final activation = None
I0123 12:48:59.395113 140169433182208 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.395209 140169433182208 nn_components.py:261] mlp: residual
I0123 12:48:59.395504 140169433182208 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.395584 140169433182208 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 12:48:59.395694 140169433182208 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:48:59.395733 140169433182208 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:48:59.395765 140169433182208 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:48:59.397616 140169433182208 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.400094 140169433182208 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:48:59.405787 140169433182208 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.406054 140169433182208 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:48:59.408726 140169433182208 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:48:59.412539 140169433182208 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:48:59.412596 140169433182208 attention.py:418] Single window, no scan.
I0123 12:48:59.412631 140169433182208 transformer_layer.py:389] tlayer: self-attention.
I0123 12:48:59.412663 140169433182208 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.412725 140169433182208 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.413291 140169433182208 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.413367 140169433182208 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.413734 140169433182208 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.414515 140169433182208 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.417050 140169433182208 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.417684 140169433182208 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.417762 140169433182208 transformer_layer.py:468] tlayer: End windows.
I0123 12:48:59.417798 140169433182208 transformer_layer.py:472] tlayer: final FFN.
I0123 12:48:59.417859 140169433182208 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.417986 140169433182208 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:48:59.418315 140169433182208 nn_components.py:325] mlp: activation = None
I0123 12:48:59.418359 140169433182208 nn_components.py:329] mlp: final activation = None
I0123 12:48:59.420674 140169433182208 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.420769 140169433182208 nn_components.py:261] mlp: residual
I0123 12:48:59.423309 140169433182208 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.423389 140169433182208 transformer_base.py:443] tbase: final FFN
I0123 12:48:59.423824 140169433182208 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:48:59.566543 140169433182208 nn_components.py:329] mlp: final activation = None
I0123 12:48:59.568825 140169433182208 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.568991 140169433182208 nn_components.py:261] mlp: residual
I0123 12:48:59.569306 140169433182208 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.569397 140169433182208 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 12:48:59.569510 140169433182208 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:48:59.569549 140169433182208 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:48:59.569580 140169433182208 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:48:59.571634 140169433182208 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.574152 140169433182208 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:48:59.579861 140169433182208 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.580143 140169433182208 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:48:59.582855 140169433182208 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:48:59.586761 140169433182208 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:48:59.586820 140169433182208 attention.py:418] Single window, no scan.
I0123 12:48:59.586857 140169433182208 transformer_layer.py:389] tlayer: self-attention.
I0123 12:48:59.586889 140169433182208 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.586952 140169433182208 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.587572 140169433182208 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.587652 140169433182208 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.588014 140169433182208 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.588805 140169433182208 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.591417 140169433182208 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.592059 140169433182208 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.592135 140169433182208 transformer_layer.py:468] tlayer: End windows.
I0123 12:48:59.592171 140169433182208 transformer_layer.py:472] tlayer: final FFN.
I0123 12:48:59.592232 140169433182208 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.592357 140169433182208 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:48:59.592688 140169433182208 nn_components.py:325] mlp: activation = None
I0123 12:48:59.592732 140169433182208 nn_components.py:329] mlp: final activation = None
I0123 12:48:59.594671 140169433182208 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.594765 140169433182208 nn_components.py:261] mlp: residual
I0123 12:48:59.597325 140169433182208 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.597403 140169433182208 transformer_base.py:443] tbase: final FFN
I0123 12:48:59.597901 140169433182208 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:48:59.600208 140169433182208 nn_components.py:329] mlp: final activation = None
I0123 12:48:59.602130 140169433182208 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.602233 140169433182208 nn_components.py:261] mlp: residual
I0123 12:48:59.602533 140169433182208 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.602613 140169433182208 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 12:48:59.602725 140169433182208 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:48:59.602765 140169433182208 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:48:59.602797 140169433182208 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:48:59.604728 140169433182208 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.607115 140169433182208 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:48:59.612780 140169433182208 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.613044 140169433182208 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:48:59.615752 140169433182208 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:48:59.619531 140169433182208 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:48:59.619587 140169433182208 attention.py:418] Single window, no scan.
I0123 12:48:59.619623 140169433182208 transformer_layer.py:389] tlayer: self-attention.
I0123 12:48:59.619655 140169433182208 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.619716 140169433182208 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.620280 140169433182208 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.620356 140169433182208 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.620714 140169433182208 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.621490 140169433182208 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.624067 140169433182208 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.624687 140169433182208 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.624764 140169433182208 transformer_layer.py:468] tlayer: End windows.
I0123 12:48:59.624800 140169433182208 transformer_layer.py:472] tlayer: final FFN.
I0123 12:48:59.624860 140169433182208 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.624986 140169433182208 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:48:59.625313 140169433182208 nn_components.py:325] mlp: activation = None
I0123 12:48:59.625356 140169433182208 nn_components.py:329] mlp: final activation = None
I0123 12:48:59.627274 140169433182208 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.627391 140169433182208 nn_components.py:261] mlp: residual
I0123 12:48:59.630145 140169433182208 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.630224 140169433182208 transformer_base.py:443] tbase: final FFN
I0123 12:48:59.630665 140169433182208 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:48:59.633081 140169433182208 nn_components.py:329] mlp: final activation = None
I0123 12:48:59.635006 140169433182208 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.635100 140169433182208 nn_components.py:261] mlp: residual
I0123 12:48:59.635391 140169433182208 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.635478 140169433182208 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 12:48:59.635588 140169433182208 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:48:59.635627 140169433182208 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:48:59.635657 140169433182208 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:48:59.637567 140169433182208 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.639965 140169433182208 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:48:59.645945 140169433182208 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.646209 140169433182208 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:48:59.648915 140169433182208 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:48:59.652656 140169433182208 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:48:59.652712 140169433182208 attention.py:418] Single window, no scan.
I0123 12:48:59.652748 140169433182208 transformer_layer.py:389] tlayer: self-attention.
I0123 12:48:59.652780 140169433182208 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.652840 140169433182208 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.653394 140169433182208 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.653470 140169433182208 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.653835 140169433182208 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.654661 140169433182208 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.657156 140169433182208 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.657773 140169433182208 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.657851 140169433182208 transformer_layer.py:468] tlayer: End windows.
I0123 12:48:59.657887 140169433182208 transformer_layer.py:472] tlayer: final FFN.
I0123 12:48:59.657946 140169433182208 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.658075 140169433182208 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:48:59.658399 140169433182208 nn_components.py:325] mlp: activation = None
I0123 12:48:59.658443 140169433182208 nn_components.py:329] mlp: final activation = None
I0123 12:48:59.660352 140169433182208 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.660446 140169433182208 nn_components.py:261] mlp: residual
I0123 12:48:59.663015 140169433182208 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.663097 140169433182208 transformer_base.py:443] tbase: final FFN
I0123 12:48:59.663529 140169433182208 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:48:59.665802 140169433182208 nn_components.py:329] mlp: final activation = None
I0123 12:48:59.667771 140169433182208 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.667866 140169433182208 nn_components.py:261] mlp: residual
I0123 12:48:59.668161 140169433182208 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.668248 140169433182208 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 12:48:59.668359 140169433182208 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:48:59.668398 140169433182208 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:48:59.668429 140169433182208 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:48:59.670287 140169433182208 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.672742 140169433182208 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:48:59.678294 140169433182208 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.678562 140169433182208 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:48:59.681241 140169433182208 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:48:59.684985 140169433182208 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:48:59.685041 140169433182208 attention.py:418] Single window, no scan.
I0123 12:48:59.685078 140169433182208 transformer_layer.py:389] tlayer: self-attention.
I0123 12:48:59.685109 140169433182208 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.685211 140169433182208 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.685775 140169433182208 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.685853 140169433182208 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.686218 140169433182208 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.686985 140169433182208 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.689487 140169433182208 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.690113 140169433182208 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.690190 140169433182208 transformer_layer.py:468] tlayer: End windows.
I0123 12:48:59.690225 140169433182208 transformer_layer.py:472] tlayer: final FFN.
I0123 12:48:59.690285 140169433182208 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.690409 140169433182208 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:48:59.690732 140169433182208 nn_components.py:325] mlp: activation = None
I0123 12:48:59.690776 140169433182208 nn_components.py:329] mlp: final activation = None
I0123 12:48:59.692740 140169433182208 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.692834 140169433182208 nn_components.py:261] mlp: residual
I0123 12:48:59.695633 140169433182208 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.695714 140169433182208 transformer_base.py:443] tbase: final FFN
I0123 12:48:59.696144 140169433182208 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:48:59.698471 140169433182208 nn_components.py:329] mlp: final activation = None
I0123 12:48:59.700376 140169433182208 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.700469 140169433182208 nn_components.py:261] mlp: residual
I0123 12:48:59.700759 140169433182208 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.700839 140169433182208 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 12:48:59.700954 140169433182208 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:48:59.700994 140169433182208 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:48:59.701024 140169433182208 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:48:59.702873 140169433182208 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.705317 140169433182208 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:48:59.710931 140169433182208 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.711195 140169433182208 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:48:59.713845 140169433182208 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:48:59.717967 140169433182208 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:48:59.718024 140169433182208 attention.py:418] Single window, no scan.
I0123 12:48:59.718059 140169433182208 transformer_layer.py:389] tlayer: self-attention.
I0123 12:48:59.718091 140169433182208 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.718152 140169433182208 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.718708 140169433182208 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.718785 140169433182208 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.719143 140169433182208 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.719914 140169433182208 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.722388 140169433182208 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.723004 140169433182208 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.723083 140169433182208 transformer_layer.py:468] tlayer: End windows.
I0123 12:48:59.723118 140169433182208 transformer_layer.py:472] tlayer: final FFN.
I0123 12:48:59.723182 140169433182208 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.723311 140169433182208 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:48:59.723630 140169433182208 nn_components.py:325] mlp: activation = None
I0123 12:48:59.723675 140169433182208 nn_components.py:329] mlp: final activation = None
I0123 12:48:59.725624 140169433182208 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.725728 140169433182208 nn_components.py:261] mlp: residual
I0123 12:48:59.728242 140169433182208 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.728321 140169433182208 transformer_base.py:443] tbase: final FFN
I0123 12:48:59.728758 140169433182208 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:48:59.731033 140169433182208 nn_components.py:329] mlp: final activation = None
I0123 12:48:59.733266 140169433182208 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.733364 140169433182208 nn_components.py:261] mlp: residual
I0123 12:48:59.733662 140169433182208 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.733955 140169433182208 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:48:59.734025 140169433182208 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:48:59.734093 140169433182208 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:48:59.734151 140169433182208 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:48:59.734205 140169433182208 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:48:59.734260 140169433182208 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:48:59.734313 140169433182208 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:48:59.734365 140169433182208 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:48:59.734417 140169433182208 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:48:59.734469 140169433182208 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:48:59.734521 140169433182208 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:48:59.734573 140169433182208 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:48:59.734610 140169433182208 decoder_stack.py:344] dstack: Final layernorm.
I0123 12:48:59.738138 140169433182208 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:48:59.786262 140169433182208 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.786351 140169433182208 decoder_stack.py:333] dstack: autoregressive generator.
I0123 12:48:59.786405 140169433182208 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 12:48:59.786509 140169433182208 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:48:59.786548 140169433182208 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:48:59.786578 140169433182208 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:48:59.786641 140169433182208 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.789067 140169433182208 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:48:59.794573 140169433182208 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.794833 140169433182208 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:48:59.797483 140169433182208 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:48:59.813993 140169433182208 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:48:59.814050 140169433182208 attention.py:418] Single window, no scan.
I0123 12:48:59.814087 140169433182208 transformer_layer.py:389] tlayer: self-attention.
I0123 12:48:59.814118 140169433182208 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.814180 140169433182208 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.815298 140169433182208 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.815377 140169433182208 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.816087 140169433182208 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.818097 140169433182208 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.822884 140169433182208 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.824203 140169433182208 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.824290 140169433182208 transformer_layer.py:468] tlayer: End windows.
I0123 12:48:59.824327 140169433182208 transformer_layer.py:472] tlayer: final FFN.
I0123 12:48:59.824389 140169433182208 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.824522 140169433182208 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:48:59.824635 140169433182208 nn_components.py:325] mlp: activation = None
I0123 12:48:59.824674 140169433182208 nn_components.py:329] mlp: final activation = None
I0123 12:48:59.826605 140169433182208 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.826701 140169433182208 nn_components.py:261] mlp: residual
I0123 12:48:59.829136 140169433182208 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.829215 140169433182208 transformer_base.py:443] tbase: final FFN
I0123 12:48:59.829324 140169433182208 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:48:59.831602 140169433182208 nn_components.py:329] mlp: final activation = None
I0123 12:48:59.833561 140169433182208 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.833662 140169433182208 nn_components.py:261] mlp: residual
I0123 12:48:59.833961 140169433182208 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.834044 140169433182208 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 12:48:59.834153 140169433182208 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:48:59.834192 140169433182208 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:48:59.834223 140169433182208 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:48:59.834288 140169433182208 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.836572 140169433182208 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:48:59.842129 140169433182208 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.842389 140169433182208 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:48:59.845108 140169433182208 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:48:59.862784 140169433182208 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:48:59.862871 140169433182208 attention.py:418] Single window, no scan.
I0123 12:48:59.862910 140169433182208 transformer_layer.py:389] tlayer: self-attention.
I0123 12:48:59.862943 140169433182208 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.863021 140169433182208 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.863656 140169433182208 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.863739 140169433182208 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.864125 140169433182208 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.864865 140169433182208 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.867454 140169433182208 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.868085 140169433182208 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.868163 140169433182208 transformer_layer.py:468] tlayer: End windows.
I0123 12:48:59.868204 140169433182208 transformer_layer.py:472] tlayer: final FFN.
I0123 12:48:59.868269 140169433182208 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.868400 140169433182208 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:48:59.868520 140169433182208 nn_components.py:325] mlp: activation = None
I0123 12:48:59.868560 140169433182208 nn_components.py:329] mlp: final activation = None
I0123 12:48:59.870639 140169433182208 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.870733 140169433182208 nn_components.py:261] mlp: residual
I0123 12:48:59.873232 140169433182208 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.873313 140169433182208 transformer_base.py:443] tbase: final FFN
I0123 12:48:59.873427 140169433182208 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:48:59.875762 140169433182208 nn_components.py:329] mlp: final activation = None
I0123 12:48:59.877758 140169433182208 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.877854 140169433182208 nn_components.py:261] mlp: residual
I0123 12:48:59.878147 140169433182208 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.878231 140169433182208 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 12:48:59.878345 140169433182208 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:48:59.878387 140169433182208 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:48:59.878419 140169433182208 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:48:59.878486 140169433182208 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.880780 140169433182208 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:48:59.886469 140169433182208 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.886917 140169433182208 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:48:59.889696 140169433182208 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:48:59.902608 140169433182208 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:48:59.902665 140169433182208 attention.py:418] Single window, no scan.
I0123 12:48:59.902702 140169433182208 transformer_layer.py:389] tlayer: self-attention.
I0123 12:48:59.902733 140169433182208 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.902795 140169433182208 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.903359 140169433182208 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.903437 140169433182208 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.903802 140169433182208 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.904499 140169433182208 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.907028 140169433182208 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.907655 140169433182208 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.907732 140169433182208 transformer_layer.py:468] tlayer: End windows.
I0123 12:48:59.907767 140169433182208 transformer_layer.py:472] tlayer: final FFN.
I0123 12:48:59.907832 140169433182208 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.907960 140169433182208 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:48:59.908084 140169433182208 nn_components.py:325] mlp: activation = None
I0123 12:48:59.908122 140169433182208 nn_components.py:329] mlp: final activation = None
I0123 12:48:59.910091 140169433182208 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.910185 140169433182208 nn_components.py:261] mlp: residual
I0123 12:48:59.912655 140169433182208 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.912735 140169433182208 transformer_base.py:443] tbase: final FFN
I0123 12:48:59.912844 140169433182208 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:48:59.915116 140169433182208 nn_components.py:329] mlp: final activation = None
I0123 12:48:59.917068 140169433182208 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.917164 140169433182208 nn_components.py:261] mlp: residual
I0123 12:48:59.917456 140169433182208 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.917537 140169433182208 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 12:48:59.917653 140169433182208 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:48:59.917694 140169433182208 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:48:59.917726 140169433182208 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:48:59.917792 140169433182208 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.920062 140169433182208 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:48:59.925615 140169433182208 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.925887 140169433182208 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:48:59.928633 140169433182208 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:48:59.941889 140169433182208 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:48:59.941946 140169433182208 attention.py:418] Single window, no scan.
I0123 12:48:59.941983 140169433182208 transformer_layer.py:389] tlayer: self-attention.
I0123 12:48:59.942015 140169433182208 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.942076 140169433182208 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.942634 140169433182208 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.942711 140169433182208 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.943075 140169433182208 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.943793 140169433182208 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.946341 140169433182208 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.946969 140169433182208 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.947046 140169433182208 transformer_layer.py:468] tlayer: End windows.
I0123 12:48:59.947081 140169433182208 transformer_layer.py:472] tlayer: final FFN.
I0123 12:48:59.947140 140169433182208 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.947280 140169433182208 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:48:59.947392 140169433182208 nn_components.py:325] mlp: activation = None
I0123 12:48:59.947431 140169433182208 nn_components.py:329] mlp: final activation = None
I0123 12:48:59.949697 140169433182208 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.949794 140169433182208 nn_components.py:261] mlp: residual
I0123 12:48:59.952255 140169433182208 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.952335 140169433182208 transformer_base.py:443] tbase: final FFN
I0123 12:48:59.952443 140169433182208 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:48:59.954747 140169433182208 nn_components.py:329] mlp: final activation = None
I0123 12:48:59.956655 140169433182208 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.956750 140169433182208 nn_components.py:261] mlp: residual
I0123 12:48:59.957043 140169433182208 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.957123 140169433182208 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 12:48:59.957231 140169433182208 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:48:59.957269 140169433182208 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:48:59.957301 140169433182208 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:48:59.957365 140169433182208 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.959735 140169433182208 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:48:59.965298 140169433182208 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.965569 140169433182208 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:48:59.968247 140169433182208 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:48:59.981155 140169433182208 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:48:59.981211 140169433182208 attention.py:418] Single window, no scan.
I0123 12:48:59.981248 140169433182208 transformer_layer.py:389] tlayer: self-attention.
I0123 12:48:59.981280 140169433182208 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.981341 140169433182208 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.981920 140169433182208 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.981997 140169433182208 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.982357 140169433182208 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.983071 140169433182208 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.985669 140169433182208 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.986308 140169433182208 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.986386 140169433182208 transformer_layer.py:468] tlayer: End windows.
I0123 12:48:59.986422 140169433182208 transformer_layer.py:472] tlayer: final FFN.
I0123 12:48:59.986484 140169433182208 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.986622 140169433182208 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:48:59.986735 140169433182208 nn_components.py:325] mlp: activation = None
I0123 12:48:59.986775 140169433182208 nn_components.py:329] mlp: final activation = None
I0123 12:48:59.988703 140169433182208 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.988797 140169433182208 nn_components.py:261] mlp: residual
I0123 12:48:59.991292 140169433182208 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.991373 140169433182208 transformer_base.py:443] tbase: final FFN
I0123 12:48:59.991484 140169433182208 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:48:59.993814 140169433182208 nn_components.py:329] mlp: final activation = None
I0123 12:48:59.995728 140169433182208 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.995824 140169433182208 nn_components.py:261] mlp: residual
I0123 12:48:59.996114 140169433182208 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.996195 140169433182208 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 12:48:59.996304 140169433182208 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:48:59.996343 140169433182208 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:48:59.996375 140169433182208 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:48:59.996439 140169433182208 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:48:59.998716 140169433182208 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:49:00.004245 140169433182208 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.004505 140169433182208 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:49:00.007252 140169433182208 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:49:00.020177 140169433182208 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:49:00.020236 140169433182208 attention.py:418] Single window, no scan.
I0123 12:49:00.020272 140169433182208 transformer_layer.py:389] tlayer: self-attention.
I0123 12:49:00.020303 140169433182208 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.020366 140169433182208 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.020942 140169433182208 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.021019 140169433182208 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.021381 140169433182208 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.022097 140169433182208 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.024607 140169433182208 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.025237 140169433182208 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.025315 140169433182208 transformer_layer.py:468] tlayer: End windows.
I0123 12:49:00.025349 140169433182208 transformer_layer.py:472] tlayer: final FFN.
I0123 12:49:00.025409 140169433182208 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.025537 140169433182208 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:49:00.025662 140169433182208 nn_components.py:325] mlp: activation = None
I0123 12:49:00.025704 140169433182208 nn_components.py:329] mlp: final activation = None
I0123 12:49:00.027701 140169433182208 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.027796 140169433182208 nn_components.py:261] mlp: residual
I0123 12:49:00.030258 140169433182208 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.030343 140169433182208 transformer_base.py:443] tbase: final FFN
I0123 12:49:00.030454 140169433182208 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:49:00.032679 140169433182208 nn_components.py:329] mlp: final activation = None
I0123 12:49:00.034575 140169433182208 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.034672 140169433182208 nn_components.py:261] mlp: residual
I0123 12:49:00.034964 140169433182208 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.035045 140169433182208 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 12:49:00.035155 140169433182208 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:49:00.035195 140169433182208 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:49:00.035227 140169433182208 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:49:00.035291 140169433182208 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.037631 140169433182208 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:49:00.043363 140169433182208 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.043623 140169433182208 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:49:00.046306 140169433182208 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:49:00.059514 140169433182208 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:49:00.059570 140169433182208 attention.py:418] Single window, no scan.
I0123 12:49:00.059607 140169433182208 transformer_layer.py:389] tlayer: self-attention.
I0123 12:49:00.059639 140169433182208 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.059702 140169433182208 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.060267 140169433182208 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.060348 140169433182208 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.060708 140169433182208 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.061417 140169433182208 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.063942 140169433182208 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.064609 140169433182208 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.064686 140169433182208 transformer_layer.py:468] tlayer: End windows.
I0123 12:49:00.064721 140169433182208 transformer_layer.py:472] tlayer: final FFN.
I0123 12:49:00.064781 140169433182208 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.064913 140169433182208 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:49:00.065027 140169433182208 nn_components.py:325] mlp: activation = None
I0123 12:49:00.065072 140169433182208 nn_components.py:329] mlp: final activation = None
I0123 12:49:00.067022 140169433182208 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.067116 140169433182208 nn_components.py:261] mlp: residual
I0123 12:49:00.069556 140169433182208 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.069636 140169433182208 transformer_base.py:443] tbase: final FFN
I0123 12:49:00.069754 140169433182208 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:49:00.071991 140169433182208 nn_components.py:329] mlp: final activation = None
I0123 12:49:00.073969 140169433182208 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.074067 140169433182208 nn_components.py:261] mlp: residual
I0123 12:49:00.074356 140169433182208 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.074437 140169433182208 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 12:49:00.074548 140169433182208 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:49:00.074587 140169433182208 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:49:00.074620 140169433182208 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:49:00.074684 140169433182208 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.076962 140169433182208 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:49:00.082472 140169433182208 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.082745 140169433182208 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:49:00.085471 140169433182208 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:49:00.098438 140169433182208 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:49:00.098495 140169433182208 attention.py:418] Single window, no scan.
I0123 12:49:00.098532 140169433182208 transformer_layer.py:389] tlayer: self-attention.
I0123 12:49:00.098563 140169433182208 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.098627 140169433182208 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.099233 140169433182208 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.099310 140169433182208 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.099678 140169433182208 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.100387 140169433182208 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.102929 140169433182208 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.103565 140169433182208 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.103643 140169433182208 transformer_layer.py:468] tlayer: End windows.
I0123 12:49:00.103679 140169433182208 transformer_layer.py:472] tlayer: final FFN.
I0123 12:49:00.103740 140169433182208 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.103869 140169433182208 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:49:00.103983 140169433182208 nn_components.py:325] mlp: activation = None
I0123 12:49:00.104028 140169433182208 nn_components.py:329] mlp: final activation = None
I0123 12:49:00.105946 140169433182208 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.106041 140169433182208 nn_components.py:261] mlp: residual
I0123 12:49:00.108518 140169433182208 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.108598 140169433182208 transformer_base.py:443] tbase: final FFN
I0123 12:49:00.108708 140169433182208 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:49:00.110947 140169433182208 nn_components.py:329] mlp: final activation = None
I0123 12:49:00.112837 140169433182208 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.112932 140169433182208 nn_components.py:261] mlp: residual
I0123 12:49:00.113221 140169433182208 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.113303 140169433182208 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 12:49:00.113415 140169433182208 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:49:00.113455 140169433182208 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:49:00.113488 140169433182208 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:49:00.113552 140169433182208 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.115837 140169433182208 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:49:00.121418 140169433182208 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.121691 140169433182208 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:49:00.124361 140169433182208 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:49:00.137326 140169433182208 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:49:00.137383 140169433182208 attention.py:418] Single window, no scan.
I0123 12:49:00.137419 140169433182208 transformer_layer.py:389] tlayer: self-attention.
I0123 12:49:00.137452 140169433182208 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.137514 140169433182208 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.138097 140169433182208 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.138174 140169433182208 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.138540 140169433182208 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.139252 140169433182208 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.141820 140169433182208 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.142625 140169433182208 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.142706 140169433182208 transformer_layer.py:468] tlayer: End windows.
I0123 12:49:00.142742 140169433182208 transformer_layer.py:472] tlayer: final FFN.
I0123 12:49:00.142802 140169433182208 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.142930 140169433182208 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:49:00.143040 140169433182208 nn_components.py:325] mlp: activation = None
I0123 12:49:00.143079 140169433182208 nn_components.py:329] mlp: final activation = None
I0123 12:49:00.145110 140169433182208 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.145204 140169433182208 nn_components.py:261] mlp: residual
I0123 12:49:00.147659 140169433182208 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.147739 140169433182208 transformer_base.py:443] tbase: final FFN
I0123 12:49:00.147846 140169433182208 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:49:00.150109 140169433182208 nn_components.py:329] mlp: final activation = None
I0123 12:49:00.152066 140169433182208 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.152163 140169433182208 nn_components.py:261] mlp: residual
I0123 12:49:00.152455 140169433182208 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.152536 140169433182208 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 12:49:00.152646 140169433182208 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:49:00.152685 140169433182208 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:49:00.152717 140169433182208 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:49:00.152781 140169433182208 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.155065 140169433182208 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:49:00.160584 140169433182208 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.160845 140169433182208 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:49:00.163882 140169433182208 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:49:00.176616 140169433182208 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:49:00.176673 140169433182208 attention.py:418] Single window, no scan.
I0123 12:49:00.176709 140169433182208 transformer_layer.py:389] tlayer: self-attention.
I0123 12:49:00.176741 140169433182208 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.176803 140169433182208 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.177416 140169433182208 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.177493 140169433182208 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.177868 140169433182208 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.178561 140169433182208 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.181098 140169433182208 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.181732 140169433182208 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.181810 140169433182208 transformer_layer.py:468] tlayer: End windows.
I0123 12:49:00.181847 140169433182208 transformer_layer.py:472] tlayer: final FFN.
I0123 12:49:00.181905 140169433182208 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.182037 140169433182208 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:49:00.182148 140169433182208 nn_components.py:325] mlp: activation = None
I0123 12:49:00.182187 140169433182208 nn_components.py:329] mlp: final activation = None
I0123 12:49:00.184077 140169433182208 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.184177 140169433182208 nn_components.py:261] mlp: residual
I0123 12:49:00.186665 140169433182208 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.186745 140169433182208 transformer_base.py:443] tbase: final FFN
I0123 12:49:00.186859 140169433182208 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:49:00.189089 140169433182208 nn_components.py:329] mlp: final activation = None
I0123 12:49:00.190967 140169433182208 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.191064 140169433182208 nn_components.py:261] mlp: residual
I0123 12:49:00.191351 140169433182208 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.191433 140169433182208 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 12:49:00.191542 140169433182208 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:49:00.191581 140169433182208 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:49:00.191612 140169433182208 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:49:00.191676 140169433182208 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.193941 140169433182208 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:49:00.199513 140169433182208 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.199772 140169433182208 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:49:00.202441 140169433182208 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:49:00.215202 140169433182208 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:49:00.215258 140169433182208 attention.py:418] Single window, no scan.
I0123 12:49:00.215294 140169433182208 transformer_layer.py:389] tlayer: self-attention.
I0123 12:49:00.215325 140169433182208 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.215386 140169433182208 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.215949 140169433182208 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.216027 140169433182208 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.216385 140169433182208 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.217081 140169433182208 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.219596 140169433182208 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.220265 140169433182208 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.220343 140169433182208 transformer_layer.py:468] tlayer: End windows.
I0123 12:49:00.220377 140169433182208 transformer_layer.py:472] tlayer: final FFN.
I0123 12:49:00.220436 140169433182208 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.220562 140169433182208 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:49:00.220671 140169433182208 nn_components.py:325] mlp: activation = None
I0123 12:49:00.220710 140169433182208 nn_components.py:329] mlp: final activation = None
I0123 12:49:00.222615 140169433182208 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.222714 140169433182208 nn_components.py:261] mlp: residual
I0123 12:49:00.225162 140169433182208 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.225241 140169433182208 transformer_base.py:443] tbase: final FFN
I0123 12:49:00.225347 140169433182208 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:49:00.227574 140169433182208 nn_components.py:329] mlp: final activation = None
I0123 12:49:00.229525 140169433182208 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.229619 140169433182208 nn_components.py:261] mlp: residual
I0123 12:49:00.229918 140169433182208 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.229999 140169433182208 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 12:49:00.230108 140169433182208 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:49:00.230147 140169433182208 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:49:00.230178 140169433182208 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:49:00.230241 140169433182208 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.232475 140169433182208 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:49:00.237950 140169433182208 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.238209 140169433182208 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:49:00.240834 140169433182208 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:49:00.253725 140169433182208 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:49:00.253781 140169433182208 attention.py:418] Single window, no scan.
I0123 12:49:00.253818 140169433182208 transformer_layer.py:389] tlayer: self-attention.
I0123 12:49:00.253849 140169433182208 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.253910 140169433182208 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.254466 140169433182208 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.254547 140169433182208 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.254910 140169433182208 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.255603 140169433182208 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.258199 140169433182208 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.258819 140169433182208 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.258895 140169433182208 transformer_layer.py:468] tlayer: End windows.
I0123 12:49:00.258931 140169433182208 transformer_layer.py:472] tlayer: final FFN.
I0123 12:49:00.258991 140169433182208 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.259122 140169433182208 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:49:00.259233 140169433182208 nn_components.py:325] mlp: activation = None
I0123 12:49:00.259271 140169433182208 nn_components.py:329] mlp: final activation = None
I0123 12:49:00.261165 140169433182208 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.261259 140169433182208 nn_components.py:261] mlp: residual
I0123 12:49:00.263709 140169433182208 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.263795 140169433182208 transformer_base.py:443] tbase: final FFN
I0123 12:49:00.263904 140169433182208 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:49:00.266534 140169433182208 nn_components.py:329] mlp: final activation = None
I0123 12:49:00.268436 140169433182208 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.268531 140169433182208 nn_components.py:261] mlp: residual
I0123 12:49:00.268820 140169433182208 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.268911 140169433182208 decoder_stack.py:344] dstack: Final layernorm.
I0123 12:49:00.271810 140169433182208 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:49:00.327841 140169433182208 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.327930 140169433182208 decoder_stack.py:333] dstack: autoregressive generator.
I0123 12:49:00.327984 140169433182208 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 12:49:00.328088 140169433182208 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:49:00.328127 140169433182208 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:49:00.328159 140169433182208 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:49:00.328224 140169433182208 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.330593 140169433182208 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:49:00.335980 140169433182208 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.336241 140169433182208 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:49:00.338850 140169433182208 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:49:00.351240 140169433182208 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:49:00.351297 140169433182208 attention.py:418] Single window, no scan.
I0123 12:49:00.351333 140169433182208 transformer_layer.py:389] tlayer: self-attention.
I0123 12:49:00.351365 140169433182208 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.351426 140169433182208 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.351989 140169433182208 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.352066 140169433182208 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.352422 140169433182208 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.353094 140169433182208 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.355616 140169433182208 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.356230 140169433182208 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.356308 140169433182208 transformer_layer.py:468] tlayer: End windows.
I0123 12:49:00.356343 140169433182208 transformer_layer.py:472] tlayer: final FFN.
I0123 12:49:00.356403 140169433182208 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.356535 140169433182208 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:49:00.356654 140169433182208 nn_components.py:325] mlp: activation = None
I0123 12:49:00.356693 140169433182208 nn_components.py:329] mlp: final activation = None
I0123 12:49:00.358556 140169433182208 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.358652 140169433182208 nn_components.py:261] mlp: residual
I0123 12:49:00.361065 140169433182208 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.361144 140169433182208 transformer_base.py:443] tbase: final FFN
I0123 12:49:00.361252 140169433182208 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:49:00.363533 140169433182208 nn_components.py:329] mlp: final activation = None
I0123 12:49:00.365411 140169433182208 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.365507 140169433182208 nn_components.py:261] mlp: residual
I0123 12:49:00.365803 140169433182208 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.365886 140169433182208 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 12:49:00.365993 140169433182208 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:49:00.366032 140169433182208 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:49:00.366062 140169433182208 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:49:00.366127 140169433182208 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.368396 140169433182208 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:49:00.373811 140169433182208 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.374072 140169433182208 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:49:00.376747 140169433182208 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:49:00.389350 140169433182208 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:49:00.389406 140169433182208 attention.py:418] Single window, no scan.
I0123 12:49:00.389443 140169433182208 transformer_layer.py:389] tlayer: self-attention.
I0123 12:49:00.389474 140169433182208 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.389534 140169433182208 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.390097 140169433182208 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.390174 140169433182208 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.390530 140169433182208 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.391207 140169433182208 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.393739 140169433182208 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.394362 140169433182208 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.394440 140169433182208 transformer_layer.py:468] tlayer: End windows.
I0123 12:49:00.394477 140169433182208 transformer_layer.py:472] tlayer: final FFN.
I0123 12:49:00.394538 140169433182208 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.394664 140169433182208 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:49:00.394772 140169433182208 nn_components.py:325] mlp: activation = None
I0123 12:49:00.394821 140169433182208 nn_components.py:329] mlp: final activation = None
I0123 12:49:00.396672 140169433182208 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.396766 140169433182208 nn_components.py:261] mlp: residual
I0123 12:49:00.399182 140169433182208 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.399261 140169433182208 transformer_base.py:443] tbase: final FFN
I0123 12:49:00.399370 140169433182208 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:49:00.401612 140169433182208 nn_components.py:329] mlp: final activation = None
I0123 12:49:00.403467 140169433182208 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.403562 140169433182208 nn_components.py:261] mlp: residual
I0123 12:49:00.403850 140169433182208 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.403931 140169433182208 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 12:49:00.404039 140169433182208 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:49:00.404078 140169433182208 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:49:00.404109 140169433182208 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:49:00.404173 140169433182208 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.406427 140169433182208 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:49:00.411826 140169433182208 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.412086 140169433182208 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:49:00.414795 140169433182208 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:49:00.427159 140169433182208 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:49:00.427217 140169433182208 attention.py:418] Single window, no scan.
I0123 12:49:00.427252 140169433182208 transformer_layer.py:389] tlayer: self-attention.
I0123 12:49:00.427284 140169433182208 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.427344 140169433182208 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.427900 140169433182208 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.427977 140169433182208 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.428339 140169433182208 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.429020 140169433182208 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.431996 140169433182208 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.432615 140169433182208 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.432693 140169433182208 transformer_layer.py:468] tlayer: End windows.
I0123 12:49:00.432728 140169433182208 transformer_layer.py:472] tlayer: final FFN.
I0123 12:49:00.432789 140169433182208 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.432917 140169433182208 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:49:00.433027 140169433182208 nn_components.py:325] mlp: activation = None
I0123 12:49:00.433066 140169433182208 nn_components.py:329] mlp: final activation = None
I0123 12:49:00.434948 140169433182208 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.435043 140169433182208 nn_components.py:261] mlp: residual
I0123 12:49:00.437495 140169433182208 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.437574 140169433182208 transformer_base.py:443] tbase: final FFN
I0123 12:49:00.437692 140169433182208 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:49:00.439949 140169433182208 nn_components.py:329] mlp: final activation = None
I0123 12:49:00.441823 140169433182208 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.441920 140169433182208 nn_components.py:261] mlp: residual
I0123 12:49:00.442208 140169433182208 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.442289 140169433182208 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 12:49:00.442396 140169433182208 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:49:00.442434 140169433182208 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:49:00.442465 140169433182208 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:49:00.442529 140169433182208 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.444772 140169433182208 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:49:00.450406 140169433182208 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.450665 140169433182208 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:49:00.453360 140169433182208 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:49:00.465738 140169433182208 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:49:00.465795 140169433182208 attention.py:418] Single window, no scan.
I0123 12:49:00.465843 140169433182208 transformer_layer.py:389] tlayer: self-attention.
I0123 12:49:00.465883 140169433182208 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.465946 140169433182208 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.466508 140169433182208 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.466583 140169433182208 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.466936 140169433182208 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.467629 140169433182208 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.470253 140169433182208 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.470936 140169433182208 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.471014 140169433182208 transformer_layer.py:468] tlayer: End windows.
I0123 12:49:00.471048 140169433182208 transformer_layer.py:472] tlayer: final FFN.
I0123 12:49:00.471108 140169433182208 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.471234 140169433182208 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:49:00.471353 140169433182208 nn_components.py:325] mlp: activation = None
I0123 12:49:00.471394 140169433182208 nn_components.py:329] mlp: final activation = None
I0123 12:49:00.473296 140169433182208 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.473390 140169433182208 nn_components.py:261] mlp: residual
I0123 12:49:00.475829 140169433182208 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.475908 140169433182208 transformer_base.py:443] tbase: final FFN
I0123 12:49:00.476019 140169433182208 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:49:00.478338 140169433182208 nn_components.py:329] mlp: final activation = None
I0123 12:49:00.480214 140169433182208 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.480309 140169433182208 nn_components.py:261] mlp: residual
I0123 12:49:00.480594 140169433182208 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.480674 140169433182208 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 12:49:00.480781 140169433182208 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:49:00.480818 140169433182208 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:49:00.480848 140169433182208 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:49:00.480912 140169433182208 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.483170 140169433182208 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:49:00.488611 140169433182208 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.488874 140169433182208 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:49:00.491601 140169433182208 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:49:00.504290 140169433182208 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:49:00.504349 140169433182208 attention.py:418] Single window, no scan.
I0123 12:49:00.504385 140169433182208 transformer_layer.py:389] tlayer: self-attention.
I0123 12:49:00.504416 140169433182208 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.504477 140169433182208 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.505082 140169433182208 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.505157 140169433182208 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.505522 140169433182208 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.506230 140169433182208 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.508841 140169433182208 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.509478 140169433182208 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.509557 140169433182208 transformer_layer.py:468] tlayer: End windows.
I0123 12:49:00.509592 140169433182208 transformer_layer.py:472] tlayer: final FFN.
I0123 12:49:00.509662 140169433182208 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.509797 140169433182208 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:49:00.509909 140169433182208 nn_components.py:325] mlp: activation = None
I0123 12:49:00.509946 140169433182208 nn_components.py:329] mlp: final activation = None
I0123 12:49:00.511826 140169433182208 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.511929 140169433182208 nn_components.py:261] mlp: residual
I0123 12:49:00.514390 140169433182208 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.514469 140169433182208 transformer_base.py:443] tbase: final FFN
I0123 12:49:00.514580 140169433182208 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:49:00.516868 140169433182208 nn_components.py:329] mlp: final activation = None
I0123 12:49:00.518752 140169433182208 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.518846 140169433182208 nn_components.py:261] mlp: residual
I0123 12:49:00.519134 140169433182208 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.519214 140169433182208 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 12:49:00.519321 140169433182208 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:49:00.519358 140169433182208 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:49:00.519388 140169433182208 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:49:00.519453 140169433182208 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.521711 140169433182208 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:49:00.527233 140169433182208 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.527495 140169433182208 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:49:00.530235 140169433182208 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:49:00.542852 140169433182208 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:49:00.542907 140169433182208 attention.py:418] Single window, no scan.
I0123 12:49:00.542942 140169433182208 transformer_layer.py:389] tlayer: self-attention.
I0123 12:49:00.542973 140169433182208 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.543038 140169433182208 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.543614 140169433182208 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.543688 140169433182208 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.544043 140169433182208 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.544736 140169433182208 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.548079 140169433182208 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.548710 140169433182208 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.548785 140169433182208 transformer_layer.py:468] tlayer: End windows.
I0123 12:49:00.548819 140169433182208 transformer_layer.py:472] tlayer: final FFN.
I0123 12:49:00.548878 140169433182208 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.549003 140169433182208 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:49:00.549111 140169433182208 nn_components.py:325] mlp: activation = None
I0123 12:49:00.549148 140169433182208 nn_components.py:329] mlp: final activation = None
I0123 12:49:00.551184 140169433182208 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.551280 140169433182208 nn_components.py:261] mlp: residual
I0123 12:49:00.553689 140169433182208 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.553774 140169433182208 transformer_base.py:443] tbase: final FFN
I0123 12:49:00.553881 140169433182208 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:49:00.556175 140169433182208 nn_components.py:329] mlp: final activation = None
I0123 12:49:00.558086 140169433182208 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.558179 140169433182208 nn_components.py:261] mlp: residual
I0123 12:49:00.558465 140169433182208 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.558545 140169433182208 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 12:49:00.558653 140169433182208 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:49:00.558690 140169433182208 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:49:00.558720 140169433182208 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:49:00.558783 140169433182208 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.561019 140169433182208 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:49:00.566469 140169433182208 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.566726 140169433182208 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:49:00.569419 140169433182208 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:49:00.581992 140169433182208 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:49:00.582046 140169433182208 attention.py:418] Single window, no scan.
I0123 12:49:00.582081 140169433182208 transformer_layer.py:389] tlayer: self-attention.
I0123 12:49:00.582112 140169433182208 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.582171 140169433182208 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.582726 140169433182208 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.582800 140169433182208 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.583159 140169433182208 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.583843 140169433182208 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.586421 140169433182208 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.587057 140169433182208 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.587133 140169433182208 transformer_layer.py:468] tlayer: End windows.
I0123 12:49:00.587167 140169433182208 transformer_layer.py:472] tlayer: final FFN.
I0123 12:49:00.587225 140169433182208 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.587350 140169433182208 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:49:00.587457 140169433182208 nn_components.py:325] mlp: activation = None
I0123 12:49:00.587493 140169433182208 nn_components.py:329] mlp: final activation = None
I0123 12:49:00.589385 140169433182208 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.589476 140169433182208 nn_components.py:261] mlp: residual
I0123 12:49:00.591893 140169433182208 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.591972 140169433182208 transformer_base.py:443] tbase: final FFN
I0123 12:49:00.592080 140169433182208 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:49:00.594384 140169433182208 nn_components.py:329] mlp: final activation = None
I0123 12:49:00.596262 140169433182208 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.596357 140169433182208 nn_components.py:261] mlp: residual
I0123 12:49:00.596647 140169433182208 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.596728 140169433182208 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 12:49:00.596837 140169433182208 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:49:00.596875 140169433182208 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:49:00.596905 140169433182208 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:49:00.596970 140169433182208 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.599217 140169433182208 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:49:00.604671 140169433182208 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.604929 140169433182208 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:49:00.607628 140169433182208 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:49:00.620151 140169433182208 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:49:00.620205 140169433182208 attention.py:418] Single window, no scan.
I0123 12:49:00.620240 140169433182208 transformer_layer.py:389] tlayer: self-attention.
I0123 12:49:00.620269 140169433182208 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.620329 140169433182208 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.620889 140169433182208 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.620963 140169433182208 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.621320 140169433182208 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.622013 140169433182208 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.624566 140169433182208 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.625190 140169433182208 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.625266 140169433182208 transformer_layer.py:468] tlayer: End windows.
I0123 12:49:00.625300 140169433182208 transformer_layer.py:472] tlayer: final FFN.
I0123 12:49:00.625358 140169433182208 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.625484 140169433182208 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:49:00.625590 140169433182208 nn_components.py:325] mlp: activation = None
I0123 12:49:00.625628 140169433182208 nn_components.py:329] mlp: final activation = None
I0123 12:49:00.627490 140169433182208 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.627581 140169433182208 nn_components.py:261] mlp: residual
I0123 12:49:00.629996 140169433182208 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.630079 140169433182208 transformer_base.py:443] tbase: final FFN
I0123 12:49:00.630188 140169433182208 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:49:00.632473 140169433182208 nn_components.py:329] mlp: final activation = None
I0123 12:49:00.634331 140169433182208 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.634424 140169433182208 nn_components.py:261] mlp: residual
I0123 12:49:00.634708 140169433182208 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.634788 140169433182208 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 12:49:00.634896 140169433182208 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:49:00.634933 140169433182208 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:49:00.634964 140169433182208 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:49:00.635027 140169433182208 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.637266 140169433182208 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:49:00.642695 140169433182208 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.642951 140169433182208 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:49:00.645670 140169433182208 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:49:00.658521 140169433182208 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:49:00.658576 140169433182208 attention.py:418] Single window, no scan.
I0123 12:49:00.658611 140169433182208 transformer_layer.py:389] tlayer: self-attention.
I0123 12:49:00.658641 140169433182208 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.658701 140169433182208 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.659260 140169433182208 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.659335 140169433182208 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.659696 140169433182208 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.660382 140169433182208 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.663345 140169433182208 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.663970 140169433182208 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.664046 140169433182208 transformer_layer.py:468] tlayer: End windows.
I0123 12:49:00.664081 140169433182208 transformer_layer.py:472] tlayer: final FFN.
I0123 12:49:00.664138 140169433182208 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.664264 140169433182208 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:49:00.664376 140169433182208 nn_components.py:325] mlp: activation = None
I0123 12:49:00.664414 140169433182208 nn_components.py:329] mlp: final activation = None
I0123 12:49:00.666314 140169433182208 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.666407 140169433182208 nn_components.py:261] mlp: residual
I0123 12:49:00.668822 140169433182208 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.668905 140169433182208 transformer_base.py:443] tbase: final FFN
I0123 12:49:00.669014 140169433182208 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:49:00.671307 140169433182208 nn_components.py:329] mlp: final activation = None
I0123 12:49:00.673179 140169433182208 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.673272 140169433182208 nn_components.py:261] mlp: residual
I0123 12:49:00.673562 140169433182208 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.673647 140169433182208 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 12:49:00.673757 140169433182208 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:49:00.673795 140169433182208 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:49:00.673825 140169433182208 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:49:00.673890 140169433182208 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.676149 140169433182208 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:49:00.681583 140169433182208 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.681847 140169433182208 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:49:00.684560 140169433182208 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:49:00.697112 140169433182208 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:49:00.697167 140169433182208 attention.py:418] Single window, no scan.
I0123 12:49:00.697203 140169433182208 transformer_layer.py:389] tlayer: self-attention.
I0123 12:49:00.697232 140169433182208 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.697292 140169433182208 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.697857 140169433182208 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.697932 140169433182208 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.698287 140169433182208 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.698978 140169433182208 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.701529 140169433182208 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.702154 140169433182208 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.702230 140169433182208 transformer_layer.py:468] tlayer: End windows.
I0123 12:49:00.702264 140169433182208 transformer_layer.py:472] tlayer: final FFN.
I0123 12:49:00.702322 140169433182208 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.702445 140169433182208 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:49:00.702553 140169433182208 nn_components.py:325] mlp: activation = None
I0123 12:49:00.702591 140169433182208 nn_components.py:329] mlp: final activation = None
I0123 12:49:00.705236 140169433182208 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.705328 140169433182208 nn_components.py:261] mlp: residual
I0123 12:49:00.707756 140169433182208 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.707835 140169433182208 transformer_base.py:443] tbase: final FFN
I0123 12:49:00.707947 140169433182208 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:49:00.710189 140169433182208 nn_components.py:329] mlp: final activation = None
I0123 12:49:00.712025 140169433182208 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.712117 140169433182208 nn_components.py:261] mlp: residual
I0123 12:49:00.712404 140169433182208 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.712484 140169433182208 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 12:49:00.712589 140169433182208 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:49:00.712627 140169433182208 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:49:00.712656 140169433182208 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:49:00.712719 140169433182208 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.714956 140169433182208 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:49:00.720354 140169433182208 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.720610 140169433182208 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:49:00.723289 140169433182208 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:49:00.735737 140169433182208 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:49:00.735791 140169433182208 attention.py:418] Single window, no scan.
I0123 12:49:00.735828 140169433182208 transformer_layer.py:389] tlayer: self-attention.
I0123 12:49:00.735858 140169433182208 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.735920 140169433182208 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.736469 140169433182208 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.736543 140169433182208 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.736901 140169433182208 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.737583 140169433182208 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.740132 140169433182208 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.740750 140169433182208 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.740825 140169433182208 transformer_layer.py:468] tlayer: End windows.
I0123 12:49:00.740858 140169433182208 transformer_layer.py:472] tlayer: final FFN.
I0123 12:49:00.740916 140169433182208 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.741040 140169433182208 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:49:00.741146 140169433182208 nn_components.py:325] mlp: activation = None
I0123 12:49:00.741183 140169433182208 nn_components.py:329] mlp: final activation = None
I0123 12:49:00.743067 140169433182208 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.743158 140169433182208 nn_components.py:261] mlp: residual
I0123 12:49:00.745548 140169433182208 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.745624 140169433182208 transformer_base.py:443] tbase: final FFN
I0123 12:49:00.745737 140169433182208 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:49:00.748004 140169433182208 nn_components.py:329] mlp: final activation = None
I0123 12:49:00.749870 140169433182208 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.749963 140169433182208 nn_components.py:261] mlp: residual
I0123 12:49:00.750372 140169433182208 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.750451 140169433182208 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 12:49:00.750558 140169433182208 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:49:00.750596 140169433182208 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:49:00.750625 140169433182208 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:49:00.750688 140169433182208 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.752933 140169433182208 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:49:00.758333 140169433182208 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.758588 140169433182208 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:49:00.761261 140169433182208 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:49:00.773700 140169433182208 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:49:00.773755 140169433182208 attention.py:418] Single window, no scan.
I0123 12:49:00.773789 140169433182208 transformer_layer.py:389] tlayer: self-attention.
I0123 12:49:00.773819 140169433182208 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.773879 140169433182208 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.774425 140169433182208 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.774498 140169433182208 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.774857 140169433182208 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.775549 140169433182208 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.778447 140169433182208 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.779062 140169433182208 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.779137 140169433182208 transformer_layer.py:468] tlayer: End windows.
I0123 12:49:00.779170 140169433182208 transformer_layer.py:472] tlayer: final FFN.
I0123 12:49:00.779228 140169433182208 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.779353 140169433182208 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:49:00.779461 140169433182208 nn_components.py:325] mlp: activation = None
I0123 12:49:00.779499 140169433182208 nn_components.py:329] mlp: final activation = None
I0123 12:49:00.781378 140169433182208 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.781469 140169433182208 nn_components.py:261] mlp: residual
I0123 12:49:00.783874 140169433182208 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.783953 140169433182208 transformer_base.py:443] tbase: final FFN
I0123 12:49:00.784060 140169433182208 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:49:00.786335 140169433182208 nn_components.py:329] mlp: final activation = None
I0123 12:49:00.788213 140169433182208 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.788305 140169433182208 nn_components.py:261] mlp: residual
I0123 12:49:00.788590 140169433182208 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:00.788677 140169433182208 decoder_stack.py:344] dstack: Final layernorm.
I0123 12:49:00.791512 140169433182208 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:49:05.205419 140169433182208 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
I0123 12:49:05.729784 140169433182208 training_loop.py:409] No working directory specified.
I0123 12:49:05.729928 140169433182208 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0123 12:49:05.730757 140169433182208 checkpoints.py:1062] Restoring legacy Flax checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0123 12:49:08.966922 140169433182208 training_loop.py:447] Only restoring trainable parameters.
I0123 12:49:08.967632 140169433182208 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0123 12:49:08.967711 140169433182208 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0123 12:49:08.967763 140169433182208 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:49:08.967808 140169433182208 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:49:08.967849 140169433182208 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:49:08.967889 140169433182208 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:49:08.967928 140169433182208 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:49:08.967968 140169433182208 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:49:08.968007 140169433182208 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:49:08.968044 140169433182208 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:49:08.968081 140169433182208 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:49:08.968117 140169433182208 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:49:08.968155 140169433182208 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:49:08.968192 140169433182208 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:49:08.968228 140169433182208 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:49:08.968263 140169433182208 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:49:08.968299 140169433182208 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:49:08.968334 140169433182208 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:49:08.968370 140169433182208 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:49:08.968404 140169433182208 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:49:08.968459 140169433182208 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:49:08.968497 140169433182208 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:49:08.968533 140169433182208 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:49:08.968569 140169433182208 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:49:08.968605 140169433182208 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:49:08.968641 140169433182208 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:49:08.968677 140169433182208 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:49:08.968713 140169433182208 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:49:08.968747 140169433182208 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:49:08.968782 140169433182208 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:49:08.968817 140169433182208 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:49:08.968852 140169433182208 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:49:08.968888 140169433182208 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:49:08.968922 140169433182208 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:49:08.968957 140169433182208 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:49:08.968991 140169433182208 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:49:08.969026 140169433182208 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:49:08.969060 140169433182208 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:49:08.969096 140169433182208 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:49:08.969131 140169433182208 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:49:08.969165 140169433182208 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:49:08.969200 140169433182208 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:49:08.969235 140169433182208 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:49:08.969270 140169433182208 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:49:08.969305 140169433182208 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:49:08.969339 140169433182208 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:49:08.969380 140169433182208 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:49:08.969417 140169433182208 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:49:08.969452 140169433182208 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:49:08.969487 140169433182208 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:49:08.969522 140169433182208 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:49:08.969557 140169433182208 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:49:08.969592 140169433182208 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:49:08.969627 140169433182208 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:49:08.969677 140169433182208 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:49:08.969716 140169433182208 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:49:08.969757 140169433182208 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:49:08.969792 140169433182208 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:49:08.969827 140169433182208 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:49:08.969861 140169433182208 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:49:08.969896 140169433182208 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:49:08.969930 140169433182208 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:49:08.969965 140169433182208 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:49:08.970000 140169433182208 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:49:08.970035 140169433182208 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:49:08.970070 140169433182208 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:49:08.970105 140169433182208 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:49:08.970140 140169433182208 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:49:08.970175 140169433182208 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:49:08.970210 140169433182208 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:49:08.970244 140169433182208 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:49:08.970279 140169433182208 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:49:08.970314 140169433182208 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:49:08.970355 140169433182208 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:49:08.970390 140169433182208 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:49:08.970425 140169433182208 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:49:08.970459 140169433182208 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:49:08.970494 140169433182208 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:49:08.970528 140169433182208 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:49:08.970561 140169433182208 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:49:08.970595 140169433182208 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:49:08.970630 140169433182208 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:49:08.970664 140169433182208 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:49:08.970699 140169433182208 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:49:08.970733 140169433182208 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:49:08.970768 140169433182208 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:49:08.970804 140169433182208 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:49:08.970839 140169433182208 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:49:08.970874 140169433182208 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:49:08.970908 140169433182208 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:49:08.970942 140169433182208 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:49:08.970976 140169433182208 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:49:08.971011 140169433182208 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:49:08.971046 140169433182208 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:49:08.971080 140169433182208 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:49:08.971115 140169433182208 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:49:08.971150 140169433182208 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:49:08.971184 140169433182208 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:49:08.971218 140169433182208 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:49:08.971252 140169433182208 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:49:08.971292 140169433182208 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:49:08.971328 140169433182208 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:49:08.971364 140169433182208 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:49:08.971399 140169433182208 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:49:08.971435 140169433182208 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:49:08.971472 140169433182208 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:49:08.971508 140169433182208 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:49:08.971544 140169433182208 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:49:08.971579 140169433182208 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:49:08.971615 140169433182208 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:49:08.971650 140169433182208 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:49:08.971685 140169433182208 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:49:08.971720 140169433182208 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:49:08.971755 140169433182208 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:49:08.971791 140169433182208 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:49:08.971826 140169433182208 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:49:08.971862 140169433182208 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:49:08.971897 140169433182208 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:49:08.971932 140169433182208 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:49:08.971968 140169433182208 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:49:08.972002 140169433182208 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:49:08.972037 140169433182208 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:49:08.972065 140169433182208 training_loop.py:725] Total parameters: 152072288
I0123 12:49:08.972292 140169433182208 training_loop.py:739] Total state size: 0
I0123 12:49:08.999047 140169433182208 training_loop.py:492] Training loop: creating task for mode beam_search
I0123 12:49:08.999294 140169433182208 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0123 12:49:08.999676 140169433182208 training_loop.py:652] Compiling mode beam_search with jit.
I0123 12:49:09.000038 140169433182208 training_loop.py:89] registering functions: dict_keys([])
I0123 12:49:09.020672 140169433182208 graph.py:499] a b c = triangle a b c; d = circle d a c b; e = on_circle e d b; f = on_line f a b, on_line f c e; g = on_circle g d b; h = circle h f c g; i = on_circle i d g; j = circle j f a i; k = foot k f j h; l = mirror l f k; m = on_line m b g, on_line m e i ? coll f l m
I0123 12:49:11.177440 140169433182208 ddar.py:60] Depth 1/1000 time = 2.092899799346924
I0123 12:49:17.131989 140169433182208 ddar.py:60] Depth 2/1000 time = 5.954336881637573
I0123 12:49:24.219973 140169433182208 ddar.py:60] Depth 3/1000 time = 7.087701797485352
I0123 12:49:31.163412 140169433182208 ddar.py:60] Depth 4/1000 time = 6.942862033843994
I0123 12:49:38.255050 140169433182208 ddar.py:60] Depth 5/1000 time = 7.091372966766357
I0123 12:49:45.559860 140169433182208 ddar.py:60] Depth 6/1000 time = 7.304532766342163
I0123 12:49:52.586293 140169433182208 ddar.py:60] Depth 7/1000 time = 7.026118516921997
I0123 12:49:52.597002 140169433182208 alphageometry.py:191] 
==========================
 * From theorem premises:
A B C D E F G H I J K L M : Points
DA = DC [00]
DC = DB [01]
DE = DB [02]
F,E,C are collinear [03]
F,B,A are collinear [04]
DG = DB [05]
HC = HG [06]
HF = HC [07]
DI = DG [08]
JF = JA [09]
JA = JI [10]
K,J,H are collinear [11]
FK  HJ [12]
F,K,L are collinear [13]
KF = KL [14]
B,G,M are collinear [15]
E,I,M are collinear [16]

 * Auxiliary Constructions:
: Points


 * Proof steps:
001. DA = DC [00] & DC = DB [01] & DE = DB [02] & DG = DB [05]   B,G,C,E are concyclic [17]
002. E,C,B,G are concyclic [17]   GCE = GBE [18]
003. F,K,L are collinear [13] & KF = KL [14]   K is midpoint of FL [19]
004. K,J,H are collinear [11] & F,K,L are collinear [13] & HJ  FK [12]   HK  FL [20]
005. K is midpoint of FL [19] & HK  FL [20]   HF = HL [21]
006. HC = HG [06] & HF = HC [07] & HF = HL [21]   F,G,L,C are concyclic [22]
007. F,G,L,C are concyclic [22]   FLG = FCG [23]
008. GCE = GBE [18] & FLG = FCG [23] & F,K,L are collinear [13] & F,E,C are collinear [03]   (GL-FK) = GBE [24]
009. DG = DB [05] & DI = DG [08] & DC = DB [01] & DA = DC [00] & B,G,C,E are concyclic [17]   I,E,B,A are concyclic [25]
010. I,E,B,A are concyclic [25]   BEI = BAI [26]
011. K,J,H are collinear [11] & F,K,L are collinear [13] & HJ  FK [12]   JK  FL [27]
012. K is midpoint of FL [19] & JK  FL [27]   JF = JL [28]
013. JF = JA [09] & JA = JI [10] & JF = JL [28]   F,L,I,A are concyclic [29]
014. F,L,I,A are concyclic [29]   FLI = FAI [30]
015. BEI = BAI [26] & FLI = FAI [30] & F,K,L are collinear [13] & F,B,A are collinear [04]   (FK-LI) = BEI [31]
016. (GL-FK) = GBE [24] & (FK-LI) = BEI [31]   GLI = (BG-EI) [32]
017. B,G,M are collinear [15] & E,I,M are collinear [16] & GLI = (BG-EI) [32]   GLI = GMI [33]
018. GLI = GMI [33]   L,G,I,M are concyclic [34]
019. L,G,I,M are concyclic [34]   LGI = LMI [35]
020. DG = DB [05] & DI = DG [08] & DC = DB [01] & DA = DC [00] & B,G,C,E are concyclic [17]   I,E,C,G are concyclic [36]
021. I,E,C,G are concyclic [36]   CEI = CGI [37]
022. F,E,C are collinear [03] & CGI = CEI [37]   CGI = FEI [38]
023. F,E,C are collinear [03] & FLG = FCG [23] & F,K,L are collinear [13]   CGL = EFK [39]
024. CGI = FEI [38] & CGL = EFK [39]   EIG = (FK-GL) [40]
025. LGI = LMI [35] & E,I,M are collinear [16] & EIG = (FK-GL) [40]   (FK-GL) = MLG [41]
026. (FK-GL) = MLG [41]   FK  LM [42]
027. F,K,L are collinear [13] & FK  LM [42]   LF  LM [43]
028. LF  LM [43]   F,L,M are collinear
==========================

