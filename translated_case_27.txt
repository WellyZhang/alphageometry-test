I0123 11:01:33.721318 139956176916480 inference_utils.py:69] Parsing gin configuration.
I0123 11:01:33.721557 139956176916480 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0123 11:01:33.722015 139956176916480 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0123 11:01:33.722061 139956176916480 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0123 11:01:33.722092 139956176916480 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0123 11:01:33.722119 139956176916480 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0123 11:01:33.722147 139956176916480 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0123 11:01:33.722174 139956176916480 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0123 11:01:33.722202 139956176916480 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0123 11:01:33.722229 139956176916480 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=32
I0123 11:01:33.722255 139956176916480 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0123 11:01:33.722283 139956176916480 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0123 11:01:33.722354 139956176916480 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0123 11:01:33.722601 139956176916480 resource_reader.py:55] Path not found: base_htrans.gin
I0123 11:01:33.722993 139956176916480 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0123 11:01:33.723124 139956176916480 resource_reader.py:55] Path not found: trainer_configuration.gin
I0123 11:01:33.730202 139956176916480 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0123 11:01:33.730354 139956176916480 resource_reader.py:55] Path not found: size/medium_150M.gin
I0123 11:01:33.730680 139956176916480 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0123 11:01:33.730795 139956176916480 resource_reader.py:55] Path not found: options/positions_t5.gin
I0123 11:01:33.731075 139956176916480 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0123 11:01:33.731183 139956176916480 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0123 11:01:33.731592 139956176916480 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0123 11:01:33.731700 139956176916480 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0123 11:01:33.736018 139956176916480 training_loop.py:334] ==== Training loop: initializing model ====
I0123 11:01:33.836426 139956176916480 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0123 11:01:33.837480 139956176916480 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0123 11:01:33.844388 139956176916480 training_loop.py:335] Process 0 of 1
I0123 11:01:33.844457 139956176916480 training_loop.py:336] Local device count = 1
I0123 11:01:33.844500 139956176916480 training_loop.py:337] Number of replicas = 1
I0123 11:01:33.844533 139956176916480 training_loop.py:339] Using random number seed 42
I0123 11:01:34.379088 139956176916480 training_loop.py:359] Initializing the model.
I0123 11:01:34.773683 139956176916480 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.774428 139956176916480 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 11:01:34.774548 139956176916480 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:01:34.774631 139956176916480 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:01:34.774711 139956176916480 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:01:34.774799 139956176916480 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:01:34.774875 139956176916480 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:01:34.774947 139956176916480 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:01:34.775024 139956176916480 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:01:34.775102 139956176916480 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:01:34.775176 139956176916480 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:01:34.775253 139956176916480 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:01:34.775326 139956176916480 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:01:34.775402 139956176916480 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:01:34.775451 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:34.775501 139956176916480 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 11:01:34.775624 139956176916480 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:34.775670 139956176916480 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:34.775702 139956176916480 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:34.777783 139956176916480 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.783395 139956176916480 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:34.794681 139956176916480 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.794993 139956176916480 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:34.799432 139956176916480 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:01:34.810450 139956176916480 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:34.810515 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:34.810560 139956176916480 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:34.810595 139956176916480 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.810658 139956176916480 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.811971 139956176916480 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.812056 139956176916480 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.812781 139956176916480 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.815280 139956176916480 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.821022 139956176916480 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.822759 139956176916480 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.822847 139956176916480 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:34.822884 139956176916480 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:34.822947 139956176916480 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.823088 139956176916480 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:34.823447 139956176916480 nn_components.py:325] mlp: activation = None
I0123 11:01:34.823501 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:34.825421 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.825529 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:34.828427 139956176916480 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.828513 139956176916480 transformer_base.py:443] tbase: final FFN
I0123 11:01:34.829017 139956176916480 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:34.839282 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:34.848189 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.848295 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:34.848603 139956176916480 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.848699 139956176916480 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 11:01:34.848815 139956176916480 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:34.848858 139956176916480 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:34.848890 139956176916480 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:34.850760 139956176916480 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.853256 139956176916480 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:34.858879 139956176916480 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.859164 139956176916480 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:34.861832 139956176916480 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:01:34.865677 139956176916480 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:34.865741 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:34.865779 139956176916480 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:34.865814 139956176916480 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.865886 139956176916480 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.866472 139956176916480 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.866556 139956176916480 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.866938 139956176916480 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.867749 139956176916480 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.870281 139956176916480 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.870909 139956176916480 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.870993 139956176916480 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:34.871030 139956176916480 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:34.871087 139956176916480 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.871219 139956176916480 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:34.871555 139956176916480 nn_components.py:325] mlp: activation = None
I0123 11:01:34.871603 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:34.873521 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.873619 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:34.876132 139956176916480 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.876222 139956176916480 transformer_base.py:443] tbase: final FFN
I0123 11:01:34.876655 139956176916480 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:34.878993 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:34.880872 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.880973 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:34.881270 139956176916480 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.881358 139956176916480 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 11:01:34.881470 139956176916480 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:34.881512 139956176916480 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:34.881543 139956176916480 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:34.883478 139956176916480 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.885868 139956176916480 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:34.892009 139956176916480 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.892281 139956176916480 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:34.894947 139956176916480 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:01:34.898951 139956176916480 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:34.899013 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:34.899050 139956176916480 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:34.899081 139956176916480 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.899146 139956176916480 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.899711 139956176916480 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.899793 139956176916480 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.900161 139956176916480 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.900944 139956176916480 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.903508 139956176916480 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.904177 139956176916480 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.904260 139956176916480 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:34.904297 139956176916480 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:34.904356 139956176916480 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.904490 139956176916480 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:34.904812 139956176916480 nn_components.py:325] mlp: activation = None
I0123 11:01:34.904861 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:34.906772 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.906871 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:34.909377 139956176916480 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.909469 139956176916480 transformer_base.py:443] tbase: final FFN
I0123 11:01:34.909973 139956176916480 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:34.912264 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:34.914190 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.914294 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:34.914591 139956176916480 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.914678 139956176916480 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 11:01:34.914789 139956176916480 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:34.914831 139956176916480 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:34.914863 139956176916480 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:34.916756 139956176916480 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.919151 139956176916480 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:34.924803 139956176916480 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.925076 139956176916480 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:34.927758 139956176916480 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:01:34.931576 139956176916480 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:34.931638 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:34.931676 139956176916480 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:34.931708 139956176916480 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.931773 139956176916480 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.932339 139956176916480 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.932425 139956176916480 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.932811 139956176916480 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.933589 139956176916480 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.936158 139956176916480 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.936790 139956176916480 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.936877 139956176916480 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:34.936915 139956176916480 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:34.936976 139956176916480 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.937109 139956176916480 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:34.937441 139956176916480 nn_components.py:325] mlp: activation = None
I0123 11:01:34.937491 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:34.939408 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.939509 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:34.942072 139956176916480 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.942166 139956176916480 transformer_base.py:443] tbase: final FFN
I0123 11:01:34.942617 139956176916480 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:34.944880 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:34.946793 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.946896 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:34.947192 139956176916480 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.947278 139956176916480 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 11:01:34.947390 139956176916480 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:34.947434 139956176916480 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:34.947467 139956176916480 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:34.949375 139956176916480 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.951787 139956176916480 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:34.957493 139956176916480 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.957772 139956176916480 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:34.960490 139956176916480 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:01:34.964282 139956176916480 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:34.964344 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:34.964382 139956176916480 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:34.964413 139956176916480 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.964477 139956176916480 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.965045 139956176916480 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.965128 139956176916480 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.965496 139956176916480 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.966289 139956176916480 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.969187 139956176916480 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.969828 139956176916480 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.969916 139956176916480 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:34.969953 139956176916480 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:34.970012 139956176916480 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.970154 139956176916480 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:34.970477 139956176916480 nn_components.py:325] mlp: activation = None
I0123 11:01:34.970527 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:34.972442 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.972544 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:34.975142 139956176916480 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.975229 139956176916480 transformer_base.py:443] tbase: final FFN
I0123 11:01:34.975664 139956176916480 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:34.977988 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:34.979977 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.980079 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:34.980378 139956176916480 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.980469 139956176916480 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 11:01:34.980583 139956176916480 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:34.980626 139956176916480 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:34.980659 139956176916480 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:34.982522 139956176916480 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.984939 139956176916480 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:34.990582 139956176916480 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.990851 139956176916480 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:34.993559 139956176916480 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:01:34.997298 139956176916480 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:34.997359 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:34.997397 139956176916480 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:34.997429 139956176916480 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.997493 139956176916480 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.998106 139956176916480 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.998261 139956176916480 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.998629 139956176916480 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:34.999580 139956176916480 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.002087 139956176916480 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.002714 139956176916480 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.002797 139956176916480 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:35.002834 139956176916480 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:35.002894 139956176916480 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.003023 139956176916480 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:35.003342 139956176916480 nn_components.py:325] mlp: activation = None
I0123 11:01:35.003391 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:35.005307 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.005406 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:35.007982 139956176916480 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.008070 139956176916480 transformer_base.py:443] tbase: final FFN
I0123 11:01:35.008505 139956176916480 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:35.010831 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:35.012768 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.012871 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:35.013168 139956176916480 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.013255 139956176916480 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 11:01:35.013369 139956176916480 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:35.013412 139956176916480 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:35.013443 139956176916480 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:35.015283 139956176916480 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.017750 139956176916480 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:35.023333 139956176916480 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.023606 139956176916480 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:35.026292 139956176916480 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:01:35.030076 139956176916480 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:35.030138 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:35.030176 139956176916480 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:35.030209 139956176916480 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.030273 139956176916480 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.030841 139956176916480 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.030924 139956176916480 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.031286 139956176916480 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.032072 139956176916480 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.034562 139956176916480 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.035194 139956176916480 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.035280 139956176916480 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:35.035318 139956176916480 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:35.035377 139956176916480 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.035507 139956176916480 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:35.035826 139956176916480 nn_components.py:325] mlp: activation = None
I0123 11:01:35.035874 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:35.037843 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.037943 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:35.040431 139956176916480 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.040518 139956176916480 transformer_base.py:443] tbase: final FFN
I0123 11:01:35.040947 139956176916480 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:35.043581 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:35.045492 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.045604 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:35.045914 139956176916480 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.046003 139956176916480 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 11:01:35.046115 139956176916480 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:35.046157 139956176916480 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:35.046189 139956176916480 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:35.312246 139956176916480 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.316693 139956176916480 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:35.322995 139956176916480 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.323408 139956176916480 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:35.326267 139956176916480 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:01:35.330456 139956176916480 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:35.330530 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:35.330575 139956176916480 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:35.330611 139956176916480 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.330699 139956176916480 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.331433 139956176916480 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.331518 139956176916480 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.331891 139956176916480 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.332708 139956176916480 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.335351 139956176916480 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.336036 139956176916480 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.336127 139956176916480 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:35.336166 139956176916480 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:35.336233 139956176916480 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.336371 139956176916480 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:35.336761 139956176916480 nn_components.py:325] mlp: activation = None
I0123 11:01:35.336811 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:35.338766 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.338873 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:35.341481 139956176916480 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.341568 139956176916480 transformer_base.py:443] tbase: final FFN
I0123 11:01:35.342054 139956176916480 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:35.344493 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:35.346429 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.346558 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:35.346862 139956176916480 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.346963 139956176916480 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 11:01:35.347079 139956176916480 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:35.347121 139956176916480 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:35.347153 139956176916480 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:35.349189 139956176916480 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.351639 139956176916480 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:35.357318 139956176916480 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.357591 139956176916480 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:35.360371 139956176916480 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:01:35.364277 139956176916480 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:35.364341 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:35.364382 139956176916480 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:35.364415 139956176916480 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.364479 139956176916480 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.365069 139956176916480 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.365153 139956176916480 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.365524 139956176916480 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.366338 139956176916480 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.368944 139956176916480 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.369579 139956176916480 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.369674 139956176916480 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:35.369714 139956176916480 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:35.369778 139956176916480 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.369911 139956176916480 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:35.370244 139956176916480 nn_components.py:325] mlp: activation = None
I0123 11:01:35.370293 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:35.372209 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.372308 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:35.374901 139956176916480 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.374988 139956176916480 transformer_base.py:443] tbase: final FFN
I0123 11:01:35.375440 139956176916480 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:35.377785 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:35.379780 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.379884 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:35.380178 139956176916480 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.380921 139956176916480 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 11:01:35.381039 139956176916480 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:35.381082 139956176916480 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:35.381114 139956176916480 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:35.382998 139956176916480 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.385476 139956176916480 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:35.391052 139956176916480 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.391325 139956176916480 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:35.394402 139956176916480 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:01:35.398186 139956176916480 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:35.398249 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:35.398291 139956176916480 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:35.398324 139956176916480 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.398387 139956176916480 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.398998 139956176916480 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.399085 139956176916480 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.399456 139956176916480 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.400235 139956176916480 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.402753 139956176916480 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.403388 139956176916480 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.403473 139956176916480 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:35.403512 139956176916480 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:35.403574 139956176916480 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.403707 139956176916480 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:35.404032 139956176916480 nn_components.py:325] mlp: activation = None
I0123 11:01:35.404081 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:35.406005 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.406108 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:35.408693 139956176916480 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.408781 139956176916480 transformer_base.py:443] tbase: final FFN
I0123 11:01:35.409220 139956176916480 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:35.411555 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:35.413502 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.413605 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:35.413915 139956176916480 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.414012 139956176916480 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 11:01:35.414128 139956176916480 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:35.414170 139956176916480 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:35.414202 139956176916480 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:35.416037 139956176916480 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.418531 139956176916480 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:35.424186 139956176916480 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.424459 139956176916480 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:35.427121 139956176916480 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:01:35.430906 139956176916480 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:35.430968 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:35.431007 139956176916480 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:35.431039 139956176916480 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.431102 139956176916480 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.431674 139956176916480 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.431756 139956176916480 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.432126 139956176916480 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.432910 139956176916480 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.435449 139956176916480 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.436079 139956176916480 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.436163 139956176916480 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:35.436199 139956176916480 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:35.436258 139956176916480 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.436386 139956176916480 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:35.436702 139956176916480 nn_components.py:325] mlp: activation = None
I0123 11:01:35.436751 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:35.438717 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.438819 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:35.441733 139956176916480 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.441822 139956176916480 transformer_base.py:443] tbase: final FFN
I0123 11:01:35.442250 139956176916480 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:35.444561 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:35.446480 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.446581 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:35.446878 139956176916480 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.446966 139956176916480 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 11:01:35.447090 139956176916480 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:35.447134 139956176916480 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:35.447165 139956176916480 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:35.449058 139956176916480 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.451462 139956176916480 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:35.457081 139956176916480 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.457352 139956176916480 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:35.460003 139956176916480 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:01:35.463803 139956176916480 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:35.463865 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:35.463904 139956176916480 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:35.463936 139956176916480 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.463999 139956176916480 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.464627 139956176916480 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.464711 139956176916480 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.465083 139956176916480 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.465895 139956176916480 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.468415 139956176916480 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.469405 139956176916480 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.469493 139956176916480 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:35.469530 139956176916480 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:35.469593 139956176916480 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.469731 139956176916480 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:35.470055 139956176916480 nn_components.py:325] mlp: activation = None
I0123 11:01:35.470103 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:35.472011 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.472111 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:35.474656 139956176916480 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.474743 139956176916480 transformer_base.py:443] tbase: final FFN
I0123 11:01:35.475232 139956176916480 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:35.477493 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:35.479430 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.479531 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:35.479824 139956176916480 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.480133 139956176916480 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:01:35.480209 139956176916480 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:01:35.480279 139956176916480 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:01:35.480340 139956176916480 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:01:35.480399 139956176916480 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:01:35.480455 139956176916480 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:01:35.480511 139956176916480 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:01:35.480566 139956176916480 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:01:35.480621 139956176916480 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:01:35.480674 139956176916480 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:01:35.480726 139956176916480 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:01:35.480780 139956176916480 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:01:35.480820 139956176916480 decoder_stack.py:344] dstack: Final layernorm.
I0123 11:01:35.484422 139956176916480 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:35.532617 139956176916480 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.532712 139956176916480 decoder_stack.py:333] dstack: autoregressive generator.
I0123 11:01:35.532770 139956176916480 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 11:01:35.532880 139956176916480 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:35.532923 139956176916480 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:35.532954 139956176916480 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:35.533018 139956176916480 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.535497 139956176916480 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:35.541023 139956176916480 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.541299 139956176916480 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:35.543979 139956176916480 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:35.560741 139956176916480 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:35.560805 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:35.560845 139956176916480 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:35.560880 139956176916480 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.560944 139956176916480 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.562111 139956176916480 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.562197 139956176916480 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.562913 139956176916480 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.564944 139956176916480 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.570117 139956176916480 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.571439 139956176916480 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.571535 139956176916480 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:35.571575 139956176916480 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:35.571638 139956176916480 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.571775 139956176916480 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:35.571896 139956176916480 nn_components.py:325] mlp: activation = None
I0123 11:01:35.571939 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:35.573882 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.573983 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:35.576455 139956176916480 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.576545 139956176916480 transformer_base.py:443] tbase: final FFN
I0123 11:01:35.576661 139956176916480 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:35.578917 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:35.580901 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.581004 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:35.581301 139956176916480 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.581396 139956176916480 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 11:01:35.581508 139956176916480 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:35.581550 139956176916480 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:35.581583 139956176916480 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:35.581659 139956176916480 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.583957 139956176916480 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:35.589483 139956176916480 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.589763 139956176916480 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:35.592509 139956176916480 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:35.605717 139956176916480 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:35.605782 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:35.605822 139956176916480 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:35.605855 139956176916480 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.605919 139956176916480 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.606497 139956176916480 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.606581 139956176916480 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.606972 139956176916480 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.607685 139956176916480 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.610243 139956176916480 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.610882 139956176916480 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.610965 139956176916480 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:35.611009 139956176916480 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:35.611071 139956176916480 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.611202 139956176916480 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:35.611316 139956176916480 nn_components.py:325] mlp: activation = None
I0123 11:01:35.611359 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:35.613313 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.613413 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:35.615868 139956176916480 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.615958 139956176916480 transformer_base.py:443] tbase: final FFN
I0123 11:01:35.616072 139956176916480 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:35.618306 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:35.620257 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.620358 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:35.620652 139956176916480 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.620742 139956176916480 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 11:01:35.620862 139956176916480 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:35.620904 139956176916480 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:35.620937 139956176916480 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:35.621001 139956176916480 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.623288 139956176916480 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:35.628751 139956176916480 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.629021 139956176916480 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:35.631767 139956176916480 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:35.644473 139956176916480 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:35.644536 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:35.644576 139956176916480 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:35.644609 139956176916480 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.644672 139956176916480 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.645239 139956176916480 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.645322 139956176916480 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.645698 139956176916480 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.646409 139956176916480 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.648903 139956176916480 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.649537 139956176916480 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.649621 139956176916480 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:35.649668 139956176916480 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:35.649735 139956176916480 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.649865 139956176916480 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:35.649981 139956176916480 nn_components.py:325] mlp: activation = None
I0123 11:01:35.650024 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:35.651965 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.652065 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:35.654538 139956176916480 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.654624 139956176916480 transformer_base.py:443] tbase: final FFN
I0123 11:01:35.654736 139956176916480 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:35.656953 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:35.658903 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.659006 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:35.659300 139956176916480 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.659389 139956176916480 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 11:01:35.659506 139956176916480 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:35.659548 139956176916480 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:35.659581 139956176916480 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:35.659644 139956176916480 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.661914 139956176916480 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:35.667402 139956176916480 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.667672 139956176916480 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:35.670381 139956176916480 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:35.683081 139956176916480 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:35.683143 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:35.683182 139956176916480 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:35.683214 139956176916480 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.683276 139956176916480 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.683847 139956176916480 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.683929 139956176916480 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.684297 139956176916480 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.685008 139956176916480 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.687520 139956176916480 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.688159 139956176916480 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.688243 139956176916480 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:35.688282 139956176916480 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:35.688344 139956176916480 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.688482 139956176916480 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:35.688596 139956176916480 nn_components.py:325] mlp: activation = None
I0123 11:01:35.688638 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:35.690609 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.690711 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:35.693298 139956176916480 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.693388 139956176916480 transformer_base.py:443] tbase: final FFN
I0123 11:01:35.693500 139956176916480 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:35.695754 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:35.697755 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.697858 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:35.698150 139956176916480 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.698244 139956176916480 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 11:01:35.698357 139956176916480 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:35.698401 139956176916480 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:35.698434 139956176916480 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:35.698498 139956176916480 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.701141 139956176916480 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:35.706639 139956176916480 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.706916 139956176916480 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:35.709575 139956176916480 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:35.722360 139956176916480 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:35.722420 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:35.722458 139956176916480 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:35.722490 139956176916480 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.722552 139956176916480 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.723117 139956176916480 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.723200 139956176916480 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.723563 139956176916480 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.724280 139956176916480 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.726878 139956176916480 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.727516 139956176916480 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.727601 139956176916480 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:35.727638 139956176916480 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:35.727698 139956176916480 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.727839 139956176916480 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:35.727953 139956176916480 nn_components.py:325] mlp: activation = None
I0123 11:01:35.727994 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:35.729925 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.730027 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:35.732468 139956176916480 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.732553 139956176916480 transformer_base.py:443] tbase: final FFN
I0123 11:01:35.732665 139956176916480 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:35.734955 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:35.736855 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.736958 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:35.737250 139956176916480 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.737342 139956176916480 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 11:01:35.737454 139956176916480 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:35.737496 139956176916480 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:35.737527 139956176916480 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:35.737591 139956176916480 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.739868 139956176916480 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:35.745365 139956176916480 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.745634 139956176916480 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:35.748358 139956176916480 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:35.761084 139956176916480 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:35.761146 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:35.761185 139956176916480 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:35.761219 139956176916480 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.761282 139956176916480 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.761868 139956176916480 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.761954 139956176916480 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.762319 139956176916480 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.763044 139956176916480 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.765579 139956176916480 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.766217 139956176916480 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.766303 139956176916480 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:35.766340 139956176916480 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:35.766400 139956176916480 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.766533 139956176916480 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:35.766658 139956176916480 nn_components.py:325] mlp: activation = None
I0123 11:01:35.766700 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:35.768658 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.768759 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:35.771234 139956176916480 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.771320 139956176916480 transformer_base.py:443] tbase: final FFN
I0123 11:01:35.771432 139956176916480 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:35.773680 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:35.775559 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.775661 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:35.775954 139956176916480 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.776047 139956176916480 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 11:01:35.776160 139956176916480 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:35.776203 139956176916480 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:35.776236 139956176916480 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:35.776301 139956176916480 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.778599 139956176916480 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:35.784192 139956176916480 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.784462 139956176916480 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:35.787125 139956176916480 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:35.799954 139956176916480 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:35.800017 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:35.800057 139956176916480 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:35.800090 139956176916480 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.800158 139956176916480 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.800734 139956176916480 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.800818 139956176916480 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.801185 139956176916480 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.801908 139956176916480 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.804432 139956176916480 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.805436 139956176916480 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.805521 139956176916480 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:35.805559 139956176916480 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:35.805619 139956176916480 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.805768 139956176916480 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:35.805887 139956176916480 nn_components.py:325] mlp: activation = None
I0123 11:01:35.805936 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:35.807854 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.807955 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:35.810398 139956176916480 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.810484 139956176916480 transformer_base.py:443] tbase: final FFN
I0123 11:01:35.810598 139956176916480 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:35.812825 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:35.814794 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.814899 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:35.815198 139956176916480 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.815292 139956176916480 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 11:01:35.815406 139956176916480 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:35.815449 139956176916480 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:35.815482 139956176916480 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:35.815546 139956176916480 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.817836 139956176916480 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:35.823358 139956176916480 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.823641 139956176916480 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:35.826346 139956176916480 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:35.839067 139956176916480 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:35.839130 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:35.839169 139956176916480 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:35.839203 139956176916480 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.839266 139956176916480 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.839887 139956176916480 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.839970 139956176916480 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.840341 139956176916480 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.841062 139956176916480 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.843593 139956176916480 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.844239 139956176916480 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.844326 139956176916480 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:35.844362 139956176916480 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:35.844422 139956176916480 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.844560 139956176916480 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:35.844679 139956176916480 nn_components.py:325] mlp: activation = None
I0123 11:01:35.844730 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:35.846646 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.846747 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:35.849237 139956176916480 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.849325 139956176916480 transformer_base.py:443] tbase: final FFN
I0123 11:01:35.849438 139956176916480 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:35.851681 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:35.853569 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.853679 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:35.853978 139956176916480 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.854074 139956176916480 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 11:01:35.854188 139956176916480 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:35.854231 139956176916480 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:35.854263 139956176916480 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:35.854327 139956176916480 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.856581 139956176916480 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:35.862188 139956176916480 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.862461 139956176916480 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:35.865108 139956176916480 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:35.877909 139956176916480 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:35.877973 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:35.878011 139956176916480 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:35.878044 139956176916480 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.878109 139956176916480 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.878687 139956176916480 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.878772 139956176916480 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.879142 139956176916480 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.879845 139956176916480 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.882379 139956176916480 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.883067 139956176916480 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.883152 139956176916480 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:35.883189 139956176916480 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:35.883250 139956176916480 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.883386 139956176916480 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:35.883499 139956176916480 nn_components.py:325] mlp: activation = None
I0123 11:01:35.883541 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:35.885463 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.885564 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:35.887999 139956176916480 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.888086 139956176916480 transformer_base.py:443] tbase: final FFN
I0123 11:01:35.888197 139956176916480 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:35.890452 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:35.892415 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.892518 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:35.892817 139956176916480 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.892909 139956176916480 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 11:01:35.893025 139956176916480 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:35.893069 139956176916480 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:35.893102 139956176916480 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:35.893171 139956176916480 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.895457 139956176916480 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:35.900978 139956176916480 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.901258 139956176916480 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:35.904018 139956176916480 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:35.917044 139956176916480 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:35.917108 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:35.917146 139956176916480 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:35.917181 139956176916480 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.917247 139956176916480 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.917877 139956176916480 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.917964 139956176916480 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.918334 139956176916480 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.919041 139956176916480 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.921582 139956176916480 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.922229 139956176916480 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.922314 139956176916480 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:35.922351 139956176916480 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:35.922409 139956176916480 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.922543 139956176916480 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:35.922657 139956176916480 nn_components.py:325] mlp: activation = None
I0123 11:01:35.922700 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:35.924597 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.924709 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:35.927217 139956176916480 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.927304 139956176916480 transformer_base.py:443] tbase: final FFN
I0123 11:01:35.927416 139956176916480 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:35.929674 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:35.931559 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.931662 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:35.931959 139956176916480 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.932048 139956176916480 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 11:01:35.932163 139956176916480 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:35.932207 139956176916480 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:35.932242 139956176916480 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:35.932307 139956176916480 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.934592 139956176916480 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:35.940149 139956176916480 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.940416 139956176916480 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:35.943094 139956176916480 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:35.955898 139956176916480 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:35.955960 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:35.955999 139956176916480 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:35.956031 139956176916480 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.956094 139956176916480 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.956665 139956176916480 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.956748 139956176916480 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.957113 139956176916480 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.957834 139956176916480 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.960361 139956176916480 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.961045 139956176916480 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.961129 139956176916480 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:35.961166 139956176916480 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:35.961225 139956176916480 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.961359 139956176916480 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:35.961473 139956176916480 nn_components.py:325] mlp: activation = None
I0123 11:01:35.961516 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:35.963431 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.963540 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:35.966005 139956176916480 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.966091 139956176916480 transformer_base.py:443] tbase: final FFN
I0123 11:01:35.966208 139956176916480 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:35.968430 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:35.970396 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.970498 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:35.970795 139956176916480 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.970884 139956176916480 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 11:01:35.970998 139956176916480 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:35.971042 139956176916480 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:35.971076 139956176916480 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:35.971141 139956176916480 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.973430 139956176916480 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:35.978920 139956176916480 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.979193 139956176916480 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:35.981909 139956176916480 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:35.994631 139956176916480 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:35.994694 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:35.994734 139956176916480 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:35.994767 139956176916480 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.994830 139956176916480 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.995394 139956176916480 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.995476 139956176916480 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.995846 139956176916480 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.996599 139956176916480 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.999150 139956176916480 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.999784 139956176916480 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:35.999868 139956176916480 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:35.999905 139956176916480 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:35.999966 139956176916480 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:36.000100 139956176916480 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:36.000218 139956176916480 nn_components.py:325] mlp: activation = None
I0123 11:01:36.000270 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:36.002191 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:36.002291 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:36.004734 139956176916480 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:36.004819 139956176916480 transformer_base.py:443] tbase: final FFN
I0123 11:01:36.004930 139956176916480 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:36.007230 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:36.009114 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:36.009215 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:36.009514 139956176916480 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:36.009609 139956176916480 decoder_stack.py:344] dstack: Final layernorm.
I0123 11:01:36.012498 139956176916480 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:36.068387 139956176916480 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.068491 139956176916480 decoder_stack.py:333] dstack: autoregressive generator.
I0123 11:01:36.068550 139956176916480 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 11:01:36.068657 139956176916480 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:36.068699 139956176916480 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:36.068730 139956176916480 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:36.068793 139956176916480 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.071499 139956176916480 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:36.076916 139956176916480 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.077185 139956176916480 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:36.079801 139956176916480 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:36.092240 139956176916480 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:36.092304 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:36.092342 139956176916480 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:36.092375 139956176916480 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.092437 139956176916480 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.092999 139956176916480 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.093081 139956176916480 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.093449 139956176916480 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.094147 139956176916480 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.096663 139956176916480 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.097285 139956176916480 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.097370 139956176916480 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:36.097406 139956176916480 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:36.097466 139956176916480 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.097597 139956176916480 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:36.097734 139956176916480 nn_components.py:325] mlp: activation = None
I0123 11:01:36.097779 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:36.099646 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.099746 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:36.102178 139956176916480 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.102264 139956176916480 transformer_base.py:443] tbase: final FFN
I0123 11:01:36.102378 139956176916480 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:36.104628 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:36.106500 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.106605 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:36.106903 139956176916480 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.106991 139956176916480 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 11:01:36.107103 139956176916480 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:36.107146 139956176916480 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:36.107178 139956176916480 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:36.107240 139956176916480 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.109474 139956176916480 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:36.114872 139956176916480 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.115143 139956176916480 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:36.117820 139956176916480 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:36.130059 139956176916480 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:36.130122 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:36.130160 139956176916480 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:36.130191 139956176916480 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.130253 139956176916480 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.130811 139956176916480 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.130892 139956176916480 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.131256 139956176916480 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.131940 139956176916480 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.134472 139956176916480 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.135097 139956176916480 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.135180 139956176916480 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:36.135217 139956176916480 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:36.135277 139956176916480 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.135406 139956176916480 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:36.135518 139956176916480 nn_components.py:325] mlp: activation = None
I0123 11:01:36.135568 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:36.137411 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.137511 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:36.139950 139956176916480 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.140037 139956176916480 transformer_base.py:443] tbase: final FFN
I0123 11:01:36.140151 139956176916480 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:36.142427 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:36.144280 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.144384 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:36.144682 139956176916480 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.144771 139956176916480 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 11:01:36.144883 139956176916480 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:36.144926 139956176916480 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:36.144958 139956176916480 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:36.145021 139956176916480 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.147279 139956176916480 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:36.152686 139956176916480 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.152953 139956176916480 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:36.155633 139956176916480 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:36.167902 139956176916480 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:36.167966 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:36.168005 139956176916480 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:36.168037 139956176916480 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.168101 139956176916480 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.168667 139956176916480 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.168751 139956176916480 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.169118 139956176916480 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.169826 139956176916480 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.172362 139956176916480 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.172989 139956176916480 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.173074 139956176916480 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:36.173111 139956176916480 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:36.173172 139956176916480 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.173305 139956176916480 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:36.173417 139956176916480 nn_components.py:325] mlp: activation = None
I0123 11:01:36.173459 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:36.175323 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.175424 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:36.177867 139956176916480 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.177955 139956176916480 transformer_base.py:443] tbase: final FFN
I0123 11:01:36.178068 139956176916480 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:36.180770 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:36.182641 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.182745 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:36.183044 139956176916480 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.183133 139956176916480 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 11:01:36.183247 139956176916480 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:36.183290 139956176916480 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:36.183322 139956176916480 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:36.183384 139956176916480 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.185620 139956176916480 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:36.191021 139956176916480 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.191293 139956176916480 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:36.193956 139956176916480 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:36.206346 139956176916480 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:36.206409 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:36.206457 139956176916480 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:36.206499 139956176916480 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.206563 139956176916480 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.207134 139956176916480 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.207215 139956176916480 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.207587 139956176916480 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.208290 139956176916480 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.210853 139956176916480 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.211484 139956176916480 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.211566 139956176916480 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:36.211603 139956176916480 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:36.211662 139956176916480 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.211790 139956176916480 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:36.211903 139956176916480 nn_components.py:325] mlp: activation = None
I0123 11:01:36.211950 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:36.213865 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.213966 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:36.216390 139956176916480 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.216475 139956176916480 transformer_base.py:443] tbase: final FFN
I0123 11:01:36.216587 139956176916480 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:36.218890 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:36.220778 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.220880 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:36.221171 139956176916480 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.221255 139956176916480 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 11:01:36.221364 139956176916480 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:36.221406 139956176916480 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:36.221438 139956176916480 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:36.221499 139956176916480 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.223773 139956176916480 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:36.229214 139956176916480 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.229487 139956176916480 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:36.232220 139956176916480 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:36.244707 139956176916480 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:36.244768 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:36.244805 139956176916480 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:36.244836 139956176916480 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.244898 139956176916480 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.245460 139956176916480 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.245542 139956176916480 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.245923 139956176916480 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.246627 139956176916480 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.249184 139956176916480 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.249828 139956176916480 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.249911 139956176916480 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:36.249946 139956176916480 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:36.250002 139956176916480 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.250134 139956176916480 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:36.250251 139956176916480 nn_components.py:325] mlp: activation = None
I0123 11:01:36.250295 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:36.252160 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.252267 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:36.254705 139956176916480 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.254790 139956176916480 transformer_base.py:443] tbase: final FFN
I0123 11:01:36.254901 139956176916480 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:36.257193 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:36.259066 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.259166 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:36.259457 139956176916480 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.259544 139956176916480 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 11:01:36.259655 139956176916480 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:36.259700 139956176916480 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:36.259732 139956176916480 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:36.259794 139956176916480 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.262038 139956176916480 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:36.267418 139956176916480 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.267683 139956176916480 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:36.270379 139956176916480 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:36.282862 139956176916480 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:36.282921 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:36.282958 139956176916480 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:36.282989 139956176916480 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.283051 139956176916480 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.283611 139956176916480 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.283696 139956176916480 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.284061 139956176916480 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.284761 139956176916480 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.287355 139956176916480 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.287989 139956176916480 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.288071 139956176916480 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:36.288106 139956176916480 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:36.288163 139956176916480 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.288289 139956176916480 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:36.288399 139956176916480 nn_components.py:325] mlp: activation = None
I0123 11:01:36.288438 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:36.290301 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.290408 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:36.292813 139956176916480 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.292897 139956176916480 transformer_base.py:443] tbase: final FFN
I0123 11:01:36.293010 139956176916480 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:36.295693 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:36.297548 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.297658 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:36.297951 139956176916480 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.298038 139956176916480 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 11:01:36.298148 139956176916480 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:36.298188 139956176916480 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:36.298219 139956176916480 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:36.298281 139956176916480 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.300512 139956176916480 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:36.305932 139956176916480 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.306199 139956176916480 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:36.308889 139956176916480 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:36.321386 139956176916480 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:36.321446 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:36.321484 139956176916480 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:36.321515 139956176916480 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.321576 139956176916480 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.322142 139956176916480 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.322223 139956176916480 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.322593 139956176916480 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.323290 139956176916480 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.325865 139956176916480 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.326501 139956176916480 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.326583 139956176916480 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:36.326618 139956176916480 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:36.326677 139956176916480 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.326805 139956176916480 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:36.326923 139956176916480 nn_components.py:325] mlp: activation = None
I0123 11:01:36.326972 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:36.328844 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.328943 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:36.331372 139956176916480 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.331456 139956176916480 transformer_base.py:443] tbase: final FFN
I0123 11:01:36.331567 139956176916480 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:36.333859 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:36.335721 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.335822 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:36.336114 139956176916480 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.336201 139956176916480 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 11:01:36.336312 139956176916480 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:36.336352 139956176916480 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:36.336382 139956176916480 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:36.336444 139956176916480 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.338677 139956176916480 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:36.344088 139956176916480 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.344357 139956176916480 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:36.347046 139956176916480 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:36.359538 139956176916480 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:36.359598 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:36.359635 139956176916480 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:36.359666 139956176916480 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.359727 139956176916480 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.360292 139956176916480 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.360373 139956176916480 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.360741 139956176916480 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.361437 139956176916480 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.363994 139956176916480 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.364625 139956176916480 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.364707 139956176916480 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:36.364743 139956176916480 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:36.364801 139956176916480 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.364930 139956176916480 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:36.365052 139956176916480 nn_components.py:325] mlp: activation = None
I0123 11:01:36.365093 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:36.366980 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.367079 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:36.369489 139956176916480 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.369582 139956176916480 transformer_base.py:443] tbase: final FFN
I0123 11:01:36.369704 139956176916480 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:36.371983 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:36.373873 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.373974 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:36.374269 139956176916480 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.374356 139956176916480 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 11:01:36.374467 139956176916480 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:36.374511 139956176916480 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:36.374548 139956176916480 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:36.374614 139956176916480 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.376852 139956176916480 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:36.382270 139956176916480 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.382539 139956176916480 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:36.385251 139956176916480 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:36.397814 139956176916480 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:36.397877 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:36.397912 139956176916480 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:36.397942 139956176916480 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.398003 139956176916480 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.398561 139956176916480 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.398643 139956176916480 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.399008 139956176916480 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.399704 139956176916480 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.402286 139956176916480 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.402920 139956176916480 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.403002 139956176916480 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:36.403038 139956176916480 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:36.403096 139956176916480 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.403225 139956176916480 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:36.403344 139956176916480 nn_components.py:325] mlp: activation = None
I0123 11:01:36.403387 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:36.405258 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.405357 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:36.407797 139956176916480 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.407891 139956176916480 transformer_base.py:443] tbase: final FFN
I0123 11:01:36.408007 139956176916480 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:36.410696 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:36.412571 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.412670 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:36.412964 139956176916480 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.413051 139956176916480 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 11:01:36.413160 139956176916480 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:36.413200 139956176916480 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:36.413233 139956176916480 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:36.413295 139956176916480 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.415563 139956176916480 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:36.420998 139956176916480 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.421271 139956176916480 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:36.423957 139956176916480 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:36.436482 139956176916480 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:36.436542 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:36.436578 139956176916480 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:36.436609 139956176916480 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.436675 139956176916480 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.437247 139956176916480 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.437328 139956176916480 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.437706 139956176916480 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.438414 139956176916480 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.440979 139956176916480 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.441612 139956176916480 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.441703 139956176916480 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:36.441742 139956176916480 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:36.441798 139956176916480 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.441931 139956176916480 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:36.442048 139956176916480 nn_components.py:325] mlp: activation = None
I0123 11:01:36.442096 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:36.444363 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.444462 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:36.446879 139956176916480 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.446964 139956176916480 transformer_base.py:443] tbase: final FFN
I0123 11:01:36.447082 139956176916480 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:36.449340 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:36.451211 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.451312 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:36.451607 139956176916480 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.451694 139956176916480 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 11:01:36.451805 139956176916480 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:36.451849 139956176916480 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:36.451885 139956176916480 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:36.451946 139956176916480 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.454196 139956176916480 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:36.459639 139956176916480 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.459913 139956176916480 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:36.462622 139956176916480 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:36.475213 139956176916480 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:36.475278 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:36.475315 139956176916480 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:36.475347 139956176916480 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.475410 139956176916480 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.475972 139956176916480 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.476052 139956176916480 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.476416 139956176916480 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.477122 139956176916480 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.479718 139956176916480 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.480351 139956176916480 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.480433 139956176916480 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:36.480469 139956176916480 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:36.480526 139956176916480 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.480656 139956176916480 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:36.480767 139956176916480 nn_components.py:325] mlp: activation = None
I0123 11:01:36.480808 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:36.482696 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.482794 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:36.485209 139956176916480 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.485293 139956176916480 transformer_base.py:443] tbase: final FFN
I0123 11:01:36.485403 139956176916480 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:36.487710 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:36.489585 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.489695 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:36.489990 139956176916480 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.490077 139956176916480 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 11:01:36.490187 139956176916480 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:36.490228 139956176916480 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:36.490259 139956176916480 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:36.490319 139956176916480 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.492586 139956176916480 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:36.498010 139956176916480 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.498276 139956176916480 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:36.501001 139956176916480 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:36.513625 139956176916480 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:36.513694 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:36.513731 139956176916480 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:36.513761 139956176916480 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.513823 139956176916480 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.514387 139956176916480 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.514472 139956176916480 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.514845 139956176916480 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.515552 139956176916480 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.518134 139956176916480 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.518770 139956176916480 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.518854 139956176916480 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:36.518889 139956176916480 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:36.518947 139956176916480 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.519085 139956176916480 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:36.519203 139956176916480 nn_components.py:325] mlp: activation = None
I0123 11:01:36.519246 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:36.521117 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.521213 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:36.523627 139956176916480 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.523712 139956176916480 transformer_base.py:443] tbase: final FFN
I0123 11:01:36.523822 139956176916480 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:36.526485 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:36.528386 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.528486 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:36.528782 139956176916480 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:36.528876 139956176916480 decoder_stack.py:344] dstack: Final layernorm.
I0123 11:01:36.531746 139956176916480 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:41.197898 139956176916480 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
I0123 11:01:41.813151 139956176916480 training_loop.py:409] No working directory specified.
I0123 11:01:41.813379 139956176916480 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0123 11:01:41.814441 139956176916480 checkpoints.py:1062] Restoring legacy Flax checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0123 11:01:44.731469 139956176916480 training_loop.py:447] Only restoring trainable parameters.
I0123 11:01:44.732535 139956176916480 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0123 11:01:44.732599 139956176916480 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0123 11:01:44.732646 139956176916480 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:01:44.732692 139956176916480 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:01:44.732733 139956176916480 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:44.732775 139956176916480 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:01:44.732817 139956176916480 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:44.732859 139956176916480 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:44.732902 139956176916480 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:01:44.732943 139956176916480 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:01:44.732981 139956176916480 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:44.733023 139956176916480 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:01:44.733062 139956176916480 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:01:44.733101 139956176916480 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:01:44.733140 139956176916480 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:44.733179 139956176916480 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:01:44.733217 139956176916480 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:44.733256 139956176916480 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:44.733294 139956176916480 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:01:44.733333 139956176916480 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:01:44.733386 139956176916480 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:44.733429 139956176916480 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:01:44.733469 139956176916480 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:01:44.733508 139956176916480 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:01:44.733547 139956176916480 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:44.733585 139956176916480 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:01:44.733623 139956176916480 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:44.733675 139956176916480 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:44.733715 139956176916480 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:01:44.733753 139956176916480 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:01:44.733791 139956176916480 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:44.733829 139956176916480 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:01:44.733867 139956176916480 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:01:44.733905 139956176916480 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:01:44.733943 139956176916480 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:44.733983 139956176916480 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:01:44.734022 139956176916480 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:44.734060 139956176916480 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:44.734099 139956176916480 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:01:44.734137 139956176916480 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:01:44.734175 139956176916480 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:44.734213 139956176916480 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:01:44.734251 139956176916480 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:01:44.734287 139956176916480 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:01:44.734326 139956176916480 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:44.734363 139956176916480 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:01:44.734409 139956176916480 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:44.734449 139956176916480 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:44.734487 139956176916480 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:01:44.734524 139956176916480 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:01:44.734562 139956176916480 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:44.734599 139956176916480 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:01:44.734637 139956176916480 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:01:44.734675 139956176916480 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:01:44.734714 139956176916480 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:44.734752 139956176916480 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:01:44.734790 139956176916480 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:44.734828 139956176916480 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:44.734866 139956176916480 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:01:44.734903 139956176916480 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:01:44.734939 139956176916480 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:44.734977 139956176916480 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:01:44.735014 139956176916480 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:01:44.735050 139956176916480 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:01:44.735088 139956176916480 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:44.735125 139956176916480 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:01:44.735162 139956176916480 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:44.735199 139956176916480 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:44.735236 139956176916480 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:01:44.735272 139956176916480 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:01:44.735310 139956176916480 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:44.735348 139956176916480 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:01:44.735386 139956176916480 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:01:44.735430 139956176916480 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:01:44.735469 139956176916480 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:44.735507 139956176916480 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:01:44.735545 139956176916480 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:44.735582 139956176916480 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:44.735619 139956176916480 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:01:44.735656 139956176916480 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:01:44.735694 139956176916480 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:44.735732 139956176916480 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:01:44.735769 139956176916480 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:01:44.735807 139956176916480 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:01:44.735845 139956176916480 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:44.735882 139956176916480 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:01:44.735920 139956176916480 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:44.735957 139956176916480 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:44.735994 139956176916480 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:01:44.736032 139956176916480 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:01:44.736070 139956176916480 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:44.736108 139956176916480 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:01:44.736146 139956176916480 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:01:44.736183 139956176916480 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:01:44.736221 139956176916480 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:44.736258 139956176916480 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:01:44.736296 139956176916480 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:44.736334 139956176916480 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:44.736371 139956176916480 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:01:44.736407 139956176916480 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:01:44.736451 139956176916480 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:44.736492 139956176916480 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:01:44.736531 139956176916480 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:01:44.736569 139956176916480 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:01:44.736607 139956176916480 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:44.736644 139956176916480 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:01:44.736681 139956176916480 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:44.736719 139956176916480 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:44.736756 139956176916480 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:01:44.736793 139956176916480 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:01:44.736831 139956176916480 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:44.736868 139956176916480 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:01:44.736906 139956176916480 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:01:44.736943 139956176916480 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:01:44.736980 139956176916480 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:44.737017 139956176916480 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:01:44.737054 139956176916480 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:44.737090 139956176916480 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:44.737127 139956176916480 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:01:44.737164 139956176916480 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:01:44.737200 139956176916480 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:01:44.737237 139956176916480 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:01:44.737266 139956176916480 training_loop.py:725] Total parameters: 152072288
I0123 11:01:44.737649 139956176916480 training_loop.py:739] Total state size: 0
I0123 11:01:44.805124 139956176916480 training_loop.py:492] Training loop: creating task for mode beam_search
I0123 11:01:44.805563 139956176916480 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0123 11:01:44.806285 139956176916480 training_loop.py:652] Compiling mode beam_search with jit.
I0123 11:01:44.806777 139956176916480 training_loop.py:89] registering functions: dict_keys([])
I0123 11:01:44.826958 139956176916480 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c b, on_line f a e; g = on_line g b a, on_line g e c; h = midpoint h b e; i = midpoint i c a; j = midpoint j f g ? coll i h j
I0123 11:01:45.288796 139956176916480 ddar.py:60] Depth 1/1000 time = 0.4233698844909668
I0123 11:01:46.244652 139956176916480 ddar.py:60] Depth 2/1000 time = 0.9556779861450195
I0123 11:01:47.201390 139956176916480 ddar.py:60] Depth 3/1000 time = 0.9564597606658936
I0123 11:01:48.224492 139956176916480 ddar.py:60] Depth 4/1000 time = 1.02280592918396
I0123 11:01:49.619351 139956176916480 ddar.py:60] Depth 5/1000 time = 1.3945889472961426
I0123 11:01:50.820549 139956176916480 ddar.py:60] Depth 6/1000 time = 1.200962781906128
I0123 11:01:52.033108 139956176916480 ddar.py:60] Depth 7/1000 time = 1.2120084762573242
I0123 11:01:53.297593 139956176916480 ddar.py:60] Depth 8/1000 time = 1.2474613189697266
I0123 11:01:53.297884 139956176916480 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:01:53.297969 139956176916480 alphageometry.py:540] Depth 0. There are 1 nodes to expand:
I0123 11:01:53.298005 139956176916480 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : D a d d e 02 ; f : C a e f 03 C b c f 04 ; g : C a b g 05 C c e g 06 ; h : C b e h 07 D b h e h 08 ; i : C a c i 09 D a i c i 10 ; j : C f g j 11 D f j g j 12 ? C i h j {F1} x00
I0123 11:01:53.298037 139956176916480 alphageometry.py:549] Decoding from {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : D a d d e 02 ; f : C a e f 03 C b c f 04 ; g : C a b g 05 C c e g 06 ; h : C b e h 07 D b h e h 08 ; i : C a c i 09 D a i c i 10 ; j : C f g j 11 D f j g j 12 ? C i h j {F1} x00
I0123 11:01:53.426550 139956176916480 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.426839 139956176916480 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 11:01:53.426952 139956176916480 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 11:01:53.427029 139956176916480 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 11:01:53.427101 139956176916480 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 11:01:53.427171 139956176916480 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 11:01:53.427240 139956176916480 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 11:01:53.427309 139956176916480 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 11:01:53.427378 139956176916480 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 11:01:53.427447 139956176916480 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 11:01:53.427516 139956176916480 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 11:01:53.427584 139956176916480 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 11:01:53.427651 139956176916480 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 11:01:53.427719 139956176916480 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 11:01:53.427759 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:53.427804 139956176916480 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 11:01:53.427916 139956176916480 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:53.427957 139956176916480 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:53.427988 139956176916480 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:53.429901 139956176916480 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.432475 139956176916480 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:53.438705 139956176916480 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.438990 139956176916480 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:53.441653 139956176916480 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:01:53.445653 139956176916480 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:53.445711 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:53.445748 139956176916480 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:53.445778 139956176916480 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.445842 139956176916480 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.446494 139956176916480 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.446573 139956176916480 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.446954 139956176916480 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.447749 139956176916480 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.450351 139956176916480 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.450993 139956176916480 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.451071 139956176916480 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:53.451104 139956176916480 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:53.451160 139956176916480 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.451286 139956176916480 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:53.451611 139956176916480 nn_components.py:325] mlp: activation = None
I0123 11:01:53.451654 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:53.453620 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.453720 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:53.456186 139956176916480 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.456267 139956176916480 transformer_base.py:443] tbase: final FFN
I0123 11:01:53.456701 139956176916480 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:53.459086 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:53.461032 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.461127 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:53.461424 139956176916480 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.461505 139956176916480 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 11:01:53.461611 139956176916480 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:53.461655 139956176916480 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:53.461686 139956176916480 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:53.463526 139956176916480 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.465831 139956176916480 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:53.471363 139956176916480 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.471643 139956176916480 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:53.474265 139956176916480 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:01:53.477897 139956176916480 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:53.477955 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:53.477990 139956176916480 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:53.478019 139956176916480 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.478082 139956176916480 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.478638 139956176916480 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.478713 139956176916480 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.479077 139956176916480 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.479856 139956176916480 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.482363 139956176916480 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.483042 139956176916480 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.483120 139956176916480 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:53.483155 139956176916480 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:53.483211 139956176916480 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.483340 139956176916480 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:53.483662 139956176916480 nn_components.py:325] mlp: activation = None
I0123 11:01:53.483705 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:53.485638 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.485737 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:53.488314 139956176916480 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.488394 139956176916480 transformer_base.py:443] tbase: final FFN
I0123 11:01:53.488832 139956176916480 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:53.491179 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:53.493086 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.493182 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:53.493471 139956176916480 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.493553 139956176916480 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 11:01:53.493667 139956176916480 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:53.493708 139956176916480 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:53.493738 139956176916480 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:53.495498 139956176916480 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.497824 139956176916480 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:53.503610 139956176916480 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.503881 139956176916480 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:53.506503 139956176916480 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:01:53.510250 139956176916480 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:53.510308 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:53.510344 139956176916480 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:53.510375 139956176916480 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.510441 139956176916480 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.511060 139956176916480 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.511141 139956176916480 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.511512 139956176916480 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.512310 139956176916480 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.514875 139956176916480 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.515513 139956176916480 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.515595 139956176916480 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:53.515631 139956176916480 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:53.515690 139956176916480 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.515823 139956176916480 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:53.516152 139956176916480 nn_components.py:325] mlp: activation = None
I0123 11:01:53.516197 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:53.518237 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.518337 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:53.520873 139956176916480 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.520956 139956176916480 transformer_base.py:443] tbase: final FFN
I0123 11:01:53.521392 139956176916480 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:53.523701 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:53.525657 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.525757 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:53.526060 139956176916480 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.526145 139956176916480 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 11:01:53.526257 139956176916480 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:53.526297 139956176916480 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:53.526329 139956176916480 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:53.528238 139956176916480 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.530639 139956176916480 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:53.536360 139956176916480 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.536624 139956176916480 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:53.539259 139956176916480 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:01:53.542890 139956176916480 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:53.542948 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:53.542983 139956176916480 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:53.543014 139956176916480 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.543076 139956176916480 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.543630 139956176916480 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.543707 139956176916480 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.544063 139956176916480 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.544830 139956176916480 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.547281 139956176916480 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.548310 139956176916480 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.548393 139956176916480 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:53.548428 139956176916480 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:53.548485 139956176916480 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.548614 139956176916480 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:53.548938 139956176916480 nn_components.py:325] mlp: activation = None
I0123 11:01:53.548983 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:53.550966 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.551061 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:53.553499 139956176916480 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.553580 139956176916480 transformer_base.py:443] tbase: final FFN
I0123 11:01:53.554011 139956176916480 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:53.556330 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:53.558232 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.558329 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:53.558626 139956176916480 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.558712 139956176916480 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 11:01:53.558823 139956176916480 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:53.558865 139956176916480 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:53.558897 139956176916480 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:53.560712 139956176916480 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.563043 139956176916480 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:53.568695 139956176916480 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.568953 139956176916480 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:53.571504 139956176916480 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:01:53.575142 139956176916480 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:53.575205 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:53.575240 139956176916480 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:53.575271 139956176916480 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.575383 139956176916480 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.575941 139956176916480 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.576019 139956176916480 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.576372 139956176916480 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.577132 139956176916480 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.579588 139956176916480 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.580199 139956176916480 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.580277 139956176916480 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:53.580311 139956176916480 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:53.580366 139956176916480 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.580491 139956176916480 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:53.580803 139956176916480 nn_components.py:325] mlp: activation = None
I0123 11:01:53.580845 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:53.582859 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.582956 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:53.585411 139956176916480 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.585490 139956176916480 transformer_base.py:443] tbase: final FFN
I0123 11:01:53.585917 139956176916480 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:53.588157 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:53.590054 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.590152 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:53.590454 139956176916480 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.590538 139956176916480 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 11:01:53.590649 139956176916480 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:53.590689 139956176916480 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:53.590721 139956176916480 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:53.592599 139956176916480 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.594932 139956176916480 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:53.600502 139956176916480 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.600759 139956176916480 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:53.603524 139956176916480 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:01:53.607160 139956176916480 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:53.607218 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:53.607261 139956176916480 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:53.607295 139956176916480 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.607359 139956176916480 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.607936 139956176916480 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.608019 139956176916480 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.608396 139956176916480 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.609212 139956176916480 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.611899 139956176916480 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.612571 139956176916480 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.612651 139956176916480 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:53.612685 139956176916480 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:53.612742 139956176916480 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.612913 139956176916480 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:53.613242 139956176916480 nn_components.py:325] mlp: activation = None
I0123 11:01:53.613286 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:53.615231 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.615329 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:53.617819 139956176916480 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.617902 139956176916480 transformer_base.py:443] tbase: final FFN
I0123 11:01:53.618339 139956176916480 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:53.620687 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:53.622613 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.622715 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:53.623015 139956176916480 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.623101 139956176916480 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 11:01:53.623213 139956176916480 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:53.623254 139956176916480 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:53.623285 139956176916480 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:53.625083 139956176916480 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.627447 139956176916480 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:53.633141 139956176916480 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.633401 139956176916480 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:53.636008 139956176916480 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:01:53.639688 139956176916480 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:53.639747 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:53.639783 139956176916480 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:53.639822 139956176916480 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.639938 139956176916480 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.640502 139956176916480 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.640579 139956176916480 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.640938 139956176916480 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.641715 139956176916480 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.644245 139956176916480 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.644880 139956176916480 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.644960 139956176916480 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:53.644994 139956176916480 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:53.645051 139956176916480 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.645179 139956176916480 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:53.645500 139956176916480 nn_components.py:325] mlp: activation = None
I0123 11:01:53.645544 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:53.647571 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.647670 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:53.650160 139956176916480 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.650247 139956176916480 transformer_base.py:443] tbase: final FFN
I0123 11:01:53.650693 139956176916480 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:53.652992 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:53.654937 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.655039 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:53.655342 139956176916480 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.655429 139956176916480 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 11:01:53.655541 139956176916480 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:53.655582 139956176916480 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:53.655613 139956176916480 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:53.657809 139956176916480 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.660176 139956176916480 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:53.665792 139956176916480 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.666051 139956176916480 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:53.668700 139956176916480 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:01:53.672379 139956176916480 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:53.672436 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:53.672471 139956176916480 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:53.672502 139956176916480 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.672573 139956176916480 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.673130 139956176916480 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.673206 139956176916480 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.673565 139956176916480 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.674342 139956176916480 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.676864 139956176916480 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.677537 139956176916480 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.677618 139956176916480 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:53.677664 139956176916480 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:53.677723 139956176916480 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.677854 139956176916480 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:53.678170 139956176916480 nn_components.py:325] mlp: activation = None
I0123 11:01:53.678216 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:53.680127 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.680222 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:53.682689 139956176916480 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.682773 139956176916480 transformer_base.py:443] tbase: final FFN
I0123 11:01:53.683195 139956176916480 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:53.685494 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:53.687388 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.687484 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:53.687774 139956176916480 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.687856 139956176916480 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 11:01:53.687963 139956176916480 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:53.688003 139956176916480 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:53.688033 139956176916480 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:53.689799 139956176916480 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.692088 139956176916480 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:53.697717 139956176916480 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.697974 139956176916480 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:53.700535 139956176916480 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:01:53.704162 139956176916480 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:53.704221 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:53.704256 139956176916480 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:53.704287 139956176916480 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.704358 139956176916480 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.704964 139956176916480 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.705043 139956176916480 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.705404 139956176916480 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.706181 139956176916480 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.708656 139956176916480 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.709421 139956176916480 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.709499 139956176916480 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:53.709535 139956176916480 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:53.709593 139956176916480 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.709729 139956176916480 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:53.710047 139956176916480 nn_components.py:325] mlp: activation = None
I0123 11:01:53.710091 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:53.711990 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.712085 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:53.714597 139956176916480 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.714680 139956176916480 transformer_base.py:443] tbase: final FFN
I0123 11:01:53.715101 139956176916480 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:53.717374 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:53.719289 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.719386 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:53.719679 139956176916480 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.719762 139956176916480 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 11:01:53.719871 139956176916480 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:53.719911 139956176916480 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:53.719941 139956176916480 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:53.721743 139956176916480 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.724160 139956176916480 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:53.729770 139956176916480 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.730032 139956176916480 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:53.732632 139956176916480 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:01:53.736248 139956176916480 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:53.736306 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:53.736341 139956176916480 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:53.736372 139956176916480 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.736487 139956176916480 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.737056 139956176916480 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.737134 139956176916480 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.737489 139956176916480 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.738260 139956176916480 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.740712 139956176916480 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.741324 139956176916480 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.741402 139956176916480 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:53.741436 139956176916480 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:53.741492 139956176916480 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.741621 139956176916480 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:53.741944 139956176916480 nn_components.py:325] mlp: activation = None
I0123 11:01:53.741988 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:53.743957 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.744052 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:53.746503 139956176916480 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.746585 139956176916480 transformer_base.py:443] tbase: final FFN
I0123 11:01:53.747012 139956176916480 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:53.749246 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:53.751169 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.751266 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:53.751554 139956176916480 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.751637 139956176916480 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 11:01:53.751744 139956176916480 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:53.751784 139956176916480 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:53.751816 139956176916480 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:53.753685 139956176916480 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.755986 139956176916480 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:53.761495 139956176916480 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.761760 139956176916480 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:53.764477 139956176916480 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:01:53.768161 139956176916480 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:53.768218 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:53.768252 139956176916480 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:53.768282 139956176916480 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.768344 139956176916480 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.768898 139956176916480 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.768986 139956176916480 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.769349 139956176916480 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.770124 139956176916480 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.772589 139956176916480 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.773202 139956176916480 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.773281 139956176916480 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:53.773314 139956176916480 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:53.773370 139956176916480 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.773498 139956176916480 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:53.773820 139956176916480 nn_components.py:325] mlp: activation = None
I0123 11:01:53.773864 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:53.776224 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.776321 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:53.778799 139956176916480 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.778882 139956176916480 transformer_base.py:443] tbase: final FFN
I0123 11:01:53.779310 139956176916480 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:53.781554 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:53.783454 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.783554 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:53.783845 139956176916480 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.783928 139956176916480 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 11:01:53.784037 139956176916480 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:53.784076 139956176916480 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:53.784106 139956176916480 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:53.785971 139956176916480 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.788278 139956176916480 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:53.793804 139956176916480 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.794063 139956176916480 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:53.796679 139956176916480 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:01:53.800409 139956176916480 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:53.800465 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:53.800500 139956176916480 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:53.800531 139956176916480 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.800593 139956176916480 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.801154 139956176916480 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.801240 139956176916480 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.801603 139956176916480 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.802380 139956176916480 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.804818 139956176916480 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.805447 139956176916480 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.805526 139956176916480 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:53.805560 139956176916480 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:53.805615 139956176916480 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.805755 139956176916480 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:53.806075 139956176916480 nn_components.py:325] mlp: activation = None
I0123 11:01:53.806119 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:53.808108 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.808202 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:53.810664 139956176916480 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.810747 139956176916480 transformer_base.py:443] tbase: final FFN
I0123 11:01:53.811172 139956176916480 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:53.813450 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:53.815389 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.815490 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:53.815793 139956176916480 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.816049 139956176916480 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 11:01:53.816120 139956176916480 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 11:01:53.816177 139956176916480 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 11:01:53.816231 139956176916480 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 11:01:53.816283 139956176916480 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 11:01:53.816335 139956176916480 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 11:01:53.816387 139956176916480 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 11:01:53.816437 139956176916480 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 11:01:53.816488 139956176916480 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 11:01:53.816539 139956176916480 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 11:01:53.816590 139956176916480 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 11:01:53.816642 139956176916480 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 11:01:53.816678 139956176916480 decoder_stack.py:344] dstack: Final layernorm.
I0123 11:01:53.819587 139956176916480 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:53.865731 139956176916480 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:53.865826 139956176916480 decoder_stack.py:333] dstack: autoregressive generator.
I0123 11:01:53.865880 139956176916480 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 11:01:53.865988 139956176916480 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:53.866034 139956176916480 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:53.866066 139956176916480 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:53.866129 139956176916480 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:53.868601 139956176916480 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:53.874202 139956176916480 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:53.874485 139956176916480 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:53.877077 139956176916480 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:53.890589 139956176916480 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:53.890650 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:53.890688 139956176916480 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:53.890720 139956176916480 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:53.890787 139956176916480 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:53.891408 139956176916480 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:53.891492 139956176916480 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:53.891897 139956176916480 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:53.892674 139956176916480 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:53.895441 139956176916480 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:53.896117 139956176916480 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:53.896197 139956176916480 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:53.896232 139956176916480 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:53.896292 139956176916480 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:53.896427 139956176916480 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:53.896542 139956176916480 nn_components.py:325] mlp: activation = None
I0123 11:01:53.896584 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:53.898557 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:53.898657 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:53.901148 139956176916480 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:53.901229 139956176916480 transformer_base.py:443] tbase: final FFN
I0123 11:01:53.901337 139956176916480 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:53.903688 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:53.905558 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:53.905677 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:53.905978 139956176916480 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:53.906072 139956176916480 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 11:01:53.906185 139956176916480 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:53.906225 139956176916480 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:53.906257 139956176916480 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:53.906323 139956176916480 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:53.908622 139956176916480 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:53.914077 139956176916480 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:53.914346 139956176916480 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:53.917085 139956176916480 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:53.929809 139956176916480 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:53.929867 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:53.929902 139956176916480 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:53.929932 139956176916480 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:53.929994 139956176916480 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:53.930572 139956176916480 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:53.930652 139956176916480 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:53.931026 139956176916480 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:53.931798 139956176916480 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:53.934287 139956176916480 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:53.934942 139956176916480 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:53.935024 139956176916480 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:53.935062 139956176916480 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:53.935123 139956176916480 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:53.935259 139956176916480 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:53.935373 139956176916480 nn_components.py:325] mlp: activation = None
I0123 11:01:53.935413 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:53.937288 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:53.937382 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:53.940031 139956176916480 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:53.940112 139956176916480 transformer_base.py:443] tbase: final FFN
I0123 11:01:53.940221 139956176916480 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:53.942764 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:53.944700 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:53.944800 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:53.945095 139956176916480 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:53.945185 139956176916480 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 11:01:53.945296 139956176916480 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:53.945335 139956176916480 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:53.945364 139956176916480 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:53.945424 139956176916480 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:53.947741 139956176916480 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:53.953279 139956176916480 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:53.953552 139956176916480 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:53.956349 139956176916480 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:53.969582 139956176916480 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:53.969639 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:53.969680 139956176916480 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:53.969711 139956176916480 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:53.969774 139956176916480 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:53.970341 139956176916480 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:53.970420 139956176916480 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:53.970781 139956176916480 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:53.971522 139956176916480 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:53.974038 139956176916480 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:53.974668 139956176916480 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:53.974746 139956176916480 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:53.974781 139956176916480 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:53.974838 139956176916480 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:53.974970 139956176916480 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:53.975081 139956176916480 nn_components.py:325] mlp: activation = None
I0123 11:01:53.975120 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:53.977165 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:53.977258 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:53.979717 139956176916480 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:53.979804 139956176916480 transformer_base.py:443] tbase: final FFN
I0123 11:01:53.979913 139956176916480 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:53.982181 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:53.984034 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:53.984129 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:53.984418 139956176916480 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:53.984499 139956176916480 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 11:01:53.984616 139956176916480 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:53.984656 139956176916480 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:53.984686 139956176916480 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:53.984746 139956176916480 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:53.986977 139956176916480 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:53.992392 139956176916480 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:53.992656 139956176916480 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:53.995397 139956176916480 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:54.008095 139956176916480 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:54.008153 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:54.008189 139956176916480 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:54.008219 139956176916480 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.008281 139956176916480 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.008855 139956176916480 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.008933 139956176916480 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.009294 139956176916480 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.010078 139956176916480 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.012648 139956176916480 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.013279 139956176916480 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.013359 139956176916480 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:54.013394 139956176916480 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:54.013452 139956176916480 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.013583 139956176916480 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:54.013701 139956176916480 nn_components.py:325] mlp: activation = None
I0123 11:01:54.013742 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:54.015616 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.015711 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:54.018127 139956176916480 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.018211 139956176916480 transformer_base.py:443] tbase: final FFN
I0123 11:01:54.018321 139956176916480 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:54.020611 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:54.022473 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.022573 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:54.022876 139956176916480 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.022964 139956176916480 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 11:01:54.023076 139956176916480 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:54.023126 139956176916480 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:54.023159 139956176916480 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:54.023224 139956176916480 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.025531 139956176916480 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:54.031487 139956176916480 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.031762 139956176916480 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:54.034510 139956176916480 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:54.047259 139956176916480 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:54.047320 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:54.047358 139956176916480 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:54.047389 139956176916480 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.047455 139956176916480 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.048031 139956176916480 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.048109 139956176916480 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.048467 139956176916480 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.049216 139956176916480 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.051784 139956176916480 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.052410 139956176916480 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.052488 139956176916480 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:54.052524 139956176916480 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:54.052580 139956176916480 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.052711 139956176916480 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:54.052823 139956176916480 nn_components.py:325] mlp: activation = None
I0123 11:01:54.052862 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:54.054726 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.054823 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:54.057287 139956176916480 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.057368 139956176916480 transformer_base.py:443] tbase: final FFN
I0123 11:01:54.057476 139956176916480 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:54.059801 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:54.061671 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.061768 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:54.062061 139956176916480 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.062144 139956176916480 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 11:01:54.062258 139956176916480 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:54.062298 139956176916480 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:54.062337 139956176916480 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:54.062404 139956176916480 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.064702 139956176916480 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:54.070144 139956176916480 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.070416 139956176916480 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:54.073141 139956176916480 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:54.086257 139956176916480 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:54.086315 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:54.086351 139956176916480 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:54.086383 139956176916480 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.086447 139956176916480 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.087030 139956176916480 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.087109 139956176916480 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.087478 139956176916480 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.088245 139956176916480 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.090835 139956176916480 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.091467 139956176916480 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.091549 139956176916480 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:54.091584 139956176916480 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:54.091645 139956176916480 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.091780 139956176916480 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:54.091897 139956176916480 nn_components.py:325] mlp: activation = None
I0123 11:01:54.091938 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:54.093871 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.093967 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:54.096449 139956176916480 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.096531 139956176916480 transformer_base.py:443] tbase: final FFN
I0123 11:01:54.096647 139956176916480 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:54.098991 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:54.100936 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.101038 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:54.101350 139956176916480 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.101439 139956176916480 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 11:01:54.101556 139956176916480 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:54.101598 139956176916480 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:54.101639 139956176916480 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:54.101719 139956176916480 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.104098 139956176916480 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:54.109773 139956176916480 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.110047 139956176916480 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:54.112832 139956176916480 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:54.125756 139956176916480 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:54.125815 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:54.125852 139956176916480 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:54.125884 139956176916480 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.125948 139956176916480 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.126518 139956176916480 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.126598 139956176916480 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.126965 139956176916480 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.127897 139956176916480 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.130886 139956176916480 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.131694 139956176916480 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.131778 139956176916480 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:54.131816 139956176916480 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:54.131878 139956176916480 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.132012 139956176916480 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:54.132125 139956176916480 nn_components.py:325] mlp: activation = None
I0123 11:01:54.132166 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:54.134297 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.134395 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:54.136888 139956176916480 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.136971 139956176916480 transformer_base.py:443] tbase: final FFN
I0123 11:01:54.137084 139956176916480 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:54.139438 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:54.141353 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.141452 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:54.141764 139956176916480 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.141852 139956176916480 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 11:01:54.141966 139956176916480 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:54.142006 139956176916480 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:54.142038 139956176916480 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:54.142109 139956176916480 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.144440 139956176916480 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:54.150046 139956176916480 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.150326 139956176916480 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:54.153149 139956176916480 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:54.166333 139956176916480 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:54.166395 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:54.166431 139956176916480 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:54.166461 139956176916480 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.166525 139956176916480 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.167108 139956176916480 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.167186 139956176916480 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.167567 139956176916480 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.168345 139956176916480 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.170958 139956176916480 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.171600 139956176916480 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.171681 139956176916480 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:54.171717 139956176916480 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:54.171775 139956176916480 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.171911 139956176916480 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:54.172026 139956176916480 nn_components.py:325] mlp: activation = None
I0123 11:01:54.172068 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:54.174013 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.174111 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:54.176621 139956176916480 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.176708 139956176916480 transformer_base.py:443] tbase: final FFN
I0123 11:01:54.176821 139956176916480 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:54.179222 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:54.181160 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.181259 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:54.181560 139956176916480 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.181654 139956176916480 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 11:01:54.181773 139956176916480 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:54.181817 139956176916480 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:54.181849 139956176916480 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:54.181914 139956176916480 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.184263 139956176916480 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:54.189836 139956176916480 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.190108 139956176916480 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:54.192876 139956176916480 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:54.206320 139956176916480 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:54.206378 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:54.206415 139956176916480 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:54.206445 139956176916480 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.206508 139956176916480 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.207092 139956176916480 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.207170 139956176916480 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.207543 139956176916480 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.208253 139956176916480 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.210892 139956176916480 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.211540 139956176916480 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.211620 139956176916480 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:54.211655 139956176916480 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:54.211713 139956176916480 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.211847 139956176916480 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:54.211960 139956176916480 nn_components.py:325] mlp: activation = None
I0123 11:01:54.212001 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:54.213932 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.214030 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:54.216525 139956176916480 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.216608 139956176916480 transformer_base.py:443] tbase: final FFN
I0123 11:01:54.216721 139956176916480 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:54.219073 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:54.220987 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.221087 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:54.221388 139956176916480 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.221474 139956176916480 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 11:01:54.221588 139956176916480 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:54.221629 139956176916480 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:54.221671 139956176916480 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:54.221738 139956176916480 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.224092 139956176916480 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:54.229728 139956176916480 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.230006 139956176916480 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:54.233068 139956176916480 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:54.246154 139956176916480 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:54.246211 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:54.246248 139956176916480 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:54.246279 139956176916480 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.246345 139956176916480 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.246930 139956176916480 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.247011 139956176916480 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.247394 139956176916480 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.248135 139956176916480 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.250899 139956176916480 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.251582 139956176916480 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.251667 139956176916480 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:54.251704 139956176916480 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:54.251765 139956176916480 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.251906 139956176916480 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:54.252020 139956176916480 nn_components.py:325] mlp: activation = None
I0123 11:01:54.252061 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:54.253978 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.254074 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:54.256774 139956176916480 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.256856 139956176916480 transformer_base.py:443] tbase: final FFN
I0123 11:01:54.256969 139956176916480 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:54.259341 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:54.261283 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.261382 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:54.261692 139956176916480 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.261780 139956176916480 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 11:01:54.261892 139956176916480 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:54.261934 139956176916480 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:54.261966 139956176916480 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:54.262028 139956176916480 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.264381 139956176916480 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:54.270014 139956176916480 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.270278 139956176916480 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:54.272928 139956176916480 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:54.285332 139956176916480 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:54.285390 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:54.285425 139956176916480 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:54.285455 139956176916480 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.285521 139956176916480 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.286081 139956176916480 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.286159 139956176916480 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.286515 139956176916480 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.287203 139956176916480 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.289718 139956176916480 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.290353 139956176916480 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.290435 139956176916480 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:54.290470 139956176916480 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:54.290529 139956176916480 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.290670 139956176916480 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:54.290785 139956176916480 nn_components.py:325] mlp: activation = None
I0123 11:01:54.290826 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:54.292726 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.292822 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:54.295217 139956176916480 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.295298 139956176916480 transformer_base.py:443] tbase: final FFN
I0123 11:01:54.295408 139956176916480 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:54.297636 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:54.299502 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.299597 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:54.299892 139956176916480 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.299976 139956176916480 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 11:01:54.300084 139956176916480 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:54.300123 139956176916480 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:54.300154 139956176916480 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:54.300215 139956176916480 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.302447 139956176916480 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:54.307802 139956176916480 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.308064 139956176916480 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:54.310780 139956176916480 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:54.323837 139956176916480 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:54.323897 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:54.323932 139956176916480 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:54.323962 139956176916480 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.324024 139956176916480 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.324587 139956176916480 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.324663 139956176916480 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.325023 139956176916480 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.325730 139956176916480 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.328252 139956176916480 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.328871 139956176916480 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.328950 139956176916480 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:54.328984 139956176916480 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:54.329042 139956176916480 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.329171 139956176916480 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:54.329282 139956176916480 nn_components.py:325] mlp: activation = None
I0123 11:01:54.329321 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:54.331175 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.331272 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:54.333667 139956176916480 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.333748 139956176916480 transformer_base.py:443] tbase: final FFN
I0123 11:01:54.333858 139956176916480 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:54.336097 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:54.337958 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.338057 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:54.338349 139956176916480 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.338438 139956176916480 decoder_stack.py:344] dstack: Final layernorm.
I0123 11:01:54.341282 139956176916480 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:54.392568 139956176916480 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.392662 139956176916480 decoder_stack.py:333] dstack: autoregressive generator.
I0123 11:01:54.392718 139956176916480 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 11:01:54.392825 139956176916480 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:54.392866 139956176916480 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:54.392903 139956176916480 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:54.392968 139956176916480 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.395273 139956176916480 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:54.400745 139956176916480 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.401012 139956176916480 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:54.403621 139956176916480 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:54.415985 139956176916480 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:54.416042 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:54.416077 139956176916480 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:54.416107 139956176916480 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.416171 139956176916480 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.416737 139956176916480 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.416815 139956176916480 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.417186 139956176916480 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.417888 139956176916480 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.420330 139956176916480 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.420948 139956176916480 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.421026 139956176916480 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:54.421060 139956176916480 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:54.421118 139956176916480 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.421245 139956176916480 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:54.421353 139956176916480 nn_components.py:325] mlp: activation = None
I0123 11:01:54.421391 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:54.423323 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.423420 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:54.425820 139956176916480 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.425902 139956176916480 transformer_base.py:443] tbase: final FFN
I0123 11:01:54.426010 139956176916480 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:54.428189 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:54.430044 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.430141 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:54.430443 139956176916480 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.430526 139956176916480 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 11:01:54.430633 139956176916480 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:54.430671 139956176916480 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:54.430709 139956176916480 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:54.430775 139956176916480 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.433018 139956176916480 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:54.438491 139956176916480 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.438761 139956176916480 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:54.441382 139956176916480 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:54.453627 139956176916480 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:54.453690 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:54.453725 139956176916480 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:54.453755 139956176916480 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.453817 139956176916480 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.454380 139956176916480 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.454457 139956176916480 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.454821 139956176916480 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.455515 139956176916480 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.457978 139956176916480 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.458593 139956176916480 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.458670 139956176916480 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:54.458704 139956176916480 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:54.458761 139956176916480 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.458888 139956176916480 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:54.458998 139956176916480 nn_components.py:325] mlp: activation = None
I0123 11:01:54.459038 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:54.460948 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.461041 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:54.463455 139956176916480 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.463536 139956176916480 transformer_base.py:443] tbase: final FFN
I0123 11:01:54.463645 139956176916480 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:54.465826 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:54.467655 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.467749 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:54.468049 139956176916480 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.468131 139956176916480 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 11:01:54.468238 139956176916480 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:54.468276 139956176916480 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:54.468306 139956176916480 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:54.468377 139956176916480 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.470711 139956176916480 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:54.476158 139956176916480 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.476429 139956176916480 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:54.479198 139956176916480 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:54.491571 139956176916480 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:54.491629 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:54.491664 139956176916480 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:54.491695 139956176916480 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.491758 139956176916480 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.492318 139956176916480 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.492397 139956176916480 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.492761 139956176916480 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.493446 139956176916480 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.496057 139956176916480 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.496680 139956176916480 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.496758 139956176916480 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:54.496793 139956176916480 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:54.496850 139956176916480 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.496978 139956176916480 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:54.497090 139956176916480 nn_components.py:325] mlp: activation = None
I0123 11:01:54.497129 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:54.499533 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.499630 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:54.502042 139956176916480 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.502124 139956176916480 transformer_base.py:443] tbase: final FFN
I0123 11:01:54.502233 139956176916480 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:54.504401 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:54.506244 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.506341 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:54.506631 139956176916480 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.506715 139956176916480 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 11:01:54.506822 139956176916480 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:54.506861 139956176916480 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:54.506891 139956176916480 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:54.506952 139956176916480 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.509188 139956176916480 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:54.514631 139956176916480 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.514894 139956176916480 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:54.517460 139956176916480 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:54.529733 139956176916480 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:54.529792 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:54.529828 139956176916480 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:54.529858 139956176916480 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.529921 139956176916480 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.530473 139956176916480 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.530550 139956176916480 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.530903 139956176916480 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.531588 139956176916480 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.534028 139956176916480 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.534641 139956176916480 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.534720 139956176916480 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:54.534754 139956176916480 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:54.534812 139956176916480 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.534943 139956176916480 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:54.535052 139956176916480 nn_components.py:325] mlp: activation = None
I0123 11:01:54.535092 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:54.536996 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.537091 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:54.539493 139956176916480 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.539574 139956176916480 transformer_base.py:443] tbase: final FFN
I0123 11:01:54.539683 139956176916480 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:54.541861 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:54.543686 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.543782 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:54.544073 139956176916480 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.544155 139956176916480 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 11:01:54.544262 139956176916480 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:54.544301 139956176916480 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:54.544332 139956176916480 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:54.544395 139956176916480 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.546667 139956176916480 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:54.552237 139956176916480 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.552514 139956176916480 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:54.555192 139956176916480 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:54.568136 139956176916480 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:54.568196 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:54.568232 139956176916480 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:54.568263 139956176916480 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.568326 139956176916480 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.568896 139956176916480 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.568978 139956176916480 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.569354 139956176916480 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.570082 139956176916480 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.572633 139956176916480 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.573272 139956176916480 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.573353 139956176916480 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:54.573388 139956176916480 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:54.573447 139956176916480 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.573580 139956176916480 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:54.573702 139956176916480 nn_components.py:325] mlp: activation = None
I0123 11:01:54.573745 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:54.575731 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.575830 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:54.578340 139956176916480 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.578425 139956176916480 transformer_base.py:443] tbase: final FFN
I0123 11:01:54.578538 139956176916480 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:54.580794 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:54.582678 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.582775 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:54.583065 139956176916480 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.583148 139956176916480 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 11:01:54.583257 139956176916480 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:54.583297 139956176916480 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:54.583327 139956176916480 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:54.583389 139956176916480 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.585597 139956176916480 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:54.591071 139956176916480 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.591334 139956176916480 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:54.593911 139956176916480 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:54.606239 139956176916480 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:54.606296 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:54.606331 139956176916480 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:54.606362 139956176916480 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.606426 139956176916480 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.606977 139956176916480 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.607056 139956176916480 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.607414 139956176916480 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.608093 139956176916480 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.610565 139956176916480 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.611201 139956176916480 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.611282 139956176916480 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:54.611317 139956176916480 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:54.611377 139956176916480 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.611510 139956176916480 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:54.611624 139956176916480 nn_components.py:325] mlp: activation = None
I0123 11:01:54.611665 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:54.613993 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.614090 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:54.616541 139956176916480 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.616623 139956176916480 transformer_base.py:443] tbase: final FFN
I0123 11:01:54.616731 139956176916480 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:54.618925 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:54.620796 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.620891 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:54.621183 139956176916480 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.621266 139956176916480 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 11:01:54.621374 139956176916480 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:54.621413 139956176916480 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:54.621445 139956176916480 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:54.621507 139956176916480 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.623734 139956176916480 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:54.629150 139956176916480 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.629434 139956176916480 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:54.632020 139956176916480 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:54.644386 139956176916480 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:54.644443 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:54.644478 139956176916480 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:54.644508 139956176916480 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.644569 139956176916480 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.645119 139956176916480 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.645196 139956176916480 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.645554 139956176916480 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.646240 139956176916480 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.648695 139956176916480 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.649309 139956176916480 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.649387 139956176916480 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:54.649422 139956176916480 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:54.649479 139956176916480 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.649608 139956176916480 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:54.649726 139956176916480 nn_components.py:325] mlp: activation = None
I0123 11:01:54.649768 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:54.651661 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.651756 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:54.654178 139956176916480 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.654260 139956176916480 transformer_base.py:443] tbase: final FFN
I0123 11:01:54.654370 139956176916480 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:54.656542 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:54.658376 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.658474 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:54.658767 139956176916480 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.658851 139956176916480 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 11:01:54.658961 139956176916480 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:54.659001 139956176916480 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:54.659033 139956176916480 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:54.659287 139956176916480 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.661514 139956176916480 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:54.666917 139956176916480 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.667191 139956176916480 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:54.669759 139956176916480 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:54.682094 139956176916480 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:54.682152 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:54.682186 139956176916480 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:54.682216 139956176916480 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.682279 139956176916480 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.682841 139956176916480 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.682918 139956176916480 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.683282 139956176916480 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.683962 139956176916480 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.686424 139956176916480 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.687034 139956176916480 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.687112 139956176916480 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:54.687146 139956176916480 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:54.687204 139956176916480 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.687331 139956176916480 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:54.687440 139956176916480 nn_components.py:325] mlp: activation = None
I0123 11:01:54.687479 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:54.689381 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.689473 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:54.691913 139956176916480 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.691993 139956176916480 transformer_base.py:443] tbase: final FFN
I0123 11:01:54.692103 139956176916480 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:54.694293 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:54.696123 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.696217 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:54.696509 139956176916480 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.696590 139956176916480 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 11:01:54.696699 139956176916480 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:54.696739 139956176916480 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:54.696769 139956176916480 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:54.696831 139956176916480 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.699063 139956176916480 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:54.704461 139956176916480 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.704724 139956176916480 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:54.707329 139956176916480 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:54.719563 139956176916480 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:54.719619 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:54.719654 139956176916480 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:54.719684 139956176916480 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.719745 139956176916480 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.720294 139956176916480 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.720371 139956176916480 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.720731 139956176916480 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.721413 139956176916480 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.724014 139956176916480 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.724620 139956176916480 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.724696 139956176916480 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:54.724730 139956176916480 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:54.724785 139956176916480 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.724913 139956176916480 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:54.725021 139956176916480 nn_components.py:325] mlp: activation = None
I0123 11:01:54.725061 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:54.727350 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.727446 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:54.729806 139956176916480 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.729886 139956176916480 transformer_base.py:443] tbase: final FFN
I0123 11:01:54.729994 139956176916480 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:54.732125 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:54.733930 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.734026 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:54.734318 139956176916480 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.734400 139956176916480 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 11:01:54.734507 139956176916480 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:54.734545 139956176916480 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:54.734575 139956176916480 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:54.734636 139956176916480 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.736831 139956176916480 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:54.742239 139956176916480 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.742503 139956176916480 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:54.745103 139956176916480 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:54.757325 139956176916480 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:54.757382 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:54.757417 139956176916480 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:54.757448 139956176916480 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.757509 139956176916480 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.758068 139956176916480 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.758146 139956176916480 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.758499 139956176916480 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.759174 139956176916480 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.761600 139956176916480 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.762214 139956176916480 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.762291 139956176916480 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:54.762325 139956176916480 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:54.762383 139956176916480 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.762514 139956176916480 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:54.762627 139956176916480 nn_components.py:325] mlp: activation = None
I0123 11:01:54.762667 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:54.764597 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.764690 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:54.767056 139956176916480 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.767136 139956176916480 transformer_base.py:443] tbase: final FFN
I0123 11:01:54.767243 139956176916480 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:54.769384 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:54.771231 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.771330 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:54.771630 139956176916480 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.771713 139956176916480 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 11:01:54.771824 139956176916480 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:54.771866 139956176916480 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:54.771896 139956176916480 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:54.771958 139956176916480 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.774166 139956176916480 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:54.779527 139956176916480 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.779788 139956176916480 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:54.782331 139956176916480 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:54.794717 139956176916480 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:54.794775 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:54.794810 139956176916480 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:54.794840 139956176916480 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.794900 139956176916480 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.795452 139956176916480 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.795544 139956176916480 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.795912 139956176916480 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.796620 139956176916480 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.799078 139956176916480 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.799687 139956176916480 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.799764 139956176916480 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:54.799798 139956176916480 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:54.799857 139956176916480 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.799983 139956176916480 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:54.800092 139956176916480 nn_components.py:325] mlp: activation = None
I0123 11:01:54.800132 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:54.802041 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.802134 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:54.804568 139956176916480 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.804647 139956176916480 transformer_base.py:443] tbase: final FFN
I0123 11:01:54.804757 139956176916480 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:54.806958 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:54.808813 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.808908 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:54.809197 139956176916480 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.809278 139956176916480 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 11:01:54.809384 139956176916480 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:54.809423 139956176916480 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:54.809453 139956176916480 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:54.809514 139956176916480 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.811722 139956176916480 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:54.817232 139956176916480 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.817494 139956176916480 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:54.820066 139956176916480 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:54.832695 139956176916480 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:54.832760 139956176916480 attention.py:418] Single window, no scan.
I0123 11:01:54.832797 139956176916480 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:54.832827 139956176916480 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.832887 139956176916480 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.833446 139956176916480 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.833521 139956176916480 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.833881 139956176916480 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.834561 139956176916480 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.837005 139956176916480 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.837611 139956176916480 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.837695 139956176916480 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:54.837729 139956176916480 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:54.837785 139956176916480 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.837911 139956176916480 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:54.838020 139956176916480 nn_components.py:325] mlp: activation = None
I0123 11:01:54.838059 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:54.840317 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.840410 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:54.842796 139956176916480 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.842879 139956176916480 transformer_base.py:443] tbase: final FFN
I0123 11:01:54.842987 139956176916480 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:54.845133 139956176916480 nn_components.py:329] mlp: final activation = None
I0123 11:01:54.846949 139956176916480 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.847045 139956176916480 nn_components.py:261] mlp: residual
I0123 11:01:54.847337 139956176916480 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:54.847423 139956176916480 decoder_stack.py:344] dstack: Final layernorm.
I0123 11:01:54.850582 139956176916480 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:02:12.715662 139956176916480 alphageometry.py:566] LM output (score=-0.363738): "k : C b f k 13 D b k f k 14 ;"
I0123 11:02:12.716073 139956176916480 alphageometry.py:567] Translation: "k = on_line k b f, on_bline k f b"

I0123 11:02:12.716132 139956176916480 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c b, on_line f a e; g = on_line g b a, on_line g e c; h = midpoint h b e; i = midpoint i c a; j = midpoint j f g; k = on_line k b f, on_bline k f b ? coll i h j"
I0123 11:02:12.716277 139956176916480 graph.py:498] 
I0123 11:02:12.716341 139956176916480 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a; f = on_line f c b, on_line f a e; g = on_line g b a, on_line g e c; h = midpoint h b e; i = midpoint i c a; j = midpoint j f g; k = on_line k b f, on_bline k f b ? coll i h j
I0123 11:02:13.691099 139956176916480 ddar.py:60] Depth 1/1000 time = 0.9350340366363525
I0123 11:02:15.380169 139956176916480 ddar.py:60] Depth 2/1000 time = 1.6888377666473389
I0123 11:02:17.039180 139956176916480 ddar.py:60] Depth 3/1000 time = 1.6587467193603516
I0123 11:02:18.762716 139956176916480 ddar.py:60] Depth 4/1000 time = 1.7232139110565186
I0123 11:02:20.923541 139956176916480 ddar.py:60] Depth 5/1000 time = 2.160599708557129
I0123 11:02:23.089681 139956176916480 ddar.py:60] Depth 6/1000 time = 2.165877342224121
I0123 11:02:25.162551 139956176916480 ddar.py:60] Depth 7/1000 time = 2.072597026824951
I0123 11:02:27.449287 139956176916480 ddar.py:60] Depth 8/1000 time = 2.2861857414245605
I0123 11:02:29.670701 139956176916480 ddar.py:60] Depth 9/1000 time = 2.181781530380249
I0123 11:02:31.896739 139956176916480 ddar.py:60] Depth 10/1000 time = 2.2257955074310303
I0123 11:02:31.916611 139956176916480 alphageometry.py:191] 
==========================
 * From theorem premises:
A B C D E F G H I J : Points
DC = DB [00]
DB = DA [01]
DE = DA [02]
B,F,C are collinear [03]
A,E,F are collinear [04]
E,G,C are collinear [05]
A,B,G are collinear [06]
E,B,H are collinear [07]
HB = HE [08]
A,I,C are collinear [09]
IC = IA [10]
G,F,J are collinear [11]
JF = JG [12]

 * Auxiliary Constructions:
K : Points
B,F,K are collinear [13]
KF = KB [14]

 * Proof steps:
001. E,B,H are collinear [07] & HB = HE [08]   H is midpoint of BE [15]
002. A,I,C are collinear [09] & IC = IA [10]   I is midpoint of CA [16]
003. H is midpoint of BE [15] & I is midpoint of CA [16]   EH:EB = AI:AC [17]
004. EH:EB = AI:AC [17] & BH = EH [08] & IC = AI [10]   BH:EB = IC:AC [18]
005. DC = DB [00] & DB = DA [01] & DE = DA [02]   A,E,B,C are concyclic [19]
006. A,E,B,C are concyclic [19]   ABE = ACE [20]
007. A,E,B,C are concyclic [19]   CAB = CEB [21]
008. A,E,B,C are concyclic [19]   AEB = ACB [22]
009. A,E,B,C are concyclic [19]   EAB = ECB [23]
010. E,G,C are collinear [05] & A,B,G are collinear [06] & ACE = ABE [20]   ACG = GBE [24]
011. A,G,B are collinear [06] & E,G,C are collinear [05]   AGC = BGE [25]
012. ACG = GBE [24] & AGC = BGE [25] (Similar Triangles)  AC:AG = EB:EG [26]
013. ACG = GBE [24] & AGC = BGE [25] (Similar Triangles)  AC:EB = GC:BG [27]
014. BH:EB = IC:AC [18] & AC:AG = EB:EG [26]   BH:EG = IC:AG [28]
015. BH:EG = IC:AG [28] & HB = HE [08] & IC = IA [10]   EH:EG = AI:AG [29]
016. A,I,C are collinear [09] & A,G,B are collinear [06] & E,G,C are collinear [05] & E,B,H are collinear [07] & CAB = CEB [21]   IAG = GEH [30]
017. EH:EG = AI:AG [29] & IAG = GEH [30] (Similar Triangles)  AI:IG = EH:GH [31]
018. EH:EG = AI:AG [29] & IAG = GEH [30] (Similar Triangles)  AIG = GHE [32]
019. EH:GH = AI:IG [31] & BH = EH [08] & IC = AI [10]   BH:GH = IC:IG [33]
020. G,F,J are collinear [11] & JF = JG [12]   J is midpoint of FG [34]
021. I is midpoint of CA [16] & J is midpoint of FG [34]   AI:AC = GJ:GF [35]
022. AI:AC = GJ:GF [35] & IC = AI [10]   IC:AC = GJ:GF [36]
023. B,F,C are collinear [03] & A,E,F are collinear [04] & ACB = AEB [22]   ACF = FEB [37]
024. A,E,F are collinear [04] & B,F,C are collinear [03]   AFC = EFB [38]
025. ACF = FEB [37] & AFC = EFB [38] (Similar Triangles)  AC:EB = FC:EF [39]
026. AC:EB = GC:BG [27] & AC:EB = FC:EF [39]   FC:EF = GC:BG [40]
027. K,B,F are collinear [13] & KF = KB [14]   K is midpoint of BF [41]
028. K is midpoint of BF [41] & J is midpoint of FG [34]   KJ  BG [42]
029. BG  KJ [42] & B,K,F are collinear [13] & G,F,J are collinear [11]   FB:FK = BG:KJ [43]
030. K is midpoint of BF [41] & H is midpoint of BE [15]   KH  FE [44]
031. KH  FE [44] & B,K,F are collinear [13] & E,B,H are collinear [07]   BF:BK = EF:KH [45]
032. FB:FK = BG:KJ [43] & KF = KB [14] & BF:BK = EF:KH [45]   EF:KH = BG:KJ [46]
033. FC:EF = GC:BG [40] & EF:KH = BG:KJ [46]   FC:GC = KH:KJ [47]
034. E,G,C are collinear [05] & B,F,C are collinear [03] & EAB = ECB [23] & KH  FE [44] & A,E,F are collinear [04] & KJ  BG [42] & A,B,G are collinear [06]   HKJ = GCF [48]
035. FC:GC = KH:KJ [47] & HKJ = GCF [48] (Similar Triangles)  FC:KH = GF:JH [49]
036. FC:GC = KH:KJ [47] & HKJ = GCF [48] (Similar Triangles)  KHJ = GFC [50]
037. B,F,C are collinear [03] & E,B,H are collinear [07] & ACB = AEB [22] & KH  FE [44] & A,E,F are collinear [04]   ACF = KHB [51]
038. A,E,F are collinear [04] & B,F,C are collinear [03] & B,F,K are collinear [13] & KH  FE [44]   AFC = HKB [52]
039. ACF = KHB [51] & AFC = HKB [52] (Similar Triangles)  CA:HB = CF:HK [53]
040. FC:KH = GF:JH [49] & CA:HB = CF:HK [53] & HB = HE [08]   AC:BH = GF:JH [54]
041. IC:AC = GJ:GF [36] & AC:BH = GF:JH [54]   IC:GJ = BH:JH [55]
042. BH:GH = IC:IG [33] & BH:JH = IC:GJ [55]   IG:GJ = GH:JH [56]
043. KHJ = GFC [50] & B,F,C are collinear [03] & KH  FE [44] & A,E,F are collinear [04]   (AE-HJ) = (FG-BC) [57]
044. AIG = GHE [32] & A,I,C are collinear [09] & E,B,H are collinear [07]   (AC-GI) = (GH-BE) [58]
045. AEB = ACB [22] & (AE-HJ) = (FG-BC) [57] & (AC-GI) = (GH-BE) [58] (Angle chase)  IGF = JHG [59]
046. G,F,J are collinear [11] & IGF = JHG [59]   IGJ = JHG [60]
047. IG:GJ = GH:JH [56] & IGJ = JHG [60] (Similar Triangles)  IJG = HJG [61]
048. IJG = HJG [61] & G,F,J are collinear [11]   IJ  HJ [62]
049. IJ  HJ [62]   I,J,H are collinear
==========================

I0123 11:02:31.916741 139956176916480 alphageometry.py:582] Solved.
