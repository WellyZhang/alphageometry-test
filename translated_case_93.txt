I0123 13:27:05.130445 140123132608512 inference_utils.py:69] Parsing gin configuration.
I0123 13:27:05.130544 140123132608512 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0123 13:27:05.130743 140123132608512 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0123 13:27:05.130777 140123132608512 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0123 13:27:05.130806 140123132608512 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0123 13:27:05.130836 140123132608512 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0123 13:27:05.130863 140123132608512 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0123 13:27:05.130889 140123132608512 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0123 13:27:05.130915 140123132608512 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0123 13:27:05.130940 140123132608512 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=32
I0123 13:27:05.130964 140123132608512 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0123 13:27:05.130989 140123132608512 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0123 13:27:05.131033 140123132608512 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0123 13:27:05.131166 140123132608512 resource_reader.py:55] Path not found: base_htrans.gin
I0123 13:27:05.131378 140123132608512 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0123 13:27:05.131477 140123132608512 resource_reader.py:55] Path not found: trainer_configuration.gin
I0123 13:27:05.137748 140123132608512 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0123 13:27:05.137866 140123132608512 resource_reader.py:55] Path not found: size/medium_150M.gin
I0123 13:27:05.138189 140123132608512 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0123 13:27:05.138294 140123132608512 resource_reader.py:55] Path not found: options/positions_t5.gin
I0123 13:27:05.138574 140123132608512 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0123 13:27:05.138676 140123132608512 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0123 13:27:05.139082 140123132608512 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0123 13:27:05.139184 140123132608512 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0123 13:27:05.142812 140123132608512 training_loop.py:334] ==== Training loop: initializing model ====
I0123 13:27:05.236662 140123132608512 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0123 13:27:05.237401 140123132608512 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0123 13:27:05.243645 140123132608512 training_loop.py:335] Process 0 of 1
I0123 13:27:05.243701 140123132608512 training_loop.py:336] Local device count = 1
I0123 13:27:05.243741 140123132608512 training_loop.py:337] Number of replicas = 1
I0123 13:27:05.243772 140123132608512 training_loop.py:339] Using random number seed 42
I0123 13:27:05.720069 140123132608512 training_loop.py:359] Initializing the model.
I0123 13:27:06.089501 140123132608512 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.089766 140123132608512 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 13:27:06.089872 140123132608512 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 13:27:06.089955 140123132608512 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 13:27:06.090036 140123132608512 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 13:27:06.090121 140123132608512 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 13:27:06.090198 140123132608512 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 13:27:06.090272 140123132608512 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 13:27:06.090345 140123132608512 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 13:27:06.090417 140123132608512 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 13:27:06.090490 140123132608512 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 13:27:06.090562 140123132608512 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 13:27:06.090634 140123132608512 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 13:27:06.090707 140123132608512 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 13:27:06.090747 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:06.090794 140123132608512 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 13:27:06.090913 140123132608512 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:27:06.090953 140123132608512 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:27:06.090984 140123132608512 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:27:06.093034 140123132608512 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.098526 140123132608512 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:27:06.109407 140123132608512 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.109685 140123132608512 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:27:06.114176 140123132608512 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:27:06.125041 140123132608512 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:27:06.125101 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:06.125139 140123132608512 transformer_layer.py:389] tlayer: self-attention.
I0123 13:27:06.125172 140123132608512 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.125236 140123132608512 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.126462 140123132608512 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.126545 140123132608512 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.127303 140123132608512 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.129802 140123132608512 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.135755 140123132608512 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.137498 140123132608512 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.137579 140123132608512 transformer_layer.py:468] tlayer: End windows.
I0123 13:27:06.137614 140123132608512 transformer_layer.py:472] tlayer: final FFN.
I0123 13:27:06.137681 140123132608512 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.137814 140123132608512 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:27:06.138167 140123132608512 nn_components.py:325] mlp: activation = None
I0123 13:27:06.138216 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:06.140230 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.140331 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:06.143343 140123132608512 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.143424 140123132608512 transformer_base.py:443] tbase: final FFN
I0123 13:27:06.143929 140123132608512 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:27:06.154383 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:06.163489 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.163588 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:06.163888 140123132608512 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.163969 140123132608512 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 13:27:06.164081 140123132608512 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:27:06.164120 140123132608512 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:27:06.164150 140123132608512 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:27:06.166035 140123132608512 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.168613 140123132608512 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:27:06.174363 140123132608512 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.174634 140123132608512 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:27:06.177321 140123132608512 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:27:06.181528 140123132608512 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:27:06.181583 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:06.181619 140123132608512 transformer_layer.py:389] tlayer: self-attention.
I0123 13:27:06.181657 140123132608512 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.181721 140123132608512 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.182306 140123132608512 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.182384 140123132608512 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.182769 140123132608512 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.183560 140123132608512 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.186100 140123132608512 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.186750 140123132608512 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.186829 140123132608512 transformer_layer.py:468] tlayer: End windows.
I0123 13:27:06.186865 140123132608512 transformer_layer.py:472] tlayer: final FFN.
I0123 13:27:06.186926 140123132608512 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.187058 140123132608512 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:27:06.187393 140123132608512 nn_components.py:325] mlp: activation = None
I0123 13:27:06.187436 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:06.189390 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.189484 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:06.192092 140123132608512 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.192175 140123132608512 transformer_base.py:443] tbase: final FFN
I0123 13:27:06.192612 140123132608512 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:27:06.194987 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:06.196933 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.197029 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:06.197328 140123132608512 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.197408 140123132608512 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 13:27:06.197518 140123132608512 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:27:06.197556 140123132608512 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:27:06.197587 140123132608512 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:27:06.199571 140123132608512 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.202496 140123132608512 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:27:06.208853 140123132608512 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.209136 140123132608512 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:27:06.211895 140123132608512 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:27:06.215848 140123132608512 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:27:06.215907 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:06.215944 140123132608512 transformer_layer.py:389] tlayer: self-attention.
I0123 13:27:06.215974 140123132608512 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.216036 140123132608512 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.216623 140123132608512 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.216701 140123132608512 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.217065 140123132608512 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.217844 140123132608512 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.220441 140123132608512 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.221120 140123132608512 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.221197 140123132608512 transformer_layer.py:468] tlayer: End windows.
I0123 13:27:06.221232 140123132608512 transformer_layer.py:472] tlayer: final FFN.
I0123 13:27:06.221292 140123132608512 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.221422 140123132608512 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:27:06.221763 140123132608512 nn_components.py:325] mlp: activation = None
I0123 13:27:06.221808 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:06.223783 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.223877 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:06.226474 140123132608512 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.226563 140123132608512 transformer_base.py:443] tbase: final FFN
I0123 13:27:06.227082 140123132608512 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:27:06.229402 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:06.231383 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.231480 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:06.231782 140123132608512 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.231863 140123132608512 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 13:27:06.231975 140123132608512 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:27:06.232013 140123132608512 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:27:06.232044 140123132608512 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:27:06.234000 140123132608512 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.236470 140123132608512 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:27:06.242221 140123132608512 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.242496 140123132608512 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:27:06.245174 140123132608512 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:27:06.249023 140123132608512 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:27:06.249078 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:06.249116 140123132608512 transformer_layer.py:389] tlayer: self-attention.
I0123 13:27:06.249146 140123132608512 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.249212 140123132608512 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.249787 140123132608512 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.249863 140123132608512 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.250236 140123132608512 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.251043 140123132608512 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.253615 140123132608512 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.254263 140123132608512 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.254342 140123132608512 transformer_layer.py:468] tlayer: End windows.
I0123 13:27:06.254378 140123132608512 transformer_layer.py:472] tlayer: final FFN.
I0123 13:27:06.254440 140123132608512 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.254577 140123132608512 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:27:06.254917 140123132608512 nn_components.py:325] mlp: activation = None
I0123 13:27:06.254962 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:06.256918 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.257014 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:06.259666 140123132608512 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.259757 140123132608512 transformer_base.py:443] tbase: final FFN
I0123 13:27:06.260196 140123132608512 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:27:06.262495 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:06.264436 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.264531 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:06.264825 140123132608512 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.264905 140123132608512 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 13:27:06.265016 140123132608512 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:27:06.265055 140123132608512 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:27:06.265085 140123132608512 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:27:06.267059 140123132608512 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.269465 140123132608512 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:27:06.275208 140123132608512 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.275645 140123132608512 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:27:06.278398 140123132608512 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:27:06.282222 140123132608512 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:27:06.282280 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:06.282318 140123132608512 transformer_layer.py:389] tlayer: self-attention.
I0123 13:27:06.282349 140123132608512 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.282413 140123132608512 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.283008 140123132608512 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.283088 140123132608512 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.283463 140123132608512 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.284246 140123132608512 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.287218 140123132608512 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.287855 140123132608512 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.287933 140123132608512 transformer_layer.py:468] tlayer: End windows.
I0123 13:27:06.287967 140123132608512 transformer_layer.py:472] tlayer: final FFN.
I0123 13:27:06.288028 140123132608512 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.288162 140123132608512 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:27:06.288489 140123132608512 nn_components.py:325] mlp: activation = None
I0123 13:27:06.288531 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:06.290483 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.290581 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:06.293190 140123132608512 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.293271 140123132608512 transformer_base.py:443] tbase: final FFN
I0123 13:27:06.293718 140123132608512 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:27:06.296056 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:06.298045 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.298145 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:06.298457 140123132608512 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.298540 140123132608512 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 13:27:06.298655 140123132608512 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:27:06.298695 140123132608512 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:27:06.298727 140123132608512 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:27:06.300638 140123132608512 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.303100 140123132608512 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:27:06.308819 140123132608512 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.309076 140123132608512 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:27:06.311852 140123132608512 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:27:06.315654 140123132608512 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:27:06.315709 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:06.315745 140123132608512 transformer_layer.py:389] tlayer: self-attention.
I0123 13:27:06.315776 140123132608512 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.315838 140123132608512 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.316451 140123132608512 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.316529 140123132608512 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.316895 140123132608512 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.317676 140123132608512 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.320250 140123132608512 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.320880 140123132608512 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.320956 140123132608512 transformer_layer.py:468] tlayer: End windows.
I0123 13:27:06.320991 140123132608512 transformer_layer.py:472] tlayer: final FFN.
I0123 13:27:06.321050 140123132608512 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.321178 140123132608512 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:27:06.321498 140123132608512 nn_components.py:325] mlp: activation = None
I0123 13:27:06.321546 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:06.323532 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.323626 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:06.326243 140123132608512 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.326330 140123132608512 transformer_base.py:443] tbase: final FFN
I0123 13:27:06.326787 140123132608512 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:27:06.329125 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:06.331116 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.331216 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:06.331524 140123132608512 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.331609 140123132608512 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 13:27:06.331725 140123132608512 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:27:06.331765 140123132608512 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:27:06.331796 140123132608512 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:27:06.333671 140123132608512 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.336224 140123132608512 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:27:06.341938 140123132608512 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.342212 140123132608512 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:27:06.344912 140123132608512 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:27:06.348752 140123132608512 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:27:06.348808 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:06.348844 140123132608512 transformer_layer.py:389] tlayer: self-attention.
I0123 13:27:06.348874 140123132608512 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.348937 140123132608512 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.349506 140123132608512 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.349581 140123132608512 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.349950 140123132608512 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.350762 140123132608512 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.353288 140123132608512 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.353917 140123132608512 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.353996 140123132608512 transformer_layer.py:468] tlayer: End windows.
I0123 13:27:06.354032 140123132608512 transformer_layer.py:472] tlayer: final FFN.
I0123 13:27:06.354093 140123132608512 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.354229 140123132608512 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:27:06.354563 140123132608512 nn_components.py:325] mlp: activation = None
I0123 13:27:06.354607 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:06.356601 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.356695 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:06.359256 140123132608512 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.359340 140123132608512 transformer_base.py:443] tbase: final FFN
I0123 13:27:06.359775 140123132608512 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:27:06.362442 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:06.364390 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.364491 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:06.364795 140123132608512 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.364876 140123132608512 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 13:27:06.364988 140123132608512 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:27:06.365028 140123132608512 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:27:06.365059 140123132608512 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:27:06.503175 140123132608512 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.506329 140123132608512 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:27:06.512394 140123132608512 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.512696 140123132608512 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:27:06.515549 140123132608512 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:27:06.519577 140123132608512 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:27:06.519637 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:06.519677 140123132608512 transformer_layer.py:389] tlayer: self-attention.
I0123 13:27:06.519710 140123132608512 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.519779 140123132608512 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.520601 140123132608512 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.520680 140123132608512 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.521055 140123132608512 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.521875 140123132608512 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.524559 140123132608512 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.525220 140123132608512 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.525300 140123132608512 transformer_layer.py:468] tlayer: End windows.
I0123 13:27:06.525483 140123132608512 transformer_layer.py:472] tlayer: final FFN.
I0123 13:27:06.525547 140123132608512 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.525691 140123132608512 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:27:06.526041 140123132608512 nn_components.py:325] mlp: activation = None
I0123 13:27:06.526086 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:06.528072 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.528169 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:06.530884 140123132608512 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.530966 140123132608512 transformer_base.py:443] tbase: final FFN
I0123 13:27:06.531428 140123132608512 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:27:06.533827 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:06.535822 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.535929 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:06.536239 140123132608512 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.536324 140123132608512 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 13:27:06.536438 140123132608512 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:27:06.536479 140123132608512 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:27:06.536511 140123132608512 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:27:06.538535 140123132608512 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.541029 140123132608512 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:27:06.546908 140123132608512 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.547188 140123132608512 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:27:06.549980 140123132608512 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:27:06.553930 140123132608512 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:27:06.553987 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:06.554024 140123132608512 transformer_layer.py:389] tlayer: self-attention.
I0123 13:27:06.554056 140123132608512 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.554119 140123132608512 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.554706 140123132608512 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.554782 140123132608512 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.555150 140123132608512 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.555954 140123132608512 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.558640 140123132608512 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.559281 140123132608512 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.559360 140123132608512 transformer_layer.py:468] tlayer: End windows.
I0123 13:27:06.559396 140123132608512 transformer_layer.py:472] tlayer: final FFN.
I0123 13:27:06.559457 140123132608512 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.559589 140123132608512 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:27:06.559917 140123132608512 nn_components.py:325] mlp: activation = None
I0123 13:27:06.559962 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:06.561906 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.562006 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:06.564677 140123132608512 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.564757 140123132608512 transformer_base.py:443] tbase: final FFN
I0123 13:27:06.565199 140123132608512 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:27:06.567555 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:06.569575 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.569680 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:06.569982 140123132608512 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.570070 140123132608512 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 13:27:06.570190 140123132608512 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:27:06.570231 140123132608512 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:27:06.570263 140123132608512 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:27:06.572164 140123132608512 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.574700 140123132608512 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:27:06.580399 140123132608512 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.580672 140123132608512 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:27:06.583784 140123132608512 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:27:06.587619 140123132608512 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:27:06.587676 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:06.587713 140123132608512 transformer_layer.py:389] tlayer: self-attention.
I0123 13:27:06.587744 140123132608512 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.587808 140123132608512 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.588417 140123132608512 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.588495 140123132608512 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.588865 140123132608512 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.589670 140123132608512 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.592321 140123132608512 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.592967 140123132608512 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.593047 140123132608512 transformer_layer.py:468] tlayer: End windows.
I0123 13:27:06.593085 140123132608512 transformer_layer.py:472] tlayer: final FFN.
I0123 13:27:06.593147 140123132608512 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.593290 140123132608512 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:27:06.593621 140123132608512 nn_components.py:325] mlp: activation = None
I0123 13:27:06.593675 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:06.595659 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.595759 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:06.598409 140123132608512 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.598493 140123132608512 transformer_base.py:443] tbase: final FFN
I0123 13:27:06.598937 140123132608512 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:27:06.601310 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:06.603267 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.603369 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:06.603672 140123132608512 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.603764 140123132608512 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 13:27:06.603879 140123132608512 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:27:06.603920 140123132608512 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:27:06.603954 140123132608512 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:27:06.605865 140123132608512 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.608398 140123132608512 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:27:06.614256 140123132608512 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.614535 140123132608512 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:27:06.617277 140123132608512 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:27:06.621143 140123132608512 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:27:06.621199 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:06.621236 140123132608512 transformer_layer.py:389] tlayer: self-attention.
I0123 13:27:06.621268 140123132608512 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.621330 140123132608512 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.621907 140123132608512 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.621984 140123132608512 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.622349 140123132608512 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.623156 140123132608512 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.625858 140123132608512 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.626576 140123132608512 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.626657 140123132608512 transformer_layer.py:468] tlayer: End windows.
I0123 13:27:06.626693 140123132608512 transformer_layer.py:472] tlayer: final FFN.
I0123 13:27:06.626759 140123132608512 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.626897 140123132608512 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:27:06.627222 140123132608512 nn_components.py:325] mlp: activation = None
I0123 13:27:06.627265 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:06.629256 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.629351 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:06.632155 140123132608512 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.632236 140123132608512 transformer_base.py:443] tbase: final FFN
I0123 13:27:06.632670 140123132608512 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:27:06.635058 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:06.636981 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.637076 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:06.637374 140123132608512 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.637457 140123132608512 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 13:27:06.637577 140123132608512 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:27:06.637618 140123132608512 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:27:06.637657 140123132608512 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:27:06.639610 140123132608512 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.642059 140123132608512 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:27:06.647806 140123132608512 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.648069 140123132608512 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:27:06.650786 140123132608512 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:27:06.654669 140123132608512 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:27:06.654725 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:06.654767 140123132608512 transformer_layer.py:389] tlayer: self-attention.
I0123 13:27:06.654799 140123132608512 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.654862 140123132608512 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.655439 140123132608512 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.655516 140123132608512 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.655881 140123132608512 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.656677 140123132608512 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.659237 140123132608512 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.660225 140123132608512 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.660303 140123132608512 transformer_layer.py:468] tlayer: End windows.
I0123 13:27:06.660340 140123132608512 transformer_layer.py:472] tlayer: final FFN.
I0123 13:27:06.660401 140123132608512 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.660535 140123132608512 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:27:06.660863 140123132608512 nn_components.py:325] mlp: activation = None
I0123 13:27:06.660907 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:06.662847 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.662946 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:06.665494 140123132608512 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.665574 140123132608512 transformer_base.py:443] tbase: final FFN
I0123 13:27:06.666071 140123132608512 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:27:06.668387 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:06.670335 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.670432 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:06.670728 140123132608512 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.671014 140123132608512 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 13:27:06.671084 140123132608512 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 13:27:06.671154 140123132608512 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 13:27:06.671214 140123132608512 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 13:27:06.671272 140123132608512 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 13:27:06.671326 140123132608512 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 13:27:06.671381 140123132608512 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 13:27:06.671436 140123132608512 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 13:27:06.671489 140123132608512 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 13:27:06.671541 140123132608512 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 13:27:06.671593 140123132608512 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 13:27:06.671645 140123132608512 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 13:27:06.671683 140123132608512 decoder_stack.py:344] dstack: Final layernorm.
I0123 13:27:06.675263 140123132608512 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:06.723894 140123132608512 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.723981 140123132608512 decoder_stack.py:333] dstack: autoregressive generator.
I0123 13:27:06.724037 140123132608512 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 13:27:06.724145 140123132608512 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:27:06.724184 140123132608512 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:27:06.724215 140123132608512 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:27:06.724280 140123132608512 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.726865 140123132608512 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:27:06.732614 140123132608512 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.732886 140123132608512 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:27:06.735613 140123132608512 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:27:06.752370 140123132608512 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:27:06.752427 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:06.752463 140123132608512 transformer_layer.py:389] tlayer: self-attention.
I0123 13:27:06.752494 140123132608512 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.752561 140123132608512 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.753711 140123132608512 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.753792 140123132608512 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.754515 140123132608512 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.756541 140123132608512 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.761359 140123132608512 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.762688 140123132608512 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.762773 140123132608512 transformer_layer.py:468] tlayer: End windows.
I0123 13:27:06.762810 140123132608512 transformer_layer.py:472] tlayer: final FFN.
I0123 13:27:06.762871 140123132608512 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.763006 140123132608512 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:27:06.763118 140123132608512 nn_components.py:325] mlp: activation = None
I0123 13:27:06.763159 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:06.765091 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.765186 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:06.767679 140123132608512 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.767760 140123132608512 transformer_base.py:443] tbase: final FFN
I0123 13:27:06.767871 140123132608512 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:27:06.770149 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:06.772305 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.772404 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:06.772701 140123132608512 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.772784 140123132608512 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 13:27:06.772894 140123132608512 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:27:06.772934 140123132608512 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:27:06.772967 140123132608512 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:27:06.773032 140123132608512 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.775332 140123132608512 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:27:06.780938 140123132608512 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.781206 140123132608512 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:27:06.783921 140123132608512 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:27:06.797231 140123132608512 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:27:06.797286 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:06.797322 140123132608512 transformer_layer.py:389] tlayer: self-attention.
I0123 13:27:06.797353 140123132608512 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.797414 140123132608512 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.797979 140123132608512 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.798059 140123132608512 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.798425 140123132608512 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.799123 140123132608512 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.801668 140123132608512 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.802307 140123132608512 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.802387 140123132608512 transformer_layer.py:468] tlayer: End windows.
I0123 13:27:06.802429 140123132608512 transformer_layer.py:472] tlayer: final FFN.
I0123 13:27:06.802491 140123132608512 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.802625 140123132608512 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:27:06.802740 140123132608512 nn_components.py:325] mlp: activation = None
I0123 13:27:06.802780 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:06.804739 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.804834 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:06.807293 140123132608512 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.807378 140123132608512 transformer_base.py:443] tbase: final FFN
I0123 13:27:06.807490 140123132608512 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:27:06.809743 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:06.811722 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.811819 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:06.812115 140123132608512 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.812197 140123132608512 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 13:27:06.812306 140123132608512 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:27:06.812346 140123132608512 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:27:06.812377 140123132608512 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:27:06.812442 140123132608512 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.814733 140123132608512 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:27:06.820264 140123132608512 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.820529 140123132608512 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:27:06.823319 140123132608512 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:27:06.836207 140123132608512 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:27:06.836263 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:06.836299 140123132608512 transformer_layer.py:389] tlayer: self-attention.
I0123 13:27:06.836329 140123132608512 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.836392 140123132608512 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.836954 140123132608512 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.837031 140123132608512 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.837396 140123132608512 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.838115 140123132608512 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.840685 140123132608512 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.841317 140123132608512 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.841396 140123132608512 transformer_layer.py:468] tlayer: End windows.
I0123 13:27:06.841431 140123132608512 transformer_layer.py:472] tlayer: final FFN.
I0123 13:27:06.841500 140123132608512 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.841633 140123132608512 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:27:06.841751 140123132608512 nn_components.py:325] mlp: activation = None
I0123 13:27:06.841791 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:06.843780 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.843876 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:06.846367 140123132608512 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.846450 140123132608512 transformer_base.py:443] tbase: final FFN
I0123 13:27:06.846564 140123132608512 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:27:06.848809 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:06.850758 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.850855 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:06.851151 140123132608512 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.851233 140123132608512 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 13:27:06.851345 140123132608512 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:27:06.851384 140123132608512 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:27:06.851415 140123132608512 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:27:06.851479 140123132608512 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.853784 140123132608512 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:27:06.863373 140123132608512 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.863680 140123132608512 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:27:06.866585 140123132608512 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:27:06.880155 140123132608512 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:27:06.880213 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:06.880252 140123132608512 transformer_layer.py:389] tlayer: self-attention.
I0123 13:27:06.880283 140123132608512 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.880344 140123132608512 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.880938 140123132608512 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.881015 140123132608512 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.881383 140123132608512 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.882114 140123132608512 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.884737 140123132608512 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.885391 140123132608512 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.885472 140123132608512 transformer_layer.py:468] tlayer: End windows.
I0123 13:27:06.885508 140123132608512 transformer_layer.py:472] tlayer: final FFN.
I0123 13:27:06.885570 140123132608512 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.885719 140123132608512 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:27:06.885841 140123132608512 nn_components.py:325] mlp: activation = None
I0123 13:27:06.885882 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:06.887913 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.888009 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:06.890537 140123132608512 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.890620 140123132608512 transformer_base.py:443] tbase: final FFN
I0123 13:27:06.890734 140123132608512 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:27:06.893037 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:06.894979 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.895080 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:06.895373 140123132608512 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.895457 140123132608512 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 13:27:06.895570 140123132608512 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:27:06.895612 140123132608512 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:27:06.895644 140123132608512 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:27:06.895708 140123132608512 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.898386 140123132608512 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:27:06.904031 140123132608512 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.904302 140123132608512 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:27:06.907040 140123132608512 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:27:06.920151 140123132608512 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:27:06.920208 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:06.920243 140123132608512 transformer_layer.py:389] tlayer: self-attention.
I0123 13:27:06.920273 140123132608512 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.920336 140123132608512 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.920917 140123132608512 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.920996 140123132608512 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.921369 140123132608512 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.922120 140123132608512 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.924739 140123132608512 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.925369 140123132608512 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.925447 140123132608512 transformer_layer.py:468] tlayer: End windows.
I0123 13:27:06.925482 140123132608512 transformer_layer.py:472] tlayer: final FFN.
I0123 13:27:06.925540 140123132608512 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.925682 140123132608512 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:27:06.925795 140123132608512 nn_components.py:325] mlp: activation = None
I0123 13:27:06.925832 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:06.927799 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.927893 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:06.930371 140123132608512 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.930455 140123132608512 transformer_base.py:443] tbase: final FFN
I0123 13:27:06.930568 140123132608512 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:27:06.932912 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:06.934877 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.934980 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:06.935284 140123132608512 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.935368 140123132608512 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 13:27:06.935479 140123132608512 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:27:06.935519 140123132608512 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:27:06.935549 140123132608512 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:27:06.935613 140123132608512 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.937923 140123132608512 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:27:06.943557 140123132608512 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.943824 140123132608512 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:27:06.946586 140123132608512 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:27:06.959612 140123132608512 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:27:06.959670 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:06.959706 140123132608512 transformer_layer.py:389] tlayer: self-attention.
I0123 13:27:06.959737 140123132608512 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.959798 140123132608512 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.960371 140123132608512 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.960449 140123132608512 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.960810 140123132608512 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.961531 140123132608512 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.964122 140123132608512 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.964755 140123132608512 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.964833 140123132608512 transformer_layer.py:468] tlayer: End windows.
I0123 13:27:06.964869 140123132608512 transformer_layer.py:472] tlayer: final FFN.
I0123 13:27:06.964929 140123132608512 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.965066 140123132608512 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:27:06.965186 140123132608512 nn_components.py:325] mlp: activation = None
I0123 13:27:06.965225 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:06.967267 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.967365 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:06.969855 140123132608512 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.969937 140123132608512 transformer_base.py:443] tbase: final FFN
I0123 13:27:06.970053 140123132608512 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:27:06.972364 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:06.974281 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.974382 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:06.974691 140123132608512 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.974777 140123132608512 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 13:27:06.974894 140123132608512 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:27:06.974935 140123132608512 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:27:06.974967 140123132608512 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:27:06.975034 140123132608512 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.977498 140123132608512 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:27:06.983397 140123132608512 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.983664 140123132608512 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:27:06.986353 140123132608512 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:27:06.999382 140123132608512 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:27:06.999443 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:06.999481 140123132608512 transformer_layer.py:389] tlayer: self-attention.
I0123 13:27:06.999512 140123132608512 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:06.999575 140123132608512 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.000148 140123132608512 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.000225 140123132608512 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.000592 140123132608512 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.001298 140123132608512 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.003905 140123132608512 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.004902 140123132608512 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.004985 140123132608512 transformer_layer.py:468] tlayer: End windows.
I0123 13:27:07.005021 140123132608512 transformer_layer.py:472] tlayer: final FFN.
I0123 13:27:07.005081 140123132608512 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.005218 140123132608512 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:27:07.005333 140123132608512 nn_components.py:325] mlp: activation = None
I0123 13:27:07.005377 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:07.007356 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.007452 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:07.009927 140123132608512 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.010011 140123132608512 transformer_base.py:443] tbase: final FFN
I0123 13:27:07.010125 140123132608512 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:27:07.012404 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:07.014393 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.014495 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:07.014802 140123132608512 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.014888 140123132608512 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 13:27:07.015003 140123132608512 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:27:07.015044 140123132608512 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:27:07.015076 140123132608512 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:27:07.015142 140123132608512 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.017428 140123132608512 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:27:07.023053 140123132608512 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.023333 140123132608512 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:27:07.026095 140123132608512 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:27:07.039076 140123132608512 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:27:07.039134 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:07.039169 140123132608512 transformer_layer.py:389] tlayer: self-attention.
I0123 13:27:07.039200 140123132608512 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.039263 140123132608512 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.039875 140123132608512 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.039952 140123132608512 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.040319 140123132608512 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.041028 140123132608512 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.043605 140123132608512 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.044230 140123132608512 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.044308 140123132608512 transformer_layer.py:468] tlayer: End windows.
I0123 13:27:07.044343 140123132608512 transformer_layer.py:472] tlayer: final FFN.
I0123 13:27:07.044402 140123132608512 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.044535 140123132608512 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:27:07.044645 140123132608512 nn_components.py:325] mlp: activation = None
I0123 13:27:07.044689 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:07.046629 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.046728 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:07.049251 140123132608512 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.049330 140123132608512 transformer_base.py:443] tbase: final FFN
I0123 13:27:07.049440 140123132608512 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:27:07.051747 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:07.053647 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.053746 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:07.054051 140123132608512 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.054136 140123132608512 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 13:27:07.054250 140123132608512 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:27:07.054291 140123132608512 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:27:07.054323 140123132608512 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:27:07.054390 140123132608512 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.056686 140123132608512 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:27:07.062377 140123132608512 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.062654 140123132608512 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:27:07.065346 140123132608512 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:27:07.078387 140123132608512 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:27:07.078446 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:07.078483 140123132608512 transformer_layer.py:389] tlayer: self-attention.
I0123 13:27:07.078516 140123132608512 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.078581 140123132608512 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.079171 140123132608512 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.079248 140123132608512 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.079615 140123132608512 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.080327 140123132608512 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.083088 140123132608512 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.083764 140123132608512 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.083842 140123132608512 transformer_layer.py:468] tlayer: End windows.
I0123 13:27:07.083877 140123132608512 transformer_layer.py:472] tlayer: final FFN.
I0123 13:27:07.083936 140123132608512 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.084073 140123132608512 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:27:07.084186 140123132608512 nn_components.py:325] mlp: activation = None
I0123 13:27:07.084226 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:07.086143 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.086242 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:07.088718 140123132608512 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.088796 140123132608512 transformer_base.py:443] tbase: final FFN
I0123 13:27:07.088905 140123132608512 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:27:07.091215 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:07.093182 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.093278 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:07.093573 140123132608512 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.093660 140123132608512 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 13:27:07.093778 140123132608512 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:27:07.093818 140123132608512 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:27:07.093850 140123132608512 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:27:07.093917 140123132608512 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.096240 140123132608512 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:27:07.101796 140123132608512 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.102069 140123132608512 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:27:07.104853 140123132608512 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:27:07.118105 140123132608512 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:27:07.118164 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:07.118202 140123132608512 transformer_layer.py:389] tlayer: self-attention.
I0123 13:27:07.118235 140123132608512 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.118300 140123132608512 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.118941 140123132608512 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.119019 140123132608512 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.119391 140123132608512 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.120102 140123132608512 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.122663 140123132608512 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.123307 140123132608512 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.123385 140123132608512 transformer_layer.py:468] tlayer: End windows.
I0123 13:27:07.123420 140123132608512 transformer_layer.py:472] tlayer: final FFN.
I0123 13:27:07.123480 140123132608512 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.123614 140123132608512 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:27:07.123724 140123132608512 nn_components.py:325] mlp: activation = None
I0123 13:27:07.123763 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:07.125672 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.125774 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:07.128339 140123132608512 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.128420 140123132608512 transformer_base.py:443] tbase: final FFN
I0123 13:27:07.128529 140123132608512 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:27:07.130832 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:07.132711 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.132807 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:07.133097 140123132608512 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.133179 140123132608512 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 13:27:07.133290 140123132608512 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:27:07.133330 140123132608512 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:27:07.133361 140123132608512 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:27:07.133425 140123132608512 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.135749 140123132608512 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:27:07.141387 140123132608512 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.141656 140123132608512 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:27:07.144382 140123132608512 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:27:07.157415 140123132608512 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:27:07.157471 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:07.157507 140123132608512 transformer_layer.py:389] tlayer: self-attention.
I0123 13:27:07.157538 140123132608512 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.157600 140123132608512 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.158181 140123132608512 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.158260 140123132608512 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.158640 140123132608512 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.159364 140123132608512 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.161893 140123132608512 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.162584 140123132608512 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.162664 140123132608512 transformer_layer.py:468] tlayer: End windows.
I0123 13:27:07.162700 140123132608512 transformer_layer.py:472] tlayer: final FFN.
I0123 13:27:07.162762 140123132608512 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.162899 140123132608512 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:27:07.163014 140123132608512 nn_components.py:325] mlp: activation = None
I0123 13:27:07.163054 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:07.164949 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.165049 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:07.167552 140123132608512 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.167635 140123132608512 transformer_base.py:443] tbase: final FFN
I0123 13:27:07.167748 140123132608512 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:27:07.169976 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:07.171949 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.172044 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:07.172334 140123132608512 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.172415 140123132608512 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 13:27:07.172526 140123132608512 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:27:07.172565 140123132608512 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:27:07.172595 140123132608512 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:27:07.172657 140123132608512 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.174982 140123132608512 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:27:07.180503 140123132608512 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.180767 140123132608512 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:27:07.183544 140123132608512 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:27:07.196314 140123132608512 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:27:07.196370 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:07.196405 140123132608512 transformer_layer.py:389] tlayer: self-attention.
I0123 13:27:07.196435 140123132608512 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.196496 140123132608512 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.197060 140123132608512 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.197137 140123132608512 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.197498 140123132608512 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.198243 140123132608512 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.200786 140123132608512 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.201417 140123132608512 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.201495 140123132608512 transformer_layer.py:468] tlayer: End windows.
I0123 13:27:07.201530 140123132608512 transformer_layer.py:472] tlayer: final FFN.
I0123 13:27:07.201590 140123132608512 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.201730 140123132608512 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:27:07.201842 140123132608512 nn_components.py:325] mlp: activation = None
I0123 13:27:07.201882 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:07.203789 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.203883 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:07.206336 140123132608512 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.206416 140123132608512 transformer_base.py:443] tbase: final FFN
I0123 13:27:07.206525 140123132608512 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:27:07.208817 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:07.210708 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.210805 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:07.211096 140123132608512 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.211186 140123132608512 decoder_stack.py:344] dstack: Final layernorm.
I0123 13:27:07.214078 140123132608512 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:07.270225 140123132608512 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.270311 140123132608512 decoder_stack.py:333] dstack: autoregressive generator.
I0123 13:27:07.270365 140123132608512 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 13:27:07.270470 140123132608512 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:27:07.270509 140123132608512 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:27:07.270538 140123132608512 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:27:07.270600 140123132608512 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.273269 140123132608512 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:27:07.278644 140123132608512 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.278905 140123132608512 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:27:07.281495 140123132608512 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:27:07.293946 140123132608512 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:27:07.294001 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:07.294036 140123132608512 transformer_layer.py:389] tlayer: self-attention.
I0123 13:27:07.294065 140123132608512 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.294126 140123132608512 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.294682 140123132608512 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.294761 140123132608512 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.295126 140123132608512 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.295819 140123132608512 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.298373 140123132608512 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.298992 140123132608512 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.299071 140123132608512 transformer_layer.py:468] tlayer: End windows.
I0123 13:27:07.299106 140123132608512 transformer_layer.py:472] tlayer: final FFN.
I0123 13:27:07.299165 140123132608512 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.299294 140123132608512 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:27:07.299409 140123132608512 nn_components.py:325] mlp: activation = None
I0123 13:27:07.299448 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:07.301280 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.301373 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:07.303793 140123132608512 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.303872 140123132608512 transformer_base.py:443] tbase: final FFN
I0123 13:27:07.303981 140123132608512 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:27:07.306236 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:07.308096 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.308192 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:07.308481 140123132608512 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.308562 140123132608512 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 13:27:07.308668 140123132608512 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:27:07.308707 140123132608512 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:27:07.308737 140123132608512 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:27:07.308799 140123132608512 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.311057 140123132608512 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:27:07.316460 140123132608512 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.316722 140123132608512 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:27:07.319403 140123132608512 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:27:07.331707 140123132608512 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:27:07.331763 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:07.331799 140123132608512 transformer_layer.py:389] tlayer: self-attention.
I0123 13:27:07.331830 140123132608512 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.331893 140123132608512 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.332453 140123132608512 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.332530 140123132608512 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.332894 140123132608512 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.333583 140123132608512 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.336113 140123132608512 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.336733 140123132608512 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.336810 140123132608512 transformer_layer.py:468] tlayer: End windows.
I0123 13:27:07.336844 140123132608512 transformer_layer.py:472] tlayer: final FFN.
I0123 13:27:07.336903 140123132608512 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.337031 140123132608512 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:27:07.337141 140123132608512 nn_components.py:325] mlp: activation = None
I0123 13:27:07.337187 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:07.339040 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.339135 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:07.341692 140123132608512 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.341771 140123132608512 transformer_base.py:443] tbase: final FFN
I0123 13:27:07.341879 140123132608512 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:27:07.344129 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:07.345996 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.346091 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:07.346383 140123132608512 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.346464 140123132608512 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 13:27:07.346573 140123132608512 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:27:07.346611 140123132608512 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:27:07.346641 140123132608512 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:27:07.346704 140123132608512 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.348946 140123132608512 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:27:07.354315 140123132608512 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.354576 140123132608512 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:27:07.357240 140123132608512 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:27:07.369464 140123132608512 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:27:07.369519 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:07.369555 140123132608512 transformer_layer.py:389] tlayer: self-attention.
I0123 13:27:07.369585 140123132608512 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.369652 140123132608512 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.370210 140123132608512 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.370287 140123132608512 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.370648 140123132608512 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.371331 140123132608512 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.373884 140123132608512 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.374506 140123132608512 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.374583 140123132608512 transformer_layer.py:468] tlayer: End windows.
I0123 13:27:07.374617 140123132608512 transformer_layer.py:472] tlayer: final FFN.
I0123 13:27:07.374676 140123132608512 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.374803 140123132608512 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:27:07.374912 140123132608512 nn_components.py:325] mlp: activation = None
I0123 13:27:07.374950 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:07.376791 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.376883 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:07.379344 140123132608512 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.379424 140123132608512 transformer_base.py:443] tbase: final FFN
I0123 13:27:07.379533 140123132608512 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:27:07.382224 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:07.384071 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.384167 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:07.384458 140123132608512 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.384538 140123132608512 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 13:27:07.384647 140123132608512 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:27:07.384686 140123132608512 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:27:07.384716 140123132608512 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:27:07.384779 140123132608512 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.387011 140123132608512 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:27:07.392401 140123132608512 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.392661 140123132608512 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:27:07.395354 140123132608512 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:27:07.407753 140123132608512 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:27:07.407808 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:07.407844 140123132608512 transformer_layer.py:389] tlayer: self-attention.
I0123 13:27:07.407881 140123132608512 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.407942 140123132608512 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.408507 140123132608512 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.408581 140123132608512 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.408940 140123132608512 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.409618 140123132608512 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.412158 140123132608512 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.412777 140123132608512 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.412852 140123132608512 transformer_layer.py:468] tlayer: End windows.
I0123 13:27:07.412885 140123132608512 transformer_layer.py:472] tlayer: final FFN.
I0123 13:27:07.412943 140123132608512 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.413067 140123132608512 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:27:07.413174 140123132608512 nn_components.py:325] mlp: activation = None
I0123 13:27:07.413213 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:07.415141 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.415233 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:07.417625 140123132608512 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.417709 140123132608512 transformer_base.py:443] tbase: final FFN
I0123 13:27:07.417814 140123132608512 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:27:07.420109 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:07.421972 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.422073 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:07.422373 140123132608512 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.422456 140123132608512 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 13:27:07.422567 140123132608512 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:27:07.422605 140123132608512 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:27:07.422635 140123132608512 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:27:07.422699 140123132608512 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.424932 140123132608512 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:27:07.430357 140123132608512 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.430623 140123132608512 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:27:07.433345 140123132608512 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:27:07.446002 140123132608512 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:27:07.446057 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:07.446093 140123132608512 transformer_layer.py:389] tlayer: self-attention.
I0123 13:27:07.446124 140123132608512 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.446190 140123132608512 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.446767 140123132608512 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.446844 140123132608512 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.447215 140123132608512 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.448083 140123132608512 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.450624 140123132608512 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.451249 140123132608512 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.451324 140123132608512 transformer_layer.py:468] tlayer: End windows.
I0123 13:27:07.451358 140123132608512 transformer_layer.py:472] tlayer: final FFN.
I0123 13:27:07.451416 140123132608512 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.451541 140123132608512 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:27:07.451648 140123132608512 nn_components.py:325] mlp: activation = None
I0123 13:27:07.451685 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:07.453560 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.453663 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:07.456137 140123132608512 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.456213 140123132608512 transformer_base.py:443] tbase: final FFN
I0123 13:27:07.456319 140123132608512 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:27:07.458602 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:07.460451 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.460545 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:07.460832 140123132608512 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.460911 140123132608512 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 13:27:07.461017 140123132608512 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:27:07.461055 140123132608512 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:27:07.461083 140123132608512 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:27:07.461145 140123132608512 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.463455 140123132608512 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:27:07.468899 140123132608512 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.469155 140123132608512 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:27:07.471923 140123132608512 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:27:07.484555 140123132608512 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:27:07.484608 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:07.484642 140123132608512 transformer_layer.py:389] tlayer: self-attention.
I0123 13:27:07.484670 140123132608512 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.484730 140123132608512 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.485295 140123132608512 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.485370 140123132608512 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.485739 140123132608512 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.486451 140123132608512 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.489050 140123132608512 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.489668 140123132608512 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.489745 140123132608512 transformer_layer.py:468] tlayer: End windows.
I0123 13:27:07.489778 140123132608512 transformer_layer.py:472] tlayer: final FFN.
I0123 13:27:07.489835 140123132608512 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.489961 140123132608512 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:27:07.490070 140123132608512 nn_components.py:325] mlp: activation = None
I0123 13:27:07.490107 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:07.491984 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.492080 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:07.494520 140123132608512 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.494605 140123132608512 transformer_base.py:443] tbase: final FFN
I0123 13:27:07.494717 140123132608512 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:27:07.497431 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:07.499367 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.499464 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:07.499752 140123132608512 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.499831 140123132608512 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 13:27:07.499939 140123132608512 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:27:07.499976 140123132608512 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:27:07.500005 140123132608512 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:27:07.500068 140123132608512 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.502316 140123132608512 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:27:07.507744 140123132608512 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.508002 140123132608512 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:27:07.510722 140123132608512 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:27:07.523221 140123132608512 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:27:07.523275 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:07.523308 140123132608512 transformer_layer.py:389] tlayer: self-attention.
I0123 13:27:07.523337 140123132608512 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.523398 140123132608512 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.523965 140123132608512 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.524041 140123132608512 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.524398 140123132608512 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.525091 140123132608512 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.527647 140123132608512 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.528281 140123132608512 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.528357 140123132608512 transformer_layer.py:468] tlayer: End windows.
I0123 13:27:07.528390 140123132608512 transformer_layer.py:472] tlayer: final FFN.
I0123 13:27:07.528448 140123132608512 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.528574 140123132608512 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:27:07.528680 140123132608512 nn_components.py:325] mlp: activation = None
I0123 13:27:07.528717 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:07.530584 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.530677 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:07.533094 140123132608512 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.533171 140123132608512 transformer_base.py:443] tbase: final FFN
I0123 13:27:07.533277 140123132608512 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:27:07.535572 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:07.537433 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.537526 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:07.537822 140123132608512 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.537904 140123132608512 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 13:27:07.538010 140123132608512 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:27:07.538047 140123132608512 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:27:07.538075 140123132608512 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:27:07.538137 140123132608512 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.540391 140123132608512 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:27:07.545827 140123132608512 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.546082 140123132608512 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:27:07.548796 140123132608512 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:27:07.561290 140123132608512 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:27:07.561343 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:07.561377 140123132608512 transformer_layer.py:389] tlayer: self-attention.
I0123 13:27:07.561407 140123132608512 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.561467 140123132608512 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.562031 140123132608512 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.562106 140123132608512 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.562469 140123132608512 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.563160 140123132608512 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.565703 140123132608512 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.566335 140123132608512 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.566411 140123132608512 transformer_layer.py:468] tlayer: End windows.
I0123 13:27:07.566445 140123132608512 transformer_layer.py:472] tlayer: final FFN.
I0123 13:27:07.566505 140123132608512 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.566633 140123132608512 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:27:07.566740 140123132608512 nn_components.py:325] mlp: activation = None
I0123 13:27:07.566776 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:07.568657 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.568749 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:07.571150 140123132608512 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.571232 140123132608512 transformer_base.py:443] tbase: final FFN
I0123 13:27:07.571340 140123132608512 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:27:07.573621 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:07.575487 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.575579 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:07.575866 140123132608512 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.575946 140123132608512 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 13:27:07.576053 140123132608512 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:27:07.576090 140123132608512 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:27:07.576119 140123132608512 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:27:07.576179 140123132608512 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.578479 140123132608512 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:27:07.583858 140123132608512 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.584113 140123132608512 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:27:07.586824 140123132608512 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:27:07.599287 140123132608512 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:27:07.599341 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:07.599375 140123132608512 transformer_layer.py:389] tlayer: self-attention.
I0123 13:27:07.599404 140123132608512 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.599464 140123132608512 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.600024 140123132608512 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.600098 140123132608512 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.600455 140123132608512 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.601146 140123132608512 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.603707 140123132608512 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.604328 140123132608512 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.604404 140123132608512 transformer_layer.py:468] tlayer: End windows.
I0123 13:27:07.604438 140123132608512 transformer_layer.py:472] tlayer: final FFN.
I0123 13:27:07.604494 140123132608512 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.604621 140123132608512 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:27:07.604728 140123132608512 nn_components.py:325] mlp: activation = None
I0123 13:27:07.604764 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:07.606641 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.606733 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:07.609143 140123132608512 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.609225 140123132608512 transformer_base.py:443] tbase: final FFN
I0123 13:27:07.609335 140123132608512 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:27:07.612024 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:07.613912 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.614007 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:07.614301 140123132608512 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.614381 140123132608512 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 13:27:07.614490 140123132608512 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:27:07.614528 140123132608512 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:27:07.614557 140123132608512 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:27:07.614620 140123132608512 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.616890 140123132608512 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:27:07.622352 140123132608512 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.622609 140123132608512 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:27:07.625323 140123132608512 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:27:07.637903 140123132608512 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:27:07.637957 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:07.637991 140123132608512 transformer_layer.py:389] tlayer: self-attention.
I0123 13:27:07.638020 140123132608512 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.638082 140123132608512 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.638653 140123132608512 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.638728 140123132608512 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.639094 140123132608512 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.639782 140123132608512 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.642335 140123132608512 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.642966 140123132608512 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.643042 140123132608512 transformer_layer.py:468] tlayer: End windows.
I0123 13:27:07.643075 140123132608512 transformer_layer.py:472] tlayer: final FFN.
I0123 13:27:07.643133 140123132608512 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.643260 140123132608512 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:27:07.643370 140123132608512 nn_components.py:325] mlp: activation = None
I0123 13:27:07.643409 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:07.645728 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.645820 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:07.648223 140123132608512 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.648300 140123132608512 transformer_base.py:443] tbase: final FFN
I0123 13:27:07.648413 140123132608512 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:27:07.650658 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:07.652519 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.652612 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:07.652898 140123132608512 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.652976 140123132608512 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 13:27:07.653083 140123132608512 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:27:07.653120 140123132608512 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:27:07.653150 140123132608512 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:27:07.653213 140123132608512 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.655471 140123132608512 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:27:07.660881 140123132608512 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.661140 140123132608512 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:27:07.663846 140123132608512 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:27:07.676319 140123132608512 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:27:07.676373 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:07.676407 140123132608512 transformer_layer.py:389] tlayer: self-attention.
I0123 13:27:07.676436 140123132608512 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.676500 140123132608512 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.677057 140123132608512 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.677131 140123132608512 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.677488 140123132608512 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.678189 140123132608512 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.680748 140123132608512 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.681379 140123132608512 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.681455 140123132608512 transformer_layer.py:468] tlayer: End windows.
I0123 13:27:07.681489 140123132608512 transformer_layer.py:472] tlayer: final FFN.
I0123 13:27:07.681546 140123132608512 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.681681 140123132608512 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:27:07.681791 140123132608512 nn_components.py:325] mlp: activation = None
I0123 13:27:07.681828 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:07.683695 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.683786 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:07.686178 140123132608512 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.686256 140123132608512 transformer_base.py:443] tbase: final FFN
I0123 13:27:07.686365 140123132608512 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:27:07.688654 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:07.690521 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.690614 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:07.690902 140123132608512 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.690981 140123132608512 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 13:27:07.691088 140123132608512 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:27:07.691126 140123132608512 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:27:07.691155 140123132608512 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:27:07.691216 140123132608512 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.693456 140123132608512 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:27:07.698949 140123132608512 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.699207 140123132608512 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:27:07.701924 140123132608512 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:27:07.714493 140123132608512 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:27:07.714547 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:07.714581 140123132608512 transformer_layer.py:389] tlayer: self-attention.
I0123 13:27:07.714611 140123132608512 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.714671 140123132608512 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.715225 140123132608512 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.715300 140123132608512 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.715656 140123132608512 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.716353 140123132608512 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.718971 140123132608512 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.719595 140123132608512 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.719670 140123132608512 transformer_layer.py:468] tlayer: End windows.
I0123 13:27:07.719704 140123132608512 transformer_layer.py:472] tlayer: final FFN.
I0123 13:27:07.719760 140123132608512 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.719887 140123132608512 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:27:07.719999 140123132608512 nn_components.py:325] mlp: activation = None
I0123 13:27:07.720037 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:07.721907 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.722000 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:07.724389 140123132608512 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.724465 140123132608512 transformer_base.py:443] tbase: final FFN
I0123 13:27:07.724571 140123132608512 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:27:07.727231 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:07.729125 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.729218 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:07.729505 140123132608512 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:07.729588 140123132608512 decoder_stack.py:344] dstack: Final layernorm.
I0123 13:27:07.732415 140123132608512 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:12.115743 140123132608512 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
I0123 13:27:12.596572 140123132608512 training_loop.py:409] No working directory specified.
I0123 13:27:12.596689 140123132608512 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0123 13:27:12.597439 140123132608512 checkpoints.py:1062] Restoring legacy Flax checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0123 13:27:15.610662 140123132608512 training_loop.py:447] Only restoring trainable parameters.
I0123 13:27:15.611353 140123132608512 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0123 13:27:15.611411 140123132608512 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0123 13:27:15.611458 140123132608512 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 13:27:15.611500 140123132608512 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 13:27:15.611541 140123132608512 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:27:15.611579 140123132608512 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 13:27:15.611616 140123132608512 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:27:15.611654 140123132608512 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:27:15.611691 140123132608512 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 13:27:15.611727 140123132608512 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 13:27:15.611763 140123132608512 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:27:15.611799 140123132608512 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 13:27:15.611835 140123132608512 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 13:27:15.611871 140123132608512 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 13:27:15.611908 140123132608512 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:27:15.611944 140123132608512 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 13:27:15.611980 140123132608512 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:27:15.612016 140123132608512 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:27:15.612054 140123132608512 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 13:27:15.612091 140123132608512 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 13:27:15.612140 140123132608512 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:27:15.612177 140123132608512 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 13:27:15.612213 140123132608512 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 13:27:15.612248 140123132608512 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 13:27:15.612282 140123132608512 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:27:15.612317 140123132608512 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 13:27:15.612352 140123132608512 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:27:15.612387 140123132608512 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:27:15.612422 140123132608512 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 13:27:15.612456 140123132608512 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 13:27:15.612490 140123132608512 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:27:15.612525 140123132608512 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 13:27:15.612560 140123132608512 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 13:27:15.612594 140123132608512 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 13:27:15.612629 140123132608512 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:27:15.612664 140123132608512 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 13:27:15.612699 140123132608512 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:27:15.612735 140123132608512 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:27:15.612770 140123132608512 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 13:27:15.612804 140123132608512 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 13:27:15.612839 140123132608512 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:27:15.612874 140123132608512 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 13:27:15.612908 140123132608512 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 13:27:15.612942 140123132608512 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 13:27:15.612977 140123132608512 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:27:15.613013 140123132608512 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 13:27:15.613054 140123132608512 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:27:15.613092 140123132608512 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:27:15.613127 140123132608512 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 13:27:15.613162 140123132608512 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 13:27:15.613196 140123132608512 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:27:15.613231 140123132608512 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 13:27:15.613266 140123132608512 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 13:27:15.613300 140123132608512 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 13:27:15.613335 140123132608512 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:27:15.613369 140123132608512 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 13:27:15.613403 140123132608512 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:27:15.613437 140123132608512 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:27:15.613471 140123132608512 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 13:27:15.613506 140123132608512 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 13:27:15.613541 140123132608512 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:27:15.613575 140123132608512 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 13:27:15.613610 140123132608512 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 13:27:15.613656 140123132608512 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 13:27:15.613698 140123132608512 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:27:15.613739 140123132608512 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 13:27:15.613774 140123132608512 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:27:15.613809 140123132608512 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:27:15.613843 140123132608512 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 13:27:15.613877 140123132608512 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 13:27:15.613911 140123132608512 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:27:15.613948 140123132608512 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 13:27:15.613984 140123132608512 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 13:27:15.614025 140123132608512 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 13:27:15.614063 140123132608512 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:27:15.614100 140123132608512 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 13:27:15.614136 140123132608512 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:27:15.614172 140123132608512 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:27:15.614208 140123132608512 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 13:27:15.614243 140123132608512 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 13:27:15.614279 140123132608512 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:27:15.614315 140123132608512 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 13:27:15.614351 140123132608512 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 13:27:15.614386 140123132608512 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 13:27:15.614422 140123132608512 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:27:15.614457 140123132608512 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 13:27:15.614493 140123132608512 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:27:15.614528 140123132608512 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:27:15.614564 140123132608512 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 13:27:15.614599 140123132608512 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 13:27:15.614635 140123132608512 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:27:15.614671 140123132608512 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 13:27:15.614707 140123132608512 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 13:27:15.614742 140123132608512 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 13:27:15.614778 140123132608512 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:27:15.614813 140123132608512 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 13:27:15.614849 140123132608512 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:27:15.614885 140123132608512 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:27:15.614920 140123132608512 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 13:27:15.614956 140123132608512 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 13:27:15.614997 140123132608512 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:27:15.615034 140123132608512 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 13:27:15.615071 140123132608512 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 13:27:15.615106 140123132608512 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 13:27:15.615142 140123132608512 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:27:15.615176 140123132608512 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 13:27:15.615211 140123132608512 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:27:15.615244 140123132608512 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:27:15.615278 140123132608512 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 13:27:15.615311 140123132608512 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 13:27:15.615345 140123132608512 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:27:15.615379 140123132608512 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 13:27:15.615412 140123132608512 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 13:27:15.615446 140123132608512 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 13:27:15.615480 140123132608512 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:27:15.615514 140123132608512 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 13:27:15.615548 140123132608512 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:27:15.615581 140123132608512 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:27:15.615614 140123132608512 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 13:27:15.615647 140123132608512 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 13:27:15.615681 140123132608512 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:27:15.615714 140123132608512 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 13:27:15.615742 140123132608512 training_loop.py:725] Total parameters: 152072288
I0123 13:27:15.615948 140123132608512 training_loop.py:739] Total state size: 0
I0123 13:27:15.635701 140123132608512 training_loop.py:492] Training loop: creating task for mode beam_search
I0123 13:27:15.635961 140123132608512 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0123 13:27:15.636416 140123132608512 training_loop.py:652] Compiling mode beam_search with jit.
I0123 13:27:15.636725 140123132608512 training_loop.py:89] registering functions: dict_keys([])
I0123 13:27:15.652280 140123132608512 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = lc_tangent e b d, on_line e c d; f = foot f e b a; g = foot g e c b; h = foot h e c a; i = on_line i h f, on_line i b e; j = on_line j g f, on_line j a e; k = on_line k c i, on_line k b a; l = on_line l c j, on_line l b a ? cong f k f l
I0123 13:27:16.291686 140123132608512 ddar.py:60] Depth 1/1000 time = 0.619513750076294
I0123 13:27:17.066817 140123132608512 ddar.py:60] Depth 2/1000 time = 0.7749879360198975
I0123 13:27:18.427433 140123132608512 ddar.py:60] Depth 3/1000 time = 1.3604679107666016
I0123 13:27:19.659766 140123132608512 ddar.py:60] Depth 4/1000 time = 1.2321808338165283
I0123 13:27:20.910193 140123132608512 ddar.py:60] Depth 5/1000 time = 1.2474963665008545
I0123 13:27:20.913191 140123132608512 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:27:20.913258 140123132608512 alphageometry.py:540] Depth 0. There are 1 nodes to expand:
I0123 13:27:20.913292 140123132608512 alphageometry.py:544] {S} a : ; b : ; c : ; d : ^ a b a d a d a c 00 ^ c a c d c d c b 01 ; e : C c d e 02 T b d b e 03 ; f : C a b f 04 T a b e f 05 ; g : C b c g 06 T b c e g 07 ; h : C a c h 08 T a c e h 09 ; i : C b e i 10 C f h i 11 ; j : C a e j 12 C f g j 13 ; k : C a b k 14 C c i k 15 ; l : C a b l 16 C c j l 17 ? D f k f l {F1} x00
I0123 13:27:20.913322 140123132608512 alphageometry.py:549] Decoding from {S} a : ; b : ; c : ; d : ^ a b a d a d a c 00 ^ c a c d c d c b 01 ; e : C c d e 02 T b d b e 03 ; f : C a b f 04 T a b e f 05 ; g : C b c g 06 T b c e g 07 ; h : C a c h 08 T a c e h 09 ; i : C b e i 10 C f h i 11 ; j : C a e j 12 C f g j 13 ; k : C a b k 14 C c i k 15 ; l : C a b l 16 C c j l 17 ? D f k f l {F1} x00
I0123 13:27:21.035504 140123132608512 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.035675 140123132608512 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 13:27:21.035773 140123132608512 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 13:27:21.035849 140123132608512 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 13:27:21.035919 140123132608512 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 13:27:21.035995 140123132608512 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 13:27:21.036064 140123132608512 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 13:27:21.036131 140123132608512 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 13:27:21.036198 140123132608512 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 13:27:21.036263 140123132608512 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 13:27:21.036327 140123132608512 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 13:27:21.036392 140123132608512 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 13:27:21.036456 140123132608512 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 13:27:21.036520 140123132608512 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 13:27:21.036558 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:21.036600 140123132608512 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 13:27:21.036704 140123132608512 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:27:21.036742 140123132608512 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:27:21.036771 140123132608512 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:27:21.038617 140123132608512 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.041075 140123132608512 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:27:21.046891 140123132608512 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.047166 140123132608512 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:27:21.049798 140123132608512 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:27:21.053626 140123132608512 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:27:21.053687 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:21.053727 140123132608512 transformer_layer.py:389] tlayer: self-attention.
I0123 13:27:21.053758 140123132608512 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.053819 140123132608512 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.054416 140123132608512 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.054491 140123132608512 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.054860 140123132608512 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.055639 140123132608512 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.058162 140123132608512 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.058784 140123132608512 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.058861 140123132608512 transformer_layer.py:468] tlayer: End windows.
I0123 13:27:21.058895 140123132608512 transformer_layer.py:472] tlayer: final FFN.
I0123 13:27:21.058953 140123132608512 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.059078 140123132608512 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:27:21.059396 140123132608512 nn_components.py:325] mlp: activation = None
I0123 13:27:21.059437 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:21.061394 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.061486 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:21.063957 140123132608512 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.064035 140123132608512 transformer_base.py:443] tbase: final FFN
I0123 13:27:21.064454 140123132608512 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:27:21.066747 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:21.068654 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.068747 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:21.069038 140123132608512 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.069118 140123132608512 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 13:27:21.069225 140123132608512 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:27:21.069263 140123132608512 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:27:21.069293 140123132608512 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:27:21.071167 140123132608512 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.073476 140123132608512 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:27:21.079050 140123132608512 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.079317 140123132608512 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:27:21.081982 140123132608512 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:27:21.085608 140123132608512 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:27:21.085667 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:21.085702 140123132608512 transformer_layer.py:389] tlayer: self-attention.
I0123 13:27:21.085734 140123132608512 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.085795 140123132608512 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.086349 140123132608512 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.086425 140123132608512 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.086783 140123132608512 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.087543 140123132608512 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.090018 140123132608512 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.090678 140123132608512 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.090754 140123132608512 transformer_layer.py:468] tlayer: End windows.
I0123 13:27:21.090788 140123132608512 transformer_layer.py:472] tlayer: final FFN.
I0123 13:27:21.090846 140123132608512 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.090972 140123132608512 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:27:21.091284 140123132608512 nn_components.py:325] mlp: activation = None
I0123 13:27:21.091325 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:21.093221 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.093312 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:21.095773 140123132608512 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.095852 140123132608512 transformer_base.py:443] tbase: final FFN
I0123 13:27:21.096278 140123132608512 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:27:21.098628 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:21.100558 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.100650 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:21.100945 140123132608512 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.101026 140123132608512 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 13:27:21.101134 140123132608512 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:27:21.101173 140123132608512 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:27:21.101203 140123132608512 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:27:21.103019 140123132608512 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.105354 140123132608512 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:27:21.111070 140123132608512 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.111324 140123132608512 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:27:21.113944 140123132608512 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:27:21.117758 140123132608512 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:27:21.117811 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:21.117846 140123132608512 transformer_layer.py:389] tlayer: self-attention.
I0123 13:27:21.117876 140123132608512 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.117936 140123132608512 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.118536 140123132608512 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.118610 140123132608512 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.118970 140123132608512 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.119737 140123132608512 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.122255 140123132608512 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.122870 140123132608512 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.122945 140123132608512 transformer_layer.py:468] tlayer: End windows.
I0123 13:27:21.122979 140123132608512 transformer_layer.py:472] tlayer: final FFN.
I0123 13:27:21.123037 140123132608512 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.123162 140123132608512 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:27:21.123474 140123132608512 nn_components.py:325] mlp: activation = None
I0123 13:27:21.123515 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:21.125831 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.125925 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:21.128414 140123132608512 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.128493 140123132608512 transformer_base.py:443] tbase: final FFN
I0123 13:27:21.128919 140123132608512 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:27:21.131194 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:21.133110 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.133201 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:21.133491 140123132608512 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.133571 140123132608512 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 13:27:21.133685 140123132608512 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:27:21.133724 140123132608512 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:27:21.133755 140123132608512 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:27:21.290124 140123132608512 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.292911 140123132608512 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:27:21.299169 140123132608512 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.299461 140123132608512 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:27:21.302161 140123132608512 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:27:21.306032 140123132608512 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:27:21.306091 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:21.306129 140123132608512 transformer_layer.py:389] tlayer: self-attention.
I0123 13:27:21.306160 140123132608512 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.306226 140123132608512 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.306895 140123132608512 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.306971 140123132608512 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.307340 140123132608512 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.308120 140123132608512 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.310671 140123132608512 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.311298 140123132608512 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.311375 140123132608512 transformer_layer.py:468] tlayer: End windows.
I0123 13:27:21.311409 140123132608512 transformer_layer.py:472] tlayer: final FFN.
I0123 13:27:21.311468 140123132608512 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.311598 140123132608512 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:27:21.311930 140123132608512 nn_components.py:325] mlp: activation = None
I0123 13:27:21.311972 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:21.313980 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.314075 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:21.316579 140123132608512 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.316654 140123132608512 transformer_base.py:443] tbase: final FFN
I0123 13:27:21.317083 140123132608512 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:27:21.319387 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:21.321316 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.321409 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:21.321708 140123132608512 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.321792 140123132608512 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 13:27:21.321901 140123132608512 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:27:21.321939 140123132608512 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:27:21.321969 140123132608512 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:27:21.323887 140123132608512 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.326254 140123132608512 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:27:21.331871 140123132608512 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.332132 140123132608512 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:27:21.334799 140123132608512 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:27:21.338518 140123132608512 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:27:21.338582 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:21.338618 140123132608512 transformer_layer.py:389] tlayer: self-attention.
I0123 13:27:21.338649 140123132608512 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.338709 140123132608512 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.339271 140123132608512 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.339347 140123132608512 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.339706 140123132608512 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.340474 140123132608512 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.343053 140123132608512 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.343668 140123132608512 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.343744 140123132608512 transformer_layer.py:468] tlayer: End windows.
I0123 13:27:21.343778 140123132608512 transformer_layer.py:472] tlayer: final FFN.
I0123 13:27:21.343834 140123132608512 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.343960 140123132608512 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:27:21.344275 140123132608512 nn_components.py:325] mlp: activation = None
I0123 13:27:21.344317 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:21.346235 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.346327 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:21.348796 140123132608512 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.348874 140123132608512 transformer_base.py:443] tbase: final FFN
I0123 13:27:21.349299 140123132608512 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:27:21.351657 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:21.353567 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.353668 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:21.353962 140123132608512 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.354042 140123132608512 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 13:27:21.354150 140123132608512 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:27:21.354189 140123132608512 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:27:21.354218 140123132608512 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:27:21.356007 140123132608512 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.358347 140123132608512 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:27:21.364009 140123132608512 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.364267 140123132608512 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:27:21.366837 140123132608512 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:27:21.370528 140123132608512 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:27:21.370582 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:21.370622 140123132608512 transformer_layer.py:389] tlayer: self-attention.
I0123 13:27:21.370655 140123132608512 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.370716 140123132608512 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.371274 140123132608512 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.371349 140123132608512 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.371709 140123132608512 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.372476 140123132608512 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.374953 140123132608512 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.375562 140123132608512 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.375638 140123132608512 transformer_layer.py:468] tlayer: End windows.
I0123 13:27:21.375672 140123132608512 transformer_layer.py:472] tlayer: final FFN.
I0123 13:27:21.375730 140123132608512 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.375875 140123132608512 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:27:21.376197 140123132608512 nn_components.py:325] mlp: activation = None
I0123 13:27:21.376239 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:21.378234 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.378326 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:21.380811 140123132608512 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.380887 140123132608512 transformer_base.py:443] tbase: final FFN
I0123 13:27:21.381309 140123132608512 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:27:21.383580 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:21.385486 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.385579 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:21.385881 140123132608512 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.385962 140123132608512 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 13:27:21.386071 140123132608512 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:27:21.386109 140123132608512 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:27:21.386140 140123132608512 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:27:21.387984 140123132608512 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.390316 140123132608512 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:27:21.395946 140123132608512 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.396204 140123132608512 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:27:21.399382 140123132608512 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:27:21.403249 140123132608512 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:27:21.403303 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:21.403338 140123132608512 transformer_layer.py:389] tlayer: self-attention.
I0123 13:27:21.403374 140123132608512 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.403436 140123132608512 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.403990 140123132608512 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.404064 140123132608512 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.404427 140123132608512 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.405198 140123132608512 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.407735 140123132608512 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.408357 140123132608512 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.408433 140123132608512 transformer_layer.py:468] tlayer: End windows.
I0123 13:27:21.408468 140123132608512 transformer_layer.py:472] tlayer: final FFN.
I0123 13:27:21.408526 140123132608512 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.408653 140123132608512 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:27:21.408967 140123132608512 nn_components.py:325] mlp: activation = None
I0123 13:27:21.409008 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:21.410953 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.411047 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:21.413518 140123132608512 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.413595 140123132608512 transformer_base.py:443] tbase: final FFN
I0123 13:27:21.414023 140123132608512 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:27:21.416359 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:21.418281 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.418375 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:21.418668 140123132608512 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.418749 140123132608512 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 13:27:21.418856 140123132608512 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:27:21.418895 140123132608512 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:27:21.418925 140123132608512 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:27:21.420711 140123132608512 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.423033 140123132608512 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:27:21.428707 140123132608512 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.428965 140123132608512 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:27:21.431536 140123132608512 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:27:21.435231 140123132608512 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:27:21.435285 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:21.435319 140123132608512 transformer_layer.py:389] tlayer: self-attention.
I0123 13:27:21.435349 140123132608512 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.435415 140123132608512 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.435973 140123132608512 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.436048 140123132608512 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.436406 140123132608512 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.437182 140123132608512 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.439682 140123132608512 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.440294 140123132608512 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.440370 140123132608512 transformer_layer.py:468] tlayer: End windows.
I0123 13:27:21.440404 140123132608512 transformer_layer.py:472] tlayer: final FFN.
I0123 13:27:21.440461 140123132608512 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.440588 140123132608512 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:27:21.440909 140123132608512 nn_components.py:325] mlp: activation = None
I0123 13:27:21.440951 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:21.442940 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.443034 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:21.445483 140123132608512 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.445561 140123132608512 transformer_base.py:443] tbase: final FFN
I0123 13:27:21.445995 140123132608512 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:27:21.448265 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:21.450179 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.450273 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:21.450564 140123132608512 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.450644 140123132608512 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 13:27:21.450751 140123132608512 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:27:21.450788 140123132608512 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:27:21.450818 140123132608512 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:27:21.452671 140123132608512 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.455002 140123132608512 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:27:21.460618 140123132608512 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.460877 140123132608512 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:27:21.463504 140123132608512 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:27:21.467157 140123132608512 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:27:21.467211 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:21.467245 140123132608512 transformer_layer.py:389] tlayer: self-attention.
I0123 13:27:21.467275 140123132608512 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.467335 140123132608512 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.467902 140123132608512 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.467977 140123132608512 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.468336 140123132608512 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.469092 140123132608512 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.471613 140123132608512 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.472221 140123132608512 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.472297 140123132608512 transformer_layer.py:468] tlayer: End windows.
I0123 13:27:21.472331 140123132608512 transformer_layer.py:472] tlayer: final FFN.
I0123 13:27:21.472388 140123132608512 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.472515 140123132608512 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:27:21.472829 140123132608512 nn_components.py:325] mlp: activation = None
I0123 13:27:21.472871 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:21.474786 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.474878 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:21.477328 140123132608512 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.477406 140123132608512 transformer_base.py:443] tbase: final FFN
I0123 13:27:21.477837 140123132608512 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:27:21.480166 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:21.482082 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.482177 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:21.482471 140123132608512 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.482551 140123132608512 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 13:27:21.482659 140123132608512 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:27:21.482697 140123132608512 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:27:21.482728 140123132608512 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:27:21.484515 140123132608512 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.486839 140123132608512 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:27:21.492533 140123132608512 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.492789 140123132608512 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:27:21.495392 140123132608512 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:27:21.499120 140123132608512 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:27:21.499173 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:21.499208 140123132608512 transformer_layer.py:389] tlayer: self-attention.
I0123 13:27:21.499239 140123132608512 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.499299 140123132608512 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.500061 140123132608512 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.500136 140123132608512 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.500497 140123132608512 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.501264 140123132608512 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.503937 140123132608512 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.504554 140123132608512 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.504630 140123132608512 transformer_layer.py:468] tlayer: End windows.
I0123 13:27:21.504663 140123132608512 transformer_layer.py:472] tlayer: final FFN.
I0123 13:27:21.504720 140123132608512 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.504847 140123132608512 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:27:21.505161 140123132608512 nn_components.py:325] mlp: activation = None
I0123 13:27:21.505203 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:21.507545 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.507639 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:21.510131 140123132608512 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.510210 140123132608512 transformer_base.py:443] tbase: final FFN
I0123 13:27:21.510779 140123132608512 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:27:21.513058 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:21.514980 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.515074 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:21.515367 140123132608512 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.515448 140123132608512 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 13:27:21.515557 140123132608512 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:27:21.515595 140123132608512 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:27:21.515625 140123132608512 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:27:21.517486 140123132608512 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.519852 140123132608512 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:27:21.525650 140123132608512 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.525917 140123132608512 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:27:21.528579 140123132608512 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:27:21.532285 140123132608512 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:27:21.532339 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:21.532373 140123132608512 transformer_layer.py:389] tlayer: self-attention.
I0123 13:27:21.532403 140123132608512 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.532464 140123132608512 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.533015 140123132608512 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.533096 140123132608512 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.533455 140123132608512 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.534232 140123132608512 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.536808 140123132608512 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.537430 140123132608512 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.537508 140123132608512 transformer_layer.py:468] tlayer: End windows.
I0123 13:27:21.537542 140123132608512 transformer_layer.py:472] tlayer: final FFN.
I0123 13:27:21.537600 140123132608512 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.537740 140123132608512 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:27:21.538062 140123132608512 nn_components.py:325] mlp: activation = None
I0123 13:27:21.538103 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:21.540026 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.540118 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:21.542616 140123132608512 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.542695 140123132608512 transformer_base.py:443] tbase: final FFN
I0123 13:27:21.543120 140123132608512 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:27:21.545463 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:21.547415 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.547511 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:21.547813 140123132608512 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.547894 140123132608512 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 13:27:21.548002 140123132608512 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:27:21.548039 140123132608512 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:27:21.548070 140123132608512 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:27:21.549863 140123132608512 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.552188 140123132608512 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:27:21.557861 140123132608512 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.558116 140123132608512 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:27:21.560702 140123132608512 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:27:21.564407 140123132608512 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:27:21.564460 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:21.564494 140123132608512 transformer_layer.py:389] tlayer: self-attention.
I0123 13:27:21.564523 140123132608512 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.564585 140123132608512 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.565141 140123132608512 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.565221 140123132608512 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.565582 140123132608512 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.566361 140123132608512 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.568834 140123132608512 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.569446 140123132608512 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.569521 140123132608512 transformer_layer.py:468] tlayer: End windows.
I0123 13:27:21.569555 140123132608512 transformer_layer.py:472] tlayer: final FFN.
I0123 13:27:21.569612 140123132608512 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.569753 140123132608512 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:27:21.570068 140123132608512 nn_components.py:325] mlp: activation = None
I0123 13:27:21.570110 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:21.572079 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.572171 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:21.574635 140123132608512 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.574713 140123132608512 transformer_base.py:443] tbase: final FFN
I0123 13:27:21.575133 140123132608512 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:27:21.577377 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:21.579294 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.579388 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:21.579678 140123132608512 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.579927 140123132608512 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 13:27:21.579994 140123132608512 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 13:27:21.580050 140123132608512 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 13:27:21.580104 140123132608512 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 13:27:21.580155 140123132608512 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 13:27:21.580209 140123132608512 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 13:27:21.580261 140123132608512 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 13:27:21.580313 140123132608512 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 13:27:21.580364 140123132608512 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 13:27:21.580416 140123132608512 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 13:27:21.580467 140123132608512 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 13:27:21.580518 140123132608512 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 13:27:21.580553 140123132608512 decoder_stack.py:344] dstack: Final layernorm.
I0123 13:27:21.583512 140123132608512 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:27:21.629200 140123132608512 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.629290 140123132608512 decoder_stack.py:333] dstack: autoregressive generator.
I0123 13:27:21.629344 140123132608512 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 13:27:21.629448 140123132608512 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:27:21.629485 140123132608512 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:27:21.629516 140123132608512 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:27:21.629578 140123132608512 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.632034 140123132608512 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:27:21.637543 140123132608512 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.637811 140123132608512 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:27:21.640475 140123132608512 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:27:21.653379 140123132608512 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:27:21.653434 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:21.653468 140123132608512 transformer_layer.py:389] tlayer: self-attention.
I0123 13:27:21.653498 140123132608512 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.653559 140123132608512 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.654190 140123132608512 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.654270 140123132608512 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.654651 140123132608512 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.655364 140123132608512 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.657930 140123132608512 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.658573 140123132608512 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.658651 140123132608512 transformer_layer.py:468] tlayer: End windows.
I0123 13:27:21.658686 140123132608512 transformer_layer.py:472] tlayer: final FFN.
I0123 13:27:21.658747 140123132608512 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.658880 140123132608512 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:27:21.658994 140123132608512 nn_components.py:325] mlp: activation = None
I0123 13:27:21.659034 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:21.660913 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.661005 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:21.663484 140123132608512 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.663562 140123132608512 transformer_base.py:443] tbase: final FFN
I0123 13:27:21.663670 140123132608512 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:27:21.665957 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:21.667861 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.667955 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:21.668252 140123132608512 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.668339 140123132608512 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 13:27:21.668448 140123132608512 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:27:21.668486 140123132608512 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:27:21.668515 140123132608512 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:27:21.668578 140123132608512 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.670883 140123132608512 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:27:21.676757 140123132608512 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.677019 140123132608512 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:27:21.679700 140123132608512 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:27:21.692242 140123132608512 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:27:21.692296 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:21.692331 140123132608512 transformer_layer.py:389] tlayer: self-attention.
I0123 13:27:21.692361 140123132608512 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.692421 140123132608512 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.692974 140123132608512 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.693049 140123132608512 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.693411 140123132608512 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.694105 140123132608512 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.696569 140123132608512 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.697186 140123132608512 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.697263 140123132608512 transformer_layer.py:468] tlayer: End windows.
I0123 13:27:21.697297 140123132608512 transformer_layer.py:472] tlayer: final FFN.
I0123 13:27:21.697355 140123132608512 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.697484 140123132608512 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:27:21.697591 140123132608512 nn_components.py:325] mlp: activation = None
I0123 13:27:21.697628 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:21.699545 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.699638 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:21.702062 140123132608512 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.702142 140123132608512 transformer_base.py:443] tbase: final FFN
I0123 13:27:21.702250 140123132608512 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:27:21.704440 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:21.706298 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.706392 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:21.706687 140123132608512 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.706774 140123132608512 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 13:27:21.706884 140123132608512 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:27:21.706922 140123132608512 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:27:21.706954 140123132608512 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:27:21.707017 140123132608512 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.709342 140123132608512 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:27:21.714927 140123132608512 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.715185 140123132608512 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:27:21.717793 140123132608512 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:27:21.730420 140123132608512 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:27:21.730474 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:21.730509 140123132608512 transformer_layer.py:389] tlayer: self-attention.
I0123 13:27:21.730540 140123132608512 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.730600 140123132608512 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.731159 140123132608512 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.731235 140123132608512 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.731598 140123132608512 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.732288 140123132608512 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.734828 140123132608512 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.735450 140123132608512 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.735525 140123132608512 transformer_layer.py:468] tlayer: End windows.
I0123 13:27:21.735558 140123132608512 transformer_layer.py:472] tlayer: final FFN.
I0123 13:27:21.735616 140123132608512 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.735744 140123132608512 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:27:21.735851 140123132608512 nn_components.py:325] mlp: activation = None
I0123 13:27:21.735888 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:21.737773 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.737866 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:21.740310 140123132608512 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.740387 140123132608512 transformer_base.py:443] tbase: final FFN
I0123 13:27:21.740497 140123132608512 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:27:21.742785 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:21.744633 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.744729 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:21.745025 140123132608512 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.745106 140123132608512 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 13:27:21.745221 140123132608512 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:27:21.745260 140123132608512 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:27:21.745290 140123132608512 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:27:21.745353 140123132608512 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.747626 140123132608512 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:27:21.753036 140123132608512 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.753295 140123132608512 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:27:21.756136 140123132608512 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:27:21.768581 140123132608512 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:27:21.768635 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:21.768670 140123132608512 transformer_layer.py:389] tlayer: self-attention.
I0123 13:27:21.768700 140123132608512 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.768761 140123132608512 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.769320 140123132608512 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.769395 140123132608512 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.769767 140123132608512 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.770471 140123132608512 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.772955 140123132608512 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.773567 140123132608512 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.773848 140123132608512 transformer_layer.py:468] tlayer: End windows.
I0123 13:27:21.773882 140123132608512 transformer_layer.py:472] tlayer: final FFN.
I0123 13:27:21.773945 140123132608512 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.774073 140123132608512 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:27:21.774181 140123132608512 nn_components.py:325] mlp: activation = None
I0123 13:27:21.774219 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:21.776713 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.776807 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:21.779246 140123132608512 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.779325 140123132608512 transformer_base.py:443] tbase: final FFN
I0123 13:27:21.779433 140123132608512 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:27:21.781620 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:21.783491 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.783584 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:21.783874 140123132608512 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.783954 140123132608512 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 13:27:21.784060 140123132608512 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:27:21.784105 140123132608512 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:27:21.784137 140123132608512 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:27:21.784201 140123132608512 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.786474 140123132608512 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:27:21.792166 140123132608512 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.792425 140123132608512 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:27:21.795041 140123132608512 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:27:21.807550 140123132608512 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:27:21.807615 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:21.807650 140123132608512 transformer_layer.py:389] tlayer: self-attention.
I0123 13:27:21.807680 140123132608512 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.807741 140123132608512 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.808327 140123132608512 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.808406 140123132608512 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.808783 140123132608512 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.809517 140123132608512 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.812035 140123132608512 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.812697 140123132608512 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.812772 140123132608512 transformer_layer.py:468] tlayer: End windows.
I0123 13:27:21.812806 140123132608512 transformer_layer.py:472] tlayer: final FFN.
I0123 13:27:21.812862 140123132608512 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.812989 140123132608512 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:27:21.813097 140123132608512 nn_components.py:325] mlp: activation = None
I0123 13:27:21.813134 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:21.815009 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.815101 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:21.817507 140123132608512 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.817585 140123132608512 transformer_base.py:443] tbase: final FFN
I0123 13:27:21.817703 140123132608512 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:27:21.819918 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:21.821865 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.821960 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:21.822260 140123132608512 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.822340 140123132608512 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 13:27:21.822448 140123132608512 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:27:21.822486 140123132608512 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:27:21.822523 140123132608512 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:27:21.822588 140123132608512 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.824838 140123132608512 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:27:21.830266 140123132608512 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.830527 140123132608512 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:27:21.833197 140123132608512 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:27:21.845487 140123132608512 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:27:21.845541 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:21.845575 140123132608512 transformer_layer.py:389] tlayer: self-attention.
I0123 13:27:21.845603 140123132608512 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.845672 140123132608512 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.846280 140123132608512 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.846354 140123132608512 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.846706 140123132608512 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.847380 140123132608512 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.849847 140123132608512 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.850458 140123132608512 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.850533 140123132608512 transformer_layer.py:468] tlayer: End windows.
I0123 13:27:21.850565 140123132608512 transformer_layer.py:472] tlayer: final FFN.
I0123 13:27:21.850621 140123132608512 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.850747 140123132608512 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:27:21.850852 140123132608512 nn_components.py:325] mlp: activation = None
I0123 13:27:21.850889 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:21.852720 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.852811 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:21.855260 140123132608512 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.855337 140123132608512 transformer_base.py:443] tbase: final FFN
I0123 13:27:21.855443 140123132608512 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:27:21.857603 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:21.859436 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.859529 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:21.859817 140123132608512 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.859896 140123132608512 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 13:27:21.860001 140123132608512 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:27:21.860038 140123132608512 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:27:21.860072 140123132608512 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:27:21.860133 140123132608512 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.862367 140123132608512 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:27:21.867828 140123132608512 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.868082 140123132608512 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:27:21.870651 140123132608512 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:27:21.883257 140123132608512 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:27:21.883311 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:21.883345 140123132608512 transformer_layer.py:389] tlayer: self-attention.
I0123 13:27:21.883373 140123132608512 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.883432 140123132608512 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.883985 140123132608512 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.884061 140123132608512 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.884418 140123132608512 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.885106 140123132608512 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.887772 140123132608512 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.888439 140123132608512 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.888515 140123132608512 transformer_layer.py:468] tlayer: End windows.
I0123 13:27:21.888549 140123132608512 transformer_layer.py:472] tlayer: final FFN.
I0123 13:27:21.888606 140123132608512 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.888894 140123132608512 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:27:21.889001 140123132608512 nn_components.py:325] mlp: activation = None
I0123 13:27:21.889038 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:21.890878 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.890970 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:21.893358 140123132608512 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.893435 140123132608512 transformer_base.py:443] tbase: final FFN
I0123 13:27:21.893542 140123132608512 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:27:21.895719 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:21.897604 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.897708 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:21.897998 140123132608512 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.898077 140123132608512 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 13:27:21.898182 140123132608512 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:27:21.898219 140123132608512 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:27:21.898248 140123132608512 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:27:21.898314 140123132608512 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.900525 140123132608512 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:27:21.905902 140123132608512 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.906162 140123132608512 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:27:21.908706 140123132608512 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:27:21.920933 140123132608512 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:27:21.920987 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:21.921020 140123132608512 transformer_layer.py:389] tlayer: self-attention.
I0123 13:27:21.921048 140123132608512 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.921107 140123132608512 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.921670 140123132608512 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.921745 140123132608512 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.922104 140123132608512 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.922783 140123132608512 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.925298 140123132608512 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.925910 140123132608512 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.925986 140123132608512 transformer_layer.py:468] tlayer: End windows.
I0123 13:27:21.926019 140123132608512 transformer_layer.py:472] tlayer: final FFN.
I0123 13:27:21.926075 140123132608512 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.926202 140123132608512 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:27:21.926306 140123132608512 nn_components.py:325] mlp: activation = None
I0123 13:27:21.926343 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:21.928189 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.928281 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:21.930686 140123132608512 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.930763 140123132608512 transformer_base.py:443] tbase: final FFN
I0123 13:27:21.930869 140123132608512 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:27:21.933103 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:21.935080 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.935350 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:21.935640 140123132608512 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.935720 140123132608512 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 13:27:21.935827 140123132608512 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:27:21.935864 140123132608512 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:27:21.935893 140123132608512 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:27:21.935954 140123132608512 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.938200 140123132608512 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:27:21.943522 140123132608512 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.943778 140123132608512 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:27:21.946478 140123132608512 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:27:21.958694 140123132608512 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:27:21.958748 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:21.958782 140123132608512 transformer_layer.py:389] tlayer: self-attention.
I0123 13:27:21.958812 140123132608512 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.958872 140123132608512 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.959477 140123132608512 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.959552 140123132608512 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.959905 140123132608512 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.960590 140123132608512 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.963042 140123132608512 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.963654 140123132608512 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.963730 140123132608512 transformer_layer.py:468] tlayer: End windows.
I0123 13:27:21.963764 140123132608512 transformer_layer.py:472] tlayer: final FFN.
I0123 13:27:21.963820 140123132608512 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.963947 140123132608512 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:27:21.964056 140123132608512 nn_components.py:325] mlp: activation = None
I0123 13:27:21.964094 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:21.965916 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.966007 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:21.968434 140123132608512 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.968510 140123132608512 transformer_base.py:443] tbase: final FFN
I0123 13:27:21.968616 140123132608512 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:27:21.970777 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:21.972599 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.972692 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:21.972978 140123132608512 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.973057 140123132608512 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 13:27:21.973162 140123132608512 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:27:21.973200 140123132608512 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:27:21.973230 140123132608512 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:27:21.973291 140123132608512 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.975542 140123132608512 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:27:21.980943 140123132608512 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.981198 140123132608512 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:27:21.983764 140123132608512 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:27:21.996371 140123132608512 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:27:21.996425 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:21.996459 140123132608512 transformer_layer.py:389] tlayer: self-attention.
I0123 13:27:21.996487 140123132608512 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.996548 140123132608512 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.997100 140123132608512 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.997176 140123132608512 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.997532 140123132608512 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:21.998226 140123132608512 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:22.000672 140123132608512 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:22.001331 140123132608512 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:22.001407 140123132608512 transformer_layer.py:468] tlayer: End windows.
I0123 13:27:22.001441 140123132608512 transformer_layer.py:472] tlayer: final FFN.
I0123 13:27:22.001497 140123132608512 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:22.001626 140123132608512 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:27:22.001745 140123132608512 nn_components.py:325] mlp: activation = None
I0123 13:27:22.001782 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:22.003597 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:22.003689 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:22.006054 140123132608512 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:22.006131 140123132608512 transformer_base.py:443] tbase: final FFN
I0123 13:27:22.006240 140123132608512 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:27:22.008403 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:22.010328 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:22.010422 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:22.010716 140123132608512 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:22.010797 140123132608512 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 13:27:22.010905 140123132608512 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:27:22.010943 140123132608512 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:27:22.010974 140123132608512 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:27:22.011038 140123132608512 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:22.013293 140123132608512 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:27:22.018686 140123132608512 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:22.018941 140123132608512 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:27:22.021519 140123132608512 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:27:22.033917 140123132608512 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:27:22.033971 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:22.034006 140123132608512 transformer_layer.py:389] tlayer: self-attention.
I0123 13:27:22.034036 140123132608512 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:22.034097 140123132608512 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:22.034650 140123132608512 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:22.034724 140123132608512 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:22.035208 140123132608512 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:22.036082 140123132608512 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:22.038627 140123132608512 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:22.039252 140123132608512 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:22.039330 140123132608512 transformer_layer.py:468] tlayer: End windows.
I0123 13:27:22.039364 140123132608512 transformer_layer.py:472] tlayer: final FFN.
I0123 13:27:22.039423 140123132608512 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:22.039553 140123132608512 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:27:22.039663 140123132608512 nn_components.py:325] mlp: activation = None
I0123 13:27:22.039700 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:22.041551 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:22.041648 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:22.044053 140123132608512 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:22.044131 140123132608512 transformer_base.py:443] tbase: final FFN
I0123 13:27:22.044240 140123132608512 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:27:22.046506 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:22.048368 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:22.048461 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:22.048755 140123132608512 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:22.048835 140123132608512 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 13:27:22.048942 140123132608512 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:27:22.048980 140123132608512 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:27:22.049011 140123132608512 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:27:22.049073 140123132608512 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:22.051337 140123132608512 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:27:22.056745 140123132608512 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:22.057016 140123132608512 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:27:22.059727 140123132608512 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:27:22.072082 140123132608512 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:27:22.072136 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:22.072171 140123132608512 transformer_layer.py:389] tlayer: self-attention.
I0123 13:27:22.072201 140123132608512 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:22.072262 140123132608512 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:22.072870 140123132608512 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:22.072947 140123132608512 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:22.073309 140123132608512 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:22.074000 140123132608512 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:22.076455 140123132608512 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:22.077062 140123132608512 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:22.077137 140123132608512 transformer_layer.py:468] tlayer: End windows.
I0123 13:27:22.077172 140123132608512 transformer_layer.py:472] tlayer: final FFN.
I0123 13:27:22.077230 140123132608512 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:22.077358 140123132608512 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:27:22.077466 140123132608512 nn_components.py:325] mlp: activation = None
I0123 13:27:22.077503 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:22.079352 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:22.079445 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:22.081921 140123132608512 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:22.081998 140123132608512 transformer_base.py:443] tbase: final FFN
I0123 13:27:22.082105 140123132608512 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:27:22.084300 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:22.086154 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:22.086249 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:22.086542 140123132608512 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:22.086629 140123132608512 decoder_stack.py:344] dstack: Final layernorm.
I0123 13:27:22.089447 140123132608512 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:27:22.139578 140123132608512 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.139663 140123132608512 decoder_stack.py:333] dstack: autoregressive generator.
I0123 13:27:22.139716 140123132608512 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 13:27:22.139820 140123132608512 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:27:22.139858 140123132608512 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:27:22.139893 140123132608512 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:27:22.139957 140123132608512 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.142340 140123132608512 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:27:22.147782 140123132608512 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.148039 140123132608512 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:27:22.150670 140123132608512 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:27:22.163637 140123132608512 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:27:22.163691 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:22.163727 140123132608512 transformer_layer.py:389] tlayer: self-attention.
I0123 13:27:22.163758 140123132608512 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.163821 140123132608512 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.164382 140123132608512 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.164458 140123132608512 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.164820 140123132608512 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.165517 140123132608512 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.168080 140123132608512 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.168702 140123132608512 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.168779 140123132608512 transformer_layer.py:468] tlayer: End windows.
I0123 13:27:22.168814 140123132608512 transformer_layer.py:472] tlayer: final FFN.
I0123 13:27:22.168874 140123132608512 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.169004 140123132608512 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:27:22.169114 140123132608512 nn_components.py:325] mlp: activation = None
I0123 13:27:22.169152 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:22.171005 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.171098 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:22.173692 140123132608512 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.173771 140123132608512 transformer_base.py:443] tbase: final FFN
I0123 13:27:22.173879 140123132608512 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:27:22.176228 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:22.178101 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.178196 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:22.178494 140123132608512 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.178575 140123132608512 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 13:27:22.178682 140123132608512 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:27:22.178720 140123132608512 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:27:22.178750 140123132608512 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:27:22.178820 140123132608512 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.181086 140123132608512 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:27:22.186586 140123132608512 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.186844 140123132608512 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:27:22.189553 140123132608512 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:27:22.201979 140123132608512 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:27:22.202033 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:22.202068 140123132608512 transformer_layer.py:389] tlayer: self-attention.
I0123 13:27:22.202098 140123132608512 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.202160 140123132608512 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.202711 140123132608512 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.202785 140123132608512 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.203142 140123132608512 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.203820 140123132608512 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.206356 140123132608512 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.206968 140123132608512 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.207043 140123132608512 transformer_layer.py:468] tlayer: End windows.
I0123 13:27:22.207077 140123132608512 transformer_layer.py:472] tlayer: final FFN.
I0123 13:27:22.207137 140123132608512 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.207267 140123132608512 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:27:22.207377 140123132608512 nn_components.py:325] mlp: activation = None
I0123 13:27:22.207415 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:22.209272 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.209363 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:22.211822 140123132608512 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.211902 140123132608512 transformer_base.py:443] tbase: final FFN
I0123 13:27:22.212011 140123132608512 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:27:22.214298 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:22.216163 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.216257 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:22.216550 140123132608512 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.216629 140123132608512 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 13:27:22.216737 140123132608512 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:27:22.216775 140123132608512 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:27:22.216805 140123132608512 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:27:22.216876 140123132608512 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.219147 140123132608512 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:27:22.224576 140123132608512 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.224838 140123132608512 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:27:22.227545 140123132608512 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:27:22.239884 140123132608512 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:27:22.239938 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:22.239974 140123132608512 transformer_layer.py:389] tlayer: self-attention.
I0123 13:27:22.240005 140123132608512 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.240067 140123132608512 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.240618 140123132608512 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.240693 140123132608512 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.241050 140123132608512 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.241744 140123132608512 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.244288 140123132608512 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.244900 140123132608512 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.244976 140123132608512 transformer_layer.py:468] tlayer: End windows.
I0123 13:27:22.245010 140123132608512 transformer_layer.py:472] tlayer: final FFN.
I0123 13:27:22.245069 140123132608512 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.245199 140123132608512 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:27:22.245307 140123132608512 nn_components.py:325] mlp: activation = None
I0123 13:27:22.245344 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:22.247209 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.247301 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:22.249742 140123132608512 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.249820 140123132608512 transformer_base.py:443] tbase: final FFN
I0123 13:27:22.249928 140123132608512 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:27:22.252210 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:22.254094 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.254189 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:22.254485 140123132608512 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.254565 140123132608512 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 13:27:22.254673 140123132608512 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:27:22.254711 140123132608512 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:27:22.254741 140123132608512 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:27:22.254805 140123132608512 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.257062 140123132608512 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:27:22.262452 140123132608512 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.262710 140123132608512 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:27:22.265379 140123132608512 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:27:22.278507 140123132608512 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:27:22.278562 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:22.278598 140123132608512 transformer_layer.py:389] tlayer: self-attention.
I0123 13:27:22.278629 140123132608512 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.278691 140123132608512 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.279247 140123132608512 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.279323 140123132608512 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.279687 140123132608512 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.280377 140123132608512 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.283069 140123132608512 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.283679 140123132608512 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.283755 140123132608512 transformer_layer.py:468] tlayer: End windows.
I0123 13:27:22.283790 140123132608512 transformer_layer.py:472] tlayer: final FFN.
I0123 13:27:22.283848 140123132608512 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.283976 140123132608512 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:27:22.284083 140123132608512 nn_components.py:325] mlp: activation = None
I0123 13:27:22.284120 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:22.285984 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.286076 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:22.288485 140123132608512 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.288561 140123132608512 transformer_base.py:443] tbase: final FFN
I0123 13:27:22.288670 140123132608512 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:27:22.290955 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:22.292817 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.292911 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:22.293210 140123132608512 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.293291 140123132608512 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 13:27:22.293400 140123132608512 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:27:22.293437 140123132608512 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:27:22.293468 140123132608512 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:27:22.293530 140123132608512 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.295836 140123132608512 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:27:22.301252 140123132608512 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.301508 140123132608512 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:27:22.304187 140123132608512 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:27:22.316480 140123132608512 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:27:22.316534 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:22.316569 140123132608512 transformer_layer.py:389] tlayer: self-attention.
I0123 13:27:22.316599 140123132608512 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.316660 140123132608512 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.317212 140123132608512 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.317287 140123132608512 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.317653 140123132608512 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.318343 140123132608512 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.320858 140123132608512 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.321472 140123132608512 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.321547 140123132608512 transformer_layer.py:468] tlayer: End windows.
I0123 13:27:22.321581 140123132608512 transformer_layer.py:472] tlayer: final FFN.
I0123 13:27:22.321638 140123132608512 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.321775 140123132608512 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:27:22.321883 140123132608512 nn_components.py:325] mlp: activation = None
I0123 13:27:22.321920 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:22.323759 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.323851 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:22.326316 140123132608512 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.326394 140123132608512 transformer_base.py:443] tbase: final FFN
I0123 13:27:22.326502 140123132608512 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:27:22.328754 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:22.330615 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.330709 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:22.331004 140123132608512 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.331085 140123132608512 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 13:27:22.331192 140123132608512 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:27:22.331230 140123132608512 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:27:22.331261 140123132608512 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:27:22.331324 140123132608512 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.333542 140123132608512 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:27:22.338969 140123132608512 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.339228 140123132608512 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:27:22.341916 140123132608512 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:27:22.354293 140123132608512 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:27:22.354348 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:22.354384 140123132608512 transformer_layer.py:389] tlayer: self-attention.
I0123 13:27:22.354414 140123132608512 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.354475 140123132608512 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.355029 140123132608512 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.355103 140123132608512 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.355463 140123132608512 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.356142 140123132608512 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.358663 140123132608512 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.359274 140123132608512 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.359349 140123132608512 transformer_layer.py:468] tlayer: End windows.
I0123 13:27:22.359383 140123132608512 transformer_layer.py:472] tlayer: final FFN.
I0123 13:27:22.359442 140123132608512 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.359568 140123132608512 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:27:22.359677 140123132608512 nn_components.py:325] mlp: activation = None
I0123 13:27:22.359714 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:22.361554 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.361651 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:22.364053 140123132608512 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.364130 140123132608512 transformer_base.py:443] tbase: final FFN
I0123 13:27:22.364238 140123132608512 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:27:22.366487 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:22.368327 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.368419 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:22.368711 140123132608512 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.368791 140123132608512 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 13:27:22.368899 140123132608512 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:27:22.368937 140123132608512 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:27:22.368968 140123132608512 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:27:22.369031 140123132608512 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.371281 140123132608512 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:27:22.376667 140123132608512 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.376934 140123132608512 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:27:22.379591 140123132608512 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:27:22.392700 140123132608512 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:27:22.392755 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:22.392791 140123132608512 transformer_layer.py:389] tlayer: self-attention.
I0123 13:27:22.392821 140123132608512 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.392882 140123132608512 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.393447 140123132608512 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.393520 140123132608512 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.393881 140123132608512 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.394563 140123132608512 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.397088 140123132608512 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.397705 140123132608512 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.397781 140123132608512 transformer_layer.py:468] tlayer: End windows.
I0123 13:27:22.397815 140123132608512 transformer_layer.py:472] tlayer: final FFN.
I0123 13:27:22.397873 140123132608512 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.397999 140123132608512 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:27:22.398109 140123132608512 nn_components.py:325] mlp: activation = None
I0123 13:27:22.398146 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:22.400015 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.400106 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:22.402517 140123132608512 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.402599 140123132608512 transformer_base.py:443] tbase: final FFN
I0123 13:27:22.402707 140123132608512 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:27:22.404973 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:22.406830 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.406924 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:22.407219 140123132608512 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.407299 140123132608512 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 13:27:22.407405 140123132608512 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:27:22.407442 140123132608512 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:27:22.407472 140123132608512 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:27:22.407534 140123132608512 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.409766 140123132608512 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:27:22.415167 140123132608512 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.415436 140123132608512 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:27:22.418143 140123132608512 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:27:22.430595 140123132608512 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:27:22.430649 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:22.430684 140123132608512 transformer_layer.py:389] tlayer: self-attention.
I0123 13:27:22.430714 140123132608512 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.430776 140123132608512 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.431337 140123132608512 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.431416 140123132608512 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.431780 140123132608512 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.432465 140123132608512 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.435021 140123132608512 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.435637 140123132608512 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.435716 140123132608512 transformer_layer.py:468] tlayer: End windows.
I0123 13:27:22.435751 140123132608512 transformer_layer.py:472] tlayer: final FFN.
I0123 13:27:22.435810 140123132608512 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.435940 140123132608512 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:27:22.436048 140123132608512 nn_components.py:325] mlp: activation = None
I0123 13:27:22.436085 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:22.437949 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.438040 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:22.440456 140123132608512 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.440533 140123132608512 transformer_base.py:443] tbase: final FFN
I0123 13:27:22.440641 140123132608512 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:27:22.442922 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:22.444791 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.444884 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:22.445180 140123132608512 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.445259 140123132608512 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 13:27:22.445367 140123132608512 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:27:22.445406 140123132608512 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:27:22.445436 140123132608512 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:27:22.445499 140123132608512 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.447740 140123132608512 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:27:22.453166 140123132608512 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.453421 140123132608512 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:27:22.456142 140123132608512 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:27:22.468549 140123132608512 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:27:22.468603 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:22.468638 140123132608512 transformer_layer.py:389] tlayer: self-attention.
I0123 13:27:22.468668 140123132608512 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.468734 140123132608512 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.469283 140123132608512 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.469356 140123132608512 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.469730 140123132608512 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.470415 140123132608512 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.472950 140123132608512 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.473565 140123132608512 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.473644 140123132608512 transformer_layer.py:468] tlayer: End windows.
I0123 13:27:22.473680 140123132608512 transformer_layer.py:472] tlayer: final FFN.
I0123 13:27:22.473739 140123132608512 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.473865 140123132608512 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:27:22.473972 140123132608512 nn_components.py:325] mlp: activation = None
I0123 13:27:22.474010 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:22.475872 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.475962 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:22.478383 140123132608512 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.478460 140123132608512 transformer_base.py:443] tbase: final FFN
I0123 13:27:22.478567 140123132608512 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:27:22.480843 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:22.482849 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.482941 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:22.483233 140123132608512 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.483311 140123132608512 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 13:27:22.483418 140123132608512 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:27:22.483456 140123132608512 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:27:22.483486 140123132608512 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:27:22.483549 140123132608512 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.485797 140123132608512 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:27:22.491543 140123132608512 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.491802 140123132608512 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:27:22.494713 140123132608512 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:27:22.507624 140123132608512 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:27:22.507679 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:22.507714 140123132608512 transformer_layer.py:389] tlayer: self-attention.
I0123 13:27:22.507744 140123132608512 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.507804 140123132608512 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.508358 140123132608512 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.508430 140123132608512 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.508793 140123132608512 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.509478 140123132608512 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.512019 140123132608512 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.512644 140123132608512 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.512719 140123132608512 transformer_layer.py:468] tlayer: End windows.
I0123 13:27:22.512754 140123132608512 transformer_layer.py:472] tlayer: final FFN.
I0123 13:27:22.512812 140123132608512 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.512938 140123132608512 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:27:22.513046 140123132608512 nn_components.py:325] mlp: activation = None
I0123 13:27:22.513084 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:22.515153 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.515244 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:22.517651 140123132608512 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.517728 140123132608512 transformer_base.py:443] tbase: final FFN
I0123 13:27:22.517836 140123132608512 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:27:22.520108 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:22.522055 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.522147 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:22.522438 140123132608512 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.522516 140123132608512 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 13:27:22.522623 140123132608512 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:27:22.522661 140123132608512 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:27:22.522691 140123132608512 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:27:22.522753 140123132608512 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.525012 140123132608512 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:27:22.530396 140123132608512 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.530654 140123132608512 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:27:22.533334 140123132608512 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:27:22.545993 140123132608512 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:27:22.546047 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:22.546085 140123132608512 transformer_layer.py:389] tlayer: self-attention.
I0123 13:27:22.546117 140123132608512 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.546180 140123132608512 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.546756 140123132608512 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.546833 140123132608512 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.547207 140123132608512 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.547894 140123132608512 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.550456 140123132608512 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.551104 140123132608512 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.551182 140123132608512 transformer_layer.py:468] tlayer: End windows.
I0123 13:27:22.551218 140123132608512 transformer_layer.py:472] tlayer: final FFN.
I0123 13:27:22.551275 140123132608512 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.551402 140123132608512 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:27:22.551510 140123132608512 nn_components.py:325] mlp: activation = None
I0123 13:27:22.551548 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:22.553410 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.553500 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:22.555896 140123132608512 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.555973 140123132608512 transformer_base.py:443] tbase: final FFN
I0123 13:27:22.556081 140123132608512 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:27:22.558363 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:22.560225 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.560317 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:22.560611 140123132608512 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.560690 140123132608512 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 13:27:22.560798 140123132608512 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:27:22.560835 140123132608512 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:27:22.560866 140123132608512 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:27:22.560929 140123132608512 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.563161 140123132608512 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:27:22.568548 140123132608512 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.568800 140123132608512 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:27:22.571505 140123132608512 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:27:22.584028 140123132608512 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:27:22.584087 140123132608512 attention.py:418] Single window, no scan.
I0123 13:27:22.584122 140123132608512 transformer_layer.py:389] tlayer: self-attention.
I0123 13:27:22.584153 140123132608512 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.584213 140123132608512 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.584765 140123132608512 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.584841 140123132608512 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.585200 140123132608512 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.585903 140123132608512 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.588439 140123132608512 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.589063 140123132608512 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.589139 140123132608512 transformer_layer.py:468] tlayer: End windows.
I0123 13:27:22.589174 140123132608512 transformer_layer.py:472] tlayer: final FFN.
I0123 13:27:22.589232 140123132608512 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.589357 140123132608512 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:27:22.589465 140123132608512 nn_components.py:325] mlp: activation = None
I0123 13:27:22.589502 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:22.591387 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.591478 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:22.593878 140123132608512 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.593954 140123132608512 transformer_base.py:443] tbase: final FFN
I0123 13:27:22.594062 140123132608512 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:27:22.596334 140123132608512 nn_components.py:329] mlp: final activation = None
I0123 13:27:22.598196 140123132608512 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.598288 140123132608512 nn_components.py:261] mlp: residual
I0123 13:27:22.598578 140123132608512 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:22.598661 140123132608512 decoder_stack.py:344] dstack: Final layernorm.
I0123 13:27:22.601504 140123132608512 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:27:33.518451 140123132608512 alphageometry.py:566] LM output (score=-1.257666): "m : D c e f m 18 D c m e f 19 ;"
I0123 13:27:33.518710 140123132608512 alphageometry.py:567] Translation: "m = eqdistance m f c e, eqdistance m c e f"

I0123 13:27:33.518764 140123132608512 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = lc_tangent e b d, on_line e c d; f = foot f e b a; g = foot g e c b; h = foot h e c a; i = on_line i h f, on_line i b e; j = on_line j g f, on_line j a e; k = on_line k c i, on_line k b a; l = on_line l c j, on_line l b a; m = eqdistance m f c e, eqdistance m c e f ? cong f k f l"
I0123 13:27:33.518921 140123132608512 graph.py:498] 
I0123 13:27:33.518978 140123132608512 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = lc_tangent e b d, on_line e c d; f = foot f e b a; g = foot g e c b; h = foot h e c a; i = on_line i h f, on_line i b e; j = on_line j g f, on_line j a e; k = on_line k c i, on_line k b a; l = on_line l c j, on_line l b a; m = eqdistance m f c e, eqdistance m c e f ? cong f k f l
I0123 13:27:34.317824 140123132608512 ddar.py:60] Depth 1/1000 time = 0.7812850475311279
I0123 13:27:35.295677 140123132608512 ddar.py:60] Depth 2/1000 time = 0.9777059555053711
I0123 13:27:36.862945 140123132608512 ddar.py:60] Depth 3/1000 time = 1.567107915878296
I0123 13:27:38.626697 140123132608512 ddar.py:60] Depth 4/1000 time = 1.7635917663574219
I0123 13:27:40.202238 140123132608512 ddar.py:60] Depth 5/1000 time = 1.5753810405731201
I0123 13:27:41.787657 140123132608512 ddar.py:60] Depth 6/1000 time = 1.5822114944458008
I0123 13:27:41.792882 140123132608512 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:27:41.792966 140123132608512 alphageometry.py:566] LM output (score=-1.271127): "m : D c i f m 18 D c m f i 19 ;"
I0123 13:27:41.793003 140123132608512 alphageometry.py:567] Translation: "m = eqdistance m f c i, eqdistance m c f i"

I0123 13:27:41.793038 140123132608512 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = lc_tangent e b d, on_line e c d; f = foot f e b a; g = foot g e c b; h = foot h e c a; i = on_line i h f, on_line i b e; j = on_line j g f, on_line j a e; k = on_line k c i, on_line k b a; l = on_line l c j, on_line l b a; m = eqdistance m f c i, eqdistance m c f i ? cong f k f l"
I0123 13:27:41.793186 140123132608512 graph.py:498] 
I0123 13:27:41.793237 140123132608512 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = lc_tangent e b d, on_line e c d; f = foot f e b a; g = foot g e c b; h = foot h e c a; i = on_line i h f, on_line i b e; j = on_line j g f, on_line j a e; k = on_line k c i, on_line k b a; l = on_line l c j, on_line l b a; m = eqdistance m f c i, eqdistance m c f i ? cong f k f l
I0123 13:27:42.550858 140123132608512 ddar.py:60] Depth 1/1000 time = 0.739985466003418
I0123 13:27:43.767443 140123132608512 ddar.py:60] Depth 2/1000 time = 1.2164366245269775
I0123 13:27:45.410652 140123132608512 ddar.py:60] Depth 3/1000 time = 1.6430597305297852
I0123 13:27:47.095646 140123132608512 ddar.py:60] Depth 4/1000 time = 1.684826374053955
I0123 13:27:48.757887 140123132608512 ddar.py:60] Depth 5/1000 time = 1.6620664596557617
I0123 13:27:50.593782 140123132608512 ddar.py:60] Depth 6/1000 time = 1.832733392715454
I0123 13:27:50.598540 140123132608512 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:27:50.598610 140123132608512 alphageometry.py:566] LM output (score=-1.379735): "m : D c e c m 18 T c e c m 19 ;"
I0123 13:27:50.598645 140123132608512 alphageometry.py:567] Translation: "m = on_circle m c e, on_tline m c c e"

I0123 13:27:50.598679 140123132608512 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = lc_tangent e b d, on_line e c d; f = foot f e b a; g = foot g e c b; h = foot h e c a; i = on_line i h f, on_line i b e; j = on_line j g f, on_line j a e; k = on_line k c i, on_line k b a; l = on_line l c j, on_line l b a; m = on_circle m c e, on_tline m c c e ? cong f k f l"
I0123 13:27:50.598827 140123132608512 graph.py:498] 
I0123 13:27:50.598877 140123132608512 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = lc_tangent e b d, on_line e c d; f = foot f e b a; g = foot g e c b; h = foot h e c a; i = on_line i h f, on_line i b e; j = on_line j g f, on_line j a e; k = on_line k c i, on_line k b a; l = on_line l c j, on_line l b a; m = on_circle m c e, on_tline m c c e ? cong f k f l
I0123 13:27:51.347306 140123132608512 ddar.py:60] Depth 1/1000 time = 0.7280128002166748
I0123 13:27:52.301958 140123132608512 ddar.py:60] Depth 2/1000 time = 0.954512357711792
I0123 13:27:53.765267 140123132608512 ddar.py:60] Depth 3/1000 time = 1.4631626605987549
I0123 13:27:55.249811 140123132608512 ddar.py:60] Depth 4/1000 time = 1.4843742847442627
I0123 13:27:56.894261 140123132608512 ddar.py:60] Depth 5/1000 time = 1.6439664363861084
I0123 13:27:58.399576 140123132608512 ddar.py:60] Depth 6/1000 time = 1.5000941753387451
I0123 13:27:59.899750 140123132608512 ddar.py:60] Depth 7/1000 time = 1.4987123012542725
I0123 13:27:59.903441 140123132608512 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:27:59.903546 140123132608512 alphageometry.py:566] LM output (score=-1.628312): "m : D c e c m 18 T c i e m 19 ;"
I0123 13:27:59.903585 140123132608512 alphageometry.py:567] Translation: "m = on_circle m c e, on_tline m e c i"

I0123 13:27:59.903624 140123132608512 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = lc_tangent e b d, on_line e c d; f = foot f e b a; g = foot g e c b; h = foot h e c a; i = on_line i h f, on_line i b e; j = on_line j g f, on_line j a e; k = on_line k c i, on_line k b a; l = on_line l c j, on_line l b a; m = on_circle m c e, on_tline m e c i ? cong f k f l"
I0123 13:27:59.903789 140123132608512 graph.py:498] 
I0123 13:27:59.903848 140123132608512 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = lc_tangent e b d, on_line e c d; f = foot f e b a; g = foot g e c b; h = foot h e c a; i = on_line i h f, on_line i b e; j = on_line j g f, on_line j a e; k = on_line k c i, on_line k b a; l = on_line l c j, on_line l b a; m = on_circle m c e, on_tline m e c i ? cong f k f l
I0123 13:28:00.657577 140123132608512 ddar.py:60] Depth 1/1000 time = 0.7321047782897949
I0123 13:28:01.793019 140123132608512 ddar.py:60] Depth 2/1000 time = 1.1352925300598145
I0123 13:28:03.329006 140123132608512 ddar.py:60] Depth 3/1000 time = 1.5358448028564453
I0123 13:28:04.918401 140123132608512 ddar.py:60] Depth 4/1000 time = 1.5892283916473389
I0123 13:28:06.866959 140123132608512 ddar.py:60] Depth 5/1000 time = 1.948401689529419
I0123 13:28:08.852183 140123132608512 ddar.py:60] Depth 6/1000 time = 1.9850640296936035
I0123 13:28:11.034247 140123132608512 ddar.py:60] Depth 7/1000 time = 2.176630735397339
I0123 13:28:11.042802 140123132608512 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:28:11.042905 140123132608512 alphageometry.py:566] LM output (score=-1.654724): "m : D c e c m 18 T a c e m 19 ;"
I0123 13:28:11.042944 140123132608512 alphageometry.py:567] Translation: "m = on_circle m c e, on_tline m e a c"

I0123 13:28:11.042980 140123132608512 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = lc_tangent e b d, on_line e c d; f = foot f e b a; g = foot g e c b; h = foot h e c a; i = on_line i h f, on_line i b e; j = on_line j g f, on_line j a e; k = on_line k c i, on_line k b a; l = on_line l c j, on_line l b a; m = on_circle m c e, on_tline m e a c ? cong f k f l"
I0123 13:28:11.043144 140123132608512 graph.py:498] 
I0123 13:28:11.043202 140123132608512 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = lc_tangent e b d, on_line e c d; f = foot f e b a; g = foot g e c b; h = foot h e c a; i = on_line i h f, on_line i b e; j = on_line j g f, on_line j a e; k = on_line k c i, on_line k b a; l = on_line l c j, on_line l b a; m = on_circle m c e, on_tline m e a c ? cong f k f l
I0123 13:28:11.824537 140123132608512 ddar.py:60] Depth 1/1000 time = 0.7650001049041748
I0123 13:28:12.807857 140123132608512 ddar.py:60] Depth 2/1000 time = 0.9831533432006836
I0123 13:28:14.420196 140123132608512 ddar.py:60] Depth 3/1000 time = 1.6121814250946045
I0123 13:28:16.395674 140123132608512 ddar.py:60] Depth 4/1000 time = 1.9753172397613525
I0123 13:28:18.193272 140123132608512 ddar.py:60] Depth 5/1000 time = 1.797438621520996
I0123 13:28:19.973557 140123132608512 ddar.py:60] Depth 6/1000 time = 1.7799952030181885
I0123 13:28:21.958753 140123132608512 ddar.py:60] Depth 7/1000 time = 1.9821434020996094
I0123 13:28:21.964570 140123132608512 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:28:21.964646 140123132608512 alphageometry.py:566] LM output (score=-1.753087): "m : D c i c m 18 D d i d m 19 ;"
I0123 13:28:21.964682 140123132608512 alphageometry.py:567] Translation: "m = on_circle m c i, on_circle m d i"

I0123 13:28:21.964719 140123132608512 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = lc_tangent e b d, on_line e c d; f = foot f e b a; g = foot g e c b; h = foot h e c a; i = on_line i h f, on_line i b e; j = on_line j g f, on_line j a e; k = on_line k c i, on_line k b a; l = on_line l c j, on_line l b a; m = on_circle m c i, on_circle m d i ? cong f k f l"
I0123 13:28:21.964885 140123132608512 graph.py:498] 
I0123 13:28:21.964938 140123132608512 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = lc_tangent e b d, on_line e c d; f = foot f e b a; g = foot g e c b; h = foot h e c a; i = on_line i h f, on_line i b e; j = on_line j g f, on_line j a e; k = on_line k c i, on_line k b a; l = on_line l c j, on_line l b a; m = on_circle m c i, on_circle m d i ? cong f k f l
I0123 13:28:22.744270 140123132608512 ddar.py:60] Depth 1/1000 time = 0.7607958316802979
I0123 13:28:23.835088 140123132608512 ddar.py:60] Depth 2/1000 time = 1.090672254562378
I0123 13:28:25.706077 140123132608512 ddar.py:60] Depth 3/1000 time = 1.8707940578460693
I0123 13:28:27.820900 140123132608512 ddar.py:60] Depth 4/1000 time = 2.1145660877227783
I0123 13:28:29.752707 140123132608512 ddar.py:60] Depth 5/1000 time = 1.9283583164215088
I0123 13:28:31.774816 140123132608512 ddar.py:60] Depth 6/1000 time = 2.014357328414917
I0123 13:28:34.203672 140123132608512 ddar.py:60] Depth 7/1000 time = 2.428699254989624
I0123 13:28:37.086505 140123132608512 ddar.py:60] Depth 8/1000 time = 2.8826727867126465
I0123 13:28:39.847784 140123132608512 ddar.py:60] Depth 9/1000 time = 2.7611236572265625
I0123 13:28:42.625323 140123132608512 ddar.py:60] Depth 10/1000 time = 2.76308012008667
I0123 13:28:45.450904 140123132608512 ddar.py:60] Depth 11/1000 time = 2.805812120437622
I0123 13:28:45.451097 140123132608512 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:28:45.451189 140123132608512 alphageometry.py:566] LM output (score=-1.970633): "m : D c i h m 18 D c m h i 19 ;"
I0123 13:28:45.451225 140123132608512 alphageometry.py:567] Translation: "ERROR: Traceback (most recent call last):
  File "/home/chi/alphageometry-test/alphageometry.py", line 438, in try_translate_constrained_to_construct
    g.copy().add_clause(clause, 0, DEFINITIONS)
  File "/home/chi/alphageometry-test/graph.py", line 2635, in add_clause
    raise PointTooCloseError()
graph.PointTooCloseError
"

I0123 13:28:45.451262 140123132608512 alphageometry.py:566] LM output (score=-2.022653): "m : D c e e m 18 D c g g m 19 ;"
I0123 13:28:45.451290 140123132608512 alphageometry.py:567] Translation: "m = on_circle m e c, on_circle m g c"

I0123 13:28:45.451321 140123132608512 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = lc_tangent e b d, on_line e c d; f = foot f e b a; g = foot g e c b; h = foot h e c a; i = on_line i h f, on_line i b e; j = on_line j g f, on_line j a e; k = on_line k c i, on_line k b a; l = on_line l c j, on_line l b a; m = on_circle m e c, on_circle m g c ? cong f k f l"
I0123 13:28:45.451477 140123132608512 graph.py:498] 
I0123 13:28:45.451537 140123132608512 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = lc_tangent e b d, on_line e c d; f = foot f e b a; g = foot g e c b; h = foot h e c a; i = on_line i h f, on_line i b e; j = on_line j g f, on_line j a e; k = on_line k c i, on_line k b a; l = on_line l c j, on_line l b a; m = on_circle m e c, on_circle m g c ? cong f k f l
I0123 13:28:46.448072 140123132608512 ddar.py:60] Depth 1/1000 time = 0.9776525497436523
I0123 13:28:47.621511 140123132608512 ddar.py:60] Depth 2/1000 time = 1.1732940673828125
I0123 13:28:49.571858 140123132608512 ddar.py:60] Depth 3/1000 time = 1.9501986503601074
I0123 13:28:51.516137 140123132608512 ddar.py:60] Depth 4/1000 time = 1.944131851196289
I0123 13:28:53.472743 140123132608512 ddar.py:60] Depth 5/1000 time = 1.9535064697265625
I0123 13:28:53.477824 140123132608512 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:28:53.477901 140123132608512 alphageometry.py:566] LM output (score=-2.048237): "m : C e k m 18 D e m k m 19 ;"
I0123 13:28:53.477952 140123132608512 alphageometry.py:567] Translation: "m = on_line m e k, on_bline m k e"

I0123 13:28:53.477991 140123132608512 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = lc_tangent e b d, on_line e c d; f = foot f e b a; g = foot g e c b; h = foot h e c a; i = on_line i h f, on_line i b e; j = on_line j g f, on_line j a e; k = on_line k c i, on_line k b a; l = on_line l c j, on_line l b a; m = on_line m e k, on_bline m k e ? cong f k f l"
I0123 13:28:53.478140 140123132608512 graph.py:498] 
I0123 13:28:53.478190 140123132608512 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = lc_tangent e b d, on_line e c d; f = foot f e b a; g = foot g e c b; h = foot h e c a; i = on_line i h f, on_line i b e; j = on_line j g f, on_line j a e; k = on_line k c i, on_line k b a; l = on_line l c j, on_line l b a; m = on_line m e k, on_bline m k e ? cong f k f l
I0123 13:28:54.264868 140123132608512 ddar.py:60] Depth 1/1000 time = 0.7699055671691895
I0123 13:28:56.599493 140123132608512 ddar.py:60] Depth 2/1000 time = 2.3344838619232178
I0123 13:28:59.781076 140123132608512 ddar.py:60] Depth 3/1000 time = 3.181429386138916
I0123 13:29:03.012823 140123132608512 ddar.py:60] Depth 4/1000 time = 3.2315683364868164
I0123 13:29:06.247182 140123132608512 ddar.py:60] Depth 5/1000 time = 3.2340328693389893
I0123 13:29:09.463316 140123132608512 ddar.py:60] Depth 6/1000 time = 3.213064670562744
I0123 13:29:09.466746 140123132608512 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:29:09.466842 140123132608512 alphageometry.py:566] LM output (score=-2.104396): "m : D c i c m 18 ;"
I0123 13:29:09.466878 140123132608512 alphageometry.py:567] Translation: "m = on_circle m c i"

I0123 13:29:09.466916 140123132608512 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = lc_tangent e b d, on_line e c d; f = foot f e b a; g = foot g e c b; h = foot h e c a; i = on_line i h f, on_line i b e; j = on_line j g f, on_line j a e; k = on_line k c i, on_line k b a; l = on_line l c j, on_line l b a; m = on_circle m c i ? cong f k f l"
I0123 13:29:09.467071 140123132608512 graph.py:498] 
I0123 13:29:09.467127 140123132608512 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = lc_tangent e b d, on_line e c d; f = foot f e b a; g = foot g e c b; h = foot h e c a; i = on_line i h f, on_line i b e; j = on_line j g f, on_line j a e; k = on_line k c i, on_line k b a; l = on_line l c j, on_line l b a; m = on_circle m c i ? cong f k f l
I0123 13:29:10.201368 140123132608512 ddar.py:60] Depth 1/1000 time = 0.7181901931762695
I0123 13:29:11.126539 140123132608512 ddar.py:60] Depth 2/1000 time = 0.925015926361084
I0123 13:29:12.734719 140123132608512 ddar.py:60] Depth 3/1000 time = 1.608022928237915
I0123 13:29:14.182872 140123132608512 ddar.py:60] Depth 4/1000 time = 1.4479944705963135
I0123 13:29:15.640859 140123132608512 ddar.py:60] Depth 5/1000 time = 1.4549832344055176
I0123 13:29:15.644186 140123132608512 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:29:15.644258 140123132608512 alphageometry.py:566] LM output (score=-2.148977): "m : D c e c m 18 D e g g m 19 ;"
I0123 13:29:15.644294 140123132608512 alphageometry.py:567] Translation: "m = on_circle m c e, on_circle m g e"

I0123 13:29:15.644329 140123132608512 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = lc_tangent e b d, on_line e c d; f = foot f e b a; g = foot g e c b; h = foot h e c a; i = on_line i h f, on_line i b e; j = on_line j g f, on_line j a e; k = on_line k c i, on_line k b a; l = on_line l c j, on_line l b a; m = on_circle m c e, on_circle m g e ? cong f k f l"
I0123 13:29:15.644479 140123132608512 graph.py:498] 
I0123 13:29:15.644529 140123132608512 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = lc_tangent e b d, on_line e c d; f = foot f e b a; g = foot g e c b; h = foot h e c a; i = on_line i h f, on_line i b e; j = on_line j g f, on_line j a e; k = on_line k c i, on_line k b a; l = on_line l c j, on_line l b a; m = on_circle m c e, on_circle m g e ? cong f k f l
I0123 13:29:16.433380 140123132608512 ddar.py:60] Depth 1/1000 time = 0.7703914642333984
I0123 13:29:17.521247 140123132608512 ddar.py:60] Depth 2/1000 time = 1.0877234935760498
I0123 13:29:19.323868 140123132608512 ddar.py:60] Depth 3/1000 time = 1.8024749755859375
I0123 13:29:21.347017 140123132608512 ddar.py:60] Depth 4/1000 time = 2.022996187210083
I0123 13:29:23.181320 140123132608512 ddar.py:60] Depth 5/1000 time = 1.8310472965240479
I0123 13:29:23.190526 140123132608512 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:29:23.190606 140123132608512 alphageometry.py:566] LM output (score=-2.210831): "m : D c d c m 18 T c d c m 19 ;"
I0123 13:29:23.190642 140123132608512 alphageometry.py:567] Translation: "m = on_circle m c d, on_tline m c c d"

I0123 13:29:23.190678 140123132608512 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = lc_tangent e b d, on_line e c d; f = foot f e b a; g = foot g e c b; h = foot h e c a; i = on_line i h f, on_line i b e; j = on_line j g f, on_line j a e; k = on_line k c i, on_line k b a; l = on_line l c j, on_line l b a; m = on_circle m c d, on_tline m c c d ? cong f k f l"
I0123 13:29:23.190828 140123132608512 graph.py:498] 
I0123 13:29:23.190879 140123132608512 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = lc_tangent e b d, on_line e c d; f = foot f e b a; g = foot g e c b; h = foot h e c a; i = on_line i h f, on_line i b e; j = on_line j g f, on_line j a e; k = on_line k c i, on_line k b a; l = on_line l c j, on_line l b a; m = on_circle m c d, on_tline m c c d ? cong f k f l
I0123 13:29:23.979022 140123132608512 ddar.py:60] Depth 1/1000 time = 0.7675256729125977
I0123 13:29:24.986488 140123132608512 ddar.py:60] Depth 2/1000 time = 1.0072765350341797
I0123 13:29:26.532763 140123132608512 ddar.py:60] Depth 3/1000 time = 1.5460522174835205
I0123 13:29:28.076616 140123132608512 ddar.py:60] Depth 4/1000 time = 1.5436787605285645
I0123 13:29:29.618029 140123132608512 ddar.py:60] Depth 5/1000 time = 1.5409057140350342
I0123 13:29:31.158736 140123132608512 ddar.py:60] Depth 6/1000 time = 1.5355358123779297
I0123 13:29:32.708714 140123132608512 ddar.py:60] Depth 7/1000 time = 1.5486443042755127
I0123 13:29:32.712355 140123132608512 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:29:32.712437 140123132608512 alphageometry.py:566] LM output (score=-2.211735): "m : D c i i m 18 ;"
I0123 13:29:32.712472 140123132608512 alphageometry.py:567] Translation: "m = on_circle m i c"

I0123 13:29:32.712506 140123132608512 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = lc_tangent e b d, on_line e c d; f = foot f e b a; g = foot g e c b; h = foot h e c a; i = on_line i h f, on_line i b e; j = on_line j g f, on_line j a e; k = on_line k c i, on_line k b a; l = on_line l c j, on_line l b a; m = on_circle m i c ? cong f k f l"
I0123 13:29:32.712660 140123132608512 graph.py:498] 
I0123 13:29:32.712716 140123132608512 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = lc_tangent e b d, on_line e c d; f = foot f e b a; g = foot g e c b; h = foot h e c a; i = on_line i h f, on_line i b e; j = on_line j g f, on_line j a e; k = on_line k c i, on_line k b a; l = on_line l c j, on_line l b a; m = on_circle m i c ? cong f k f l
I0123 13:29:33.456469 140123132608512 ddar.py:60] Depth 1/1000 time = 0.7278013229370117
I0123 13:29:34.393498 140123132608512 ddar.py:60] Depth 2/1000 time = 0.9368898868560791
I0123 13:29:35.834191 140123132608512 ddar.py:60] Depth 3/1000 time = 1.440537929534912
I0123 13:29:37.520131 140123132608512 ddar.py:60] Depth 4/1000 time = 1.6857879161834717
I0123 13:29:39.010250 140123132608512 ddar.py:60] Depth 5/1000 time = 1.487137794494629
I0123 13:29:39.013403 140123132608512 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:29:39.013482 140123132608512 alphageometry.py:566] LM output (score=-2.391267): "m : D c i g m 18 D c m g i 19 ;"
I0123 13:29:39.013519 140123132608512 alphageometry.py:567] Translation: "m = eqdistance m g c i, eqdistance m c g i"

I0123 13:29:39.013568 140123132608512 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = lc_tangent e b d, on_line e c d; f = foot f e b a; g = foot g e c b; h = foot h e c a; i = on_line i h f, on_line i b e; j = on_line j g f, on_line j a e; k = on_line k c i, on_line k b a; l = on_line l c j, on_line l b a; m = eqdistance m g c i, eqdistance m c g i ? cong f k f l"
I0123 13:29:39.013733 140123132608512 graph.py:498] 
I0123 13:29:39.013787 140123132608512 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = lc_tangent e b d, on_line e c d; f = foot f e b a; g = foot g e c b; h = foot h e c a; i = on_line i h f, on_line i b e; j = on_line j g f, on_line j a e; k = on_line k c i, on_line k b a; l = on_line l c j, on_line l b a; m = eqdistance m g c i, eqdistance m c g i ? cong f k f l
I0123 13:29:39.822234 140123132608512 ddar.py:60] Depth 1/1000 time = 0.7901511192321777
I0123 13:29:40.877926 140123132608512 ddar.py:60] Depth 2/1000 time = 1.0555524826049805
I0123 13:29:42.496372 140123132608512 ddar.py:60] Depth 3/1000 time = 1.6182961463928223
I0123 13:29:44.182625 140123132608512 ddar.py:60] Depth 4/1000 time = 1.6860952377319336
I0123 13:29:45.900032 140123132608512 ddar.py:60] Depth 5/1000 time = 1.7172446250915527
I0123 13:29:47.650790 140123132608512 ddar.py:60] Depth 6/1000 time = 1.7475695610046387
I0123 13:29:49.378943 140123132608512 ddar.py:60] Depth 7/1000 time = 1.725525140762329
I0123 13:29:49.382495 140123132608512 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:29:49.382593 140123132608512 alphageometry.py:566] LM output (score=-2.412746): "m : D c e e m 18 ;"
I0123 13:29:49.382630 140123132608512 alphageometry.py:567] Translation: "m = on_circle m e c"

I0123 13:29:49.382670 140123132608512 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = lc_tangent e b d, on_line e c d; f = foot f e b a; g = foot g e c b; h = foot h e c a; i = on_line i h f, on_line i b e; j = on_line j g f, on_line j a e; k = on_line k c i, on_line k b a; l = on_line l c j, on_line l b a; m = on_circle m e c ? cong f k f l"
I0123 13:29:49.382826 140123132608512 graph.py:498] 
I0123 13:29:49.382881 140123132608512 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = lc_tangent e b d, on_line e c d; f = foot f e b a; g = foot g e c b; h = foot h e c a; i = on_line i h f, on_line i b e; j = on_line j g f, on_line j a e; k = on_line k c i, on_line k b a; l = on_line l c j, on_line l b a; m = on_circle m e c ? cong f k f l
I0123 13:29:50.142399 140123132608512 ddar.py:60] Depth 1/1000 time = 0.7436187267303467
I0123 13:29:51.075303 140123132608512 ddar.py:60] Depth 2/1000 time = 0.9327630996704102
I0123 13:29:52.478018 140123132608512 ddar.py:60] Depth 3/1000 time = 1.4025638103485107
I0123 13:29:53.909268 140123132608512 ddar.py:60] Depth 4/1000 time = 1.4310777187347412
I0123 13:29:55.340353 140123132608512 ddar.py:60] Depth 5/1000 time = 1.428095817565918
I0123 13:29:55.343508 140123132608512 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:29:55.343585 140123132608512 alphageometry.py:566] LM output (score=-2.422678): "m : C i k m 18 D i m k m 19 ;"
I0123 13:29:55.343621 140123132608512 alphageometry.py:567] Translation: "m = on_line m i k, on_bline m k i"

I0123 13:29:55.343655 140123132608512 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = lc_tangent e b d, on_line e c d; f = foot f e b a; g = foot g e c b; h = foot h e c a; i = on_line i h f, on_line i b e; j = on_line j g f, on_line j a e; k = on_line k c i, on_line k b a; l = on_line l c j, on_line l b a; m = on_line m i k, on_bline m k i ? cong f k f l"
I0123 13:29:55.343802 140123132608512 graph.py:498] 
I0123 13:29:55.343852 140123132608512 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = lc_tangent e b d, on_line e c d; f = foot f e b a; g = foot g e c b; h = foot h e c a; i = on_line i h f, on_line i b e; j = on_line j g f, on_line j a e; k = on_line k c i, on_line k b a; l = on_line l c j, on_line l b a; m = on_line m i k, on_bline m k i ? cong f k f l
I0123 13:29:56.441686 140123132608512 ddar.py:60] Depth 1/1000 time = 1.0811858177185059
I0123 13:29:57.489063 140123132608512 ddar.py:60] Depth 2/1000 time = 1.0472264289855957
I0123 13:29:59.224331 140123132608512 ddar.py:60] Depth 3/1000 time = 1.7351081371307373
I0123 13:30:00.767530 140123132608512 ddar.py:60] Depth 4/1000 time = 1.5428121089935303
I0123 13:30:02.531104 140123132608512 ddar.py:60] Depth 5/1000 time = 1.7633028030395508
I0123 13:30:04.086413 140123132608512 ddar.py:60] Depth 6/1000 time = 1.5523288249969482
I0123 13:30:04.089659 140123132608512 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:30:04.089740 140123132608512 alphageometry.py:566] LM output (score=-2.434055): "m : D a m c m 18 D c m d m 19 ;"
I0123 13:30:04.089776 140123132608512 alphageometry.py:567] Translation: "m = on_bline m c a, on_bline m d c"

I0123 13:30:04.089812 140123132608512 alphageometry.py:576] Solving: "a b c = triangle a b c; d = incenter d c b a; e = lc_tangent e b d, on_line e c d; f = foot f e b a; g = foot g e c b; h = foot h e c a; i = on_line i h f, on_line i b e; j = on_line j g f, on_line j a e; k = on_line k c i, on_line k b a; l = on_line l c j, on_line l b a; m = on_bline m c a, on_bline m d c ? cong f k f l"
I0123 13:30:04.089959 140123132608512 graph.py:498] 
I0123 13:30:04.090008 140123132608512 graph.py:499] a b c = triangle a b c; d = incenter d c b a; e = lc_tangent e b d, on_line e c d; f = foot f e b a; g = foot g e c b; h = foot h e c a; i = on_line i h f, on_line i b e; j = on_line j g f, on_line j a e; k = on_line k c i, on_line k b a; l = on_line l c j, on_line l b a; m = on_bline m c a, on_bline m d c ? cong f k f l
I0123 13:30:06.231483 140123132608512 ddar.py:60] Depth 1/1000 time = 2.1192586421966553
I0123 13:30:08.580883 140123132608512 ddar.py:60] Depth 2/1000 time = 2.3492164611816406
I0123 13:30:11.685167 140123132608512 ddar.py:60] Depth 3/1000 time = 3.1040894985198975
I0123 13:30:14.637256 140123132608512 ddar.py:60] Depth 4/1000 time = 2.9519169330596924
I0123 13:30:17.645284 140123132608512 ddar.py:60] Depth 5/1000 time = 3.0024867057800293
I0123 13:30:20.738548 140123132608512 ddar.py:60] Depth 6/1000 time = 3.093104600906372
I0123 13:30:23.884335 140123132608512 ddar.py:60] Depth 7/1000 time = 3.1455979347229004
I0123 13:30:27.035745 140123132608512 ddar.py:60] Depth 8/1000 time = 3.1512324810028076
I0123 13:30:30.433015 140123132608512 ddar.py:60] Depth 9/1000 time = 3.3970835208892822
I0123 13:30:33.819819 140123132608512 ddar.py:60] Depth 10/1000 time = 3.3865602016448975
I0123 13:30:38.208539 140123132608512 ddar.py:60] Depth 11/1000 time = 4.388556241989136
I0123 13:30:43.748574 140123132608512 ddar.py:60] Depth 12/1000 time = 5.5398736000061035
I0123 13:30:52.767265 140123132608512 ddar.py:60] Depth 13/1000 time = 9.01842999458313
I0123 13:31:03.469541 140123132608512 ddar.py:60] Depth 14/1000 time = 10.701900959014893
I0123 13:31:21.919078 140123132608512 ddar.py:60] Depth 15/1000 time = 18.449238061904907
I0123 13:31:22.452530 140123132608512 alphageometry.py:191] 
==========================
 * From theorem premises:
A B C D E F G H I J K L : Points
DCB = ACD [00]
DAC = BAD [01]
BE  BD [02]
E,D,C are collinear [03]
(CD-BE) = (CD-BE) [04]
F,A,B are collinear [05]
EF  AB [06]
G,B,C are collinear [07]
EG  BC [08]
H,A,C are collinear [09]
EH  AC [10]
E,B,I are collinear [11]
F,I,H are collinear [12]
G,F,J are collinear [13]
E,A,J are collinear [14]
K,A,B are collinear [15]
K,I,C are collinear [16]
J,L,C are collinear [17]
L,A,B are collinear [18]

 * Auxiliary Constructions:
M : Points
MD = MC [19]
MC = MA [20]
MCA = CAM [21]
MDC = DCM [22]

 * Proof steps:
001. G,B,C are collinear [07] & F,A,B are collinear [05] & EF  AB [06] & EG  BC [08]   EGB = EFB [23]
002. EGB = EFB [23]   G,E,F,B are concyclic [24]
003. G,B,C are collinear [07] & EG  BC [08]   GE  GB [25]
004. BE  BD [02] & GE  GB [25]   (DB-GE) = EBG [26]
005. BE  BD [02] & GE  GB [25]   DBG = BEG [27]
006. G,B,C are collinear [07] & (DB-GE) = EBG [26]   DBE = EGB [28]
007. G,B,C are collinear [07] & A,C,H are collinear [09] & EH  AC [10] & EG  BC [08]   EGC = CHE [29]
008. E,D,C are collinear [03] & G,B,C are collinear [07] & H,A,C are collinear [09] & DCB = ACD [00]   ECG = HCE [30]
009. EGC = CHE [29] & ECG = HCE [30] (Similar Triangles)  CG = CH [31]
010. EGC = CHE [29] & ECG = HCE [30] (Similar Triangles)  EG = EH [32]
011. MD = MC [19] & MC = MA [20] & CG = CH [31]   MA:MD = CG:CH [33]
012. MD = MC [19] & MC = MA [20] & CG = CH [31]   MA:MD = CH:CG [34]
013. C,A,H are collinear [09] & EH  AC [10]   EH  HA [35]
014. EH  HA [35] & GE  GB [25]   HEG = (HA-GB) [36]
015. BCD = DCA [00] & EG  BC [08] & EH  AC [10] & MCA = CAM [21] & MDC = DCM [22] (Angle chase)  HEG = AMD [37]
016. H,A,C are collinear [09] & G,B,C are collinear [07] & HEG = (HA-GB) [36] & HEG = AMD [37]   AMD = HCG [38]
017. MA:MD = CG:CH [33] & AMD = HCG [38] (Similar Triangles)  AM:AD = GC:GH [39]
018. MA:MD = CG:CH [33] & AMD = HCG [38] (Similar Triangles)  ADM = CHG [40]
019. AM:AD = GC:GH [39] & MD = MC [19] & MC = MA [20]   DM:AD = GC:GH [41]
020. GC:GH = DM:AD [41] & HC = GC [31] & MD = MC [19] & MC = MA [20]   HC:HG = AM:AD [42]
021. HC:HG = AM:AD [42] & DM:AD = GC:GH [41] (Similar Triangles)  CGH = MDA [43]
022. K,A,B are collinear [15] & ADM = CHG [40] & H,A,C are collinear [09] & DAC = BAD [01]   KAD = (DM-GH) [44]
023. MA:MD = CH:CG [34] & AMD = HCG [38] (Similar Triangles)  MAD = CHG [45]
024. C,A,H are collinear [09] & MAD = CHG [45]   DAM = GHA [46]
025. KAD = (DM-GH) [44] & DAM = GHA [46]   KAM = (DM-HA) [47]
026. BCD = DCA [00] & MDC = DCM [22] (Angle chase)  MCB = (AC-DM) [48]
027. BCD = DCA [00] & MDC = DCM [22] (Angle chase)  MCA = (BC-DM) [49]
028. KAM = (DM-HA) [47] & K,A,B are collinear [15] & H,A,C are collinear [09] & MCB = (AC-DM) [48]   MCB = MAB [50]
029. MCB = MAB [50]   M,A,B,C are concyclic [51]
030. M,A,B,C are concyclic [51]   MAC = MBC [52]
031. G,B,C are collinear [07] & MAC = MBC [52] & HCG = AMD [38] & H,A,C are collinear [09]   (DM-GB) = MBG [53]
032. (DM-GB) = MBG [53]   DM  BM [54]
033. DM  BM [54]   D,B,M are collinear [55]
034. K,A,B are collinear [15] & C,A,H are collinear [09] & BAD = DAC [01]   KAD = DAH [56]
035. C,A,H are collinear [09] & ADM = CHG [40]   ADM = AHG [57]
036. KAD = DAH [56] & ADM = AHG [57]   (AK-DM) = (AD-GH) [58]
037. K,A,B are collinear [15] & DBG = BEG [27] & G,B,C are collinear [07] & CGH = MDA [43] & D,B,M are collinear [55] & (AK-DM) = (AD-GH) [58] & DM  BM [54]   (DB-AK) = GEB [59]
038. DBE = EGB [28] & (DB-AK) = GEB [59]   (EB-AK) = GBE [60]
039. G,E,F,B are concyclic [24]   GFE = GBE [61]
040. G,E,F,B are concyclic [24]   GEB = GFB [62]
041. F,A,B are collinear [05] & (EB-AK) = GBE [60] & K,A,B are collinear [15] & G,B,C are collinear [07] & GFE = GBE [61]   GFE = EBF [63]
042. G,E,F,B are concyclic [24] & GFE = EBF [63]   GE = EF [64]
043. GE = EF [64] & EG = EH [32]   EH = EF [65]
044. K,A,B are collinear [15] & AB  EF [06]   AK  EF [66]
045. AK  EF [66] & EH  HA [35]   (AK-EH) = (EF-HA) [67]
046. F,A,B are collinear [05] & C,A,H are collinear [09] & (AK-EH) = (EF-HA) [67] & K,A,B are collinear [15]   EFA = AHE [68]
047. EG = EH [32] & CG = CH [31]   GH  EC [69]
048. E,D,C are collinear [03] & GH  EC [69]   GH  ED [70]
049. BE  BD [02] & GH  ED [70]   (DB-GH) = BED [71]
050. E,D,C are collinear [03] & DAC = BAD [01] & CHG = ADM [40] & H,A,C are collinear [09] & D,B,M are collinear [55] & (DB-GH) = BED [71]   BED = BAD [72]
051. BED = BAD [72]   D,E,A,B are concyclic [73]
052. D,E,A,B are concyclic [73]   DEA = DBA [74]
053. D,E,A,B are concyclic [73]   DAE = DBE [75]
054. K,A,B are collinear [15] & KAM = (DM-HA) [47] & H,A,C are collinear [09] & HCG = AMD [38] & G,B,C are collinear [07] & D,B,M are collinear [55] & DBG = BEG [27] & GEB = GFB [62] & F,A,B are collinear [05] & DM  BM [54]   (GF-AK) = (DB-AK) [76]
055. (GF-AK) = (DB-AK) [76]   GF  DB [77]
056. E,D,C are collinear [03] & DCA = BCD [00]   DCA = BCE [78]
057. E,D,C are collinear [03] & DCA = BCD [00]   DCB = ACE [79]
058. E,D,C are collinear [03] & (DB-GH) = BED [71] & CHG = ADM [40] & H,A,C are collinear [09] & D,B,M are collinear [55]   DAC = BEC [80]
059. DCA = BCE [78] & DAC = BEC [80] (Similar Triangles)  DC:BC = AC:EC [81]
060. DC:BC = AC:EC [81] & DCB = ACE [79] (Similar Triangles)  CDB = CAE [82]
061. DC:BC = AC:EC [81] & DCB = ACE [79] (Similar Triangles)  DBC = AEC [83]
062. C,A,H are collinear [09] & F,A,B are collinear [05] & DEA = DBA [74] & E,D,C are collinear [03] & D,B,M are collinear [55] & FG  BD [77] & CDB = CAE [82]   EAH = FAE [84]
063. EFA = AHE [68] & EAH = FAE [84] (Similar Triangles)  FE:HE = FA:HA [85]
064. FE:HE = FA:HA [85] & EH = EF [65]   FA = HA [86]
065. EH = EF [65] & FA = HA [86]   EA  HF [87]
066. G,F,J are collinear [13] & G,B,C are collinear [07] & E,A,J are collinear [14] & E,D,C are collinear [03] & DBC = AEC [83] & BD  FG [77]   JGC = JEC [88]
067. JGC = JEC [88]   G,E,J,C are concyclic [89]
068. G,E,J,C are concyclic [89]   JEG = JCG [90]
069. G,B,C are collinear [07] & A,C,H are collinear [09] & EH  AC [10] & EG  BC [08]   EGC = EHC [91]
070. EGC = EHC [91]   G,E,H,C are concyclic [92]
071. G,E,C,J are concyclic [89] & G,E,H,C are concyclic [92]   J,H,E,C are concyclic [93]
072. J,H,E,C are concyclic [93]   JEH = JCH [94]
073. J,L,C are collinear [17] & EA  HF [87] & JEG = JCG [90] & E,A,J are collinear [14] & G,B,C are collinear [07] & EH  AC [10] & EG  BC [08] & JEH = JCH [94] & H,A,C are collinear [09]   LC  FH [95]
074. L,A,B are collinear [18] & F,A,B are collinear [05]   A,L,F are collinear [96]
075. LC  FH [95] & A,L,F are collinear [96] & H,A,C are collinear [09]   FA:HA = FL:HC [97]
076. AK  EF [66] & BE  BD [02]   (AK-DB) = FEB [98]
077. F,A,B are collinear [05] & (AK-DB) = FEB [98] & K,A,B are collinear [15] & DAE = DBE [75]   DAE = BFE [99]
078. E,D,C are collinear [03] & DEA = DBA [74] & (AK-DB) = FEB [98] & K,A,B are collinear [15]   FEB = AED [100]
079. DAE = BFE [99] & FEB = AED [100] (Similar Triangles)  ED:EB = EA:EF [101]
080. E,B,I are collinear [11] & E,D,C are collinear [03] & DEA = DBA [74] & (AK-DB) = FEB [98] & K,A,B are collinear [15]   IEF = CEA [102]
081. F,A,B are collinear [05] & C,A,H are collinear [09] & EH  AC [10] & EF  AB [06]   EFA = EHA [103]
082. EFA = EHA [103]   E,H,A,F are concyclic [104]
083. E,H,A,F are concyclic [104]   EAH = EFH [105]
084. F,I,H are collinear [12] & EAH = EFH [105] & H,A,C are collinear [09]   IFE = CAE [106]
085. IEF = CEA [102] & IFE = CAE [106] (Similar Triangles)  EC:EI = EA:EF [107]
086. ED:EB = EA:EF [101] & EC:EI = EA:EF [107]   EC:EI = ED:EB [108]
087. E,D,C are collinear [03] & E,B,I are collinear [11] & (CD-BE) = (CD-BE) [04]   CEI = DEB [109]
088. EC:EI = ED:EB [108] & CEI = DEB [109] (Similar Triangles)  ECI = EDB [110]
089. K,I,C are collinear [16] & ECI = EDB [110] & E,D,C are collinear [03] & FG  BD [77]   GF  CK [111]
090. K,A,B are collinear [15] & F,A,B are collinear [05]   B,F,K are collinear [112]
091. GF  CK [111] & G,B,C are collinear [07] & B,F,K are collinear [112]   GB:FB = GC:FK [113]
092. F,A,B are collinear [05] & GEB = GFB [62] & DBG = BEG [27] & G,B,C are collinear [07] & HCG = AMD [38] & H,A,C are collinear [09] & D,B,M are collinear [55]   CAM = GFB [114]
093. G,B,C are collinear [07] & C,A,H are collinear [09] & (BC-DM) = MCA [49]   (GB-DM) = (MC-HA) [115]
094. K,A,B are collinear [15] & C,A,H are collinear [09] & KAM = (DM-HA) [47]   (DM-AK) = HAM [116]
095. (GB-DM) = (MC-HA) [115] & (DM-AK) = HAM [116]   (GB-AK) = CMA [117]
096. G,B,C are collinear [07] & F,A,B are collinear [05] & (GB-AK) = CMA [117] & K,A,B are collinear [15]   CMA = GBF [118]
097. CAM = GFB [114] & CMA = GBF [118] (Similar Triangles)  MC:MA = BG:BF [119]
098. FA:HA = FL:HC [97] & FA = HA [86] & CG = CH [31] & GB:FB = GC:FK [113] & MC:MA = BG:BF [119] & MC = MA [20]   FK = FL
==========================

I0123 13:31:22.452650 140123132608512 alphageometry.py:582] Solved.
