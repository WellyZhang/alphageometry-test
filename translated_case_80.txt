I0123 11:23:06.809021 139686199676928 inference_utils.py:69] Parsing gin configuration.
I0123 11:23:06.809120 139686199676928 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0123 11:23:06.809319 139686199676928 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0123 11:23:06.809352 139686199676928 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0123 11:23:06.809381 139686199676928 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0123 11:23:06.809409 139686199676928 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0123 11:23:06.809435 139686199676928 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0123 11:23:06.809461 139686199676928 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0123 11:23:06.809487 139686199676928 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0123 11:23:06.809512 139686199676928 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=32
I0123 11:23:06.809537 139686199676928 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0123 11:23:06.809562 139686199676928 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0123 11:23:06.809607 139686199676928 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0123 11:23:06.809748 139686199676928 resource_reader.py:55] Path not found: base_htrans.gin
I0123 11:23:06.809950 139686199676928 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0123 11:23:06.810047 139686199676928 resource_reader.py:55] Path not found: trainer_configuration.gin
I0123 11:23:06.816347 139686199676928 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0123 11:23:06.816469 139686199676928 resource_reader.py:55] Path not found: size/medium_150M.gin
I0123 11:23:06.816796 139686199676928 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0123 11:23:06.816902 139686199676928 resource_reader.py:55] Path not found: options/positions_t5.gin
I0123 11:23:06.817185 139686199676928 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0123 11:23:06.817288 139686199676928 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0123 11:23:06.817699 139686199676928 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0123 11:23:06.817802 139686199676928 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0123 11:23:06.821428 139686199676928 training_loop.py:334] ==== Training loop: initializing model ====
I0123 11:23:06.917212 139686199676928 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0123 11:23:06.917948 139686199676928 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0123 11:23:06.924689 139686199676928 training_loop.py:335] Process 0 of 1
I0123 11:23:06.924746 139686199676928 training_loop.py:336] Local device count = 1
I0123 11:23:06.924787 139686199676928 training_loop.py:337] Number of replicas = 1
I0123 11:23:06.924820 139686199676928 training_loop.py:339] Using random number seed 42
I0123 11:23:07.387216 139686199676928 training_loop.py:359] Initializing the model.
I0123 11:23:07.785907 139686199676928 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.786155 139686199676928 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 11:23:07.786255 139686199676928 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:23:07.786331 139686199676928 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:23:07.786405 139686199676928 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:23:07.786636 139686199676928 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:23:07.786705 139686199676928 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:23:07.786774 139686199676928 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:23:07.786842 139686199676928 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:23:07.786910 139686199676928 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:23:07.786977 139686199676928 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:23:07.787043 139686199676928 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:23:07.787110 139686199676928 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:23:07.787176 139686199676928 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:23:07.787214 139686199676928 attention.py:418] Single window, no scan.
I0123 11:23:07.787257 139686199676928 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 11:23:07.787369 139686199676928 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:23:07.787407 139686199676928 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:23:07.787436 139686199676928 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:23:07.789421 139686199676928 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.794662 139686199676928 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:23:07.805266 139686199676928 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.805538 139686199676928 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:23:07.809854 139686199676928 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:23:07.820321 139686199676928 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:23:07.820378 139686199676928 attention.py:418] Single window, no scan.
I0123 11:23:07.820415 139686199676928 transformer_layer.py:389] tlayer: self-attention.
I0123 11:23:07.820449 139686199676928 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.820511 139686199676928 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.821689 139686199676928 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.821768 139686199676928 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.822474 139686199676928 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.824889 139686199676928 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.831070 139686199676928 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.832360 139686199676928 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.832439 139686199676928 transformer_layer.py:468] tlayer: End windows.
I0123 11:23:07.832473 139686199676928 transformer_layer.py:472] tlayer: final FFN.
I0123 11:23:07.832532 139686199676928 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.832657 139686199676928 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:23:07.832986 139686199676928 nn_components.py:325] mlp: activation = None
I0123 11:23:07.833032 139686199676928 nn_components.py:329] mlp: final activation = None
I0123 11:23:07.834910 139686199676928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.835011 139686199676928 nn_components.py:261] mlp: residual
I0123 11:23:07.837850 139686199676928 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.837930 139686199676928 transformer_base.py:443] tbase: final FFN
I0123 11:23:07.838431 139686199676928 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:23:07.848559 139686199676928 nn_components.py:329] mlp: final activation = None
I0123 11:23:07.857364 139686199676928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.857461 139686199676928 nn_components.py:261] mlp: residual
I0123 11:23:07.857765 139686199676928 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.857847 139686199676928 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 11:23:07.857955 139686199676928 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:23:07.857993 139686199676928 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:23:07.858023 139686199676928 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:23:07.859876 139686199676928 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.862453 139686199676928 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:23:07.868116 139686199676928 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.868374 139686199676928 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:23:07.871058 139686199676928 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:23:07.874920 139686199676928 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:23:07.874976 139686199676928 attention.py:418] Single window, no scan.
I0123 11:23:07.875013 139686199676928 transformer_layer.py:389] tlayer: self-attention.
I0123 11:23:07.875044 139686199676928 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.875107 139686199676928 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.875697 139686199676928 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.875775 139686199676928 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.876140 139686199676928 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.876905 139686199676928 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.879449 139686199676928 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.880090 139686199676928 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.880167 139686199676928 transformer_layer.py:468] tlayer: End windows.
I0123 11:23:07.880200 139686199676928 transformer_layer.py:472] tlayer: final FFN.
I0123 11:23:07.880257 139686199676928 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.880380 139686199676928 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:23:07.880702 139686199676928 nn_components.py:325] mlp: activation = None
I0123 11:23:07.880744 139686199676928 nn_components.py:329] mlp: final activation = None
I0123 11:23:07.882701 139686199676928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.882796 139686199676928 nn_components.py:261] mlp: residual
I0123 11:23:07.885319 139686199676928 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.885399 139686199676928 transformer_base.py:443] tbase: final FFN
I0123 11:23:07.885863 139686199676928 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:23:07.888298 139686199676928 nn_components.py:329] mlp: final activation = None
I0123 11:23:07.890322 139686199676928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.890423 139686199676928 nn_components.py:261] mlp: residual
I0123 11:23:07.890727 139686199676928 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.890810 139686199676928 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 11:23:07.890920 139686199676928 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:23:07.890960 139686199676928 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:23:07.890990 139686199676928 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:23:07.893244 139686199676928 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.895647 139686199676928 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:23:07.901264 139686199676928 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.901525 139686199676928 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:23:07.904242 139686199676928 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:23:07.908131 139686199676928 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:23:07.908186 139686199676928 attention.py:418] Single window, no scan.
I0123 11:23:07.908221 139686199676928 transformer_layer.py:389] tlayer: self-attention.
I0123 11:23:07.908252 139686199676928 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.908312 139686199676928 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.908866 139686199676928 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.908941 139686199676928 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.909298 139686199676928 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.910070 139686199676928 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.912648 139686199676928 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.913309 139686199676928 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.913385 139686199676928 transformer_layer.py:468] tlayer: End windows.
I0123 11:23:07.913419 139686199676928 transformer_layer.py:472] tlayer: final FFN.
I0123 11:23:07.913476 139686199676928 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.913605 139686199676928 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:23:07.913931 139686199676928 nn_components.py:325] mlp: activation = None
I0123 11:23:07.913975 139686199676928 nn_components.py:329] mlp: final activation = None
I0123 11:23:07.915940 139686199676928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.916033 139686199676928 nn_components.py:261] mlp: residual
I0123 11:23:07.918578 139686199676928 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.918664 139686199676928 transformer_base.py:443] tbase: final FFN
I0123 11:23:07.919165 139686199676928 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:23:07.921475 139686199676928 nn_components.py:329] mlp: final activation = None
I0123 11:23:07.923436 139686199676928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.923533 139686199676928 nn_components.py:261] mlp: residual
I0123 11:23:07.923838 139686199676928 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.923917 139686199676928 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 11:23:07.924023 139686199676928 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:23:07.924060 139686199676928 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:23:07.924089 139686199676928 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:23:07.925984 139686199676928 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.928415 139686199676928 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:23:07.934069 139686199676928 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.934331 139686199676928 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:23:07.937016 139686199676928 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:23:07.940838 139686199676928 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:23:07.940893 139686199676928 attention.py:418] Single window, no scan.
I0123 11:23:07.940928 139686199676928 transformer_layer.py:389] tlayer: self-attention.
I0123 11:23:07.940958 139686199676928 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.941019 139686199676928 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.941569 139686199676928 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.941650 139686199676928 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.942013 139686199676928 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.942801 139686199676928 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.945381 139686199676928 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.946011 139686199676928 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.946099 139686199676928 transformer_layer.py:468] tlayer: End windows.
I0123 11:23:07.946134 139686199676928 transformer_layer.py:472] tlayer: final FFN.
I0123 11:23:07.946193 139686199676928 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.946324 139686199676928 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:23:07.946660 139686199676928 nn_components.py:325] mlp: activation = None
I0123 11:23:07.946704 139686199676928 nn_components.py:329] mlp: final activation = None
I0123 11:23:07.948632 139686199676928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.948725 139686199676928 nn_components.py:261] mlp: residual
I0123 11:23:07.951331 139686199676928 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.951421 139686199676928 transformer_base.py:443] tbase: final FFN
I0123 11:23:07.951866 139686199676928 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:23:07.954146 139686199676928 nn_components.py:329] mlp: final activation = None
I0123 11:23:07.956116 139686199676928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.956212 139686199676928 nn_components.py:261] mlp: residual
I0123 11:23:07.956504 139686199676928 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.956584 139686199676928 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 11:23:07.956691 139686199676928 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:23:07.956729 139686199676928 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:23:07.956758 139686199676928 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:23:07.958679 139686199676928 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.961117 139686199676928 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:23:07.966791 139686199676928 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.967053 139686199676928 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:23:07.970085 139686199676928 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:23:07.973832 139686199676928 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:23:07.973887 139686199676928 attention.py:418] Single window, no scan.
I0123 11:23:07.973922 139686199676928 transformer_layer.py:389] tlayer: self-attention.
I0123 11:23:07.973953 139686199676928 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.974015 139686199676928 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.974583 139686199676928 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.974660 139686199676928 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.975024 139686199676928 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.975794 139686199676928 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.978356 139686199676928 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.978972 139686199676928 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.979048 139686199676928 transformer_layer.py:468] tlayer: End windows.
I0123 11:23:07.979082 139686199676928 transformer_layer.py:472] tlayer: final FFN.
I0123 11:23:07.979139 139686199676928 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.979268 139686199676928 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:23:07.979588 139686199676928 nn_components.py:325] mlp: activation = None
I0123 11:23:07.979631 139686199676928 nn_components.py:329] mlp: final activation = None
I0123 11:23:07.981511 139686199676928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.981603 139686199676928 nn_components.py:261] mlp: residual
I0123 11:23:07.984157 139686199676928 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.984236 139686199676928 transformer_base.py:443] tbase: final FFN
I0123 11:23:07.984677 139686199676928 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:23:07.986923 139686199676928 nn_components.py:329] mlp: final activation = None
I0123 11:23:07.989044 139686199676928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.989139 139686199676928 nn_components.py:261] mlp: residual
I0123 11:23:07.989437 139686199676928 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.989515 139686199676928 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 11:23:07.989622 139686199676928 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:23:07.989668 139686199676928 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:23:07.989701 139686199676928 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:23:07.991518 139686199676928 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:07.993904 139686199676928 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:23:08.000336 139686199676928 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.000664 139686199676928 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:23:08.003367 139686199676928 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:23:08.007150 139686199676928 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:23:08.007208 139686199676928 attention.py:418] Single window, no scan.
I0123 11:23:08.007244 139686199676928 transformer_layer.py:389] tlayer: self-attention.
I0123 11:23:08.007275 139686199676928 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.007339 139686199676928 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.007957 139686199676928 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.008036 139686199676928 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.008398 139686199676928 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.009184 139686199676928 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.011686 139686199676928 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.012306 139686199676928 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.012384 139686199676928 transformer_layer.py:468] tlayer: End windows.
I0123 11:23:08.012418 139686199676928 transformer_layer.py:472] tlayer: final FFN.
I0123 11:23:08.012475 139686199676928 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.012601 139686199676928 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:23:08.012926 139686199676928 nn_components.py:325] mlp: activation = None
I0123 11:23:08.012969 139686199676928 nn_components.py:329] mlp: final activation = None
I0123 11:23:08.014868 139686199676928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.014960 139686199676928 nn_components.py:261] mlp: residual
I0123 11:23:08.017506 139686199676928 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.017585 139686199676928 transformer_base.py:443] tbase: final FFN
I0123 11:23:08.018028 139686199676928 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:23:08.020309 139686199676928 nn_components.py:329] mlp: final activation = None
I0123 11:23:08.022232 139686199676928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.022327 139686199676928 nn_components.py:261] mlp: residual
I0123 11:23:08.022624 139686199676928 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.022704 139686199676928 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 11:23:08.022812 139686199676928 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:23:08.022850 139686199676928 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:23:08.022881 139686199676928 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:23:08.024692 139686199676928 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.027152 139686199676928 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:23:08.032723 139686199676928 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.032982 139686199676928 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:23:08.035591 139686199676928 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:23:08.039328 139686199676928 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:23:08.039383 139686199676928 attention.py:418] Single window, no scan.
I0123 11:23:08.039418 139686199676928 transformer_layer.py:389] tlayer: self-attention.
I0123 11:23:08.039447 139686199676928 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.039507 139686199676928 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.040064 139686199676928 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.040140 139686199676928 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.040499 139686199676928 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.041269 139686199676928 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.043735 139686199676928 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.044356 139686199676928 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.044432 139686199676928 transformer_layer.py:468] tlayer: End windows.
I0123 11:23:08.044466 139686199676928 transformer_layer.py:472] tlayer: final FFN.
I0123 11:23:08.044522 139686199676928 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.044647 139686199676928 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:23:08.044965 139686199676928 nn_components.py:325] mlp: activation = None
I0123 11:23:08.045009 139686199676928 nn_components.py:329] mlp: final activation = None
I0123 11:23:08.047308 139686199676928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.047404 139686199676928 nn_components.py:261] mlp: residual
I0123 11:23:08.049918 139686199676928 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.049998 139686199676928 transformer_base.py:443] tbase: final FFN
I0123 11:23:08.050425 139686199676928 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:23:08.190173 139686199676928 nn_components.py:329] mlp: final activation = None
I0123 11:23:08.192377 139686199676928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.192529 139686199676928 nn_components.py:261] mlp: residual
I0123 11:23:08.192849 139686199676928 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.192938 139686199676928 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 11:23:08.193054 139686199676928 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:23:08.193095 139686199676928 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:23:08.193127 139686199676928 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:23:08.195213 139686199676928 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.197785 139686199676928 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:23:08.203581 139686199676928 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.203856 139686199676928 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:23:08.206568 139686199676928 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:23:08.210488 139686199676928 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:23:08.210545 139686199676928 attention.py:418] Single window, no scan.
I0123 11:23:08.210586 139686199676928 transformer_layer.py:389] tlayer: self-attention.
I0123 11:23:08.210619 139686199676928 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.210682 139686199676928 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.211293 139686199676928 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.211370 139686199676928 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.211735 139686199676928 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.212530 139686199676928 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.215149 139686199676928 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.215779 139686199676928 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.215857 139686199676928 transformer_layer.py:468] tlayer: End windows.
I0123 11:23:08.215892 139686199676928 transformer_layer.py:472] tlayer: final FFN.
I0123 11:23:08.215951 139686199676928 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.216079 139686199676928 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:23:08.216407 139686199676928 nn_components.py:325] mlp: activation = None
I0123 11:23:08.216451 139686199676928 nn_components.py:329] mlp: final activation = None
I0123 11:23:08.218348 139686199676928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.218442 139686199676928 nn_components.py:261] mlp: residual
I0123 11:23:08.220988 139686199676928 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.221068 139686199676928 transformer_base.py:443] tbase: final FFN
I0123 11:23:08.221553 139686199676928 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:23:08.223854 139686199676928 nn_components.py:329] mlp: final activation = None
I0123 11:23:08.225780 139686199676928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.225880 139686199676928 nn_components.py:261] mlp: residual
I0123 11:23:08.226181 139686199676928 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.226262 139686199676928 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 11:23:08.226373 139686199676928 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:23:08.226411 139686199676928 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:23:08.226441 139686199676928 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:23:08.228347 139686199676928 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.230727 139686199676928 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:23:08.236325 139686199676928 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.236586 139686199676928 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:23:08.239264 139686199676928 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:23:08.243012 139686199676928 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:23:08.243067 139686199676928 attention.py:418] Single window, no scan.
I0123 11:23:08.243103 139686199676928 transformer_layer.py:389] tlayer: self-attention.
I0123 11:23:08.243134 139686199676928 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.243196 139686199676928 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.243757 139686199676928 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.243832 139686199676928 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.244190 139686199676928 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.244955 139686199676928 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.247494 139686199676928 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.248108 139686199676928 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.248184 139686199676928 transformer_layer.py:468] tlayer: End windows.
I0123 11:23:08.248217 139686199676928 transformer_layer.py:472] tlayer: final FFN.
I0123 11:23:08.248274 139686199676928 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.248397 139686199676928 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:23:08.248720 139686199676928 nn_components.py:325] mlp: activation = None
I0123 11:23:08.248763 139686199676928 nn_components.py:329] mlp: final activation = None
I0123 11:23:08.250662 139686199676928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.250755 139686199676928 nn_components.py:261] mlp: residual
I0123 11:23:08.253281 139686199676928 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.253359 139686199676928 transformer_base.py:443] tbase: final FFN
I0123 11:23:08.253793 139686199676928 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:23:08.256047 139686199676928 nn_components.py:329] mlp: final activation = None
I0123 11:23:08.257939 139686199676928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.258032 139686199676928 nn_components.py:261] mlp: residual
I0123 11:23:08.258324 139686199676928 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.258409 139686199676928 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 11:23:08.258520 139686199676928 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:23:08.258558 139686199676928 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:23:08.258589 139686199676928 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:23:08.260468 139686199676928 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.262832 139686199676928 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:23:08.268723 139686199676928 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.268981 139686199676928 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:23:08.271673 139686199676928 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:23:08.275374 139686199676928 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:23:08.275429 139686199676928 attention.py:418] Single window, no scan.
I0123 11:23:08.275464 139686199676928 transformer_layer.py:389] tlayer: self-attention.
I0123 11:23:08.275495 139686199676928 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.275555 139686199676928 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.276114 139686199676928 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.276192 139686199676928 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.276549 139686199676928 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.277367 139686199676928 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.279834 139686199676928 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.280447 139686199676928 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.280523 139686199676928 transformer_layer.py:468] tlayer: End windows.
I0123 11:23:08.280557 139686199676928 transformer_layer.py:472] tlayer: final FFN.
I0123 11:23:08.280615 139686199676928 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.280743 139686199676928 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:23:08.281064 139686199676928 nn_components.py:325] mlp: activation = None
I0123 11:23:08.281106 139686199676928 nn_components.py:329] mlp: final activation = None
I0123 11:23:08.283097 139686199676928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.283190 139686199676928 nn_components.py:261] mlp: residual
I0123 11:23:08.285915 139686199676928 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.285995 139686199676928 transformer_base.py:443] tbase: final FFN
I0123 11:23:08.286425 139686199676928 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:23:08.288689 139686199676928 nn_components.py:329] mlp: final activation = None
I0123 11:23:08.290647 139686199676928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.290743 139686199676928 nn_components.py:261] mlp: residual
I0123 11:23:08.291036 139686199676928 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.291120 139686199676928 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 11:23:08.291230 139686199676928 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:23:08.291269 139686199676928 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:23:08.291300 139686199676928 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:23:08.293119 139686199676928 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.295549 139686199676928 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:23:08.301402 139686199676928 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.301668 139686199676928 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:23:08.304352 139686199676928 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:23:08.308075 139686199676928 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:23:08.308129 139686199676928 attention.py:418] Single window, no scan.
I0123 11:23:08.308164 139686199676928 transformer_layer.py:389] tlayer: self-attention.
I0123 11:23:08.308195 139686199676928 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.308297 139686199676928 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.308863 139686199676928 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.308939 139686199676928 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.309304 139686199676928 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.310084 139686199676928 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.312571 139686199676928 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.313189 139686199676928 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.313265 139686199676928 transformer_layer.py:468] tlayer: End windows.
I0123 11:23:08.313299 139686199676928 transformer_layer.py:472] tlayer: final FFN.
I0123 11:23:08.313357 139686199676928 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.313481 139686199676928 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:23:08.313809 139686199676928 nn_components.py:325] mlp: activation = None
I0123 11:23:08.313854 139686199676928 nn_components.py:329] mlp: final activation = None
I0123 11:23:08.315791 139686199676928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.315886 139686199676928 nn_components.py:261] mlp: residual
I0123 11:23:08.318600 139686199676928 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.318680 139686199676928 transformer_base.py:443] tbase: final FFN
I0123 11:23:08.319109 139686199676928 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:23:08.321411 139686199676928 nn_components.py:329] mlp: final activation = None
I0123 11:23:08.323301 139686199676928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.323396 139686199676928 nn_components.py:261] mlp: residual
I0123 11:23:08.323693 139686199676928 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.323773 139686199676928 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 11:23:08.323889 139686199676928 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:23:08.323929 139686199676928 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:23:08.323960 139686199676928 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:23:08.325796 139686199676928 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.328242 139686199676928 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:23:08.333822 139686199676928 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.334087 139686199676928 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:23:08.336692 139686199676928 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:23:08.340786 139686199676928 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:23:08.340841 139686199676928 attention.py:418] Single window, no scan.
I0123 11:23:08.340877 139686199676928 transformer_layer.py:389] tlayer: self-attention.
I0123 11:23:08.340908 139686199676928 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.340970 139686199676928 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.341532 139686199676928 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.341607 139686199676928 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.341978 139686199676928 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.342752 139686199676928 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.345237 139686199676928 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.345868 139686199676928 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.345945 139686199676928 transformer_layer.py:468] tlayer: End windows.
I0123 11:23:08.345980 139686199676928 transformer_layer.py:472] tlayer: final FFN.
I0123 11:23:08.346039 139686199676928 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.346170 139686199676928 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:23:08.346494 139686199676928 nn_components.py:325] mlp: activation = None
I0123 11:23:08.346537 139686199676928 nn_components.py:329] mlp: final activation = None
I0123 11:23:08.348469 139686199676928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.348562 139686199676928 nn_components.py:261] mlp: residual
I0123 11:23:08.351068 139686199676928 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.351147 139686199676928 transformer_base.py:443] tbase: final FFN
I0123 11:23:08.351576 139686199676928 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:23:08.353881 139686199676928 nn_components.py:329] mlp: final activation = None
I0123 11:23:08.355766 139686199676928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.355859 139686199676928 nn_components.py:261] mlp: residual
I0123 11:23:08.356149 139686199676928 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.356428 139686199676928 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:23:08.356498 139686199676928 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:23:08.356568 139686199676928 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:23:08.356625 139686199676928 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:23:08.356679 139686199676928 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:23:08.356732 139686199676928 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:23:08.356784 139686199676928 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:23:08.356837 139686199676928 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:23:08.356888 139686199676928 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:23:08.356940 139686199676928 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:23:08.356993 139686199676928 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:23:08.357045 139686199676928 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:23:08.357082 139686199676928 decoder_stack.py:344] dstack: Final layernorm.
I0123 11:23:08.360622 139686199676928 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:23:08.408271 139686199676928 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.408355 139686199676928 decoder_stack.py:333] dstack: autoregressive generator.
I0123 11:23:08.408408 139686199676928 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 11:23:08.408512 139686199676928 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:23:08.408549 139686199676928 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:23:08.408578 139686199676928 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:23:08.408642 139686199676928 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.411063 139686199676928 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:23:08.416543 139686199676928 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.416800 139686199676928 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:23:08.419448 139686199676928 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:23:08.436126 139686199676928 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:23:08.436181 139686199676928 attention.py:418] Single window, no scan.
I0123 11:23:08.436216 139686199676928 transformer_layer.py:389] tlayer: self-attention.
I0123 11:23:08.436247 139686199676928 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.436308 139686199676928 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.437434 139686199676928 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.437512 139686199676928 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.438229 139686199676928 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.440240 139686199676928 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.445046 139686199676928 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.446360 139686199676928 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.446446 139686199676928 transformer_layer.py:468] tlayer: End windows.
I0123 11:23:08.446481 139686199676928 transformer_layer.py:472] tlayer: final FFN.
I0123 11:23:08.446539 139686199676928 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.446668 139686199676928 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:23:08.446780 139686199676928 nn_components.py:325] mlp: activation = None
I0123 11:23:08.446819 139686199676928 nn_components.py:329] mlp: final activation = None
I0123 11:23:08.448725 139686199676928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.448819 139686199676928 nn_components.py:261] mlp: residual
I0123 11:23:08.451292 139686199676928 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.451376 139686199676928 transformer_base.py:443] tbase: final FFN
I0123 11:23:08.451485 139686199676928 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:23:08.453723 139686199676928 nn_components.py:329] mlp: final activation = None
I0123 11:23:08.455708 139686199676928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.455804 139686199676928 nn_components.py:261] mlp: residual
I0123 11:23:08.456098 139686199676928 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.456179 139686199676928 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 11:23:08.456288 139686199676928 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:23:08.456327 139686199676928 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:23:08.456357 139686199676928 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:23:08.456420 139686199676928 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.458717 139686199676928 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:23:08.464267 139686199676928 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.464528 139686199676928 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:23:08.467246 139686199676928 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:23:08.480317 139686199676928 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:23:08.480372 139686199676928 attention.py:418] Single window, no scan.
I0123 11:23:08.480407 139686199676928 transformer_layer.py:389] tlayer: self-attention.
I0123 11:23:08.480437 139686199676928 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.480499 139686199676928 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.481064 139686199676928 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.481141 139686199676928 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.481499 139686199676928 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.482208 139686199676928 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.484752 139686199676928 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.485377 139686199676928 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.485455 139686199676928 transformer_layer.py:468] tlayer: End windows.
I0123 11:23:08.485494 139686199676928 transformer_layer.py:472] tlayer: final FFN.
I0123 11:23:08.485552 139686199676928 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.485688 139686199676928 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:23:08.485799 139686199676928 nn_components.py:325] mlp: activation = None
I0123 11:23:08.485837 139686199676928 nn_components.py:329] mlp: final activation = None
I0123 11:23:08.487764 139686199676928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.487858 139686199676928 nn_components.py:261] mlp: residual
I0123 11:23:08.490292 139686199676928 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.490371 139686199676928 transformer_base.py:443] tbase: final FFN
I0123 11:23:08.490478 139686199676928 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:23:08.492684 139686199676928 nn_components.py:329] mlp: final activation = None
I0123 11:23:08.494621 139686199676928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.494717 139686199676928 nn_components.py:261] mlp: residual
I0123 11:23:08.495007 139686199676928 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.495088 139686199676928 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 11:23:08.495197 139686199676928 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:23:08.495235 139686199676928 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:23:08.495265 139686199676928 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:23:08.495326 139686199676928 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.497583 139686199676928 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:23:08.503088 139686199676928 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.503348 139686199676928 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:23:08.506055 139686199676928 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:23:08.518720 139686199676928 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:23:08.518776 139686199676928 attention.py:418] Single window, no scan.
I0123 11:23:08.518811 139686199676928 transformer_layer.py:389] tlayer: self-attention.
I0123 11:23:08.518841 139686199676928 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.518903 139686199676928 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.519457 139686199676928 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.519533 139686199676928 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.519888 139686199676928 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.520585 139686199676928 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.523093 139686199676928 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.523711 139686199676928 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.523788 139686199676928 transformer_layer.py:468] tlayer: End windows.
I0123 11:23:08.523822 139686199676928 transformer_layer.py:472] tlayer: final FFN.
I0123 11:23:08.523888 139686199676928 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.524018 139686199676928 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:23:08.524126 139686199676928 nn_components.py:325] mlp: activation = None
I0123 11:23:08.524165 139686199676928 nn_components.py:329] mlp: final activation = None
I0123 11:23:08.526106 139686199676928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.526200 139686199676928 nn_components.py:261] mlp: residual
I0123 11:23:08.528675 139686199676928 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.528755 139686199676928 transformer_base.py:443] tbase: final FFN
I0123 11:23:08.528863 139686199676928 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:23:08.531109 139686199676928 nn_components.py:329] mlp: final activation = None
I0123 11:23:08.533039 139686199676928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.533133 139686199676928 nn_components.py:261] mlp: residual
I0123 11:23:08.533422 139686199676928 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.533504 139686199676928 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 11:23:08.533612 139686199676928 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:23:08.533657 139686199676928 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:23:08.533689 139686199676928 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:23:08.533752 139686199676928 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.536365 139686199676928 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:23:08.541848 139686199676928 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.542110 139686199676928 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:23:08.544887 139686199676928 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:23:08.557650 139686199676928 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:23:08.557706 139686199676928 attention.py:418] Single window, no scan.
I0123 11:23:08.557741 139686199676928 transformer_layer.py:389] tlayer: self-attention.
I0123 11:23:08.557771 139686199676928 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.557833 139686199676928 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.558402 139686199676928 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.558478 139686199676928 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.558838 139686199676928 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.559549 139686199676928 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.562102 139686199676928 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.562749 139686199676928 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.562829 139686199676928 transformer_layer.py:468] tlayer: End windows.
I0123 11:23:08.562864 139686199676928 transformer_layer.py:472] tlayer: final FFN.
I0123 11:23:08.562923 139686199676928 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.563064 139686199676928 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:23:08.563179 139686199676928 nn_components.py:325] mlp: activation = None
I0123 11:23:08.563220 139686199676928 nn_components.py:329] mlp: final activation = None
I0123 11:23:08.565488 139686199676928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.565582 139686199676928 nn_components.py:261] mlp: residual
I0123 11:23:08.568145 139686199676928 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.568224 139686199676928 transformer_base.py:443] tbase: final FFN
I0123 11:23:08.568338 139686199676928 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:23:08.570570 139686199676928 nn_components.py:329] mlp: final activation = None
I0123 11:23:08.572448 139686199676928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.572542 139686199676928 nn_components.py:261] mlp: residual
I0123 11:23:08.572830 139686199676928 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.572911 139686199676928 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 11:23:08.573019 139686199676928 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:23:08.573058 139686199676928 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:23:08.573088 139686199676928 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:23:08.573149 139686199676928 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.575470 139686199676928 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:23:08.580935 139686199676928 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.581197 139686199676928 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:23:08.583838 139686199676928 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:23:08.596599 139686199676928 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:23:08.596655 139686199676928 attention.py:418] Single window, no scan.
I0123 11:23:08.596691 139686199676928 transformer_layer.py:389] tlayer: self-attention.
I0123 11:23:08.596721 139686199676928 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.596783 139686199676928 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.597345 139686199676928 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.597420 139686199676928 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.597789 139686199676928 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.598493 139686199676928 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.601072 139686199676928 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.601706 139686199676928 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.601783 139686199676928 transformer_layer.py:468] tlayer: End windows.
I0123 11:23:08.601819 139686199676928 transformer_layer.py:472] tlayer: final FFN.
I0123 11:23:08.601877 139686199676928 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.602016 139686199676928 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:23:08.602127 139686199676928 nn_components.py:325] mlp: activation = None
I0123 11:23:08.602166 139686199676928 nn_components.py:329] mlp: final activation = None
I0123 11:23:08.604059 139686199676928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.604153 139686199676928 nn_components.py:261] mlp: residual
I0123 11:23:08.606593 139686199676928 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.606673 139686199676928 transformer_base.py:443] tbase: final FFN
I0123 11:23:08.606780 139686199676928 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:23:08.609087 139686199676928 nn_components.py:329] mlp: final activation = None
I0123 11:23:08.610977 139686199676928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.611072 139686199676928 nn_components.py:261] mlp: residual
I0123 11:23:08.611363 139686199676928 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.611445 139686199676928 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 11:23:08.611556 139686199676928 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:23:08.611596 139686199676928 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:23:08.611626 139686199676928 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:23:08.611689 139686199676928 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.613964 139686199676928 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:23:08.619418 139686199676928 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.619675 139686199676928 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:23:08.622372 139686199676928 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:23:08.635199 139686199676928 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:23:08.635257 139686199676928 attention.py:418] Single window, no scan.
I0123 11:23:08.635293 139686199676928 transformer_layer.py:389] tlayer: self-attention.
I0123 11:23:08.635323 139686199676928 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.635383 139686199676928 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.635946 139686199676928 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.636023 139686199676928 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.636385 139686199676928 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.637089 139686199676928 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.639592 139686199676928 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.640209 139686199676928 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.640285 139686199676928 transformer_layer.py:468] tlayer: End windows.
I0123 11:23:08.640319 139686199676928 transformer_layer.py:472] tlayer: final FFN.
I0123 11:23:08.640377 139686199676928 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.640509 139686199676928 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:23:08.640625 139686199676928 nn_components.py:325] mlp: activation = None
I0123 11:23:08.640664 139686199676928 nn_components.py:329] mlp: final activation = None
I0123 11:23:08.642811 139686199676928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.642906 139686199676928 nn_components.py:261] mlp: residual
I0123 11:23:08.645474 139686199676928 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.645553 139686199676928 transformer_base.py:443] tbase: final FFN
I0123 11:23:08.645666 139686199676928 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:23:08.648065 139686199676928 nn_components.py:329] mlp: final activation = None
I0123 11:23:08.650130 139686199676928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.650224 139686199676928 nn_components.py:261] mlp: residual
I0123 11:23:08.650511 139686199676928 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.650591 139686199676928 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 11:23:08.650698 139686199676928 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:23:08.650737 139686199676928 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:23:08.650768 139686199676928 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:23:08.650831 139686199676928 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.653085 139686199676928 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:23:08.664323 139686199676928 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.664629 139686199676928 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:23:08.667371 139686199676928 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:23:08.680718 139686199676928 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:23:08.680777 139686199676928 attention.py:418] Single window, no scan.
I0123 11:23:08.680815 139686199676928 transformer_layer.py:389] tlayer: self-attention.
I0123 11:23:08.680846 139686199676928 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.680907 139686199676928 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.681494 139686199676928 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.681571 139686199676928 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.681946 139686199676928 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.682642 139686199676928 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.685175 139686199676928 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.685874 139686199676928 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.685953 139686199676928 transformer_layer.py:468] tlayer: End windows.
I0123 11:23:08.685986 139686199676928 transformer_layer.py:472] tlayer: final FFN.
I0123 11:23:08.686046 139686199676928 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.686174 139686199676928 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:23:08.686288 139686199676928 nn_components.py:325] mlp: activation = None
I0123 11:23:08.686331 139686199676928 nn_components.py:329] mlp: final activation = None
I0123 11:23:08.688250 139686199676928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.688343 139686199676928 nn_components.py:261] mlp: residual
I0123 11:23:08.690816 139686199676928 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.690895 139686199676928 transformer_base.py:443] tbase: final FFN
I0123 11:23:08.691005 139686199676928 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:23:08.693267 139686199676928 nn_components.py:329] mlp: final activation = None
I0123 11:23:08.695221 139686199676928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.695317 139686199676928 nn_components.py:261] mlp: residual
I0123 11:23:08.695607 139686199676928 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.695688 139686199676928 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 11:23:08.695796 139686199676928 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:23:08.695837 139686199676928 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:23:08.695867 139686199676928 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:23:08.695929 139686199676928 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.698204 139686199676928 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:23:08.703694 139686199676928 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.703969 139686199676928 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:23:08.706683 139686199676928 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:23:08.719428 139686199676928 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:23:08.719483 139686199676928 attention.py:418] Single window, no scan.
I0123 11:23:08.719517 139686199676928 transformer_layer.py:389] tlayer: self-attention.
I0123 11:23:08.719547 139686199676928 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.719608 139686199676928 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.720216 139686199676928 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.720292 139686199676928 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.720652 139686199676928 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.721346 139686199676928 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.723875 139686199676928 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.724506 139686199676928 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.724583 139686199676928 transformer_layer.py:468] tlayer: End windows.
I0123 11:23:08.724617 139686199676928 transformer_layer.py:472] tlayer: final FFN.
I0123 11:23:08.724674 139686199676928 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.724802 139686199676928 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:23:08.724912 139686199676928 nn_components.py:325] mlp: activation = None
I0123 11:23:08.724956 139686199676928 nn_components.py:329] mlp: final activation = None
I0123 11:23:08.726870 139686199676928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.726963 139686199676928 nn_components.py:261] mlp: residual
I0123 11:23:08.729454 139686199676928 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.729533 139686199676928 transformer_base.py:443] tbase: final FFN
I0123 11:23:08.729645 139686199676928 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:23:08.731895 139686199676928 nn_components.py:329] mlp: final activation = None
I0123 11:23:08.733784 139686199676928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.733882 139686199676928 nn_components.py:261] mlp: residual
I0123 11:23:08.734175 139686199676928 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.734255 139686199676928 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 11:23:08.734363 139686199676928 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:23:08.734402 139686199676928 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:23:08.734432 139686199676928 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:23:08.734493 139686199676928 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.736742 139686199676928 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:23:08.742398 139686199676928 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.742659 139686199676928 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:23:08.745290 139686199676928 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:23:08.758142 139686199676928 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:23:08.758198 139686199676928 attention.py:418] Single window, no scan.
I0123 11:23:08.758233 139686199676928 transformer_layer.py:389] tlayer: self-attention.
I0123 11:23:08.758264 139686199676928 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.758327 139686199676928 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.758891 139686199676928 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.758967 139686199676928 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.759328 139686199676928 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.760021 139686199676928 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.762695 139686199676928 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.763398 139686199676928 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.763480 139686199676928 transformer_layer.py:468] tlayer: End windows.
I0123 11:23:08.763516 139686199676928 transformer_layer.py:472] tlayer: final FFN.
I0123 11:23:08.763707 139686199676928 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.763844 139686199676928 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:23:08.763958 139686199676928 nn_components.py:325] mlp: activation = None
I0123 11:23:08.763999 139686199676928 nn_components.py:329] mlp: final activation = None
I0123 11:23:08.765925 139686199676928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.766021 139686199676928 nn_components.py:261] mlp: residual
I0123 11:23:08.768467 139686199676928 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.768546 139686199676928 transformer_base.py:443] tbase: final FFN
I0123 11:23:08.768653 139686199676928 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:23:08.770902 139686199676928 nn_components.py:329] mlp: final activation = None
I0123 11:23:08.772854 139686199676928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.772949 139686199676928 nn_components.py:261] mlp: residual
I0123 11:23:08.773239 139686199676928 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.773319 139686199676928 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 11:23:08.773427 139686199676928 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:23:08.773466 139686199676928 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:23:08.773497 139686199676928 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:23:08.773559 139686199676928 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.775841 139686199676928 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:23:08.781319 139686199676928 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.781576 139686199676928 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:23:08.784597 139686199676928 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:23:08.797328 139686199676928 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:23:08.797384 139686199676928 attention.py:418] Single window, no scan.
I0123 11:23:08.797419 139686199676928 transformer_layer.py:389] tlayer: self-attention.
I0123 11:23:08.797450 139686199676928 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.797514 139686199676928 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.798130 139686199676928 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.798208 139686199676928 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.798567 139686199676928 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.799262 139686199676928 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.801779 139686199676928 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.802409 139686199676928 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.802486 139686199676928 transformer_layer.py:468] tlayer: End windows.
I0123 11:23:08.802520 139686199676928 transformer_layer.py:472] tlayer: final FFN.
I0123 11:23:08.802576 139686199676928 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.802704 139686199676928 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:23:08.802812 139686199676928 nn_components.py:325] mlp: activation = None
I0123 11:23:08.802854 139686199676928 nn_components.py:329] mlp: final activation = None
I0123 11:23:08.804751 139686199676928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.804852 139686199676928 nn_components.py:261] mlp: residual
I0123 11:23:08.807338 139686199676928 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.807420 139686199676928 transformer_base.py:443] tbase: final FFN
I0123 11:23:08.807527 139686199676928 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:23:08.809753 139686199676928 nn_components.py:329] mlp: final activation = None
I0123 11:23:08.811621 139686199676928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.811716 139686199676928 nn_components.py:261] mlp: residual
I0123 11:23:08.812003 139686199676928 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.812083 139686199676928 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 11:23:08.812191 139686199676928 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:23:08.812229 139686199676928 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:23:08.812259 139686199676928 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:23:08.812321 139686199676928 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.814591 139686199676928 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:23:08.820096 139686199676928 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.820357 139686199676928 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:23:08.823022 139686199676928 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:23:08.835637 139686199676928 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:23:08.835693 139686199676928 attention.py:418] Single window, no scan.
I0123 11:23:08.835728 139686199676928 transformer_layer.py:389] tlayer: self-attention.
I0123 11:23:08.835757 139686199676928 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.835819 139686199676928 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.836377 139686199676928 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.836454 139686199676928 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.836816 139686199676928 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.837510 139686199676928 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.840030 139686199676928 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.840702 139686199676928 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.840779 139686199676928 transformer_layer.py:468] tlayer: End windows.
I0123 11:23:08.840813 139686199676928 transformer_layer.py:472] tlayer: final FFN.
I0123 11:23:08.840873 139686199676928 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.841005 139686199676928 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:23:08.841113 139686199676928 nn_components.py:325] mlp: activation = None
I0123 11:23:08.841150 139686199676928 nn_components.py:329] mlp: final activation = None
I0123 11:23:08.843040 139686199676928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.843139 139686199676928 nn_components.py:261] mlp: residual
I0123 11:23:08.845595 139686199676928 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.845680 139686199676928 transformer_base.py:443] tbase: final FFN
I0123 11:23:08.845788 139686199676928 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:23:08.848018 139686199676928 nn_components.py:329] mlp: final activation = None
I0123 11:23:08.849965 139686199676928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.850060 139686199676928 nn_components.py:261] mlp: residual
I0123 11:23:08.850349 139686199676928 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.850429 139686199676928 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 11:23:08.850539 139686199676928 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:23:08.850578 139686199676928 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:23:08.850608 139686199676928 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:23:08.850671 139686199676928 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.852940 139686199676928 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:23:08.858388 139686199676928 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.858648 139686199676928 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:23:08.861296 139686199676928 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:23:08.874193 139686199676928 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:23:08.874248 139686199676928 attention.py:418] Single window, no scan.
I0123 11:23:08.874283 139686199676928 transformer_layer.py:389] tlayer: self-attention.
I0123 11:23:08.874313 139686199676928 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.874378 139686199676928 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.874942 139686199676928 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.875017 139686199676928 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.875374 139686199676928 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.876084 139686199676928 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.878690 139686199676928 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.879314 139686199676928 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.879393 139686199676928 transformer_layer.py:468] tlayer: End windows.
I0123 11:23:08.879427 139686199676928 transformer_layer.py:472] tlayer: final FFN.
I0123 11:23:08.879485 139686199676928 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.879612 139686199676928 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:23:08.879720 139686199676928 nn_components.py:325] mlp: activation = None
I0123 11:23:08.879758 139686199676928 nn_components.py:329] mlp: final activation = None
I0123 11:23:08.881657 139686199676928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.881750 139686199676928 nn_components.py:261] mlp: residual
I0123 11:23:08.884174 139686199676928 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.884251 139686199676928 transformer_base.py:443] tbase: final FFN
I0123 11:23:08.884358 139686199676928 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:23:08.886978 139686199676928 nn_components.py:329] mlp: final activation = None
I0123 11:23:08.888864 139686199676928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.888958 139686199676928 nn_components.py:261] mlp: residual
I0123 11:23:08.889243 139686199676928 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.889330 139686199676928 decoder_stack.py:344] dstack: Final layernorm.
I0123 11:23:08.892242 139686199676928 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:23:08.948210 139686199676928 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:08.948295 139686199676928 decoder_stack.py:333] dstack: autoregressive generator.
I0123 11:23:08.948348 139686199676928 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 11:23:08.948451 139686199676928 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:23:08.948490 139686199676928 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:23:08.948520 139686199676928 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:23:08.948582 139686199676928 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:08.950962 139686199676928 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:23:08.956370 139686199676928 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:08.956628 139686199676928 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:23:08.959232 139686199676928 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:23:08.971610 139686199676928 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:23:08.971665 139686199676928 attention.py:418] Single window, no scan.
I0123 11:23:08.971700 139686199676928 transformer_layer.py:389] tlayer: self-attention.
I0123 11:23:08.971730 139686199676928 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:08.971791 139686199676928 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:08.972344 139686199676928 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:08.972419 139686199676928 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:08.972774 139686199676928 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:08.973452 139686199676928 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:08.975997 139686199676928 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:08.976615 139686199676928 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:08.976692 139686199676928 transformer_layer.py:468] tlayer: End windows.
I0123 11:23:08.976727 139686199676928 transformer_layer.py:472] tlayer: final FFN.
I0123 11:23:08.976785 139686199676928 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:08.976913 139686199676928 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:23:08.977029 139686199676928 nn_components.py:325] mlp: activation = None
I0123 11:23:08.977068 139686199676928 nn_components.py:329] mlp: final activation = None
I0123 11:23:08.978929 139686199676928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:08.979023 139686199676928 nn_components.py:261] mlp: residual
I0123 11:23:08.981430 139686199676928 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:08.981509 139686199676928 transformer_base.py:443] tbase: final FFN
I0123 11:23:08.981618 139686199676928 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:23:08.983913 139686199676928 nn_components.py:329] mlp: final activation = None
I0123 11:23:08.985787 139686199676928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:08.985883 139686199676928 nn_components.py:261] mlp: residual
I0123 11:23:08.986170 139686199676928 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:08.986251 139686199676928 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 11:23:08.986358 139686199676928 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:23:08.986397 139686199676928 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:23:08.986427 139686199676928 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:23:08.986488 139686199676928 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:08.988730 139686199676928 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:23:08.994107 139686199676928 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:08.994362 139686199676928 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:23:08.997034 139686199676928 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:23:09.009455 139686199676928 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:23:09.009511 139686199676928 attention.py:418] Single window, no scan.
I0123 11:23:09.009547 139686199676928 transformer_layer.py:389] tlayer: self-attention.
I0123 11:23:09.009577 139686199676928 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.009638 139686199676928 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.010202 139686199676928 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.010278 139686199676928 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.010639 139686199676928 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.011317 139686199676928 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.013890 139686199676928 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.014508 139686199676928 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.014585 139686199676928 transformer_layer.py:468] tlayer: End windows.
I0123 11:23:09.014620 139686199676928 transformer_layer.py:472] tlayer: final FFN.
I0123 11:23:09.014678 139686199676928 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.014805 139686199676928 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:23:09.014913 139686199676928 nn_components.py:325] mlp: activation = None
I0123 11:23:09.014958 139686199676928 nn_components.py:329] mlp: final activation = None
I0123 11:23:09.016829 139686199676928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.016921 139686199676928 nn_components.py:261] mlp: residual
I0123 11:23:09.019354 139686199676928 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.019434 139686199676928 transformer_base.py:443] tbase: final FFN
I0123 11:23:09.019542 139686199676928 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:23:09.021806 139686199676928 nn_components.py:329] mlp: final activation = None
I0123 11:23:09.023656 139686199676928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.023751 139686199676928 nn_components.py:261] mlp: residual
I0123 11:23:09.024040 139686199676928 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.024121 139686199676928 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 11:23:09.024228 139686199676928 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:23:09.024267 139686199676928 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:23:09.024298 139686199676928 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:23:09.024360 139686199676928 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.026613 139686199676928 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:23:09.031994 139686199676928 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.032251 139686199676928 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:23:09.034929 139686199676928 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:23:09.047288 139686199676928 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:23:09.047344 139686199676928 attention.py:418] Single window, no scan.
I0123 11:23:09.047379 139686199676928 transformer_layer.py:389] tlayer: self-attention.
I0123 11:23:09.047410 139686199676928 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.047470 139686199676928 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.048022 139686199676928 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.048098 139686199676928 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.048454 139686199676928 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.049138 139686199676928 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.052116 139686199676928 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.052737 139686199676928 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.052814 139686199676928 transformer_layer.py:468] tlayer: End windows.
I0123 11:23:09.052848 139686199676928 transformer_layer.py:472] tlayer: final FFN.
I0123 11:23:09.052906 139686199676928 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.053032 139686199676928 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:23:09.053140 139686199676928 nn_components.py:325] mlp: activation = None
I0123 11:23:09.053178 139686199676928 nn_components.py:329] mlp: final activation = None
I0123 11:23:09.055034 139686199676928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.055127 139686199676928 nn_components.py:261] mlp: residual
I0123 11:23:09.057509 139686199676928 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.057588 139686199676928 transformer_base.py:443] tbase: final FFN
I0123 11:23:09.057704 139686199676928 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:23:09.059955 139686199676928 nn_components.py:329] mlp: final activation = None
I0123 11:23:09.061801 139686199676928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.061896 139686199676928 nn_components.py:261] mlp: residual
I0123 11:23:09.062181 139686199676928 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.062262 139686199676928 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 11:23:09.062369 139686199676928 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:23:09.062407 139686199676928 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:23:09.062437 139686199676928 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:23:09.062499 139686199676928 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.064728 139686199676928 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:23:09.070063 139686199676928 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.070320 139686199676928 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:23:09.072983 139686199676928 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:23:09.085427 139686199676928 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:23:09.085483 139686199676928 attention.py:418] Single window, no scan.
I0123 11:23:09.085520 139686199676928 transformer_layer.py:389] tlayer: self-attention.
I0123 11:23:09.085562 139686199676928 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.085626 139686199676928 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.086191 139686199676928 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.086267 139686199676928 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.086626 139686199676928 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.087310 139686199676928 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.089864 139686199676928 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.090484 139686199676928 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.090559 139686199676928 transformer_layer.py:468] tlayer: End windows.
I0123 11:23:09.090593 139686199676928 transformer_layer.py:472] tlayer: final FFN.
I0123 11:23:09.090650 139686199676928 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.090774 139686199676928 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:23:09.090879 139686199676928 nn_components.py:325] mlp: activation = None
I0123 11:23:09.090921 139686199676928 nn_components.py:329] mlp: final activation = None
I0123 11:23:09.092797 139686199676928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.092889 139686199676928 nn_components.py:261] mlp: residual
I0123 11:23:09.095301 139686199676928 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.095379 139686199676928 transformer_base.py:443] tbase: final FFN
I0123 11:23:09.095485 139686199676928 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:23:09.097768 139686199676928 nn_components.py:329] mlp: final activation = None
I0123 11:23:09.099613 139686199676928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.099707 139686199676928 nn_components.py:261] mlp: residual
I0123 11:23:09.099992 139686199676928 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.100069 139686199676928 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 11:23:09.100174 139686199676928 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:23:09.100211 139686199676928 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:23:09.100239 139686199676928 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:23:09.100301 139686199676928 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.102537 139686199676928 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:23:09.107915 139686199676928 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.108173 139686199676928 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:23:09.110851 139686199676928 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:23:09.123298 139686199676928 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:23:09.123352 139686199676928 attention.py:418] Single window, no scan.
I0123 11:23:09.123387 139686199676928 transformer_layer.py:389] tlayer: self-attention.
I0123 11:23:09.123416 139686199676928 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.123476 139686199676928 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.124033 139686199676928 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.124108 139686199676928 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.124463 139686199676928 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.125152 139686199676928 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.127699 139686199676928 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.128320 139686199676928 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.128395 139686199676928 transformer_layer.py:468] tlayer: End windows.
I0123 11:23:09.128427 139686199676928 transformer_layer.py:472] tlayer: final FFN.
I0123 11:23:09.128484 139686199676928 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.128608 139686199676928 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:23:09.128714 139686199676928 nn_components.py:325] mlp: activation = None
I0123 11:23:09.128750 139686199676928 nn_components.py:329] mlp: final activation = None
I0123 11:23:09.130619 139686199676928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.130716 139686199676928 nn_components.py:261] mlp: residual
I0123 11:23:09.133134 139686199676928 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.133211 139686199676928 transformer_base.py:443] tbase: final FFN
I0123 11:23:09.133319 139686199676928 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:23:09.135599 139686199676928 nn_components.py:329] mlp: final activation = None
I0123 11:23:09.137460 139686199676928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.137553 139686199676928 nn_components.py:261] mlp: residual
I0123 11:23:09.137849 139686199676928 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.137931 139686199676928 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 11:23:09.138037 139686199676928 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:23:09.138075 139686199676928 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:23:09.138103 139686199676928 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:23:09.138163 139686199676928 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.140405 139686199676928 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:23:09.145817 139686199676928 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.146073 139686199676928 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:23:09.148759 139686199676928 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:23:09.161226 139686199676928 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:23:09.161281 139686199676928 attention.py:418] Single window, no scan.
I0123 11:23:09.161314 139686199676928 transformer_layer.py:389] tlayer: self-attention.
I0123 11:23:09.161343 139686199676928 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.161404 139686199676928 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.161961 139686199676928 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.162037 139686199676928 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.162396 139686199676928 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.163086 139686199676928 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.166066 139686199676928 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.166676 139686199676928 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.166751 139686199676928 transformer_layer.py:468] tlayer: End windows.
I0123 11:23:09.166783 139686199676928 transformer_layer.py:472] tlayer: final FFN.
I0123 11:23:09.166839 139686199676928 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.166963 139686199676928 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:23:09.167070 139686199676928 nn_components.py:325] mlp: activation = None
I0123 11:23:09.167107 139686199676928 nn_components.py:329] mlp: final activation = None
I0123 11:23:09.168977 139686199676928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.169074 139686199676928 nn_components.py:261] mlp: residual
I0123 11:23:09.171486 139686199676928 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.171564 139686199676928 transformer_base.py:443] tbase: final FFN
I0123 11:23:09.171671 139686199676928 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:23:09.173961 139686199676928 nn_components.py:329] mlp: final activation = None
I0123 11:23:09.175893 139686199676928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.175986 139686199676928 nn_components.py:261] mlp: residual
I0123 11:23:09.176272 139686199676928 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.176351 139686199676928 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 11:23:09.176457 139686199676928 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:23:09.176495 139686199676928 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:23:09.176524 139686199676928 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:23:09.176584 139686199676928 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.178815 139686199676928 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:23:09.184216 139686199676928 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.184468 139686199676928 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:23:09.187171 139686199676928 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:23:09.199604 139686199676928 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:23:09.199657 139686199676928 attention.py:418] Single window, no scan.
I0123 11:23:09.199691 139686199676928 transformer_layer.py:389] tlayer: self-attention.
I0123 11:23:09.199720 139686199676928 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.199781 139686199676928 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.200342 139686199676928 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.200419 139686199676928 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.200775 139686199676928 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.201451 139686199676928 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.204009 139686199676928 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.204629 139686199676928 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.204705 139686199676928 transformer_layer.py:468] tlayer: End windows.
I0123 11:23:09.204738 139686199676928 transformer_layer.py:472] tlayer: final FFN.
I0123 11:23:09.204794 139686199676928 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.204919 139686199676928 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:23:09.205029 139686199676928 nn_components.py:325] mlp: activation = None
I0123 11:23:09.205066 139686199676928 nn_components.py:329] mlp: final activation = None
I0123 11:23:09.206924 139686199676928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.207017 139686199676928 nn_components.py:261] mlp: residual
I0123 11:23:09.209426 139686199676928 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.209505 139686199676928 transformer_base.py:443] tbase: final FFN
I0123 11:23:09.209616 139686199676928 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:23:09.211896 139686199676928 nn_components.py:329] mlp: final activation = None
I0123 11:23:09.213746 139686199676928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.213840 139686199676928 nn_components.py:261] mlp: residual
I0123 11:23:09.214124 139686199676928 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.214203 139686199676928 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 11:23:09.214307 139686199676928 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:23:09.214344 139686199676928 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:23:09.214373 139686199676928 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:23:09.214433 139686199676928 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.216658 139686199676928 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:23:09.222125 139686199676928 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.222381 139686199676928 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:23:09.225066 139686199676928 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:23:09.237438 139686199676928 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:23:09.237493 139686199676928 attention.py:418] Single window, no scan.
I0123 11:23:09.237526 139686199676928 transformer_layer.py:389] tlayer: self-attention.
I0123 11:23:09.237555 139686199676928 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.237614 139686199676928 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.238176 139686199676928 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.238251 139686199676928 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.238602 139686199676928 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.239285 139686199676928 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.241846 139686199676928 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.242462 139686199676928 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.242537 139686199676928 transformer_layer.py:468] tlayer: End windows.
I0123 11:23:09.242569 139686199676928 transformer_layer.py:472] tlayer: final FFN.
I0123 11:23:09.242625 139686199676928 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.242748 139686199676928 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:23:09.242854 139686199676928 nn_components.py:325] mlp: activation = None
I0123 11:23:09.242891 139686199676928 nn_components.py:329] mlp: final activation = None
I0123 11:23:09.244755 139686199676928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.244845 139686199676928 nn_components.py:261] mlp: residual
I0123 11:23:09.247248 139686199676928 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.247333 139686199676928 transformer_base.py:443] tbase: final FFN
I0123 11:23:09.247441 139686199676928 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:23:09.249702 139686199676928 nn_components.py:329] mlp: final activation = None
I0123 11:23:09.251555 139686199676928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.251648 139686199676928 nn_components.py:261] mlp: residual
I0123 11:23:09.251934 139686199676928 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.252014 139686199676928 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 11:23:09.252120 139686199676928 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:23:09.252156 139686199676928 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:23:09.252186 139686199676928 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:23:09.252247 139686199676928 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.254484 139686199676928 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:23:09.259870 139686199676928 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.260127 139686199676928 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:23:09.262905 139686199676928 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:23:09.275270 139686199676928 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:23:09.275324 139686199676928 attention.py:418] Single window, no scan.
I0123 11:23:09.275358 139686199676928 transformer_layer.py:389] tlayer: self-attention.
I0123 11:23:09.275388 139686199676928 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.275448 139686199676928 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.276002 139686199676928 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.276076 139686199676928 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.276434 139686199676928 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.277117 139686199676928 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.280049 139686199676928 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.280672 139686199676928 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.280749 139686199676928 transformer_layer.py:468] tlayer: End windows.
I0123 11:23:09.280782 139686199676928 transformer_layer.py:472] tlayer: final FFN.
I0123 11:23:09.280839 139686199676928 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.280965 139686199676928 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:23:09.281071 139686199676928 nn_components.py:325] mlp: activation = None
I0123 11:23:09.281108 139686199676928 nn_components.py:329] mlp: final activation = None
I0123 11:23:09.282984 139686199676928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.283076 139686199676928 nn_components.py:261] mlp: residual
I0123 11:23:09.285484 139686199676928 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.285565 139686199676928 transformer_base.py:443] tbase: final FFN
I0123 11:23:09.285678 139686199676928 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:23:09.287966 139686199676928 nn_components.py:329] mlp: final activation = None
I0123 11:23:09.289835 139686199676928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.289931 139686199676928 nn_components.py:261] mlp: residual
I0123 11:23:09.290219 139686199676928 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.290298 139686199676928 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 11:23:09.290403 139686199676928 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:23:09.290440 139686199676928 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:23:09.290469 139686199676928 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:23:09.290529 139686199676928 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.292782 139686199676928 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:23:09.298184 139686199676928 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.298438 139686199676928 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:23:09.301102 139686199676928 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:23:09.313552 139686199676928 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:23:09.313605 139686199676928 attention.py:418] Single window, no scan.
I0123 11:23:09.313644 139686199676928 transformer_layer.py:389] tlayer: self-attention.
I0123 11:23:09.313676 139686199676928 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.313736 139686199676928 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.314289 139686199676928 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.314363 139686199676928 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.314721 139686199676928 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.315412 139686199676928 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.317969 139686199676928 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.318592 139686199676928 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.318668 139686199676928 transformer_layer.py:468] tlayer: End windows.
I0123 11:23:09.318701 139686199676928 transformer_layer.py:472] tlayer: final FFN.
I0123 11:23:09.318756 139686199676928 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.318879 139686199676928 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:23:09.318985 139686199676928 nn_components.py:325] mlp: activation = None
I0123 11:23:09.319022 139686199676928 nn_components.py:329] mlp: final activation = None
I0123 11:23:09.321416 139686199676928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.321508 139686199676928 nn_components.py:261] mlp: residual
I0123 11:23:09.323917 139686199676928 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.323995 139686199676928 transformer_base.py:443] tbase: final FFN
I0123 11:23:09.324109 139686199676928 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:23:09.326360 139686199676928 nn_components.py:329] mlp: final activation = None
I0123 11:23:09.328198 139686199676928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.328290 139686199676928 nn_components.py:261] mlp: residual
I0123 11:23:09.328576 139686199676928 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.328656 139686199676928 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 11:23:09.328761 139686199676928 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:23:09.328799 139686199676928 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:23:09.328828 139686199676928 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:23:09.328889 139686199676928 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.331123 139686199676928 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:23:09.336525 139686199676928 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.336783 139686199676928 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:23:09.339462 139686199676928 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:23:09.351917 139686199676928 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:23:09.351972 139686199676928 attention.py:418] Single window, no scan.
I0123 11:23:09.352006 139686199676928 transformer_layer.py:389] tlayer: self-attention.
I0123 11:23:09.352035 139686199676928 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.352096 139686199676928 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.352651 139686199676928 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.352725 139686199676928 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.353082 139686199676928 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.353771 139686199676928 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.356340 139686199676928 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.356952 139686199676928 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.357027 139686199676928 transformer_layer.py:468] tlayer: End windows.
I0123 11:23:09.357059 139686199676928 transformer_layer.py:472] tlayer: final FFN.
I0123 11:23:09.357115 139686199676928 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.357239 139686199676928 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:23:09.357346 139686199676928 nn_components.py:325] mlp: activation = None
I0123 11:23:09.357382 139686199676928 nn_components.py:329] mlp: final activation = None
I0123 11:23:09.359246 139686199676928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.359337 139686199676928 nn_components.py:261] mlp: residual
I0123 11:23:09.361748 139686199676928 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.361825 139686199676928 transformer_base.py:443] tbase: final FFN
I0123 11:23:09.361932 139686199676928 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:23:09.364261 139686199676928 nn_components.py:329] mlp: final activation = None
I0123 11:23:09.366096 139686199676928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.366189 139686199676928 nn_components.py:261] mlp: residual
I0123 11:23:09.366473 139686199676928 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.366551 139686199676928 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 11:23:09.366656 139686199676928 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:23:09.366693 139686199676928 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:23:09.366722 139686199676928 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:23:09.366781 139686199676928 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.369000 139686199676928 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:23:09.374377 139686199676928 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.374632 139686199676928 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:23:09.377312 139686199676928 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:23:09.389669 139686199676928 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:23:09.389724 139686199676928 attention.py:418] Single window, no scan.
I0123 11:23:09.389758 139686199676928 transformer_layer.py:389] tlayer: self-attention.
I0123 11:23:09.389787 139686199676928 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.389847 139686199676928 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.390406 139686199676928 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.390482 139686199676928 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.390841 139686199676928 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.391526 139686199676928 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.394555 139686199676928 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.395179 139686199676928 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.395258 139686199676928 transformer_layer.py:468] tlayer: End windows.
I0123 11:23:09.395291 139686199676928 transformer_layer.py:472] tlayer: final FFN.
I0123 11:23:09.395346 139686199676928 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.395472 139686199676928 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:23:09.395581 139686199676928 nn_components.py:325] mlp: activation = None
I0123 11:23:09.395619 139686199676928 nn_components.py:329] mlp: final activation = None
I0123 11:23:09.397482 139686199676928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.397572 139686199676928 nn_components.py:261] mlp: residual
I0123 11:23:09.399967 139686199676928 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.400049 139686199676928 transformer_base.py:443] tbase: final FFN
I0123 11:23:09.400154 139686199676928 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:23:09.402420 139686199676928 nn_components.py:329] mlp: final activation = None
I0123 11:23:09.404277 139686199676928 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.404368 139686199676928 nn_components.py:261] mlp: residual
I0123 11:23:09.404651 139686199676928 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:09.404732 139686199676928 decoder_stack.py:344] dstack: Final layernorm.
I0123 11:23:09.407563 139686199676928 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:23:13.763652 139686199676928 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
I0123 11:23:14.251425 139686199676928 training_loop.py:409] No working directory specified.
I0123 11:23:14.251556 139686199676928 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0123 11:23:14.252347 139686199676928 checkpoints.py:1062] Restoring legacy Flax checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0123 11:23:17.168211 139686199676928 training_loop.py:447] Only restoring trainable parameters.
I0123 11:23:17.168909 139686199676928 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0123 11:23:17.168967 139686199676928 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0123 11:23:17.169012 139686199676928 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:23:17.169054 139686199676928 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:23:17.169093 139686199676928 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:23:17.169131 139686199676928 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:23:17.169170 139686199676928 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:23:17.169208 139686199676928 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:23:17.169245 139686199676928 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:23:17.169281 139686199676928 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:23:17.169316 139686199676928 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:23:17.169353 139686199676928 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:23:17.169388 139686199676928 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:23:17.169424 139686199676928 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:23:17.169459 139686199676928 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:23:17.169495 139686199676928 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:23:17.169530 139686199676928 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:23:17.169565 139686199676928 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:23:17.169600 139686199676928 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:23:17.169636 139686199676928 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:23:17.169692 139686199676928 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:23:17.169730 139686199676928 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:23:17.169767 139686199676928 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:23:17.169802 139686199676928 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:23:17.169840 139686199676928 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:23:17.169876 139686199676928 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:23:17.169911 139686199676928 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:23:17.169947 139686199676928 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:23:17.169982 139686199676928 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:23:17.170017 139686199676928 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:23:17.170053 139686199676928 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:23:17.170089 139686199676928 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:23:17.170126 139686199676928 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:23:17.170162 139686199676928 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:23:17.170198 139686199676928 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:23:17.170233 139686199676928 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:23:17.170269 139686199676928 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:23:17.170304 139686199676928 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:23:17.170339 139686199676928 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:23:17.170374 139686199676928 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:23:17.170408 139686199676928 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:23:17.170443 139686199676928 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:23:17.170478 139686199676928 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:23:17.170513 139686199676928 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:23:17.170547 139686199676928 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:23:17.170582 139686199676928 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:23:17.170622 139686199676928 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:23:17.170659 139686199676928 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:23:17.170694 139686199676928 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:23:17.170730 139686199676928 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:23:17.170765 139686199676928 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:23:17.170800 139686199676928 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:23:17.170835 139686199676928 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:23:17.170871 139686199676928 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:23:17.170906 139686199676928 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:23:17.170940 139686199676928 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:23:17.170975 139686199676928 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:23:17.171009 139686199676928 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:23:17.171043 139686199676928 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:23:17.171078 139686199676928 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:23:17.171112 139686199676928 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:23:17.171147 139686199676928 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:23:17.171181 139686199676928 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:23:17.171216 139686199676928 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:23:17.171251 139686199676928 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:23:17.171285 139686199676928 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:23:17.171320 139686199676928 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:23:17.171354 139686199676928 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:23:17.171389 139686199676928 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:23:17.171423 139686199676928 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:23:17.171457 139686199676928 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:23:17.171493 139686199676928 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:23:17.171527 139686199676928 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:23:17.171567 139686199676928 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:23:17.171603 139686199676928 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:23:17.171638 139686199676928 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:23:17.171673 139686199676928 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:23:17.171708 139686199676928 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:23:17.171742 139686199676928 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:23:17.171777 139686199676928 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:23:17.171812 139686199676928 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:23:17.171847 139686199676928 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:23:17.171881 139686199676928 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:23:17.171917 139686199676928 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:23:17.171952 139686199676928 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:23:17.171988 139686199676928 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:23:17.172023 139686199676928 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:23:17.172058 139686199676928 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:23:17.172093 139686199676928 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:23:17.172128 139686199676928 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:23:17.172163 139686199676928 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:23:17.172197 139686199676928 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:23:17.172232 139686199676928 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:23:17.172267 139686199676928 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:23:17.172302 139686199676928 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:23:17.172337 139686199676928 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:23:17.172373 139686199676928 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:23:17.172407 139686199676928 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:23:17.172442 139686199676928 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:23:17.172478 139686199676928 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:23:17.172518 139686199676928 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:23:17.172555 139686199676928 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:23:17.172590 139686199676928 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:23:17.172626 139686199676928 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:23:17.172661 139686199676928 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:23:17.172696 139686199676928 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:23:17.172732 139686199676928 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:23:17.172767 139686199676928 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:23:17.172801 139686199676928 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:23:17.172836 139686199676928 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:23:17.172870 139686199676928 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:23:17.172905 139686199676928 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:23:17.172941 139686199676928 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:23:17.172975 139686199676928 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:23:17.173009 139686199676928 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:23:17.173044 139686199676928 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:23:17.173079 139686199676928 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:23:17.173114 139686199676928 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:23:17.173148 139686199676928 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:23:17.173183 139686199676928 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:23:17.173218 139686199676928 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:23:17.173253 139686199676928 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:23:17.173280 139686199676928 training_loop.py:725] Total parameters: 152072288
I0123 11:23:17.173486 139686199676928 training_loop.py:739] Total state size: 0
I0123 11:23:17.194408 139686199676928 training_loop.py:492] Training loop: creating task for mode beam_search
I0123 11:23:17.194691 139686199676928 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0123 11:23:17.195065 139686199676928 training_loop.py:652] Compiling mode beam_search with jit.
I0123 11:23:17.195401 139686199676928 training_loop.py:89] registering functions: dict_keys([])
I0123 11:23:17.212044 139686199676928 graph.py:499] a b c = triangle a b c; d = on_line d c a; e = on_line e c d; f = lc_tangent f c d, on_line f b d; g = on_circle g c d, on_line g c f; h = on_circle h c d, on_line h c f; i = lc_tangent i a d, on_line i b d; j = on_circle j a e, on_line j a i; k = on_circle k a e, on_line k a i; l = on_circle l g c, on_circle l j a; m = on_circle m g c, on_circle m j a; n = on_pline n m c a, on_line n a j; o = on_circle o g m, on_line o n m; p = on_circle p j m, on_line p n m; q = on_line q c o, on_line q a p; r = on_line r c l, on_line r o p; s = on_line s a l, on_line s o p ? cong q r q s
I0123 11:24:00.176850 139686199676928 ddar.py:60] Depth 1/1000 time = 42.866894245147705
I0123 11:24:53.763425 139686199676928 ddar.py:60] Depth 2/1000 time = 53.58622598648071
I0123 11:25:53.078685 139686199676928 ddar.py:60] Depth 3/1000 time = 59.31497049331665
I0123 11:27:35.009921 139686199676928 ddar.py:60] Depth 4/1000 time = 101.93086624145508
I0123 11:29:18.039165 139686199676928 ddar.py:60] Depth 5/1000 time = 103.02887654304504
I0123 11:29:18.070067 139686199676928 alphageometry.py:191] 
==========================
 * From theorem premises:
A C D F G I J L M N O P Q R S : Points
C,A,D are collinear [00]
CF  CD [01]
C,F,G are collinear [02]
AD  AI [03]
A,J,I are collinear [04]
JL = JA [05]
GL = GC [06]
JM = JA [07]
GM = GC [08]
MAL = MAL [09]
NM  CA [10]
M,O,N are collinear [11]
GO = GM [12]
OCL = OCL [13]
M,P,N are collinear [14]
JP = JM [15]
LAP = LAP [16]
A,Q,P are collinear [17]
C,Q,O are collinear [18]
R,O,P are collinear [19]
C,R,L are collinear [20]
P,O,S are collinear [21]
A,S,L are collinear [22]
SP:MS = SP:MS [23]

 * Auxiliary Constructions:
: Points


 * Proof steps:
001. JL = JA [05] & JM = JA [07] & JP = JM [15]   M,A,P,L are concyclic [24]
002. M,A,P,L are concyclic [24]   MPA = MLA [25]
003. GL = GC [06] & GM = GC [08] & GO = GM [12]   M,C,O,L are concyclic [26]
004. M,C,O,L are concyclic [26]   MOC = MLC [27]
005. C,Q,O are collinear [18] & A,Q,P are collinear [17] & MPA = MLA [25] & M,P,N are collinear [14] & MN  AC [10] & MOC = MLC [27] & M,O,N are collinear [11]   QCL = QAL [28]
006. QCL = QAL [28]   C,A,Q,L are concyclic [29]
007. C,A,Q,L are concyclic [29]   CAL = CQL [30]
008. C,A,Q,L are concyclic [29]   CAQ = CLQ [31]
009. JM = JA [07] & JL = JA [05]   J is the circumcenter of \Delta AML [32]
010. A,J,I are collinear [04] & CF  CD [01] & C,A,D are collinear [00] & AD  AI [03]   JA  AC [33]
011. J is the circumcenter of \Delta AML [32] & JA  AC [33]   MAC = MLA [34]
012. P,O,S are collinear [21] & M,O,N are collinear [11] & M,P,N are collinear [14] & MLA = MAC [34] & AC  MN [10]   MLA = AMS [35]
013. A,S,L are collinear [22] & MAL = MAL [09]   MAL = MAS [36]
014. MLA = AMS [35] & MAL = MAS [36] (Similar Triangles)  MA:AL = AS:MA [37]
015. GM = GC [08] & GO = GM [12]   G is the circumcenter of \Delta CMO [38]
016. C,F,G are collinear [02] & CF  CD [01] & C,A,D are collinear [00]   GC  CA [39]
017. G is the circumcenter of \Delta CMO [38] & GC  CA [39]   ACM = COM [40]
018. C,Q,O are collinear [18] & ACM = COM [40] & M,O,N are collinear [11] & MN  AC [10]   MCA = ACQ [41]
019. JM = JA [07] & JP = JM [15]   J is the circumcenter of \Delta AMP [42]
020. J is the circumcenter of \Delta AMP [42] & JA  AC [33]   CAM = APM [43]
021. A,Q,P are collinear [17] & CAM = APM [43] & M,P,N are collinear [14] & MN  AC [10]   MAC = CAQ [44]
022. MCA = ACQ [41] & MAC = CAQ [44] (Similar Triangles)  AM = AQ [45]
023. MCA = ACQ [41] & MAC = CAQ [44] (Similar Triangles)  CM = CQ [46]
024. AS:MA = MA:AL [37] & AQ = MA [45]   AS:AQ = AQ:AL [47]
025. A,S,L are collinear [22] & A,Q,P are collinear [17] & LAP = LAP [16]   SAQ = LAQ [48]
026. AS:AQ = AQ:AL [47] & SAQ = LAQ [48] (Similar Triangles)  ASQ = LQA [49]
027. A,Q,P are collinear [17] & M,P,N are collinear [14] & M,O,N are collinear [11] & C,Q,O are collinear [18] & CAL = CQL [30] & ASQ = LQA [49] & A,S,L are collinear [22] & AC  MN [10]   SQP = POQ [50]
028. P,O,S are collinear [21] & M,O,N are collinear [11] & M,P,N are collinear [14] & A,Q,P are collinear [17] & AC  MN [10]   SPQ = OPQ [51]
029. SQP = POQ [50] & SPQ = OPQ [51] (Similar Triangles)  QS:SP = QO:QP [52]
030. R,O,P are collinear [19] & M,O,N are collinear [11] & M,P,N are collinear [14] & C,Q,O are collinear [18] & A,Q,P are collinear [17] & CAL = CQL [30] & ASQ = LQA [49] & A,S,L are collinear [22] & AC  MN [10]   ROQ = SQP [53]
031. GL = GC [06] & GM = GC [08]   G is the circumcenter of \Delta CLM [54]
032. G is the circumcenter of \Delta CLM [54] & GC  CA [39]   MCA = MLC [55]
033. G is the circumcenter of \Delta CLM [54] & GC  CA [39]   LCA = LMC [56]
034. R,O,P are collinear [19] & M,O,N are collinear [11] & M,P,N are collinear [14] & MCA = MLC [55] & AC  MN [10]   CMR = MLC [57]
035. C,R,L are collinear [20] & R,O,P are collinear [19] & M,O,N are collinear [11] & M,P,N are collinear [14] & LCA = LMC [56] & AC  MN [10]   CRM = LMC [58]
036. CMR = MLC [57] & CRM = LMC [58] (Similar Triangles)  MC:CR = CL:MC [59]
037. CL:MC = MC:CR [59] & CQ = MC [46]   CL:CQ = CQ:CR [60]
038. C,Q,O are collinear [18] & C,R,L are collinear [20] & OCL = OCL [13]   QCL = QCR [61]
039. CL:CQ = CQ:CR [60] & QCL = QCR [61] (Similar Triangles)  CQL = QRC [62]
040. C,Q,O are collinear [18] & P,O,S are collinear [21] & M,O,N are collinear [11] & M,P,N are collinear [14] & A,Q,P are collinear [17] & CAQ = CLQ [31] & CQL = QRC [62] & C,R,L are collinear [20] & AC  MN [10]   RQO = SPQ [63]
041. ROQ = SQP [53] & RQO = SPQ [63] (Similar Triangles)  QR:SP = QO:QP [64]
042. QS:SP = QO:QP [52] & QR:SP = QO:QP [64]   QR:SP = QS:SP [65]
043. QR:SP = QS:SP [65] & SP:MS = SP:MS [23]   QS = QR
==========================

