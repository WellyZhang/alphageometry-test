I0123 18:33:52.715633 140408815788032 inference_utils.py:69] Parsing gin configuration.
I0123 18:33:52.715743 140408815788032 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0123 18:33:52.715959 140408815788032 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0123 18:33:52.715992 140408815788032 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0123 18:33:52.716021 140408815788032 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0123 18:33:52.716049 140408815788032 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0123 18:33:52.716076 140408815788032 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0123 18:33:52.716102 140408815788032 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0123 18:33:52.716130 140408815788032 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0123 18:33:52.716157 140408815788032 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=32
I0123 18:33:52.716184 140408815788032 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0123 18:33:52.716210 140408815788032 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0123 18:33:52.716257 140408815788032 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0123 18:33:52.716398 140408815788032 resource_reader.py:55] Path not found: base_htrans.gin
I0123 18:33:52.716621 140408815788032 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0123 18:33:52.716719 140408815788032 resource_reader.py:55] Path not found: trainer_configuration.gin
I0123 18:33:52.723148 140408815788032 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0123 18:33:52.723267 140408815788032 resource_reader.py:55] Path not found: size/medium_150M.gin
I0123 18:33:52.723594 140408815788032 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0123 18:33:52.723698 140408815788032 resource_reader.py:55] Path not found: options/positions_t5.gin
I0123 18:33:52.723983 140408815788032 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0123 18:33:52.724082 140408815788032 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0123 18:33:52.724495 140408815788032 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0123 18:33:52.724594 140408815788032 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0123 18:33:52.728266 140408815788032 training_loop.py:334] ==== Training loop: initializing model ====
I0123 18:33:52.838682 140408815788032 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0123 18:33:52.839390 140408815788032 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0123 18:33:52.846012 140408815788032 training_loop.py:335] Process 0 of 1
I0123 18:33:52.846067 140408815788032 training_loop.py:336] Local device count = 1
I0123 18:33:52.846108 140408815788032 training_loop.py:337] Number of replicas = 1
I0123 18:33:52.846141 140408815788032 training_loop.py:339] Using random number seed 42
I0123 18:33:53.332625 140408815788032 training_loop.py:359] Initializing the model.
I0123 18:33:53.722495 140408815788032 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.722745 140408815788032 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 18:33:53.722849 140408815788032 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 18:33:53.722928 140408815788032 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 18:33:53.723005 140408815788032 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 18:33:53.723085 140408815788032 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 18:33:53.723157 140408815788032 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 18:33:53.723227 140408815788032 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 18:33:53.723296 140408815788032 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 18:33:53.723364 140408815788032 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 18:33:53.723433 140408815788032 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 18:33:53.723501 140408815788032 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 18:33:53.723569 140408815788032 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 18:33:53.723639 140408815788032 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 18:33:53.723679 140408815788032 attention.py:418] Single window, no scan.
I0123 18:33:53.723726 140408815788032 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 18:33:53.723837 140408815788032 transformer_layer.py:154] tlayer: recurrent = False
I0123 18:33:53.723877 140408815788032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 18:33:53.723908 140408815788032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 18:33:53.725902 140408815788032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.731162 140408815788032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 18:33:53.741701 140408815788032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.741980 140408815788032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 18:33:53.746315 140408815788032 transformer_layer.py:213] tlayer: windowed attention.
I0123 18:33:53.756949 140408815788032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 18:33:53.757007 140408815788032 attention.py:418] Single window, no scan.
I0123 18:33:53.757045 140408815788032 transformer_layer.py:389] tlayer: self-attention.
I0123 18:33:53.757078 140408815788032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.757141 140408815788032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.758328 140408815788032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.758408 140408815788032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.759114 140408815788032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.761562 140408815788032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.767699 140408815788032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.769005 140408815788032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.769087 140408815788032 transformer_layer.py:468] tlayer: End windows.
I0123 18:33:53.769122 140408815788032 transformer_layer.py:472] tlayer: final FFN.
I0123 18:33:53.769183 140408815788032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.769311 140408815788032 transformer_base.py:410] tbase: post-attention MLP.
I0123 18:33:53.769637 140408815788032 nn_components.py:325] mlp: activation = None
I0123 18:33:53.769691 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:33:53.771581 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.771681 140408815788032 nn_components.py:261] mlp: residual
I0123 18:33:53.774536 140408815788032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.774618 140408815788032 transformer_base.py:443] tbase: final FFN
I0123 18:33:53.775113 140408815788032 nn_components.py:320] mlp: hidden 4096, relu
I0123 18:33:53.785093 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:33:53.793830 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.793928 140408815788032 nn_components.py:261] mlp: residual
I0123 18:33:53.794226 140408815788032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.794306 140408815788032 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 18:33:53.794417 140408815788032 transformer_layer.py:154] tlayer: recurrent = False
I0123 18:33:53.794456 140408815788032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 18:33:53.794487 140408815788032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 18:33:53.796306 140408815788032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.798784 140408815788032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 18:33:53.804309 140408815788032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.804570 140408815788032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 18:33:53.807167 140408815788032 transformer_layer.py:213] tlayer: windowed attention.
I0123 18:33:53.810944 140408815788032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 18:33:53.811000 140408815788032 attention.py:418] Single window, no scan.
I0123 18:33:53.811035 140408815788032 transformer_layer.py:389] tlayer: self-attention.
I0123 18:33:53.811065 140408815788032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.811128 140408815788032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.811697 140408815788032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.811774 140408815788032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.812129 140408815788032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.812889 140408815788032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.815995 140408815788032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.816720 140408815788032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.816802 140408815788032 transformer_layer.py:468] tlayer: End windows.
I0123 18:33:53.816838 140408815788032 transformer_layer.py:472] tlayer: final FFN.
I0123 18:33:53.816899 140408815788032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.817034 140408815788032 transformer_base.py:410] tbase: post-attention MLP.
I0123 18:33:53.817398 140408815788032 nn_components.py:325] mlp: activation = None
I0123 18:33:53.817441 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:33:53.819408 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.819503 140408815788032 nn_components.py:261] mlp: residual
I0123 18:33:53.821996 140408815788032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.822077 140408815788032 transformer_base.py:443] tbase: final FFN
I0123 18:33:53.822506 140408815788032 nn_components.py:320] mlp: hidden 4096, relu
I0123 18:33:53.824776 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:33:53.826663 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.826759 140408815788032 nn_components.py:261] mlp: residual
I0123 18:33:53.827050 140408815788032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.827130 140408815788032 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 18:33:53.827238 140408815788032 transformer_layer.py:154] tlayer: recurrent = False
I0123 18:33:53.827276 140408815788032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 18:33:53.827307 140408815788032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 18:33:53.829497 140408815788032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.831847 140408815788032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 18:33:53.837309 140408815788032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.837571 140408815788032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 18:33:53.840209 140408815788032 transformer_layer.py:213] tlayer: windowed attention.
I0123 18:33:53.844038 140408815788032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 18:33:53.844094 140408815788032 attention.py:418] Single window, no scan.
I0123 18:33:53.844133 140408815788032 transformer_layer.py:389] tlayer: self-attention.
I0123 18:33:53.844164 140408815788032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.844225 140408815788032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.844773 140408815788032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.844850 140408815788032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.845203 140408815788032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.845975 140408815788032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.848436 140408815788032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.849102 140408815788032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.849179 140408815788032 transformer_layer.py:468] tlayer: End windows.
I0123 18:33:53.849213 140408815788032 transformer_layer.py:472] tlayer: final FFN.
I0123 18:33:53.849274 140408815788032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.849401 140408815788032 transformer_base.py:410] tbase: post-attention MLP.
I0123 18:33:53.849725 140408815788032 nn_components.py:325] mlp: activation = None
I0123 18:33:53.849770 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:33:53.851662 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.851759 140408815788032 nn_components.py:261] mlp: residual
I0123 18:33:53.854251 140408815788032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.854341 140408815788032 transformer_base.py:443] tbase: final FFN
I0123 18:33:53.854823 140408815788032 nn_components.py:320] mlp: hidden 4096, relu
I0123 18:33:53.857080 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:33:53.858995 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.859091 140408815788032 nn_components.py:261] mlp: residual
I0123 18:33:53.859384 140408815788032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.859466 140408815788032 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 18:33:53.859575 140408815788032 transformer_layer.py:154] tlayer: recurrent = False
I0123 18:33:53.859614 140408815788032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 18:33:53.859645 140408815788032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 18:33:53.861518 140408815788032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.863968 140408815788032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 18:33:53.869619 140408815788032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.869889 140408815788032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 18:33:53.872565 140408815788032 transformer_layer.py:213] tlayer: windowed attention.
I0123 18:33:53.876329 140408815788032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 18:33:53.876385 140408815788032 attention.py:418] Single window, no scan.
I0123 18:33:53.876421 140408815788032 transformer_layer.py:389] tlayer: self-attention.
I0123 18:33:53.876451 140408815788032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.876512 140408815788032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.877061 140408815788032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.877137 140408815788032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.877498 140408815788032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.878258 140408815788032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.880811 140408815788032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.881433 140408815788032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.881511 140408815788032 transformer_layer.py:468] tlayer: End windows.
I0123 18:33:53.881546 140408815788032 transformer_layer.py:472] tlayer: final FFN.
I0123 18:33:53.881604 140408815788032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.881742 140408815788032 transformer_base.py:410] tbase: post-attention MLP.
I0123 18:33:53.882060 140408815788032 nn_components.py:325] mlp: activation = None
I0123 18:33:53.882104 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:33:53.883974 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.884067 140408815788032 nn_components.py:261] mlp: residual
I0123 18:33:53.886609 140408815788032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.886698 140408815788032 transformer_base.py:443] tbase: final FFN
I0123 18:33:53.887136 140408815788032 nn_components.py:320] mlp: hidden 4096, relu
I0123 18:33:53.889404 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:33:53.891339 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.891438 140408815788032 nn_components.py:261] mlp: residual
I0123 18:33:53.891736 140408815788032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.891818 140408815788032 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 18:33:53.891925 140408815788032 transformer_layer.py:154] tlayer: recurrent = False
I0123 18:33:53.891964 140408815788032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 18:33:53.891994 140408815788032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 18:33:53.893862 140408815788032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.896234 140408815788032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 18:33:53.901837 140408815788032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.902097 140408815788032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 18:33:53.905116 140408815788032 transformer_layer.py:213] tlayer: windowed attention.
I0123 18:33:53.908842 140408815788032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 18:33:53.908897 140408815788032 attention.py:418] Single window, no scan.
I0123 18:33:53.908933 140408815788032 transformer_layer.py:389] tlayer: self-attention.
I0123 18:33:53.908963 140408815788032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.909026 140408815788032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.909595 140408815788032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.909679 140408815788032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.910036 140408815788032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.910798 140408815788032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.913328 140408815788032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.913968 140408815788032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.914046 140408815788032 transformer_layer.py:468] tlayer: End windows.
I0123 18:33:53.914080 140408815788032 transformer_layer.py:472] tlayer: final FFN.
I0123 18:33:53.914140 140408815788032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.914276 140408815788032 transformer_base.py:410] tbase: post-attention MLP.
I0123 18:33:53.914599 140408815788032 nn_components.py:325] mlp: activation = None
I0123 18:33:53.914644 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:33:53.916527 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.916621 140408815788032 nn_components.py:261] mlp: residual
I0123 18:33:53.919146 140408815788032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.919228 140408815788032 transformer_base.py:443] tbase: final FFN
I0123 18:33:53.919652 140408815788032 nn_components.py:320] mlp: hidden 4096, relu
I0123 18:33:53.921911 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:33:53.923832 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.923928 140408815788032 nn_components.py:261] mlp: residual
I0123 18:33:53.924222 140408815788032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.924307 140408815788032 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 18:33:53.924417 140408815788032 transformer_layer.py:154] tlayer: recurrent = False
I0123 18:33:53.924456 140408815788032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 18:33:53.924487 140408815788032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 18:33:53.926331 140408815788032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.928686 140408815788032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 18:33:53.934205 140408815788032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.934464 140408815788032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 18:33:53.937116 140408815788032 transformer_layer.py:213] tlayer: windowed attention.
I0123 18:33:53.940820 140408815788032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 18:33:53.940876 140408815788032 attention.py:418] Single window, no scan.
I0123 18:33:53.940912 140408815788032 transformer_layer.py:389] tlayer: self-attention.
I0123 18:33:53.940942 140408815788032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.941002 140408815788032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.941608 140408815788032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.941692 140408815788032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.942056 140408815788032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.942843 140408815788032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.945323 140408815788032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.945951 140408815788032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.946029 140408815788032 transformer_layer.py:468] tlayer: End windows.
I0123 18:33:53.946064 140408815788032 transformer_layer.py:472] tlayer: final FFN.
I0123 18:33:53.946123 140408815788032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.946258 140408815788032 transformer_base.py:410] tbase: post-attention MLP.
I0123 18:33:53.946580 140408815788032 nn_components.py:325] mlp: activation = None
I0123 18:33:53.946623 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:33:53.948510 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.948605 140408815788032 nn_components.py:261] mlp: residual
I0123 18:33:53.951129 140408815788032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.951210 140408815788032 transformer_base.py:443] tbase: final FFN
I0123 18:33:53.951644 140408815788032 nn_components.py:320] mlp: hidden 4096, relu
I0123 18:33:53.953952 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:33:53.955877 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.955974 140408815788032 nn_components.py:261] mlp: residual
I0123 18:33:53.956271 140408815788032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.956352 140408815788032 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 18:33:53.956461 140408815788032 transformer_layer.py:154] tlayer: recurrent = False
I0123 18:33:53.956500 140408815788032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 18:33:53.956532 140408815788032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 18:33:53.958363 140408815788032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.960798 140408815788032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 18:33:53.966350 140408815788032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.966606 140408815788032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 18:33:53.969224 140408815788032 transformer_layer.py:213] tlayer: windowed attention.
I0123 18:33:53.973023 140408815788032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 18:33:53.973079 140408815788032 attention.py:418] Single window, no scan.
I0123 18:33:53.973115 140408815788032 transformer_layer.py:389] tlayer: self-attention.
I0123 18:33:53.973147 140408815788032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.973209 140408815788032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.973785 140408815788032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.973864 140408815788032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.974220 140408815788032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.974992 140408815788032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.977455 140408815788032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.978087 140408815788032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.978166 140408815788032 transformer_layer.py:468] tlayer: End windows.
I0123 18:33:53.978201 140408815788032 transformer_layer.py:472] tlayer: final FFN.
I0123 18:33:53.978262 140408815788032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.978393 140408815788032 transformer_base.py:410] tbase: post-attention MLP.
I0123 18:33:53.978714 140408815788032 nn_components.py:325] mlp: activation = None
I0123 18:33:53.978759 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:33:53.981001 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.981097 140408815788032 nn_components.py:261] mlp: residual
I0123 18:33:53.983561 140408815788032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:53.983641 140408815788032 transformer_base.py:443] tbase: final FFN
I0123 18:33:53.984059 140408815788032 nn_components.py:320] mlp: hidden 4096, relu
I0123 18:33:54.124626 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:33:54.126839 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.126996 140408815788032 nn_components.py:261] mlp: residual
I0123 18:33:54.127337 140408815788032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.127433 140408815788032 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 18:33:54.127551 140408815788032 transformer_layer.py:154] tlayer: recurrent = False
I0123 18:33:54.127593 140408815788032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 18:33:54.127626 140408815788032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 18:33:54.129690 140408815788032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.132234 140408815788032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 18:33:54.137993 140408815788032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.138463 140408815788032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 18:33:54.141337 140408815788032 transformer_layer.py:213] tlayer: windowed attention.
I0123 18:33:54.145255 140408815788032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 18:33:54.145313 140408815788032 attention.py:418] Single window, no scan.
I0123 18:33:54.145351 140408815788032 transformer_layer.py:389] tlayer: self-attention.
I0123 18:33:54.145383 140408815788032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.145445 140408815788032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.146063 140408815788032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.146141 140408815788032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.146507 140408815788032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.147285 140408815788032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.149893 140408815788032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.150526 140408815788032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.150606 140408815788032 transformer_layer.py:468] tlayer: End windows.
I0123 18:33:54.150641 140408815788032 transformer_layer.py:472] tlayer: final FFN.
I0123 18:33:54.150702 140408815788032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.150831 140408815788032 transformer_base.py:410] tbase: post-attention MLP.
I0123 18:33:54.151157 140408815788032 nn_components.py:325] mlp: activation = None
I0123 18:33:54.151201 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:33:54.153106 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.153203 140408815788032 nn_components.py:261] mlp: residual
I0123 18:33:54.155686 140408815788032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.155766 140408815788032 transformer_base.py:443] tbase: final FFN
I0123 18:33:54.156245 140408815788032 nn_components.py:320] mlp: hidden 4096, relu
I0123 18:33:54.158523 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:33:54.160416 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.160517 140408815788032 nn_components.py:261] mlp: residual
I0123 18:33:54.160809 140408815788032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.160893 140408815788032 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 18:33:54.161005 140408815788032 transformer_layer.py:154] tlayer: recurrent = False
I0123 18:33:54.161046 140408815788032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 18:33:54.161078 140408815788032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 18:33:54.162980 140408815788032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.165342 140408815788032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 18:33:54.170952 140408815788032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.171217 140408815788032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 18:33:54.173892 140408815788032 transformer_layer.py:213] tlayer: windowed attention.
I0123 18:33:54.177625 140408815788032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 18:33:54.177690 140408815788032 attention.py:418] Single window, no scan.
I0123 18:33:54.177726 140408815788032 transformer_layer.py:389] tlayer: self-attention.
I0123 18:33:54.177757 140408815788032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.177820 140408815788032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.178385 140408815788032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.178462 140408815788032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.178819 140408815788032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.179583 140408815788032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.182101 140408815788032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.182712 140408815788032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.182793 140408815788032 transformer_layer.py:468] tlayer: End windows.
I0123 18:33:54.182829 140408815788032 transformer_layer.py:472] tlayer: final FFN.
I0123 18:33:54.182887 140408815788032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.183012 140408815788032 transformer_base.py:410] tbase: post-attention MLP.
I0123 18:33:54.183335 140408815788032 nn_components.py:325] mlp: activation = None
I0123 18:33:54.183379 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:33:54.185251 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.185344 140408815788032 nn_components.py:261] mlp: residual
I0123 18:33:54.187883 140408815788032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.187964 140408815788032 transformer_base.py:443] tbase: final FFN
I0123 18:33:54.188394 140408815788032 nn_components.py:320] mlp: hidden 4096, relu
I0123 18:33:54.190651 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:33:54.192533 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.192629 140408815788032 nn_components.py:261] mlp: residual
I0123 18:33:54.192921 140408815788032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.193008 140408815788032 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 18:33:54.193120 140408815788032 transformer_layer.py:154] tlayer: recurrent = False
I0123 18:33:54.193159 140408815788032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 18:33:54.193191 140408815788032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 18:33:54.195058 140408815788032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.197421 140408815788032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 18:33:54.203302 140408815788032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.203563 140408815788032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 18:33:54.206248 140408815788032 transformer_layer.py:213] tlayer: windowed attention.
I0123 18:33:54.209986 140408815788032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 18:33:54.210042 140408815788032 attention.py:418] Single window, no scan.
I0123 18:33:54.210078 140408815788032 transformer_layer.py:389] tlayer: self-attention.
I0123 18:33:54.210108 140408815788032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.210171 140408815788032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.210731 140408815788032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.210807 140408815788032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.211166 140408815788032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.211973 140408815788032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.214462 140408815788032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.215079 140408815788032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.215162 140408815788032 transformer_layer.py:468] tlayer: End windows.
I0123 18:33:54.215197 140408815788032 transformer_layer.py:472] tlayer: final FFN.
I0123 18:33:54.215256 140408815788032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.215386 140408815788032 transformer_base.py:410] tbase: post-attention MLP.
I0123 18:33:54.215711 140408815788032 nn_components.py:325] mlp: activation = None
I0123 18:33:54.215755 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:33:54.217648 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.217743 140408815788032 nn_components.py:261] mlp: residual
I0123 18:33:54.220277 140408815788032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.220360 140408815788032 transformer_base.py:443] tbase: final FFN
I0123 18:33:54.220787 140408815788032 nn_components.py:320] mlp: hidden 4096, relu
I0123 18:33:54.223023 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:33:54.224956 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.225051 140408815788032 nn_components.py:261] mlp: residual
I0123 18:33:54.225343 140408815788032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.225430 140408815788032 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 18:33:54.225542 140408815788032 transformer_layer.py:154] tlayer: recurrent = False
I0123 18:33:54.225581 140408815788032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 18:33:54.225612 140408815788032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 18:33:54.227422 140408815788032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.229850 140408815788032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 18:33:54.235330 140408815788032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.235590 140408815788032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 18:33:54.238249 140408815788032 transformer_layer.py:213] tlayer: windowed attention.
I0123 18:33:54.242125 140408815788032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 18:33:54.242181 140408815788032 attention.py:418] Single window, no scan.
I0123 18:33:54.242218 140408815788032 transformer_layer.py:389] tlayer: self-attention.
I0123 18:33:54.242249 140408815788032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.242351 140408815788032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.242917 140408815788032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.242994 140408815788032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.243350 140408815788032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.244117 140408815788032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.246577 140408815788032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.247194 140408815788032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.247272 140408815788032 transformer_layer.py:468] tlayer: End windows.
I0123 18:33:54.247305 140408815788032 transformer_layer.py:472] tlayer: final FFN.
I0123 18:33:54.247364 140408815788032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.247493 140408815788032 transformer_base.py:410] tbase: post-attention MLP.
I0123 18:33:54.247811 140408815788032 nn_components.py:325] mlp: activation = None
I0123 18:33:54.247855 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:33:54.249814 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.249911 140408815788032 nn_components.py:261] mlp: residual
I0123 18:33:54.252649 140408815788032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.252730 140408815788032 transformer_base.py:443] tbase: final FFN
I0123 18:33:54.253156 140408815788032 nn_components.py:320] mlp: hidden 4096, relu
I0123 18:33:54.255538 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:33:54.257742 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.257847 140408815788032 nn_components.py:261] mlp: residual
I0123 18:33:54.258165 140408815788032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.258250 140408815788032 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 18:33:54.258378 140408815788032 transformer_layer.py:154] tlayer: recurrent = False
I0123 18:33:54.258419 140408815788032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 18:33:54.258457 140408815788032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 18:33:54.260345 140408815788032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.262883 140408815788032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 18:33:54.268806 140408815788032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.269074 140408815788032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 18:33:54.271821 140408815788032 transformer_layer.py:213] tlayer: windowed attention.
I0123 18:33:54.276107 140408815788032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 18:33:54.276165 140408815788032 attention.py:418] Single window, no scan.
I0123 18:33:54.276202 140408815788032 transformer_layer.py:389] tlayer: self-attention.
I0123 18:33:54.276233 140408815788032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.276298 140408815788032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.276895 140408815788032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.276976 140408815788032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.277349 140408815788032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.278170 140408815788032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.280749 140408815788032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.281406 140408815788032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.281489 140408815788032 transformer_layer.py:468] tlayer: End windows.
I0123 18:33:54.281526 140408815788032 transformer_layer.py:472] tlayer: final FFN.
I0123 18:33:54.281588 140408815788032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.281736 140408815788032 transformer_base.py:410] tbase: post-attention MLP.
I0123 18:33:54.282070 140408815788032 nn_components.py:325] mlp: activation = None
I0123 18:33:54.282116 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:33:54.284260 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.284362 140408815788032 nn_components.py:261] mlp: residual
I0123 18:33:54.286935 140408815788032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.287020 140408815788032 transformer_base.py:443] tbase: final FFN
I0123 18:33:54.287470 140408815788032 nn_components.py:320] mlp: hidden 4096, relu
I0123 18:33:54.289871 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:33:54.291837 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.291938 140408815788032 nn_components.py:261] mlp: residual
I0123 18:33:54.292247 140408815788032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.292549 140408815788032 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 18:33:54.292623 140408815788032 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 18:33:54.292693 140408815788032 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 18:33:54.292753 140408815788032 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 18:33:54.292811 140408815788032 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 18:33:54.292867 140408815788032 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 18:33:54.292922 140408815788032 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 18:33:54.292976 140408815788032 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 18:33:54.293030 140408815788032 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 18:33:54.293083 140408815788032 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 18:33:54.293136 140408815788032 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 18:33:54.293189 140408815788032 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 18:33:54.293228 140408815788032 decoder_stack.py:344] dstack: Final layernorm.
I0123 18:33:54.296898 140408815788032 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:33:54.347042 140408815788032 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.347236 140408815788032 decoder_stack.py:333] dstack: autoregressive generator.
I0123 18:33:54.347295 140408815788032 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 18:33:54.347407 140408815788032 transformer_layer.py:154] tlayer: recurrent = False
I0123 18:33:54.347448 140408815788032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 18:33:54.347481 140408815788032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 18:33:54.347554 140408815788032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.350190 140408815788032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 18:33:54.355971 140408815788032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.356242 140408815788032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 18:33:54.359065 140408815788032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 18:33:54.376473 140408815788032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 18:33:54.376535 140408815788032 attention.py:418] Single window, no scan.
I0123 18:33:54.376574 140408815788032 transformer_layer.py:389] tlayer: self-attention.
I0123 18:33:54.376607 140408815788032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.376673 140408815788032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.377902 140408815788032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.377987 140408815788032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.378726 140408815788032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.380820 140408815788032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.386031 140408815788032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.387405 140408815788032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.387502 140408815788032 transformer_layer.py:468] tlayer: End windows.
I0123 18:33:54.387541 140408815788032 transformer_layer.py:472] tlayer: final FFN.
I0123 18:33:54.387604 140408815788032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.387739 140408815788032 transformer_base.py:410] tbase: post-attention MLP.
I0123 18:33:54.387858 140408815788032 nn_components.py:325] mlp: activation = None
I0123 18:33:54.387899 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:33:54.389921 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.390022 140408815788032 nn_components.py:261] mlp: residual
I0123 18:33:54.392557 140408815788032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.392643 140408815788032 transformer_base.py:443] tbase: final FFN
I0123 18:33:54.392756 140408815788032 nn_components.py:320] mlp: hidden 4096, relu
I0123 18:33:54.395110 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:33:54.397154 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.397254 140408815788032 nn_components.py:261] mlp: residual
I0123 18:33:54.397554 140408815788032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.397648 140408815788032 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 18:33:54.397764 140408815788032 transformer_layer.py:154] tlayer: recurrent = False
I0123 18:33:54.397805 140408815788032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 18:33:54.397839 140408815788032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 18:33:54.397907 140408815788032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.400258 140408815788032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 18:33:54.405941 140408815788032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.406209 140408815788032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 18:33:54.408993 140408815788032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 18:33:54.422630 140408815788032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 18:33:54.422688 140408815788032 attention.py:418] Single window, no scan.
I0123 18:33:54.422727 140408815788032 transformer_layer.py:389] tlayer: self-attention.
I0123 18:33:54.422760 140408815788032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.422831 140408815788032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.423419 140408815788032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.423500 140408815788032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.423877 140408815788032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.424598 140408815788032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.427179 140408815788032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.427825 140408815788032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.427906 140408815788032 transformer_layer.py:468] tlayer: End windows.
I0123 18:33:54.427948 140408815788032 transformer_layer.py:472] tlayer: final FFN.
I0123 18:33:54.428010 140408815788032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.428145 140408815788032 transformer_base.py:410] tbase: post-attention MLP.
I0123 18:33:54.428257 140408815788032 nn_components.py:325] mlp: activation = None
I0123 18:33:54.428297 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:33:54.430280 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.430379 140408815788032 nn_components.py:261] mlp: residual
I0123 18:33:54.432887 140408815788032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.432971 140408815788032 transformer_base.py:443] tbase: final FFN
I0123 18:33:54.433085 140408815788032 nn_components.py:320] mlp: hidden 4096, relu
I0123 18:33:54.435403 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:33:54.437397 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.437499 140408815788032 nn_components.py:261] mlp: residual
I0123 18:33:54.437804 140408815788032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.437891 140408815788032 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 18:33:54.438006 140408815788032 transformer_layer.py:154] tlayer: recurrent = False
I0123 18:33:54.438048 140408815788032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 18:33:54.438081 140408815788032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 18:33:54.438150 140408815788032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.440482 140408815788032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 18:33:54.446139 140408815788032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.446409 140408815788032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 18:33:54.449185 140408815788032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 18:33:54.462370 140408815788032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 18:33:54.462428 140408815788032 attention.py:418] Single window, no scan.
I0123 18:33:54.462466 140408815788032 transformer_layer.py:389] tlayer: self-attention.
I0123 18:33:54.462498 140408815788032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.462564 140408815788032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.463145 140408815788032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.463227 140408815788032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.463608 140408815788032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.464337 140408815788032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.466915 140408815788032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.467566 140408815788032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.467648 140408815788032 transformer_layer.py:468] tlayer: End windows.
I0123 18:33:54.467684 140408815788032 transformer_layer.py:472] tlayer: final FFN.
I0123 18:33:54.467752 140408815788032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.467890 140408815788032 transformer_base.py:410] tbase: post-attention MLP.
I0123 18:33:54.468003 140408815788032 nn_components.py:325] mlp: activation = None
I0123 18:33:54.468044 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:33:54.474456 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.474603 140408815788032 nn_components.py:261] mlp: residual
I0123 18:33:54.477309 140408815788032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.477393 140408815788032 transformer_base.py:443] tbase: final FFN
I0123 18:33:54.477510 140408815788032 nn_components.py:320] mlp: hidden 4096, relu
I0123 18:33:54.479910 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:33:54.481944 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.482045 140408815788032 nn_components.py:261] mlp: residual
I0123 18:33:54.482343 140408815788032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.482434 140408815788032 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 18:33:54.482554 140408815788032 transformer_layer.py:154] tlayer: recurrent = False
I0123 18:33:54.482599 140408815788032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 18:33:54.482632 140408815788032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 18:33:54.482703 140408815788032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.485137 140408815788032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 18:33:54.490905 140408815788032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.491179 140408815788032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 18:33:54.494021 140408815788032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 18:33:54.507492 140408815788032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 18:33:54.507551 140408815788032 attention.py:418] Single window, no scan.
I0123 18:33:54.507591 140408815788032 transformer_layer.py:389] tlayer: self-attention.
I0123 18:33:54.507623 140408815788032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.507688 140408815788032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.508295 140408815788032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.508378 140408815788032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.508752 140408815788032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.509487 140408815788032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.512091 140408815788032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.512739 140408815788032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.512822 140408815788032 transformer_layer.py:468] tlayer: End windows.
I0123 18:33:54.512859 140408815788032 transformer_layer.py:472] tlayer: final FFN.
I0123 18:33:54.512921 140408815788032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.513060 140408815788032 transformer_base.py:410] tbase: post-attention MLP.
I0123 18:33:54.513173 140408815788032 nn_components.py:325] mlp: activation = None
I0123 18:33:54.513213 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:33:54.515541 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.515641 140408815788032 nn_components.py:261] mlp: residual
I0123 18:33:54.518141 140408815788032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.518225 140408815788032 transformer_base.py:443] tbase: final FFN
I0123 18:33:54.518341 140408815788032 nn_components.py:320] mlp: hidden 4096, relu
I0123 18:33:54.520640 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:33:54.522603 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.522703 140408815788032 nn_components.py:261] mlp: residual
I0123 18:33:54.523002 140408815788032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.523087 140408815788032 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 18:33:54.523200 140408815788032 transformer_layer.py:154] tlayer: recurrent = False
I0123 18:33:54.523242 140408815788032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 18:33:54.523274 140408815788032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 18:33:54.523341 140408815788032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.525747 140408815788032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 18:33:54.531408 140408815788032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.531684 140408815788032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 18:33:54.534391 140408815788032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 18:33:54.547276 140408815788032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 18:33:54.547335 140408815788032 attention.py:418] Single window, no scan.
I0123 18:33:54.547373 140408815788032 transformer_layer.py:389] tlayer: self-attention.
I0123 18:33:54.547405 140408815788032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.547471 140408815788032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.548045 140408815788032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.548122 140408815788032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.548474 140408815788032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.549171 140408815788032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.551767 140408815788032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.552394 140408815788032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.552474 140408815788032 transformer_layer.py:468] tlayer: End windows.
I0123 18:33:54.552510 140408815788032 transformer_layer.py:472] tlayer: final FFN.
I0123 18:33:54.552569 140408815788032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.552705 140408815788032 transformer_base.py:410] tbase: post-attention MLP.
I0123 18:33:54.552815 140408815788032 nn_components.py:325] mlp: activation = None
I0123 18:33:54.552854 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:33:54.554734 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.554830 140408815788032 nn_components.py:261] mlp: residual
I0123 18:33:54.557237 140408815788032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.557318 140408815788032 transformer_base.py:443] tbase: final FFN
I0123 18:33:54.557426 140408815788032 nn_components.py:320] mlp: hidden 4096, relu
I0123 18:33:54.559720 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:33:54.561584 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.561686 140408815788032 nn_components.py:261] mlp: residual
I0123 18:33:54.561972 140408815788032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.562054 140408815788032 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 18:33:54.562163 140408815788032 transformer_layer.py:154] tlayer: recurrent = False
I0123 18:33:54.562202 140408815788032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 18:33:54.562233 140408815788032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 18:33:54.562296 140408815788032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.564523 140408815788032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 18:33:54.569923 140408815788032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.570185 140408815788032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 18:33:54.572860 140408815788032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 18:33:54.585575 140408815788032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 18:33:54.585631 140408815788032 attention.py:418] Single window, no scan.
I0123 18:33:54.585674 140408815788032 transformer_layer.py:389] tlayer: self-attention.
I0123 18:33:54.585705 140408815788032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.585767 140408815788032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.586323 140408815788032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.586399 140408815788032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.586755 140408815788032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.587599 140408815788032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.590053 140408815788032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.590675 140408815788032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.590753 140408815788032 transformer_layer.py:468] tlayer: End windows.
I0123 18:33:54.590787 140408815788032 transformer_layer.py:472] tlayer: final FFN.
I0123 18:33:54.590847 140408815788032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.590978 140408815788032 transformer_base.py:410] tbase: post-attention MLP.
I0123 18:33:54.591094 140408815788032 nn_components.py:325] mlp: activation = None
I0123 18:33:54.591133 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:33:54.593064 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.593159 140408815788032 nn_components.py:261] mlp: residual
I0123 18:33:54.595572 140408815788032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.595654 140408815788032 transformer_base.py:443] tbase: final FFN
I0123 18:33:54.595763 140408815788032 nn_components.py:320] mlp: hidden 4096, relu
I0123 18:33:54.597982 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:33:54.599850 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.599946 140408815788032 nn_components.py:261] mlp: residual
I0123 18:33:54.600233 140408815788032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.600315 140408815788032 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 18:33:54.600424 140408815788032 transformer_layer.py:154] tlayer: recurrent = False
I0123 18:33:54.600463 140408815788032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 18:33:54.600494 140408815788032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 18:33:54.600557 140408815788032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.602785 140408815788032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 18:33:54.608299 140408815788032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.608556 140408815788032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 18:33:54.611247 140408815788032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 18:33:54.624291 140408815788032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 18:33:54.624347 140408815788032 attention.py:418] Single window, no scan.
I0123 18:33:54.624383 140408815788032 transformer_layer.py:389] tlayer: self-attention.
I0123 18:33:54.624414 140408815788032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.624477 140408815788032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.625041 140408815788032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.625118 140408815788032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.625471 140408815788032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.626178 140408815788032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.628643 140408815788032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.629303 140408815788032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.629382 140408815788032 transformer_layer.py:468] tlayer: End windows.
I0123 18:33:54.629416 140408815788032 transformer_layer.py:472] tlayer: final FFN.
I0123 18:33:54.629478 140408815788032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.629608 140408815788032 transformer_base.py:410] tbase: post-attention MLP.
I0123 18:33:54.629722 140408815788032 nn_components.py:325] mlp: activation = None
I0123 18:33:54.629766 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:33:54.631638 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.631734 140408815788032 nn_components.py:261] mlp: residual
I0123 18:33:54.634125 140408815788032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.634207 140408815788032 transformer_base.py:443] tbase: final FFN
I0123 18:33:54.634314 140408815788032 nn_components.py:320] mlp: hidden 4096, relu
I0123 18:33:54.636522 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:33:54.638448 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.638545 140408815788032 nn_components.py:261] mlp: residual
I0123 18:33:54.638864 140408815788032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.638946 140408815788032 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 18:33:54.639054 140408815788032 transformer_layer.py:154] tlayer: recurrent = False
I0123 18:33:54.639094 140408815788032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 18:33:54.639124 140408815788032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 18:33:54.639187 140408815788032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.641441 140408815788032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 18:33:54.646852 140408815788032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.647122 140408815788032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 18:33:54.649813 140408815788032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 18:33:54.662520 140408815788032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 18:33:54.662578 140408815788032 attention.py:418] Single window, no scan.
I0123 18:33:54.662616 140408815788032 transformer_layer.py:389] tlayer: self-attention.
I0123 18:33:54.662647 140408815788032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.662711 140408815788032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.663314 140408815788032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.663392 140408815788032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.663743 140408815788032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.664435 140408815788032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.666917 140408815788032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.667546 140408815788032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.667624 140408815788032 transformer_layer.py:468] tlayer: End windows.
I0123 18:33:54.667659 140408815788032 transformer_layer.py:472] tlayer: final FFN.
I0123 18:33:54.667718 140408815788032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.667848 140408815788032 transformer_base.py:410] tbase: post-attention MLP.
I0123 18:33:54.667960 140408815788032 nn_components.py:325] mlp: activation = None
I0123 18:33:54.668005 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:33:54.669894 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.669989 140408815788032 nn_components.py:261] mlp: residual
I0123 18:33:54.672429 140408815788032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.672510 140408815788032 transformer_base.py:443] tbase: final FFN
I0123 18:33:54.672619 140408815788032 nn_components.py:320] mlp: hidden 4096, relu
I0123 18:33:54.674829 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:33:54.676672 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.676767 140408815788032 nn_components.py:261] mlp: residual
I0123 18:33:54.677051 140408815788032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.677132 140408815788032 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 18:33:54.677239 140408815788032 transformer_layer.py:154] tlayer: recurrent = False
I0123 18:33:54.677278 140408815788032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 18:33:54.677309 140408815788032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 18:33:54.677373 140408815788032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.679601 140408815788032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 18:33:54.685065 140408815788032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.685323 140408815788032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 18:33:54.687922 140408815788032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 18:33:54.700658 140408815788032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 18:33:54.700716 140408815788032 attention.py:418] Single window, no scan.
I0123 18:33:54.700752 140408815788032 transformer_layer.py:389] tlayer: self-attention.
I0123 18:33:54.700782 140408815788032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.700844 140408815788032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.701403 140408815788032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.701480 140408815788032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.701836 140408815788032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.702521 140408815788032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.704982 140408815788032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.705653 140408815788032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.705732 140408815788032 transformer_layer.py:468] tlayer: End windows.
I0123 18:33:54.705766 140408815788032 transformer_layer.py:472] tlayer: final FFN.
I0123 18:33:54.705827 140408815788032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.705955 140408815788032 transformer_base.py:410] tbase: post-attention MLP.
I0123 18:33:54.706065 140408815788032 nn_components.py:325] mlp: activation = None
I0123 18:33:54.706104 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:33:54.707989 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.708083 140408815788032 nn_components.py:261] mlp: residual
I0123 18:33:54.710480 140408815788032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.710561 140408815788032 transformer_base.py:443] tbase: final FFN
I0123 18:33:54.710675 140408815788032 nn_components.py:320] mlp: hidden 4096, relu
I0123 18:33:54.712876 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:33:54.714800 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.714896 140408815788032 nn_components.py:261] mlp: residual
I0123 18:33:54.715183 140408815788032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.715266 140408815788032 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 18:33:54.715375 140408815788032 transformer_layer.py:154] tlayer: recurrent = False
I0123 18:33:54.715414 140408815788032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 18:33:54.715446 140408815788032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 18:33:54.715510 140408815788032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.717753 140408815788032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 18:33:54.723135 140408815788032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.723394 140408815788032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 18:33:54.726360 140408815788032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 18:33:54.738944 140408815788032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 18:33:54.739000 140408815788032 attention.py:418] Single window, no scan.
I0123 18:33:54.739035 140408815788032 transformer_layer.py:389] tlayer: self-attention.
I0123 18:33:54.739066 140408815788032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.739128 140408815788032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.739733 140408815788032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.739811 140408815788032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.740165 140408815788032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.740851 140408815788032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.743309 140408815788032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.743928 140408815788032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.744006 140408815788032 transformer_layer.py:468] tlayer: End windows.
I0123 18:33:54.744041 140408815788032 transformer_layer.py:472] tlayer: final FFN.
I0123 18:33:54.744098 140408815788032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.744229 140408815788032 transformer_base.py:410] tbase: post-attention MLP.
I0123 18:33:54.744338 140408815788032 nn_components.py:325] mlp: activation = None
I0123 18:33:54.744379 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:33:54.746237 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.746338 140408815788032 nn_components.py:261] mlp: residual
I0123 18:33:54.748783 140408815788032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.748865 140408815788032 transformer_base.py:443] tbase: final FFN
I0123 18:33:54.748971 140408815788032 nn_components.py:320] mlp: hidden 4096, relu
I0123 18:33:54.751184 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:33:54.753042 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.753139 140408815788032 nn_components.py:261] mlp: residual
I0123 18:33:54.753425 140408815788032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.753506 140408815788032 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 18:33:54.753615 140408815788032 transformer_layer.py:154] tlayer: recurrent = False
I0123 18:33:54.753661 140408815788032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 18:33:54.753693 140408815788032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 18:33:54.753758 140408815788032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.755976 140408815788032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 18:33:54.761471 140408815788032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.761742 140408815788032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 18:33:54.764361 140408815788032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 18:33:54.777019 140408815788032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 18:33:54.777075 140408815788032 attention.py:418] Single window, no scan.
I0123 18:33:54.777115 140408815788032 transformer_layer.py:389] tlayer: self-attention.
I0123 18:33:54.777146 140408815788032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.777209 140408815788032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.777776 140408815788032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.777853 140408815788032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.778204 140408815788032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.778891 140408815788032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.781342 140408815788032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.782008 140408815788032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.782086 140408815788032 transformer_layer.py:468] tlayer: End windows.
I0123 18:33:54.782120 140408815788032 transformer_layer.py:472] tlayer: final FFN.
I0123 18:33:54.782177 140408815788032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.782301 140408815788032 transformer_base.py:410] tbase: post-attention MLP.
I0123 18:33:54.782407 140408815788032 nn_components.py:325] mlp: activation = None
I0123 18:33:54.782449 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:33:54.784313 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.784412 140408815788032 nn_components.py:261] mlp: residual
I0123 18:33:54.786822 140408815788032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.786902 140408815788032 transformer_base.py:443] tbase: final FFN
I0123 18:33:54.787009 140408815788032 nn_components.py:320] mlp: hidden 4096, relu
I0123 18:33:54.789197 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:33:54.791132 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.791229 140408815788032 nn_components.py:261] mlp: residual
I0123 18:33:54.791515 140408815788032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.791598 140408815788032 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 18:33:54.791706 140408815788032 transformer_layer.py:154] tlayer: recurrent = False
I0123 18:33:54.791745 140408815788032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 18:33:54.791775 140408815788032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 18:33:54.791838 140408815788032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.794070 140408815788032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 18:33:54.799491 140408815788032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.799752 140408815788032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 18:33:54.802566 140408815788032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 18:33:54.815217 140408815788032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 18:33:54.815274 140408815788032 attention.py:418] Single window, no scan.
I0123 18:33:54.815309 140408815788032 transformer_layer.py:389] tlayer: self-attention.
I0123 18:33:54.815340 140408815788032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.815403 140408815788032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.815961 140408815788032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.816037 140408815788032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.816388 140408815788032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.817069 140408815788032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.819590 140408815788032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.820204 140408815788032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.820282 140408815788032 transformer_layer.py:468] tlayer: End windows.
I0123 18:33:54.820317 140408815788032 transformer_layer.py:472] tlayer: final FFN.
I0123 18:33:54.820376 140408815788032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.820507 140408815788032 transformer_base.py:410] tbase: post-attention MLP.
I0123 18:33:54.820620 140408815788032 nn_components.py:325] mlp: activation = None
I0123 18:33:54.820660 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:33:54.822546 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.822640 140408815788032 nn_components.py:261] mlp: residual
I0123 18:33:54.825052 140408815788032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.825132 140408815788032 transformer_base.py:443] tbase: final FFN
I0123 18:33:54.825239 140408815788032 nn_components.py:320] mlp: hidden 4096, relu
I0123 18:33:54.827841 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:33:54.829690 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.829788 140408815788032 nn_components.py:261] mlp: residual
I0123 18:33:54.830070 140408815788032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.830158 140408815788032 decoder_stack.py:344] dstack: Final layernorm.
I0123 18:33:54.833018 140408815788032 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:33:54.888432 140408815788032 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:54.888518 140408815788032 decoder_stack.py:333] dstack: autoregressive generator.
I0123 18:33:54.888573 140408815788032 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 18:33:54.888676 140408815788032 transformer_layer.py:154] tlayer: recurrent = False
I0123 18:33:54.888715 140408815788032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 18:33:54.888745 140408815788032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 18:33:54.888809 140408815788032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:54.891127 140408815788032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 18:33:54.896446 140408815788032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:54.896708 140408815788032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 18:33:54.899284 140408815788032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 18:33:54.911608 140408815788032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 18:33:54.911665 140408815788032 attention.py:418] Single window, no scan.
I0123 18:33:54.911701 140408815788032 transformer_layer.py:389] tlayer: self-attention.
I0123 18:33:54.911733 140408815788032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:54.911796 140408815788032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:54.912351 140408815788032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:54.912428 140408815788032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:54.912782 140408815788032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:54.913448 140408815788032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:54.915936 140408815788032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:54.916550 140408815788032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:54.916627 140408815788032 transformer_layer.py:468] tlayer: End windows.
I0123 18:33:54.916661 140408815788032 transformer_layer.py:472] tlayer: final FFN.
I0123 18:33:54.916721 140408815788032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:54.916846 140408815788032 transformer_base.py:410] tbase: post-attention MLP.
I0123 18:33:54.916960 140408815788032 nn_components.py:325] mlp: activation = None
I0123 18:33:54.916998 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:33:54.918821 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:54.918916 140408815788032 nn_components.py:261] mlp: residual
I0123 18:33:54.921267 140408815788032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:54.921347 140408815788032 transformer_base.py:443] tbase: final FFN
I0123 18:33:54.921454 140408815788032 nn_components.py:320] mlp: hidden 4096, relu
I0123 18:33:54.923687 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:33:54.925511 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:54.925606 140408815788032 nn_components.py:261] mlp: residual
I0123 18:33:54.925898 140408815788032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:54.925981 140408815788032 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 18:33:54.926087 140408815788032 transformer_layer.py:154] tlayer: recurrent = False
I0123 18:33:54.926126 140408815788032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 18:33:54.926156 140408815788032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 18:33:54.926219 140408815788032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:54.928413 140408815788032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 18:33:54.933696 140408815788032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:54.933954 140408815788032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 18:33:54.936578 140408815788032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 18:33:54.948814 140408815788032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 18:33:54.948870 140408815788032 attention.py:418] Single window, no scan.
I0123 18:33:54.948905 140408815788032 transformer_layer.py:389] tlayer: self-attention.
I0123 18:33:54.948935 140408815788032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:54.948998 140408815788032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:54.949552 140408815788032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:54.949629 140408815788032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:54.949989 140408815788032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:54.950659 140408815788032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:54.953137 140408815788032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:54.953758 140408815788032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:54.953837 140408815788032 transformer_layer.py:468] tlayer: End windows.
I0123 18:33:54.953871 140408815788032 transformer_layer.py:472] tlayer: final FFN.
I0123 18:33:54.953931 140408815788032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:54.954058 140408815788032 transformer_base.py:410] tbase: post-attention MLP.
I0123 18:33:54.954166 140408815788032 nn_components.py:325] mlp: activation = None
I0123 18:33:54.954211 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:33:54.956024 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:54.956118 140408815788032 nn_components.py:261] mlp: residual
I0123 18:33:54.958466 140408815788032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:54.958547 140408815788032 transformer_base.py:443] tbase: final FFN
I0123 18:33:54.958655 140408815788032 nn_components.py:320] mlp: hidden 4096, relu
I0123 18:33:54.960862 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:33:54.962677 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:54.962773 140408815788032 nn_components.py:261] mlp: residual
I0123 18:33:54.963198 140408815788032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:54.963280 140408815788032 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 18:33:54.963386 140408815788032 transformer_layer.py:154] tlayer: recurrent = False
I0123 18:33:54.963425 140408815788032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 18:33:54.963455 140408815788032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 18:33:54.963516 140408815788032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:54.965694 140408815788032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 18:33:54.970971 140408815788032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:54.971229 140408815788032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 18:33:54.973859 140408815788032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 18:33:54.986166 140408815788032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 18:33:54.986222 140408815788032 attention.py:418] Single window, no scan.
I0123 18:33:54.986258 140408815788032 transformer_layer.py:389] tlayer: self-attention.
I0123 18:33:54.986289 140408815788032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:54.986351 140408815788032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:54.986908 140408815788032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:54.986985 140408815788032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:54.987333 140408815788032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:54.988005 140408815788032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:54.990933 140408815788032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:54.991553 140408815788032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:54.991633 140408815788032 transformer_layer.py:468] tlayer: End windows.
I0123 18:33:54.991667 140408815788032 transformer_layer.py:472] tlayer: final FFN.
I0123 18:33:54.991726 140408815788032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:54.991852 140408815788032 transformer_base.py:410] tbase: post-attention MLP.
I0123 18:33:54.991960 140408815788032 nn_components.py:325] mlp: activation = None
I0123 18:33:54.991998 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:33:54.993830 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:54.993924 140408815788032 nn_components.py:261] mlp: residual
I0123 18:33:54.996281 140408815788032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:54.996361 140408815788032 transformer_base.py:443] tbase: final FFN
I0123 18:33:54.996468 140408815788032 nn_components.py:320] mlp: hidden 4096, relu
I0123 18:33:54.998688 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:33:55.000510 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.000606 140408815788032 nn_components.py:261] mlp: residual
I0123 18:33:55.000889 140408815788032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.000969 140408815788032 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 18:33:55.001075 140408815788032 transformer_layer.py:154] tlayer: recurrent = False
I0123 18:33:55.001114 140408815788032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 18:33:55.001143 140408815788032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 18:33:55.001204 140408815788032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.003404 140408815788032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 18:33:55.008679 140408815788032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.008934 140408815788032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 18:33:55.011554 140408815788032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 18:33:55.023916 140408815788032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 18:33:55.023972 140408815788032 attention.py:418] Single window, no scan.
I0123 18:33:55.024009 140408815788032 transformer_layer.py:389] tlayer: self-attention.
I0123 18:33:55.024055 140408815788032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.024119 140408815788032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.024680 140408815788032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.024756 140408815788032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.025113 140408815788032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.025815 140408815788032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.028313 140408815788032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.028931 140408815788032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.029007 140408815788032 transformer_layer.py:468] tlayer: End windows.
I0123 18:33:55.029042 140408815788032 transformer_layer.py:472] tlayer: final FFN.
I0123 18:33:55.029100 140408815788032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.029230 140408815788032 transformer_base.py:410] tbase: post-attention MLP.
I0123 18:33:55.029339 140408815788032 nn_components.py:325] mlp: activation = None
I0123 18:33:55.029379 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:33:55.031243 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.031336 140408815788032 nn_components.py:261] mlp: residual
I0123 18:33:55.033698 140408815788032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.033778 140408815788032 transformer_base.py:443] tbase: final FFN
I0123 18:33:55.033885 140408815788032 nn_components.py:320] mlp: hidden 4096, relu
I0123 18:33:55.036148 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:33:55.037983 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.038078 140408815788032 nn_components.py:261] mlp: residual
I0123 18:33:55.038359 140408815788032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.038440 140408815788032 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 18:33:55.038546 140408815788032 transformer_layer.py:154] tlayer: recurrent = False
I0123 18:33:55.038584 140408815788032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 18:33:55.038613 140408815788032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 18:33:55.038675 140408815788032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.040875 140408815788032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 18:33:55.046210 140408815788032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.046466 140408815788032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 18:33:55.049125 140408815788032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 18:33:55.061654 140408815788032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 18:33:55.061709 140408815788032 attention.py:418] Single window, no scan.
I0123 18:33:55.061744 140408815788032 transformer_layer.py:389] tlayer: self-attention.
I0123 18:33:55.061772 140408815788032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.061834 140408815788032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.062390 140408815788032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.062466 140408815788032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.062818 140408815788032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.063505 140408815788032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.066060 140408815788032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.066674 140408815788032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.066751 140408815788032 transformer_layer.py:468] tlayer: End windows.
I0123 18:33:55.066785 140408815788032 transformer_layer.py:472] tlayer: final FFN.
I0123 18:33:55.066843 140408815788032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.066969 140408815788032 transformer_base.py:410] tbase: post-attention MLP.
I0123 18:33:55.067077 140408815788032 nn_components.py:325] mlp: activation = None
I0123 18:33:55.067114 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:33:55.068956 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.069055 140408815788032 nn_components.py:261] mlp: residual
I0123 18:33:55.071422 140408815788032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.071502 140408815788032 transformer_base.py:443] tbase: final FFN
I0123 18:33:55.071610 140408815788032 nn_components.py:320] mlp: hidden 4096, relu
I0123 18:33:55.073860 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:33:55.075674 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.075768 140408815788032 nn_components.py:261] mlp: residual
I0123 18:33:55.076052 140408815788032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.076133 140408815788032 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 18:33:55.076239 140408815788032 transformer_layer.py:154] tlayer: recurrent = False
I0123 18:33:55.076276 140408815788032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 18:33:55.076305 140408815788032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 18:33:55.076366 140408815788032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.078599 140408815788032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 18:33:55.083954 140408815788032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.084211 140408815788032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 18:33:55.086878 140408815788032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 18:33:55.099392 140408815788032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 18:33:55.099446 140408815788032 attention.py:418] Single window, no scan.
I0123 18:33:55.099480 140408815788032 transformer_layer.py:389] tlayer: self-attention.
I0123 18:33:55.099510 140408815788032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.099571 140408815788032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.100124 140408815788032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.100204 140408815788032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.100563 140408815788032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.101241 140408815788032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.104167 140408815788032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.104778 140408815788032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.104855 140408815788032 transformer_layer.py:468] tlayer: End windows.
I0123 18:33:55.104889 140408815788032 transformer_layer.py:472] tlayer: final FFN.
I0123 18:33:55.104946 140408815788032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.105072 140408815788032 transformer_base.py:410] tbase: post-attention MLP.
I0123 18:33:55.105180 140408815788032 nn_components.py:325] mlp: activation = None
I0123 18:33:55.105217 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:33:55.107080 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.107179 140408815788032 nn_components.py:261] mlp: residual
I0123 18:33:55.109543 140408815788032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.109621 140408815788032 transformer_base.py:443] tbase: final FFN
I0123 18:33:55.109736 140408815788032 nn_components.py:320] mlp: hidden 4096, relu
I0123 18:33:55.111977 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:33:55.113802 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.113896 140408815788032 nn_components.py:261] mlp: residual
I0123 18:33:55.114177 140408815788032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.114258 140408815788032 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 18:33:55.114363 140408815788032 transformer_layer.py:154] tlayer: recurrent = False
I0123 18:33:55.114401 140408815788032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 18:33:55.114430 140408815788032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 18:33:55.114491 140408815788032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.116684 140408815788032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 18:33:55.122024 140408815788032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.122274 140408815788032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 18:33:55.124903 140408815788032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 18:33:55.137278 140408815788032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 18:33:55.137331 140408815788032 attention.py:418] Single window, no scan.
I0123 18:33:55.137365 140408815788032 transformer_layer.py:389] tlayer: self-attention.
I0123 18:33:55.137394 140408815788032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.137455 140408815788032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.138021 140408815788032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.138098 140408815788032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.138455 140408815788032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.139128 140408815788032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.141638 140408815788032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.142270 140408815788032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.142347 140408815788032 transformer_layer.py:468] tlayer: End windows.
I0123 18:33:55.142381 140408815788032 transformer_layer.py:472] tlayer: final FFN.
I0123 18:33:55.142437 140408815788032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.142561 140408815788032 transformer_base.py:410] tbase: post-attention MLP.
I0123 18:33:55.142668 140408815788032 nn_components.py:325] mlp: activation = None
I0123 18:33:55.142706 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:33:55.144551 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.144642 140408815788032 nn_components.py:261] mlp: residual
I0123 18:33:55.147021 140408815788032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.147106 140408815788032 transformer_base.py:443] tbase: final FFN
I0123 18:33:55.147213 140408815788032 nn_components.py:320] mlp: hidden 4096, relu
I0123 18:33:55.149447 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:33:55.151280 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.151375 140408815788032 nn_components.py:261] mlp: residual
I0123 18:33:55.151656 140408815788032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.151737 140408815788032 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 18:33:55.151843 140408815788032 transformer_layer.py:154] tlayer: recurrent = False
I0123 18:33:55.151880 140408815788032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 18:33:55.151910 140408815788032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 18:33:55.151970 140408815788032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.154173 140408815788032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 18:33:55.159443 140408815788032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.159701 140408815788032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 18:33:55.162341 140408815788032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 18:33:55.174717 140408815788032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 18:33:55.174770 140408815788032 attention.py:418] Single window, no scan.
I0123 18:33:55.174806 140408815788032 transformer_layer.py:389] tlayer: self-attention.
I0123 18:33:55.174835 140408815788032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.174896 140408815788032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.175457 140408815788032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.175532 140408815788032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.175882 140408815788032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.176557 140408815788032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.179075 140408815788032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.179697 140408815788032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.179774 140408815788032 transformer_layer.py:468] tlayer: End windows.
I0123 18:33:55.179808 140408815788032 transformer_layer.py:472] tlayer: final FFN.
I0123 18:33:55.179866 140408815788032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.179994 140408815788032 transformer_base.py:410] tbase: post-attention MLP.
I0123 18:33:55.180104 140408815788032 nn_components.py:325] mlp: activation = None
I0123 18:33:55.180143 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:33:55.181978 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.182071 140408815788032 nn_components.py:261] mlp: residual
I0123 18:33:55.184484 140408815788032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.184568 140408815788032 transformer_base.py:443] tbase: final FFN
I0123 18:33:55.184675 140408815788032 nn_components.py:320] mlp: hidden 4096, relu
I0123 18:33:55.186912 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:33:55.188733 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.188827 140408815788032 nn_components.py:261] mlp: residual
I0123 18:33:55.189110 140408815788032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.189191 140408815788032 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 18:33:55.189298 140408815788032 transformer_layer.py:154] tlayer: recurrent = False
I0123 18:33:55.189336 140408815788032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 18:33:55.189366 140408815788032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 18:33:55.189427 140408815788032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.191627 140408815788032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 18:33:55.196924 140408815788032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.197179 140408815788032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 18:33:55.199795 140408815788032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 18:33:55.212083 140408815788032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 18:33:55.212137 140408815788032 attention.py:418] Single window, no scan.
I0123 18:33:55.212172 140408815788032 transformer_layer.py:389] tlayer: self-attention.
I0123 18:33:55.212202 140408815788032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.212267 140408815788032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.212824 140408815788032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.212898 140408815788032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.213250 140408815788032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.213932 140408815788032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.216810 140408815788032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.217422 140408815788032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.217498 140408815788032 transformer_layer.py:468] tlayer: End windows.
I0123 18:33:55.217533 140408815788032 transformer_layer.py:472] tlayer: final FFN.
I0123 18:33:55.217590 140408815788032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.217722 140408815788032 transformer_base.py:410] tbase: post-attention MLP.
I0123 18:33:55.217831 140408815788032 nn_components.py:325] mlp: activation = None
I0123 18:33:55.217868 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:33:55.219702 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.219794 140408815788032 nn_components.py:261] mlp: residual
I0123 18:33:55.222141 140408815788032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.222226 140408815788032 transformer_base.py:443] tbase: final FFN
I0123 18:33:55.222333 140408815788032 nn_components.py:320] mlp: hidden 4096, relu
I0123 18:33:55.224579 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:33:55.226401 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.226495 140408815788032 nn_components.py:261] mlp: residual
I0123 18:33:55.226775 140408815788032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.226857 140408815788032 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 18:33:55.226962 140408815788032 transformer_layer.py:154] tlayer: recurrent = False
I0123 18:33:55.227000 140408815788032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 18:33:55.227029 140408815788032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 18:33:55.227091 140408815788032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.229280 140408815788032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 18:33:55.234611 140408815788032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.234868 140408815788032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 18:33:55.237529 140408815788032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 18:33:55.249891 140408815788032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 18:33:55.249945 140408815788032 attention.py:418] Single window, no scan.
I0123 18:33:55.249980 140408815788032 transformer_layer.py:389] tlayer: self-attention.
I0123 18:33:55.250009 140408815788032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.250071 140408815788032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.250629 140408815788032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.250704 140408815788032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.251054 140408815788032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.251726 140408815788032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.254233 140408815788032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.254852 140408815788032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.254929 140408815788032 transformer_layer.py:468] tlayer: End windows.
I0123 18:33:55.254963 140408815788032 transformer_layer.py:472] tlayer: final FFN.
I0123 18:33:55.255019 140408815788032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.255143 140408815788032 transformer_base.py:410] tbase: post-attention MLP.
I0123 18:33:55.255248 140408815788032 nn_components.py:325] mlp: activation = None
I0123 18:33:55.255286 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:33:55.257607 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.257708 140408815788032 nn_components.py:261] mlp: residual
I0123 18:33:55.260047 140408815788032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.260125 140408815788032 transformer_base.py:443] tbase: final FFN
I0123 18:33:55.260236 140408815788032 nn_components.py:320] mlp: hidden 4096, relu
I0123 18:33:55.262430 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:33:55.264225 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.264317 140408815788032 nn_components.py:261] mlp: residual
I0123 18:33:55.264597 140408815788032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.264676 140408815788032 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 18:33:55.264779 140408815788032 transformer_layer.py:154] tlayer: recurrent = False
I0123 18:33:55.264816 140408815788032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 18:33:55.264847 140408815788032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 18:33:55.264907 140408815788032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.267096 140408815788032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 18:33:55.272371 140408815788032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.272623 140408815788032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 18:33:55.275250 140408815788032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 18:33:55.287539 140408815788032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 18:33:55.287592 140408815788032 attention.py:418] Single window, no scan.
I0123 18:33:55.287627 140408815788032 transformer_layer.py:389] tlayer: self-attention.
I0123 18:33:55.287656 140408815788032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.287717 140408815788032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.288272 140408815788032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.288347 140408815788032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.288699 140408815788032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.289366 140408815788032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.291871 140408815788032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.292483 140408815788032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.292559 140408815788032 transformer_layer.py:468] tlayer: End windows.
I0123 18:33:55.292593 140408815788032 transformer_layer.py:472] tlayer: final FFN.
I0123 18:33:55.292649 140408815788032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.292779 140408815788032 transformer_base.py:410] tbase: post-attention MLP.
I0123 18:33:55.292888 140408815788032 nn_components.py:325] mlp: activation = None
I0123 18:33:55.292927 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:33:55.294762 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.294854 140408815788032 nn_components.py:261] mlp: residual
I0123 18:33:55.297188 140408815788032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.297267 140408815788032 transformer_base.py:443] tbase: final FFN
I0123 18:33:55.297373 140408815788032 nn_components.py:320] mlp: hidden 4096, relu
I0123 18:33:55.299609 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:33:55.301418 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.301510 140408815788032 nn_components.py:261] mlp: residual
I0123 18:33:55.301795 140408815788032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.301874 140408815788032 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 18:33:55.301978 140408815788032 transformer_layer.py:154] tlayer: recurrent = False
I0123 18:33:55.302014 140408815788032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 18:33:55.302043 140408815788032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 18:33:55.302102 140408815788032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.304287 140408815788032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 18:33:55.309593 140408815788032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.309855 140408815788032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 18:33:55.312499 140408815788032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 18:33:55.324950 140408815788032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 18:33:55.325004 140408815788032 attention.py:418] Single window, no scan.
I0123 18:33:55.325038 140408815788032 transformer_layer.py:389] tlayer: self-attention.
I0123 18:33:55.325067 140408815788032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.325132 140408815788032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.325699 140408815788032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.325776 140408815788032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.326135 140408815788032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.326821 140408815788032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.329698 140408815788032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.330319 140408815788032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.330397 140408815788032 transformer_layer.py:468] tlayer: End windows.
I0123 18:33:55.330430 140408815788032 transformer_layer.py:472] tlayer: final FFN.
I0123 18:33:55.330486 140408815788032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.330611 140408815788032 transformer_base.py:410] tbase: post-attention MLP.
I0123 18:33:55.330718 140408815788032 nn_components.py:325] mlp: activation = None
I0123 18:33:55.330756 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:33:55.332574 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.332665 140408815788032 nn_components.py:261] mlp: residual
I0123 18:33:55.335014 140408815788032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.335093 140408815788032 transformer_base.py:443] tbase: final FFN
I0123 18:33:55.335198 140408815788032 nn_components.py:320] mlp: hidden 4096, relu
I0123 18:33:55.337437 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:33:55.339260 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.339354 140408815788032 nn_components.py:261] mlp: residual
I0123 18:33:55.339635 140408815788032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:55.339720 140408815788032 decoder_stack.py:344] dstack: Final layernorm.
I0123 18:33:55.342527 140408815788032 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:33:59.779905 140408815788032 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
I0123 18:34:00.299917 140408815788032 training_loop.py:409] No working directory specified.
I0123 18:34:00.300060 140408815788032 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0123 18:34:00.300848 140408815788032 checkpoints.py:1062] Restoring legacy Flax checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0123 18:34:03.679124 140408815788032 training_loop.py:447] Only restoring trainable parameters.
I0123 18:34:03.679767 140408815788032 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0123 18:34:03.679847 140408815788032 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0123 18:34:03.679898 140408815788032 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 18:34:03.679942 140408815788032 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 18:34:03.679985 140408815788032 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 18:34:03.680026 140408815788032 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 18:34:03.680068 140408815788032 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 18:34:03.680109 140408815788032 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 18:34:03.680148 140408815788032 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 18:34:03.680187 140408815788032 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 18:34:03.680226 140408815788032 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 18:34:03.680265 140408815788032 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 18:34:03.680305 140408815788032 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 18:34:03.680344 140408815788032 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 18:34:03.680382 140408815788032 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 18:34:03.680422 140408815788032 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 18:34:03.680461 140408815788032 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 18:34:03.680500 140408815788032 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 18:34:03.680538 140408815788032 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 18:34:03.680576 140408815788032 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 18:34:03.680629 140408815788032 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 18:34:03.680668 140408815788032 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 18:34:03.680706 140408815788032 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 18:34:03.680743 140408815788032 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 18:34:03.680780 140408815788032 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 18:34:03.680817 140408815788032 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 18:34:03.680855 140408815788032 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 18:34:03.680892 140408815788032 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 18:34:03.680930 140408815788032 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 18:34:03.680970 140408815788032 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 18:34:03.681007 140408815788032 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 18:34:03.681044 140408815788032 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 18:34:03.681081 140408815788032 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 18:34:03.681118 140408815788032 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 18:34:03.681156 140408815788032 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 18:34:03.681192 140408815788032 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 18:34:03.681229 140408815788032 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 18:34:03.681267 140408815788032 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 18:34:03.681303 140408815788032 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 18:34:03.681340 140408815788032 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 18:34:03.681376 140408815788032 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 18:34:03.681412 140408815788032 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 18:34:03.681449 140408815788032 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 18:34:03.681485 140408815788032 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 18:34:03.681522 140408815788032 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 18:34:03.681559 140408815788032 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 18:34:03.681601 140408815788032 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 18:34:03.681638 140408815788032 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 18:34:03.681684 140408815788032 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 18:34:03.681722 140408815788032 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 18:34:03.681759 140408815788032 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 18:34:03.681796 140408815788032 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 18:34:03.681833 140408815788032 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 18:34:03.681869 140408815788032 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 18:34:03.681906 140408815788032 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 18:34:03.681943 140408815788032 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 18:34:03.681980 140408815788032 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 18:34:03.682017 140408815788032 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 18:34:03.682053 140408815788032 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 18:34:03.682091 140408815788032 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 18:34:03.682128 140408815788032 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 18:34:03.682166 140408815788032 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 18:34:03.682203 140408815788032 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 18:34:03.682240 140408815788032 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 18:34:03.682278 140408815788032 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 18:34:03.682316 140408815788032 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 18:34:03.682353 140408815788032 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 18:34:03.682390 140408815788032 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 18:34:03.682426 140408815788032 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 18:34:03.682463 140408815788032 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 18:34:03.682501 140408815788032 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 18:34:03.682537 140408815788032 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 18:34:03.682574 140408815788032 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 18:34:03.682616 140408815788032 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 18:34:03.682654 140408815788032 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 18:34:03.682691 140408815788032 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 18:34:03.682728 140408815788032 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 18:34:03.682765 140408815788032 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 18:34:03.682802 140408815788032 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 18:34:03.682839 140408815788032 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 18:34:03.682876 140408815788032 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 18:34:03.682913 140408815788032 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 18:34:03.682951 140408815788032 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 18:34:03.682987 140408815788032 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 18:34:03.683024 140408815788032 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 18:34:03.683060 140408815788032 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 18:34:03.683096 140408815788032 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 18:34:03.683133 140408815788032 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 18:34:03.683169 140408815788032 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 18:34:03.683206 140408815788032 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 18:34:03.683242 140408815788032 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 18:34:03.683278 140408815788032 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 18:34:03.683315 140408815788032 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 18:34:03.683351 140408815788032 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 18:34:03.683388 140408815788032 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 18:34:03.683424 140408815788032 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 18:34:03.683461 140408815788032 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 18:34:03.683498 140408815788032 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 18:34:03.683534 140408815788032 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 18:34:03.683570 140408815788032 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 18:34:03.683611 140408815788032 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 18:34:03.683648 140408815788032 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 18:34:03.683685 140408815788032 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 18:34:03.683722 140408815788032 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 18:34:03.683758 140408815788032 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 18:34:03.683794 140408815788032 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 18:34:03.683831 140408815788032 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 18:34:03.683868 140408815788032 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 18:34:03.683905 140408815788032 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 18:34:03.683942 140408815788032 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 18:34:03.683979 140408815788032 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 18:34:03.684015 140408815788032 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 18:34:03.684052 140408815788032 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 18:34:03.684088 140408815788032 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 18:34:03.684124 140408815788032 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 18:34:03.684160 140408815788032 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 18:34:03.684197 140408815788032 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 18:34:03.684233 140408815788032 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 18:34:03.684269 140408815788032 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 18:34:03.684304 140408815788032 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 18:34:03.684340 140408815788032 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 18:34:03.684377 140408815788032 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 18:34:03.684405 140408815788032 training_loop.py:725] Total parameters: 152072288
I0123 18:34:03.684639 140408815788032 training_loop.py:739] Total state size: 0
I0123 18:34:03.707991 140408815788032 training_loop.py:492] Training loop: creating task for mode beam_search
I0123 18:34:03.708258 140408815788032 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0123 18:34:03.708822 140408815788032 training_loop.py:652] Compiling mode beam_search with jit.
I0123 18:34:03.709158 140408815788032 training_loop.py:89] registering functions: dict_keys([])
I0123 18:34:03.725456 140408815788032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a, on_bline e a c; f = on_circle f d c, on_bline f c b; g = midpoint g e f; h = on_circle h d b; i = incenter i c b a; j = incenter j c b h; k = incenter k c a h; l = circle l h j k; m = foot m h l d; n = mirror n h m ? coll i g n
I0123 18:34:06.706280 140408815788032 ddar.py:60] Depth 1/1000 time = 2.7609822750091553
I0123 18:34:15.543498 140408815788032 ddar.py:60] Depth 2/1000 time = 8.837010145187378
I0123 18:34:31.382267 140408815788032 ddar.py:60] Depth 3/1000 time = 15.838469505310059
I0123 18:34:47.149446 140408815788032 ddar.py:60] Depth 4/1000 time = 15.766822814941406
I0123 18:35:02.641372 140408815788032 ddar.py:60] Depth 5/1000 time = 15.491390466690063
I0123 18:35:19.358260 140408815788032 ddar.py:60] Depth 6/1000 time = 16.712701559066772
I0123 18:35:37.933942 140408815788032 ddar.py:60] Depth 7/1000 time = 18.575441598892212
I0123 18:35:57.936283 140408815788032 ddar.py:60] Depth 8/1000 time = 20.002058267593384
I0123 18:36:22.867071 140408815788032 ddar.py:60] Depth 9/1000 time = 24.930368423461914
I0123 18:36:46.643459 140408815788032 ddar.py:60] Depth 10/1000 time = 23.776026964187622
I0123 18:37:10.605629 140408815788032 ddar.py:60] Depth 11/1000 time = 23.961879014968872
I0123 18:37:34.237443 140408815788032 ddar.py:60] Depth 12/1000 time = 23.631493091583252
I0123 18:37:58.381880 140408815788032 ddar.py:60] Depth 13/1000 time = 24.132142066955566
I0123 18:38:22.536267 140408815788032 ddar.py:60] Depth 14/1000 time = 24.154038190841675
I0123 18:38:46.586832 140408815788032 ddar.py:60] Depth 15/1000 time = 23.900001287460327
I0123 18:39:13.069142 140408815788032 ddar.py:60] Depth 16/1000 time = 26.4820339679718
I0123 18:39:47.936500 140408815788032 ddar.py:60] Depth 17/1000 time = 34.86704158782959
I0123 18:40:23.159906 140408815788032 ddar.py:60] Depth 18/1000 time = 35.22305727005005
I0123 18:40:58.709662 140408815788032 ddar.py:60] Depth 19/1000 time = 35.54930853843689
I0123 18:41:34.693169 140408815788032 ddar.py:60] Depth 20/1000 time = 35.792805194854736
I0123 18:41:35.001394 140408815788032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 18:41:35.001494 140408815788032 alphageometry.py:540] Depth 0. There are 1 nodes to expand:
I0123 18:41:35.001531 140408815788032 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : D a d d e 02 D a e c e 03 ^ c a c e a e a c 04 ; f : D b f c f 05 D c d d f 06 ^ c b c f b f b c 07 ; g : C e f g 08 D e g f g 09 ; h : D b d d h 10 ; i : ^ a b a i a i a c 11 ^ c a c i c i c b 12 ; j : ^ c b c j c j c h 13 ^ h b h j h j h c 14 ; k : ^ c a c k c k c h 15 ^ h a h k h k h c 16 ; l : D h l j l 17 D j l k l 18 ; m : C d l m 19 T d l h m 20 ; n : C h m n 21 D h m m n 22 ? C i g n {F1} x00
I0123 18:41:35.001563 140408815788032 alphageometry.py:549] Decoding from {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : D a d d e 02 D a e c e 03 ^ c a c e a e a c 04 ; f : D b f c f 05 D c d d f 06 ^ c b c f b f b c 07 ; g : C e f g 08 D e g f g 09 ; h : D b d d h 10 ; i : ^ a b a i a i a c 11 ^ c a c i c i c b 12 ; j : ^ c b c j c j c h 13 ^ h b h j h j h c 14 ; k : ^ c a c k c k c h 15 ^ h a h k h k h c 16 ; l : D h l j l 17 D j l k l 18 ; m : C d l m 19 T d l h m 20 ; n : C h m n 21 D h m m n 22 ? C i g n {F1} x00
I0123 18:41:35.145534 140408815788032 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.145716 140408815788032 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 18:41:35.145815 140408815788032 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 18:41:35.145891 140408815788032 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 18:41:35.145962 140408815788032 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 18:41:35.146031 140408815788032 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 18:41:35.146105 140408815788032 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 18:41:35.146174 140408815788032 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 18:41:35.146242 140408815788032 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 18:41:35.146308 140408815788032 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 18:41:35.146375 140408815788032 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 18:41:35.146441 140408815788032 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 18:41:35.146507 140408815788032 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 18:41:35.146574 140408815788032 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 18:41:35.146610 140408815788032 attention.py:418] Single window, no scan.
I0123 18:41:35.146653 140408815788032 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 18:41:35.146759 140408815788032 transformer_layer.py:154] tlayer: recurrent = False
I0123 18:41:35.146797 140408815788032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 18:41:35.146826 140408815788032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 18:41:35.148656 140408815788032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.151182 140408815788032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 18:41:35.156733 140408815788032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.157001 140408815788032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 18:41:35.159563 140408815788032 transformer_layer.py:213] tlayer: windowed attention.
I0123 18:41:35.163428 140408815788032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 18:41:35.163483 140408815788032 attention.py:418] Single window, no scan.
I0123 18:41:35.163518 140408815788032 transformer_layer.py:389] tlayer: self-attention.
I0123 18:41:35.163550 140408815788032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.163612 140408815788032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.164216 140408815788032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.164292 140408815788032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.164652 140408815788032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.165411 140408815788032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.167902 140408815788032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.168574 140408815788032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.168651 140408815788032 transformer_layer.py:468] tlayer: End windows.
I0123 18:41:35.168686 140408815788032 transformer_layer.py:472] tlayer: final FFN.
I0123 18:41:35.168743 140408815788032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.168870 140408815788032 transformer_base.py:410] tbase: post-attention MLP.
I0123 18:41:35.169191 140408815788032 nn_components.py:325] mlp: activation = None
I0123 18:41:35.169234 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:41:35.171113 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.171213 140408815788032 nn_components.py:261] mlp: residual
I0123 18:41:35.173638 140408815788032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.173725 140408815788032 transformer_base.py:443] tbase: final FFN
I0123 18:41:35.174148 140408815788032 nn_components.py:320] mlp: hidden 4096, relu
I0123 18:41:35.176487 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:41:35.178393 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.178488 140408815788032 nn_components.py:261] mlp: residual
I0123 18:41:35.178776 140408815788032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.178857 140408815788032 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 18:41:35.178965 140408815788032 transformer_layer.py:154] tlayer: recurrent = False
I0123 18:41:35.179002 140408815788032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 18:41:35.179031 140408815788032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 18:41:35.180783 140408815788032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.183067 140408815788032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 18:41:35.188637 140408815788032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.188893 140408815788032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 18:41:35.191626 140408815788032 transformer_layer.py:213] tlayer: windowed attention.
I0123 18:41:35.195266 140408815788032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 18:41:35.195320 140408815788032 attention.py:418] Single window, no scan.
I0123 18:41:35.195355 140408815788032 transformer_layer.py:389] tlayer: self-attention.
I0123 18:41:35.195385 140408815788032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.195447 140408815788032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.196061 140408815788032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.196137 140408815788032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.196492 140408815788032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.197246 140408815788032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.199687 140408815788032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.200297 140408815788032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.200372 140408815788032 transformer_layer.py:468] tlayer: End windows.
I0123 18:41:35.200407 140408815788032 transformer_layer.py:472] tlayer: final FFN.
I0123 18:41:35.200464 140408815788032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.200589 140408815788032 transformer_base.py:410] tbase: post-attention MLP.
I0123 18:41:35.200900 140408815788032 nn_components.py:325] mlp: activation = None
I0123 18:41:35.200942 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:41:35.202878 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.202976 140408815788032 nn_components.py:261] mlp: residual
I0123 18:41:35.205379 140408815788032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.205457 140408815788032 transformer_base.py:443] tbase: final FFN
I0123 18:41:35.205882 140408815788032 nn_components.py:320] mlp: hidden 4096, relu
I0123 18:41:35.208085 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:41:35.209962 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.210057 140408815788032 nn_components.py:261] mlp: residual
I0123 18:41:35.210344 140408815788032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.210423 140408815788032 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 18:41:35.210530 140408815788032 transformer_layer.py:154] tlayer: recurrent = False
I0123 18:41:35.210569 140408815788032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 18:41:35.210598 140408815788032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 18:41:35.212433 140408815788032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.214729 140408815788032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 18:41:35.220262 140408815788032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.220518 140408815788032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 18:41:35.223216 140408815788032 transformer_layer.py:213] tlayer: windowed attention.
I0123 18:41:35.226884 140408815788032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 18:41:35.226937 140408815788032 attention.py:418] Single window, no scan.
I0123 18:41:35.226970 140408815788032 transformer_layer.py:389] tlayer: self-attention.
I0123 18:41:35.226999 140408815788032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.227061 140408815788032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.227604 140408815788032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.227678 140408815788032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.228027 140408815788032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.228784 140408815788032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.231217 140408815788032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.231827 140408815788032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.231904 140408815788032 transformer_layer.py:468] tlayer: End windows.
I0123 18:41:35.231938 140408815788032 transformer_layer.py:472] tlayer: final FFN.
I0123 18:41:35.231994 140408815788032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.232120 140408815788032 transformer_base.py:410] tbase: post-attention MLP.
I0123 18:41:35.232478 140408815788032 nn_components.py:325] mlp: activation = None
I0123 18:41:35.232521 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:41:35.234401 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.234494 140408815788032 nn_components.py:261] mlp: residual
I0123 18:41:35.236888 140408815788032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.236972 140408815788032 transformer_base.py:443] tbase: final FFN
I0123 18:41:35.237391 140408815788032 nn_components.py:320] mlp: hidden 4096, relu
I0123 18:41:35.239617 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:41:35.241937 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.242033 140408815788032 nn_components.py:261] mlp: residual
I0123 18:41:35.242321 140408815788032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.242401 140408815788032 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 18:41:35.242507 140408815788032 transformer_layer.py:154] tlayer: recurrent = False
I0123 18:41:35.242546 140408815788032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 18:41:35.242576 140408815788032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 18:41:35.244354 140408815788032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.246669 140408815788032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 18:41:35.252262 140408815788032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.252518 140408815788032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 18:41:35.255070 140408815788032 transformer_layer.py:213] tlayer: windowed attention.
I0123 18:41:35.258700 140408815788032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 18:41:35.258755 140408815788032 attention.py:418] Single window, no scan.
I0123 18:41:35.258788 140408815788032 transformer_layer.py:389] tlayer: self-attention.
I0123 18:41:35.258818 140408815788032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.258879 140408815788032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.259479 140408815788032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.259554 140408815788032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.259905 140408815788032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.260657 140408815788032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.263081 140408815788032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.263694 140408815788032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.263771 140408815788032 transformer_layer.py:468] tlayer: End windows.
I0123 18:41:35.263804 140408815788032 transformer_layer.py:472] tlayer: final FFN.
I0123 18:41:35.263860 140408815788032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.263986 140408815788032 transformer_base.py:410] tbase: post-attention MLP.
I0123 18:41:35.264294 140408815788032 nn_components.py:325] mlp: activation = None
I0123 18:41:35.264335 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:41:35.266266 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.266358 140408815788032 nn_components.py:261] mlp: residual
I0123 18:41:35.268761 140408815788032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.268845 140408815788032 transformer_base.py:443] tbase: final FFN
I0123 18:41:35.269263 140408815788032 nn_components.py:320] mlp: hidden 4096, relu
I0123 18:41:35.271474 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:41:35.273354 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.273448 140408815788032 nn_components.py:261] mlp: residual
I0123 18:41:35.273741 140408815788032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.273824 140408815788032 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 18:41:35.273931 140408815788032 transformer_layer.py:154] tlayer: recurrent = False
I0123 18:41:35.273968 140408815788032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 18:41:35.273998 140408815788032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 18:41:35.275822 140408815788032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.278105 140408815788032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 18:41:35.283531 140408815788032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.283782 140408815788032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 18:41:35.286314 140408815788032 transformer_layer.py:213] tlayer: windowed attention.
I0123 18:41:35.289985 140408815788032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 18:41:35.290038 140408815788032 attention.py:418] Single window, no scan.
I0123 18:41:35.290072 140408815788032 transformer_layer.py:389] tlayer: self-attention.
I0123 18:41:35.290102 140408815788032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.290164 140408815788032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.290713 140408815788032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.290789 140408815788032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.291139 140408815788032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.291890 140408815788032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.294312 140408815788032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.294922 140408815788032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.294999 140408815788032 transformer_layer.py:468] tlayer: End windows.
I0123 18:41:35.295032 140408815788032 transformer_layer.py:472] tlayer: final FFN.
I0123 18:41:35.295089 140408815788032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.295214 140408815788032 transformer_base.py:410] tbase: post-attention MLP.
I0123 18:41:35.295577 140408815788032 nn_components.py:325] mlp: activation = None
I0123 18:41:35.295619 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:41:35.297506 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.297599 140408815788032 nn_components.py:261] mlp: residual
I0123 18:41:35.300014 140408815788032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.300093 140408815788032 transformer_base.py:443] tbase: final FFN
I0123 18:41:35.300516 140408815788032 nn_components.py:320] mlp: hidden 4096, relu
I0123 18:41:35.302726 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:41:35.304655 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.304749 140408815788032 nn_components.py:261] mlp: residual
I0123 18:41:35.305036 140408815788032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.305116 140408815788032 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 18:41:35.305223 140408815788032 transformer_layer.py:154] tlayer: recurrent = False
I0123 18:41:35.305261 140408815788032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 18:41:35.305291 140408815788032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 18:41:35.307039 140408815788032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.309303 140408815788032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 18:41:35.314815 140408815788032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.315069 140408815788032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 18:41:35.317563 140408815788032 transformer_layer.py:213] tlayer: windowed attention.
I0123 18:41:35.321252 140408815788032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 18:41:35.321306 140408815788032 attention.py:418] Single window, no scan.
I0123 18:41:35.321340 140408815788032 transformer_layer.py:389] tlayer: self-attention.
I0123 18:41:35.321369 140408815788032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.321430 140408815788032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.322038 140408815788032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.322115 140408815788032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.322464 140408815788032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.323203 140408815788032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.325787 140408815788032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.326394 140408815788032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.326469 140408815788032 transformer_layer.py:468] tlayer: End windows.
I0123 18:41:35.326502 140408815788032 transformer_layer.py:472] tlayer: final FFN.
I0123 18:41:35.326557 140408815788032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.326700 140408815788032 transformer_base.py:410] tbase: post-attention MLP.
I0123 18:41:35.327013 140408815788032 nn_components.py:325] mlp: activation = None
I0123 18:41:35.327054 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:41:35.328958 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.329051 140408815788032 nn_components.py:261] mlp: residual
I0123 18:41:35.331449 140408815788032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.331528 140408815788032 transformer_base.py:443] tbase: final FFN
I0123 18:41:35.331940 140408815788032 nn_components.py:320] mlp: hidden 4096, relu
I0123 18:41:35.334138 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:41:35.336007 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.336099 140408815788032 nn_components.py:261] mlp: residual
I0123 18:41:35.336382 140408815788032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.336460 140408815788032 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 18:41:35.336564 140408815788032 transformer_layer.py:154] tlayer: recurrent = False
I0123 18:41:35.336601 140408815788032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 18:41:35.336630 140408815788032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 18:41:35.338453 140408815788032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.340715 140408815788032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 18:41:35.346192 140408815788032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.346444 140408815788032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 18:41:35.348962 140408815788032 transformer_layer.py:213] tlayer: windowed attention.
I0123 18:41:35.352936 140408815788032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 18:41:35.352990 140408815788032 attention.py:418] Single window, no scan.
I0123 18:41:35.353024 140408815788032 transformer_layer.py:389] tlayer: self-attention.
I0123 18:41:35.353054 140408815788032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.353115 140408815788032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.353671 140408815788032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.353748 140408815788032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.354098 140408815788032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.354857 140408815788032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.357260 140408815788032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.357879 140408815788032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.357957 140408815788032 transformer_layer.py:468] tlayer: End windows.
I0123 18:41:35.357990 140408815788032 transformer_layer.py:472] tlayer: final FFN.
I0123 18:41:35.358047 140408815788032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.358172 140408815788032 transformer_base.py:410] tbase: post-attention MLP.
I0123 18:41:35.527468 140408815788032 nn_components.py:325] mlp: activation = None
I0123 18:41:35.527597 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:41:35.530053 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.530192 140408815788032 nn_components.py:261] mlp: residual
I0123 18:41:35.532850 140408815788032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.532941 140408815788032 transformer_base.py:443] tbase: final FFN
I0123 18:41:35.533385 140408815788032 nn_components.py:320] mlp: hidden 4096, relu
I0123 18:41:35.535817 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:41:35.537796 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.537894 140408815788032 nn_components.py:261] mlp: residual
I0123 18:41:35.538194 140408815788032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.538282 140408815788032 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 18:41:35.538392 140408815788032 transformer_layer.py:154] tlayer: recurrent = False
I0123 18:41:35.538431 140408815788032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 18:41:35.538463 140408815788032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 18:41:35.540293 140408815788032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.542702 140408815788032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 18:41:35.548368 140408815788032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.548638 140408815788032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 18:41:35.551234 140408815788032 transformer_layer.py:213] tlayer: windowed attention.
I0123 18:41:35.555040 140408815788032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 18:41:35.555097 140408815788032 attention.py:418] Single window, no scan.
I0123 18:41:35.555134 140408815788032 transformer_layer.py:389] tlayer: self-attention.
I0123 18:41:35.555166 140408815788032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.555228 140408815788032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.555838 140408815788032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.555914 140408815788032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.556279 140408815788032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.557049 140408815788032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.559573 140408815788032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.560195 140408815788032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.560273 140408815788032 transformer_layer.py:468] tlayer: End windows.
I0123 18:41:35.560308 140408815788032 transformer_layer.py:472] tlayer: final FFN.
I0123 18:41:35.560367 140408815788032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.560495 140408815788032 transformer_base.py:410] tbase: post-attention MLP.
I0123 18:41:35.560814 140408815788032 nn_components.py:325] mlp: activation = None
I0123 18:41:35.560856 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:41:35.562855 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.562949 140408815788032 nn_components.py:261] mlp: residual
I0123 18:41:35.565387 140408815788032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.565466 140408815788032 transformer_base.py:443] tbase: final FFN
I0123 18:41:35.565896 140408815788032 nn_components.py:320] mlp: hidden 4096, relu
I0123 18:41:35.568140 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:41:35.570053 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.570156 140408815788032 nn_components.py:261] mlp: residual
I0123 18:41:35.570451 140408815788032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.570533 140408815788032 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 18:41:35.570642 140408815788032 transformer_layer.py:154] tlayer: recurrent = False
I0123 18:41:35.570682 140408815788032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 18:41:35.570712 140408815788032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 18:41:35.572574 140408815788032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.574912 140408815788032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 18:41:35.580522 140408815788032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.580783 140408815788032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 18:41:35.583459 140408815788032 transformer_layer.py:213] tlayer: windowed attention.
I0123 18:41:35.587168 140408815788032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 18:41:35.587222 140408815788032 attention.py:418] Single window, no scan.
I0123 18:41:35.587257 140408815788032 transformer_layer.py:389] tlayer: self-attention.
I0123 18:41:35.587286 140408815788032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.587348 140408815788032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.587907 140408815788032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.587984 140408815788032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.588336 140408815788032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.589098 140408815788032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.591602 140408815788032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.592218 140408815788032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.592296 140408815788032 transformer_layer.py:468] tlayer: End windows.
I0123 18:41:35.592331 140408815788032 transformer_layer.py:472] tlayer: final FFN.
I0123 18:41:35.592391 140408815788032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.592518 140408815788032 transformer_base.py:410] tbase: post-attention MLP.
I0123 18:41:35.592835 140408815788032 nn_components.py:325] mlp: activation = None
I0123 18:41:35.592877 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:41:35.594802 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.594896 140408815788032 nn_components.py:261] mlp: residual
I0123 18:41:35.597342 140408815788032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.597421 140408815788032 transformer_base.py:443] tbase: final FFN
I0123 18:41:35.597854 140408815788032 nn_components.py:320] mlp: hidden 4096, relu
I0123 18:41:35.600184 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:41:35.602096 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.602192 140408815788032 nn_components.py:261] mlp: residual
I0123 18:41:35.602494 140408815788032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.602575 140408815788032 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 18:41:35.602683 140408815788032 transformer_layer.py:154] tlayer: recurrent = False
I0123 18:41:35.602721 140408815788032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 18:41:35.602753 140408815788032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 18:41:35.604526 140408815788032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.606884 140408815788032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 18:41:35.612537 140408815788032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.612794 140408815788032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 18:41:35.615381 140408815788032 transformer_layer.py:213] tlayer: windowed attention.
I0123 18:41:35.619063 140408815788032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 18:41:35.619117 140408815788032 attention.py:418] Single window, no scan.
I0123 18:41:35.619152 140408815788032 transformer_layer.py:389] tlayer: self-attention.
I0123 18:41:35.619182 140408815788032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.619244 140408815788032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.619798 140408815788032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.619874 140408815788032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.620226 140408815788032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.620989 140408815788032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.623430 140408815788032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.624047 140408815788032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.624124 140408815788032 transformer_layer.py:468] tlayer: End windows.
I0123 18:41:35.624158 140408815788032 transformer_layer.py:472] tlayer: final FFN.
I0123 18:41:35.624215 140408815788032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.624341 140408815788032 transformer_base.py:410] tbase: post-attention MLP.
I0123 18:41:35.624659 140408815788032 nn_components.py:325] mlp: activation = None
I0123 18:41:35.624701 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:41:35.627024 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.627118 140408815788032 nn_components.py:261] mlp: residual
I0123 18:41:35.629542 140408815788032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.629621 140408815788032 transformer_base.py:443] tbase: final FFN
I0123 18:41:35.630060 140408815788032 nn_components.py:320] mlp: hidden 4096, relu
I0123 18:41:35.632308 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:41:35.634201 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.634296 140408815788032 nn_components.py:261] mlp: residual
I0123 18:41:35.634584 140408815788032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.634670 140408815788032 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 18:41:35.634776 140408815788032 transformer_layer.py:154] tlayer: recurrent = False
I0123 18:41:35.634814 140408815788032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 18:41:35.634845 140408815788032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 18:41:35.637006 140408815788032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.639306 140408815788032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 18:41:35.644806 140408815788032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.645063 140408815788032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 18:41:35.647646 140408815788032 transformer_layer.py:213] tlayer: windowed attention.
I0123 18:41:35.651266 140408815788032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 18:41:35.651320 140408815788032 attention.py:418] Single window, no scan.
I0123 18:41:35.651355 140408815788032 transformer_layer.py:389] tlayer: self-attention.
I0123 18:41:35.651386 140408815788032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.651449 140408815788032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.652001 140408815788032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.652075 140408815788032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.652425 140408815788032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.653176 140408815788032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.655635 140408815788032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.656241 140408815788032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.656316 140408815788032 transformer_layer.py:468] tlayer: End windows.
I0123 18:41:35.656349 140408815788032 transformer_layer.py:472] tlayer: final FFN.
I0123 18:41:35.656405 140408815788032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.656531 140408815788032 transformer_base.py:410] tbase: post-attention MLP.
I0123 18:41:35.656843 140408815788032 nn_components.py:325] mlp: activation = None
I0123 18:41:35.656883 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:41:35.658763 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.658855 140408815788032 nn_components.py:261] mlp: residual
I0123 18:41:35.661249 140408815788032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.661327 140408815788032 transformer_base.py:443] tbase: final FFN
I0123 18:41:35.661751 140408815788032 nn_components.py:320] mlp: hidden 4096, relu
I0123 18:41:35.664026 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:41:35.665919 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.666013 140408815788032 nn_components.py:261] mlp: residual
I0123 18:41:35.666296 140408815788032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.666376 140408815788032 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 18:41:35.666487 140408815788032 transformer_layer.py:154] tlayer: recurrent = False
I0123 18:41:35.666525 140408815788032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 18:41:35.666554 140408815788032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 18:41:35.668303 140408815788032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.670573 140408815788032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 18:41:35.676177 140408815788032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.676431 140408815788032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 18:41:35.678974 140408815788032 transformer_layer.py:213] tlayer: windowed attention.
I0123 18:41:35.682622 140408815788032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 18:41:35.682675 140408815788032 attention.py:418] Single window, no scan.
I0123 18:41:35.682709 140408815788032 transformer_layer.py:389] tlayer: self-attention.
I0123 18:41:35.682739 140408815788032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.682801 140408815788032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.683351 140408815788032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.683426 140408815788032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.683778 140408815788032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.684524 140408815788032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.686924 140408815788032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.687540 140408815788032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.687616 140408815788032 transformer_layer.py:468] tlayer: End windows.
I0123 18:41:35.687649 140408815788032 transformer_layer.py:472] tlayer: final FFN.
I0123 18:41:35.687705 140408815788032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.687829 140408815788032 transformer_base.py:410] tbase: post-attention MLP.
I0123 18:41:35.688139 140408815788032 nn_components.py:325] mlp: activation = None
I0123 18:41:35.688181 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:41:35.690143 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.690238 140408815788032 nn_components.py:261] mlp: residual
I0123 18:41:35.692655 140408815788032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.692734 140408815788032 transformer_base.py:443] tbase: final FFN
I0123 18:41:35.693153 140408815788032 nn_components.py:320] mlp: hidden 4096, relu
I0123 18:41:35.695378 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:41:35.697265 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.697360 140408815788032 nn_components.py:261] mlp: residual
I0123 18:41:35.697656 140408815788032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.697908 140408815788032 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 18:41:35.697983 140408815788032 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 18:41:35.698041 140408815788032 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 18:41:35.698095 140408815788032 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 18:41:35.698148 140408815788032 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 18:41:35.698200 140408815788032 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 18:41:35.698253 140408815788032 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 18:41:35.698305 140408815788032 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 18:41:35.698357 140408815788032 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 18:41:35.698409 140408815788032 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 18:41:35.698461 140408815788032 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 18:41:35.698512 140408815788032 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 18:41:35.698548 140408815788032 decoder_stack.py:344] dstack: Final layernorm.
I0123 18:41:35.701475 140408815788032 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 18:41:35.745991 140408815788032 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.746076 140408815788032 decoder_stack.py:333] dstack: autoregressive generator.
I0123 18:41:35.746129 140408815788032 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 18:41:35.746232 140408815788032 transformer_layer.py:154] tlayer: recurrent = False
I0123 18:41:35.746270 140408815788032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 18:41:35.746299 140408815788032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 18:41:35.746359 140408815788032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.748701 140408815788032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 18:41:35.754007 140408815788032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.754264 140408815788032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 18:41:35.756793 140408815788032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 18:41:35.769332 140408815788032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 18:41:35.769388 140408815788032 attention.py:418] Single window, no scan.
I0123 18:41:35.769423 140408815788032 transformer_layer.py:389] tlayer: self-attention.
I0123 18:41:35.769452 140408815788032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.769513 140408815788032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.770132 140408815788032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.770211 140408815788032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.770572 140408815788032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.771253 140408815788032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.773672 140408815788032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.774285 140408815788032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.774367 140408815788032 transformer_layer.py:468] tlayer: End windows.
I0123 18:41:35.774401 140408815788032 transformer_layer.py:472] tlayer: final FFN.
I0123 18:41:35.774458 140408815788032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.774587 140408815788032 transformer_base.py:410] tbase: post-attention MLP.
I0123 18:41:35.774693 140408815788032 nn_components.py:325] mlp: activation = None
I0123 18:41:35.774730 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:41:35.776550 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.776642 140408815788032 nn_components.py:261] mlp: residual
I0123 18:41:35.779044 140408815788032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.779123 140408815788032 transformer_base.py:443] tbase: final FFN
I0123 18:41:35.779227 140408815788032 nn_components.py:320] mlp: hidden 4096, relu
I0123 18:41:35.781367 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:41:35.783191 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.783286 140408815788032 nn_components.py:261] mlp: residual
I0123 18:41:35.783573 140408815788032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.783653 140408815788032 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 18:41:35.783761 140408815788032 transformer_layer.py:154] tlayer: recurrent = False
I0123 18:41:35.783799 140408815788032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 18:41:35.783829 140408815788032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 18:41:35.783890 140408815788032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.786085 140408815788032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 18:41:35.791473 140408815788032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.791732 140408815788032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 18:41:35.794388 140408815788032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 18:41:35.807078 140408815788032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 18:41:35.807134 140408815788032 attention.py:418] Single window, no scan.
I0123 18:41:35.807169 140408815788032 transformer_layer.py:389] tlayer: self-attention.
I0123 18:41:35.807199 140408815788032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.807260 140408815788032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.807810 140408815788032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.807885 140408815788032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.808236 140408815788032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.808900 140408815788032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.811334 140408815788032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.811934 140408815788032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.812018 140408815788032 transformer_layer.py:468] tlayer: End windows.
I0123 18:41:35.812053 140408815788032 transformer_layer.py:472] tlayer: final FFN.
I0123 18:41:35.812110 140408815788032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.812237 140408815788032 transformer_base.py:410] tbase: post-attention MLP.
I0123 18:41:35.812345 140408815788032 nn_components.py:325] mlp: activation = None
I0123 18:41:35.812383 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:41:35.814185 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.814277 140408815788032 nn_components.py:261] mlp: residual
I0123 18:41:35.816609 140408815788032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.816686 140408815788032 transformer_base.py:443] tbase: final FFN
I0123 18:41:35.816792 140408815788032 nn_components.py:320] mlp: hidden 4096, relu
I0123 18:41:35.818994 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:41:35.820799 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.820892 140408815788032 nn_components.py:261] mlp: residual
I0123 18:41:35.821179 140408815788032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.821259 140408815788032 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 18:41:35.821365 140408815788032 transformer_layer.py:154] tlayer: recurrent = False
I0123 18:41:35.821403 140408815788032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 18:41:35.821432 140408815788032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 18:41:35.821493 140408815788032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.823662 140408815788032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 18:41:35.829188 140408815788032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.829445 140408815788032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 18:41:35.832164 140408815788032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 18:41:35.844362 140408815788032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 18:41:35.844417 140408815788032 attention.py:418] Single window, no scan.
I0123 18:41:35.844451 140408815788032 transformer_layer.py:389] tlayer: self-attention.
I0123 18:41:35.844481 140408815788032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.844545 140408815788032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.845092 140408815788032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.845170 140408815788032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.845524 140408815788032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.846205 140408815788032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.848689 140408815788032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.849299 140408815788032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.849376 140408815788032 transformer_layer.py:468] tlayer: End windows.
I0123 18:41:35.849415 140408815788032 transformer_layer.py:472] tlayer: final FFN.
I0123 18:41:35.849474 140408815788032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.849600 140408815788032 transformer_base.py:410] tbase: post-attention MLP.
I0123 18:41:35.849712 140408815788032 nn_components.py:325] mlp: activation = None
I0123 18:41:35.849751 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:41:35.851573 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.851664 140408815788032 nn_components.py:261] mlp: residual
I0123 18:41:35.854064 140408815788032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.854143 140408815788032 transformer_base.py:443] tbase: final FFN
I0123 18:41:35.854249 140408815788032 nn_components.py:320] mlp: hidden 4096, relu
I0123 18:41:35.856460 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:41:35.858285 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.858385 140408815788032 nn_components.py:261] mlp: residual
I0123 18:41:35.858673 140408815788032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.858755 140408815788032 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 18:41:35.858860 140408815788032 transformer_layer.py:154] tlayer: recurrent = False
I0123 18:41:35.858899 140408815788032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 18:41:35.858930 140408815788032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 18:41:35.858992 140408815788032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.861192 140408815788032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 18:41:35.866576 140408815788032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.866829 140408815788032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 18:41:35.869439 140408815788032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 18:41:35.881964 140408815788032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 18:41:35.882019 140408815788032 attention.py:418] Single window, no scan.
I0123 18:41:35.882054 140408815788032 transformer_layer.py:389] tlayer: self-attention.
I0123 18:41:35.882084 140408815788032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.882147 140408815788032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.882705 140408815788032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.882782 140408815788032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.883136 140408815788032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.883818 140408815788032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.886246 140408815788032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.886863 140408815788032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.886940 140408815788032 transformer_layer.py:468] tlayer: End windows.
I0123 18:41:35.886974 140408815788032 transformer_layer.py:472] tlayer: final FFN.
I0123 18:41:35.887037 140408815788032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.887166 140408815788032 transformer_base.py:410] tbase: post-attention MLP.
I0123 18:41:35.887273 140408815788032 nn_components.py:325] mlp: activation = None
I0123 18:41:35.887310 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:41:35.889544 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.889636 140408815788032 nn_components.py:261] mlp: residual
I0123 18:41:35.892115 140408815788032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.892194 140408815788032 transformer_base.py:443] tbase: final FFN
I0123 18:41:35.892301 140408815788032 nn_components.py:320] mlp: hidden 4096, relu
I0123 18:41:35.894491 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:41:35.896311 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.896405 140408815788032 nn_components.py:261] mlp: residual
I0123 18:41:35.896693 140408815788032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.896775 140408815788032 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 18:41:35.896883 140408815788032 transformer_layer.py:154] tlayer: recurrent = False
I0123 18:41:35.896922 140408815788032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 18:41:35.896952 140408815788032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 18:41:35.897012 140408815788032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.899225 140408815788032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 18:41:35.904610 140408815788032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.904870 140408815788032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 18:41:35.907448 140408815788032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 18:41:35.919749 140408815788032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 18:41:35.919802 140408815788032 attention.py:418] Single window, no scan.
I0123 18:41:35.919838 140408815788032 transformer_layer.py:389] tlayer: self-attention.
I0123 18:41:35.919868 140408815788032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.919928 140408815788032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.920474 140408815788032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.920549 140408815788032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.920896 140408815788032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.921557 140408815788032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.923956 140408815788032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.924610 140408815788032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.924688 140408815788032 transformer_layer.py:468] tlayer: End windows.
I0123 18:41:35.924721 140408815788032 transformer_layer.py:472] tlayer: final FFN.
I0123 18:41:35.924782 140408815788032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.924911 140408815788032 transformer_base.py:410] tbase: post-attention MLP.
I0123 18:41:35.925019 140408815788032 nn_components.py:325] mlp: activation = None
I0123 18:41:35.925057 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:41:35.926892 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.926986 140408815788032 nn_components.py:261] mlp: residual
I0123 18:41:35.929340 140408815788032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.929420 140408815788032 transformer_base.py:443] tbase: final FFN
I0123 18:41:35.929527 140408815788032 nn_components.py:320] mlp: hidden 4096, relu
I0123 18:41:35.931835 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:41:35.933712 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.933807 140408815788032 nn_components.py:261] mlp: residual
I0123 18:41:35.934093 140408815788032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.934174 140408815788032 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 18:41:35.934280 140408815788032 transformer_layer.py:154] tlayer: recurrent = False
I0123 18:41:35.934318 140408815788032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 18:41:35.934348 140408815788032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 18:41:35.934408 140408815788032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.936589 140408815788032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 18:41:35.941890 140408815788032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.942145 140408815788032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 18:41:35.944718 140408815788032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 18:41:35.956933 140408815788032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 18:41:35.956988 140408815788032 attention.py:418] Single window, no scan.
I0123 18:41:35.957024 140408815788032 transformer_layer.py:389] tlayer: self-attention.
I0123 18:41:35.957054 140408815788032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.957118 140408815788032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.957734 140408815788032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.957813 140408815788032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.958169 140408815788032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.958852 140408815788032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.961290 140408815788032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.961907 140408815788032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.961986 140408815788032 transformer_layer.py:468] tlayer: End windows.
I0123 18:41:35.962019 140408815788032 transformer_layer.py:472] tlayer: final FFN.
I0123 18:41:35.962076 140408815788032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.962211 140408815788032 transformer_base.py:410] tbase: post-attention MLP.
I0123 18:41:35.962320 140408815788032 nn_components.py:325] mlp: activation = None
I0123 18:41:35.962358 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:41:35.964177 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.964268 140408815788032 nn_components.py:261] mlp: residual
I0123 18:41:35.966703 140408815788032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.966783 140408815788032 transformer_base.py:443] tbase: final FFN
I0123 18:41:35.966892 140408815788032 nn_components.py:320] mlp: hidden 4096, relu
I0123 18:41:35.969044 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:41:35.970876 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.970971 140408815788032 nn_components.py:261] mlp: residual
I0123 18:41:35.971259 140408815788032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.971341 140408815788032 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 18:41:35.971449 140408815788032 transformer_layer.py:154] tlayer: recurrent = False
I0123 18:41:35.971487 140408815788032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 18:41:35.971518 140408815788032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 18:41:35.971578 140408815788032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.973769 140408815788032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 18:41:35.979166 140408815788032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.979421 140408815788032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 18:41:35.981936 140408815788032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 18:41:35.994567 140408815788032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 18:41:35.994622 140408815788032 attention.py:418] Single window, no scan.
I0123 18:41:35.994657 140408815788032 transformer_layer.py:389] tlayer: self-attention.
I0123 18:41:35.994687 140408815788032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.994749 140408815788032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.995299 140408815788032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.995375 140408815788032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.995725 140408815788032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.996396 140408815788032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.998807 140408815788032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.999471 140408815788032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.999548 140408815788032 transformer_layer.py:468] tlayer: End windows.
I0123 18:41:35.999582 140408815788032 transformer_layer.py:472] tlayer: final FFN.
I0123 18:41:35.999639 140408815788032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:35.999776 140408815788032 transformer_base.py:410] tbase: post-attention MLP.
I0123 18:41:35.999884 140408815788032 nn_components.py:325] mlp: activation = None
I0123 18:41:35.999922 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:41:36.001769 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.001863 140408815788032 nn_components.py:261] mlp: residual
I0123 18:41:36.004225 140408815788032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.004304 140408815788032 transformer_base.py:443] tbase: final FFN
I0123 18:41:36.004411 140408815788032 nn_components.py:320] mlp: hidden 4096, relu
I0123 18:41:36.006589 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:41:36.008460 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.008554 140408815788032 nn_components.py:261] mlp: residual
I0123 18:41:36.008842 140408815788032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.008924 140408815788032 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 18:41:36.009032 140408815788032 transformer_layer.py:154] tlayer: recurrent = False
I0123 18:41:36.009071 140408815788032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 18:41:36.009101 140408815788032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 18:41:36.009163 140408815788032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.011361 140408815788032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 18:41:36.016687 140408815788032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.016948 140408815788032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 18:41:36.019548 140408815788032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 18:41:36.031836 140408815788032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 18:41:36.031890 140408815788032 attention.py:418] Single window, no scan.
I0123 18:41:36.031924 140408815788032 transformer_layer.py:389] tlayer: self-attention.
I0123 18:41:36.031953 140408815788032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.032016 140408815788032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.032615 140408815788032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.032692 140408815788032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.033047 140408815788032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.033736 140408815788032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.036153 140408815788032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.036762 140408815788032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.036840 140408815788032 transformer_layer.py:468] tlayer: End windows.
I0123 18:41:36.036874 140408815788032 transformer_layer.py:472] tlayer: final FFN.
I0123 18:41:36.036930 140408815788032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.037057 140408815788032 transformer_base.py:410] tbase: post-attention MLP.
I0123 18:41:36.037171 140408815788032 nn_components.py:325] mlp: activation = None
I0123 18:41:36.037209 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:41:36.039036 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.039130 140408815788032 nn_components.py:261] mlp: residual
I0123 18:41:36.041553 140408815788032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.041632 140408815788032 transformer_base.py:443] tbase: final FFN
I0123 18:41:36.041748 140408815788032 nn_components.py:320] mlp: hidden 4096, relu
I0123 18:41:36.043902 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:41:36.045723 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.045819 140408815788032 nn_components.py:261] mlp: residual
I0123 18:41:36.046105 140408815788032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.046186 140408815788032 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 18:41:36.046290 140408815788032 transformer_layer.py:154] tlayer: recurrent = False
I0123 18:41:36.046329 140408815788032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 18:41:36.046358 140408815788032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 18:41:36.046418 140408815788032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.048586 140408815788032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 18:41:36.054001 140408815788032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.054260 140408815788032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 18:41:36.056797 140408815788032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 18:41:36.069114 140408815788032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 18:41:36.069169 140408815788032 attention.py:418] Single window, no scan.
I0123 18:41:36.069204 140408815788032 transformer_layer.py:389] tlayer: self-attention.
I0123 18:41:36.069234 140408815788032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.069296 140408815788032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.069859 140408815788032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.069937 140408815788032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.070294 140408815788032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.070976 140408815788032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.073401 140408815788032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.074080 140408815788032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.074159 140408815788032 transformer_layer.py:468] tlayer: End windows.
I0123 18:41:36.074193 140408815788032 transformer_layer.py:472] tlayer: final FFN.
I0123 18:41:36.074250 140408815788032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.074378 140408815788032 transformer_base.py:410] tbase: post-attention MLP.
I0123 18:41:36.074486 140408815788032 nn_components.py:325] mlp: activation = None
I0123 18:41:36.074532 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:41:36.076358 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.076452 140408815788032 nn_components.py:261] mlp: residual
I0123 18:41:36.078826 140408815788032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.078906 140408815788032 transformer_base.py:443] tbase: final FFN
I0123 18:41:36.079013 140408815788032 nn_components.py:320] mlp: hidden 4096, relu
I0123 18:41:36.081172 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:41:36.083055 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.083150 140408815788032 nn_components.py:261] mlp: residual
I0123 18:41:36.083439 140408815788032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.083521 140408815788032 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 18:41:36.083629 140408815788032 transformer_layer.py:154] tlayer: recurrent = False
I0123 18:41:36.083667 140408815788032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 18:41:36.083697 140408815788032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 18:41:36.083759 140408815788032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.085995 140408815788032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 18:41:36.091321 140408815788032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.091577 140408815788032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 18:41:36.094159 140408815788032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 18:41:36.106744 140408815788032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 18:41:36.106798 140408815788032 attention.py:418] Single window, no scan.
I0123 18:41:36.106832 140408815788032 transformer_layer.py:389] tlayer: self-attention.
I0123 18:41:36.106862 140408815788032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.106925 140408815788032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.107476 140408815788032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.107553 140408815788032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.107903 140408815788032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.108573 140408815788032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.111055 140408815788032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.111669 140408815788032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.111747 140408815788032 transformer_layer.py:468] tlayer: End windows.
I0123 18:41:36.111780 140408815788032 transformer_layer.py:472] tlayer: final FFN.
I0123 18:41:36.111837 140408815788032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.111963 140408815788032 transformer_base.py:410] tbase: post-attention MLP.
I0123 18:41:36.112068 140408815788032 nn_components.py:325] mlp: activation = None
I0123 18:41:36.112106 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:41:36.113938 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.114031 140408815788032 nn_components.py:261] mlp: residual
I0123 18:41:36.116375 140408815788032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.116453 140408815788032 transformer_base.py:443] tbase: final FFN
I0123 18:41:36.116559 140408815788032 nn_components.py:320] mlp: hidden 4096, relu
I0123 18:41:36.118786 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:41:36.120602 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.120695 140408815788032 nn_components.py:261] mlp: residual
I0123 18:41:36.120983 140408815788032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.121063 140408815788032 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 18:41:36.121171 140408815788032 transformer_layer.py:154] tlayer: recurrent = False
I0123 18:41:36.121210 140408815788032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 18:41:36.121240 140408815788032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 18:41:36.121301 140408815788032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.123517 140408815788032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 18:41:36.128813 140408815788032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.129068 140408815788032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 18:41:36.131711 140408815788032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 18:41:36.143914 140408815788032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 18:41:36.143969 140408815788032 attention.py:418] Single window, no scan.
I0123 18:41:36.144003 140408815788032 transformer_layer.py:389] tlayer: self-attention.
I0123 18:41:36.144032 140408815788032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.144093 140408815788032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.144689 140408815788032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.144765 140408815788032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.145116 140408815788032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.145797 140408815788032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.148202 140408815788032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.148805 140408815788032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.148882 140408815788032 transformer_layer.py:468] tlayer: End windows.
I0123 18:41:36.148916 140408815788032 transformer_layer.py:472] tlayer: final FFN.
I0123 18:41:36.148972 140408815788032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.149098 140408815788032 transformer_base.py:410] tbase: post-attention MLP.
I0123 18:41:36.149204 140408815788032 nn_components.py:325] mlp: activation = None
I0123 18:41:36.149242 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:41:36.151057 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.151154 140408815788032 nn_components.py:261] mlp: residual
I0123 18:41:36.153564 140408815788032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.153651 140408815788032 transformer_base.py:443] tbase: final FFN
I0123 18:41:36.153760 140408815788032 nn_components.py:320] mlp: hidden 4096, relu
I0123 18:41:36.155917 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:41:36.157733 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.157827 140408815788032 nn_components.py:261] mlp: residual
I0123 18:41:36.158113 140408815788032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.158194 140408815788032 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 18:41:36.158300 140408815788032 transformer_layer.py:154] tlayer: recurrent = False
I0123 18:41:36.158338 140408815788032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 18:41:36.158367 140408815788032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 18:41:36.158428 140408815788032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.160621 140408815788032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 18:41:36.166009 140408815788032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.166266 140408815788032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 18:41:36.168811 140408815788032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 18:41:36.181115 140408815788032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 18:41:36.181170 140408815788032 attention.py:418] Single window, no scan.
I0123 18:41:36.181205 140408815788032 transformer_layer.py:389] tlayer: self-attention.
I0123 18:41:36.181234 140408815788032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.181297 140408815788032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.181857 140408815788032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.181934 140408815788032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.182282 140408815788032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.182957 140408815788032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.185357 140408815788032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.186027 140408815788032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.186106 140408815788032 transformer_layer.py:468] tlayer: End windows.
I0123 18:41:36.186141 140408815788032 transformer_layer.py:472] tlayer: final FFN.
I0123 18:41:36.186198 140408815788032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.186327 140408815788032 transformer_base.py:410] tbase: post-attention MLP.
I0123 18:41:36.186434 140408815788032 nn_components.py:325] mlp: activation = None
I0123 18:41:36.186472 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:41:36.188285 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.188382 140408815788032 nn_components.py:261] mlp: residual
I0123 18:41:36.190750 140408815788032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.190831 140408815788032 transformer_base.py:443] tbase: final FFN
I0123 18:41:36.190939 140408815788032 nn_components.py:320] mlp: hidden 4096, relu
I0123 18:41:36.193108 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:41:36.195377 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.195472 140408815788032 nn_components.py:261] mlp: residual
I0123 18:41:36.195767 140408815788032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.195852 140408815788032 decoder_stack.py:344] dstack: Final layernorm.
I0123 18:41:36.198662 140408815788032 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 18:41:36.248497 140408815788032 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.248586 140408815788032 decoder_stack.py:333] dstack: autoregressive generator.
I0123 18:41:36.248639 140408815788032 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 18:41:36.248741 140408815788032 transformer_layer.py:154] tlayer: recurrent = False
I0123 18:41:36.248779 140408815788032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 18:41:36.248808 140408815788032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 18:41:36.248868 140408815788032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.251184 140408815788032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 18:41:36.256468 140408815788032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.256722 140408815788032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 18:41:36.259273 140408815788032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 18:41:36.271831 140408815788032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 18:41:36.271888 140408815788032 attention.py:418] Single window, no scan.
I0123 18:41:36.271923 140408815788032 transformer_layer.py:389] tlayer: self-attention.
I0123 18:41:36.271953 140408815788032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.272015 140408815788032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.272562 140408815788032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.272638 140408815788032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.272985 140408815788032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.273654 140408815788032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.276087 140408815788032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.276688 140408815788032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.276764 140408815788032 transformer_layer.py:468] tlayer: End windows.
I0123 18:41:36.276798 140408815788032 transformer_layer.py:472] tlayer: final FFN.
I0123 18:41:36.276857 140408815788032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.276991 140408815788032 transformer_base.py:410] tbase: post-attention MLP.
I0123 18:41:36.277099 140408815788032 nn_components.py:325] mlp: activation = None
I0123 18:41:36.277137 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:41:36.278925 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.279017 140408815788032 nn_components.py:261] mlp: residual
I0123 18:41:36.281352 140408815788032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.281430 140408815788032 transformer_base.py:443] tbase: final FFN
I0123 18:41:36.281536 140408815788032 nn_components.py:320] mlp: hidden 4096, relu
I0123 18:41:36.283746 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:41:36.285550 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.285650 140408815788032 nn_components.py:261] mlp: residual
I0123 18:41:36.285941 140408815788032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.286021 140408815788032 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 18:41:36.286127 140408815788032 transformer_layer.py:154] tlayer: recurrent = False
I0123 18:41:36.286166 140408815788032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 18:41:36.286195 140408815788032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 18:41:36.286256 140408815788032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.288449 140408815788032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 18:41:36.293716 140408815788032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.293975 140408815788032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 18:41:36.296600 140408815788032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 18:41:36.308815 140408815788032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 18:41:36.308871 140408815788032 attention.py:418] Single window, no scan.
I0123 18:41:36.308904 140408815788032 transformer_layer.py:389] tlayer: self-attention.
I0123 18:41:36.308933 140408815788032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.308995 140408815788032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.309541 140408815788032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.309617 140408815788032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.309981 140408815788032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.310654 140408815788032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.313114 140408815788032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.313732 140408815788032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.313810 140408815788032 transformer_layer.py:468] tlayer: End windows.
I0123 18:41:36.313844 140408815788032 transformer_layer.py:472] tlayer: final FFN.
I0123 18:41:36.313902 140408815788032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.314037 140408815788032 transformer_base.py:410] tbase: post-attention MLP.
I0123 18:41:36.314146 140408815788032 nn_components.py:325] mlp: activation = None
I0123 18:41:36.314184 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:41:36.315993 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.316085 140408815788032 nn_components.py:261] mlp: residual
I0123 18:41:36.318458 140408815788032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.318539 140408815788032 transformer_base.py:443] tbase: final FFN
I0123 18:41:36.318646 140408815788032 nn_components.py:320] mlp: hidden 4096, relu
I0123 18:41:36.320859 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:41:36.322665 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.322761 140408815788032 nn_components.py:261] mlp: residual
I0123 18:41:36.323044 140408815788032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.323125 140408815788032 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 18:41:36.323230 140408815788032 transformer_layer.py:154] tlayer: recurrent = False
I0123 18:41:36.323269 140408815788032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 18:41:36.323299 140408815788032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 18:41:36.323359 140408815788032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.325539 140408815788032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 18:41:36.330787 140408815788032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.331051 140408815788032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 18:41:36.333638 140408815788032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 18:41:36.345847 140408815788032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 18:41:36.345901 140408815788032 attention.py:418] Single window, no scan.
I0123 18:41:36.345936 140408815788032 transformer_layer.py:389] tlayer: self-attention.
I0123 18:41:36.345965 140408815788032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.346026 140408815788032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.346575 140408815788032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.346649 140408815788032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.346998 140408815788032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.347666 140408815788032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.350161 140408815788032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.350767 140408815788032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.350845 140408815788032 transformer_layer.py:468] tlayer: End windows.
I0123 18:41:36.350879 140408815788032 transformer_layer.py:472] tlayer: final FFN.
I0123 18:41:36.350937 140408815788032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.351065 140408815788032 transformer_base.py:410] tbase: post-attention MLP.
I0123 18:41:36.351182 140408815788032 nn_components.py:325] mlp: activation = None
I0123 18:41:36.351221 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:41:36.353018 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.353111 140408815788032 nn_components.py:261] mlp: residual
I0123 18:41:36.355467 140408815788032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.355546 140408815788032 transformer_base.py:443] tbase: final FFN
I0123 18:41:36.355653 140408815788032 nn_components.py:320] mlp: hidden 4096, relu
I0123 18:41:36.357875 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:41:36.359691 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.359785 140408815788032 nn_components.py:261] mlp: residual
I0123 18:41:36.360069 140408815788032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.360149 140408815788032 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 18:41:36.360254 140408815788032 transformer_layer.py:154] tlayer: recurrent = False
I0123 18:41:36.360292 140408815788032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 18:41:36.360321 140408815788032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 18:41:36.360381 140408815788032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.362558 140408815788032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 18:41:36.367827 140408815788032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.368083 140408815788032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 18:41:36.371160 140408815788032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 18:41:36.383286 140408815788032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 18:41:36.383341 140408815788032 attention.py:418] Single window, no scan.
I0123 18:41:36.383375 140408815788032 transformer_layer.py:389] tlayer: self-attention.
I0123 18:41:36.383405 140408815788032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.383466 140408815788032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.384018 140408815788032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.384093 140408815788032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.384449 140408815788032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.385116 140408815788032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.387583 140408815788032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.388195 140408815788032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.388272 140408815788032 transformer_layer.py:468] tlayer: End windows.
I0123 18:41:36.388305 140408815788032 transformer_layer.py:472] tlayer: final FFN.
I0123 18:41:36.388362 140408815788032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.388489 140408815788032 transformer_base.py:410] tbase: post-attention MLP.
I0123 18:41:36.388596 140408815788032 nn_components.py:325] mlp: activation = None
I0123 18:41:36.388638 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:41:36.390449 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.390542 140408815788032 nn_components.py:261] mlp: residual
I0123 18:41:36.392909 140408815788032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.392988 140408815788032 transformer_base.py:443] tbase: final FFN
I0123 18:41:36.393096 140408815788032 nn_components.py:320] mlp: hidden 4096, relu
I0123 18:41:36.395310 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:41:36.397104 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.397199 140408815788032 nn_components.py:261] mlp: residual
I0123 18:41:36.397487 140408815788032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.397569 140408815788032 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 18:41:36.397680 140408815788032 transformer_layer.py:154] tlayer: recurrent = False
I0123 18:41:36.397718 140408815788032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 18:41:36.397747 140408815788032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 18:41:36.397808 140408815788032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.400000 140408815788032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 18:41:36.405236 140408815788032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.405491 140408815788032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 18:41:36.408098 140408815788032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 18:41:36.420189 140408815788032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 18:41:36.420243 140408815788032 attention.py:418] Single window, no scan.
I0123 18:41:36.420279 140408815788032 transformer_layer.py:389] tlayer: self-attention.
I0123 18:41:36.420309 140408815788032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.420372 140408815788032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.420923 140408815788032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.420998 140408815788032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.421350 140408815788032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.422028 140408815788032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.424473 140408815788032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.425079 140408815788032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.425156 140408815788032 transformer_layer.py:468] tlayer: End windows.
I0123 18:41:36.425189 140408815788032 transformer_layer.py:472] tlayer: final FFN.
I0123 18:41:36.425245 140408815788032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.425371 140408815788032 transformer_base.py:410] tbase: post-attention MLP.
I0123 18:41:36.425477 140408815788032 nn_components.py:325] mlp: activation = None
I0123 18:41:36.425514 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:41:36.427318 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.427410 140408815788032 nn_components.py:261] mlp: residual
I0123 18:41:36.429765 140408815788032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.429845 140408815788032 transformer_base.py:443] tbase: final FFN
I0123 18:41:36.429952 140408815788032 nn_components.py:320] mlp: hidden 4096, relu
I0123 18:41:36.432342 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:41:36.434147 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.434240 140408815788032 nn_components.py:261] mlp: residual
I0123 18:41:36.434526 140408815788032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.434606 140408815788032 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 18:41:36.434711 140408815788032 transformer_layer.py:154] tlayer: recurrent = False
I0123 18:41:36.434749 140408815788032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 18:41:36.434778 140408815788032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 18:41:36.434841 140408815788032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.437014 140408815788032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 18:41:36.442295 140408815788032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.442551 140408815788032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 18:41:36.445151 140408815788032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 18:41:36.457359 140408815788032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 18:41:36.457414 140408815788032 attention.py:418] Single window, no scan.
I0123 18:41:36.457449 140408815788032 transformer_layer.py:389] tlayer: self-attention.
I0123 18:41:36.457479 140408815788032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.457540 140408815788032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.458097 140408815788032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.458174 140408815788032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.458528 140408815788032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.459198 140408815788032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.461710 140408815788032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.462316 140408815788032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.462392 140408815788032 transformer_layer.py:468] tlayer: End windows.
I0123 18:41:36.462426 140408815788032 transformer_layer.py:472] tlayer: final FFN.
I0123 18:41:36.462484 140408815788032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.462611 140408815788032 transformer_base.py:410] tbase: post-attention MLP.
I0123 18:41:36.462716 140408815788032 nn_components.py:325] mlp: activation = None
I0123 18:41:36.462754 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:41:36.464571 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.464670 140408815788032 nn_components.py:261] mlp: residual
I0123 18:41:36.467041 140408815788032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.467120 140408815788032 transformer_base.py:443] tbase: final FFN
I0123 18:41:36.467226 140408815788032 nn_components.py:320] mlp: hidden 4096, relu
I0123 18:41:36.469429 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:41:36.471237 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.471331 140408815788032 nn_components.py:261] mlp: residual
I0123 18:41:36.471615 140408815788032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.471694 140408815788032 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 18:41:36.471799 140408815788032 transformer_layer.py:154] tlayer: recurrent = False
I0123 18:41:36.471837 140408815788032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 18:41:36.471866 140408815788032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 18:41:36.471928 140408815788032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.474095 140408815788032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 18:41:36.479310 140408815788032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.479565 140408815788032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 18:41:36.482593 140408815788032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 18:41:36.494688 140408815788032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 18:41:36.494743 140408815788032 attention.py:418] Single window, no scan.
I0123 18:41:36.494777 140408815788032 transformer_layer.py:389] tlayer: self-attention.
I0123 18:41:36.494806 140408815788032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.494866 140408815788032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.495411 140408815788032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.495487 140408815788032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.495840 140408815788032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.496503 140408815788032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.498965 140408815788032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.499566 140408815788032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.499642 140408815788032 transformer_layer.py:468] tlayer: End windows.
I0123 18:41:36.499676 140408815788032 transformer_layer.py:472] tlayer: final FFN.
I0123 18:41:36.499733 140408815788032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.499861 140408815788032 transformer_base.py:410] tbase: post-attention MLP.
I0123 18:41:36.499968 140408815788032 nn_components.py:325] mlp: activation = None
I0123 18:41:36.500006 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:41:36.501802 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.501900 140408815788032 nn_components.py:261] mlp: residual
I0123 18:41:36.504229 140408815788032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.504307 140408815788032 transformer_base.py:443] tbase: final FFN
I0123 18:41:36.504413 140408815788032 nn_components.py:320] mlp: hidden 4096, relu
I0123 18:41:36.506630 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:41:36.508431 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.508525 140408815788032 nn_components.py:261] mlp: residual
I0123 18:41:36.508811 140408815788032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.508892 140408815788032 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 18:41:36.508998 140408815788032 transformer_layer.py:154] tlayer: recurrent = False
I0123 18:41:36.509036 140408815788032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 18:41:36.509066 140408815788032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 18:41:36.509126 140408815788032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.511297 140408815788032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 18:41:36.516499 140408815788032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.516755 140408815788032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 18:41:36.519344 140408815788032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 18:41:36.531560 140408815788032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 18:41:36.531616 140408815788032 attention.py:418] Single window, no scan.
I0123 18:41:36.531651 140408815788032 transformer_layer.py:389] tlayer: self-attention.
I0123 18:41:36.531681 140408815788032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.531746 140408815788032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.532289 140408815788032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.532364 140408815788032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.532712 140408815788032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.533377 140408815788032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.535831 140408815788032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.536441 140408815788032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.536517 140408815788032 transformer_layer.py:468] tlayer: End windows.
I0123 18:41:36.536550 140408815788032 transformer_layer.py:472] tlayer: final FFN.
I0123 18:41:36.536607 140408815788032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.536732 140408815788032 transformer_base.py:410] tbase: post-attention MLP.
I0123 18:41:36.536839 140408815788032 nn_components.py:325] mlp: activation = None
I0123 18:41:36.536877 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:41:36.538681 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.538773 140408815788032 nn_components.py:261] mlp: residual
I0123 18:41:36.541114 140408815788032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.541191 140408815788032 transformer_base.py:443] tbase: final FFN
I0123 18:41:36.541298 140408815788032 nn_components.py:320] mlp: hidden 4096, relu
I0123 18:41:36.543492 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:41:36.545295 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.545387 140408815788032 nn_components.py:261] mlp: residual
I0123 18:41:36.545680 140408815788032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.545761 140408815788032 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 18:41:36.545869 140408815788032 transformer_layer.py:154] tlayer: recurrent = False
I0123 18:41:36.545907 140408815788032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 18:41:36.545938 140408815788032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 18:41:36.545999 140408815788032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.548174 140408815788032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 18:41:36.553417 140408815788032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.553680 140408815788032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 18:41:36.556274 140408815788032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 18:41:36.568567 140408815788032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 18:41:36.568623 140408815788032 attention.py:418] Single window, no scan.
I0123 18:41:36.568657 140408815788032 transformer_layer.py:389] tlayer: self-attention.
I0123 18:41:36.568687 140408815788032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.568748 140408815788032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.569302 140408815788032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.569376 140408815788032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.569735 140408815788032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.570402 140408815788032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.572871 140408815788032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.573477 140408815788032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.573552 140408815788032 transformer_layer.py:468] tlayer: End windows.
I0123 18:41:36.573586 140408815788032 transformer_layer.py:472] tlayer: final FFN.
I0123 18:41:36.573648 140408815788032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.573776 140408815788032 transformer_base.py:410] tbase: post-attention MLP.
I0123 18:41:36.573884 140408815788032 nn_components.py:325] mlp: activation = None
I0123 18:41:36.573923 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:41:36.575716 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.575807 140408815788032 nn_components.py:261] mlp: residual
I0123 18:41:36.578149 140408815788032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.578233 140408815788032 transformer_base.py:443] tbase: final FFN
I0123 18:41:36.578341 140408815788032 nn_components.py:320] mlp: hidden 4096, relu
I0123 18:41:36.580542 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:41:36.582359 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.582451 140408815788032 nn_components.py:261] mlp: residual
I0123 18:41:36.582735 140408815788032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.582814 140408815788032 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 18:41:36.582922 140408815788032 transformer_layer.py:154] tlayer: recurrent = False
I0123 18:41:36.582961 140408815788032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 18:41:36.582991 140408815788032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 18:41:36.583051 140408815788032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.585223 140408815788032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 18:41:36.590470 140408815788032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.590721 140408815788032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 18:41:36.593739 140408815788032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 18:41:36.605743 140408815788032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 18:41:36.605797 140408815788032 attention.py:418] Single window, no scan.
I0123 18:41:36.605831 140408815788032 transformer_layer.py:389] tlayer: self-attention.
I0123 18:41:36.605861 140408815788032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.605921 140408815788032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.606470 140408815788032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.606545 140408815788032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.606897 140408815788032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.607566 140408815788032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.610013 140408815788032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.610617 140408815788032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.610692 140408815788032 transformer_layer.py:468] tlayer: End windows.
I0123 18:41:36.610726 140408815788032 transformer_layer.py:472] tlayer: final FFN.
I0123 18:41:36.610782 140408815788032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.610908 140408815788032 transformer_base.py:410] tbase: post-attention MLP.
I0123 18:41:36.611016 140408815788032 nn_components.py:325] mlp: activation = None
I0123 18:41:36.611054 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:41:36.612848 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.612938 140408815788032 nn_components.py:261] mlp: residual
I0123 18:41:36.615291 140408815788032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.615375 140408815788032 transformer_base.py:443] tbase: final FFN
I0123 18:41:36.615486 140408815788032 nn_components.py:320] mlp: hidden 4096, relu
I0123 18:41:36.617699 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:41:36.619520 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.619612 140408815788032 nn_components.py:261] mlp: residual
I0123 18:41:36.619898 140408815788032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.619977 140408815788032 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 18:41:36.620081 140408815788032 transformer_layer.py:154] tlayer: recurrent = False
I0123 18:41:36.620119 140408815788032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 18:41:36.620149 140408815788032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 18:41:36.620210 140408815788032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.622385 140408815788032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 18:41:36.627620 140408815788032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.627875 140408815788032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 18:41:36.630654 140408815788032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 18:41:36.642909 140408815788032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 18:41:36.642964 140408815788032 attention.py:418] Single window, no scan.
I0123 18:41:36.642999 140408815788032 transformer_layer.py:389] tlayer: self-attention.
I0123 18:41:36.643029 140408815788032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.643090 140408815788032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.643638 140408815788032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.643712 140408815788032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.644061 140408815788032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.644727 140408815788032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.647198 140408815788032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.647807 140408815788032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.647882 140408815788032 transformer_layer.py:468] tlayer: End windows.
I0123 18:41:36.647916 140408815788032 transformer_layer.py:472] tlayer: final FFN.
I0123 18:41:36.647971 140408815788032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.648097 140408815788032 transformer_base.py:410] tbase: post-attention MLP.
I0123 18:41:36.648204 140408815788032 nn_components.py:325] mlp: activation = None
I0123 18:41:36.648241 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:41:36.650050 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.650141 140408815788032 nn_components.py:261] mlp: residual
I0123 18:41:36.652484 140408815788032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.652568 140408815788032 transformer_base.py:443] tbase: final FFN
I0123 18:41:36.652675 140408815788032 nn_components.py:320] mlp: hidden 4096, relu
I0123 18:41:36.654876 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:41:36.656690 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.656784 140408815788032 nn_components.py:261] mlp: residual
I0123 18:41:36.657069 140408815788032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.657148 140408815788032 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 18:41:36.657254 140408815788032 transformer_layer.py:154] tlayer: recurrent = False
I0123 18:41:36.657291 140408815788032 transformer_layer.py:155] tlayer: compute_importance = False
I0123 18:41:36.657320 140408815788032 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 18:41:36.657379 140408815788032 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.659555 140408815788032 transformer_base.py:161] kvq: pre_attn dropout.
I0123 18:41:36.664800 140408815788032 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.665054 140408815788032 transformer_base.py:194] kvq: normalize keys, queries.
I0123 18:41:36.667659 140408815788032 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 18:41:36.679772 140408815788032 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 18:41:36.679827 140408815788032 attention.py:418] Single window, no scan.
I0123 18:41:36.679862 140408815788032 transformer_layer.py:389] tlayer: self-attention.
I0123 18:41:36.679891 140408815788032 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.679951 140408815788032 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.680495 140408815788032 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.680569 140408815788032 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.680915 140408815788032 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.681580 140408815788032 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.684040 140408815788032 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.684639 140408815788032 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.684715 140408815788032 transformer_layer.py:468] tlayer: End windows.
I0123 18:41:36.684748 140408815788032 transformer_layer.py:472] tlayer: final FFN.
I0123 18:41:36.684804 140408815788032 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.684928 140408815788032 transformer_base.py:410] tbase: post-attention MLP.
I0123 18:41:36.685035 140408815788032 nn_components.py:325] mlp: activation = None
I0123 18:41:36.685072 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:41:36.686872 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.686962 140408815788032 nn_components.py:261] mlp: residual
I0123 18:41:36.689304 140408815788032 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.689381 140408815788032 transformer_base.py:443] tbase: final FFN
I0123 18:41:36.689493 140408815788032 nn_components.py:320] mlp: hidden 4096, relu
I0123 18:41:36.691693 140408815788032 nn_components.py:329] mlp: final activation = None
I0123 18:41:36.693476 140408815788032 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.693567 140408815788032 nn_components.py:261] mlp: residual
I0123 18:41:36.693860 140408815788032 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:36.693945 140408815788032 decoder_stack.py:344] dstack: Final layernorm.
I0123 18:41:36.696723 140408815788032 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 18:41:52.670132 140408815788032 alphageometry.py:566] LM output (score=-0.821630): "o : C d e o 23 D d o e o 24 ;"
I0123 18:41:52.670293 140408815788032 alphageometry.py:567] Translation: "o = on_line o d e, on_bline o e d"

I0123 18:41:52.670337 140408815788032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a, on_bline e a c; f = on_circle f d c, on_bline f c b; g = midpoint g e f; h = on_circle h d b; i = incenter i c b a; j = incenter j c b h; k = incenter k c a h; l = circle l h j k; m = foot m h l d; n = mirror n h m; o = on_line o d e, on_bline o e d ? coll i g n"
I0123 18:41:52.670494 140408815788032 graph.py:498] 
I0123 18:41:52.670548 140408815788032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a, on_bline e a c; f = on_circle f d c, on_bline f c b; g = midpoint g e f; h = on_circle h d b; i = incenter i c b a; j = incenter j c b h; k = incenter k c a h; l = circle l h j k; m = foot m h l d; n = mirror n h m; o = on_line o d e, on_bline o e d ? coll i g n
I0123 18:41:56.025133 140408815788032 ddar.py:60] Depth 1/1000 time = 3.1691043376922607
I0123 18:42:05.799533 140408815788032 ddar.py:60] Depth 2/1000 time = 9.774229764938354
I0123 18:42:23.474112 140408815788032 ddar.py:60] Depth 3/1000 time = 17.674323081970215
I0123 18:42:41.346621 140408815788032 ddar.py:60] Depth 4/1000 time = 17.87220811843872
I0123 18:42:58.909204 140408815788032 ddar.py:60] Depth 5/1000 time = 17.56207299232483
I0123 18:43:18.045739 140408815788032 ddar.py:60] Depth 6/1000 time = 19.131909370422363
I0123 18:43:38.963615 140408815788032 ddar.py:60] Depth 7/1000 time = 20.917623281478882
I0123 18:44:01.515771 140408815788032 ddar.py:60] Depth 8/1000 time = 22.55185866355896
I0123 18:44:32.028044 140408815788032 ddar.py:60] Depth 9/1000 time = 30.511825799942017
I0123 18:44:59.616962 140408815788032 ddar.py:60] Depth 10/1000 time = 27.58844232559204
I0123 18:45:27.110657 140408815788032 ddar.py:60] Depth 11/1000 time = 27.493221282958984
I0123 18:45:54.975678 140408815788032 ddar.py:60] Depth 12/1000 time = 27.850274562835693
I0123 18:46:22.754126 140408815788032 ddar.py:60] Depth 13/1000 time = 27.778135061264038
I0123 18:46:50.885987 140408815788032 ddar.py:60] Depth 14/1000 time = 27.96647882461548
I0123 18:47:22.228256 140408815788032 ddar.py:60] Depth 15/1000 time = 31.341821432113647
I0123 18:48:01.115863 140408815788032 ddar.py:60] Depth 16/1000 time = 38.88728952407837
I0123 18:48:40.226100 140408815788032 ddar.py:60] Depth 17/1000 time = 39.10988783836365
I0123 18:49:20.129360 140408815788032 ddar.py:60] Depth 18/1000 time = 39.90277600288391
I0123 18:50:00.361350 140408815788032 ddar.py:60] Depth 19/1000 time = 40.23152756690979
I0123 18:50:40.806786 140408815788032 ddar.py:60] Depth 20/1000 time = 40.23335552215576
I0123 18:50:41.122033 140408815788032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 18:50:41.122157 140408815788032 alphageometry.py:566] LM output (score=-1.130286): "o : C a c o 23 T a c o k 24 ;"
I0123 18:50:41.122196 140408815788032 alphageometry.py:567] Translation: "o = on_line o a c, on_tline o k a c"

I0123 18:50:41.122241 140408815788032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a, on_bline e a c; f = on_circle f d c, on_bline f c b; g = midpoint g e f; h = on_circle h d b; i = incenter i c b a; j = incenter j c b h; k = incenter k c a h; l = circle l h j k; m = foot m h l d; n = mirror n h m; o = on_line o a c, on_tline o k a c ? coll i g n"
I0123 18:50:41.122423 140408815788032 graph.py:498] 
I0123 18:50:41.122483 140408815788032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a, on_bline e a c; f = on_circle f d c, on_bline f c b; g = midpoint g e f; h = on_circle h d b; i = incenter i c b a; j = incenter j c b h; k = incenter k c a h; l = circle l h j k; m = foot m h l d; n = mirror n h m; o = on_line o a c, on_tline o k a c ? coll i g n
I0123 18:50:44.306310 140408815788032 ddar.py:60] Depth 1/1000 time = 2.8995795249938965
I0123 18:50:54.112564 140408815788032 ddar.py:60] Depth 2/1000 time = 9.805964946746826
I0123 18:51:10.637130 140408815788032 ddar.py:60] Depth 3/1000 time = 16.524298429489136
I0123 18:51:26.916721 140408815788032 ddar.py:60] Depth 4/1000 time = 16.279157638549805
I0123 18:51:43.392275 140408815788032 ddar.py:60] Depth 5/1000 time = 16.474852800369263
I0123 18:52:00.878464 140408815788032 ddar.py:60] Depth 6/1000 time = 17.482388257980347
I0123 18:52:20.368404 140408815788032 ddar.py:60] Depth 7/1000 time = 19.489694356918335
I0123 18:52:41.361764 140408815788032 ddar.py:60] Depth 8/1000 time = 20.993102073669434
I0123 18:53:07.710253 140408815788032 ddar.py:60] Depth 9/1000 time = 26.348210096359253
I0123 18:53:32.974698 140408815788032 ddar.py:60] Depth 10/1000 time = 25.264118909835815
I0123 18:53:58.217750 140408815788032 ddar.py:60] Depth 11/1000 time = 25.242581129074097
I0123 18:54:23.570196 140408815788032 ddar.py:60] Depth 12/1000 time = 25.352003574371338
I0123 18:54:49.008959 140408815788032 ddar.py:60] Depth 13/1000 time = 25.42469620704651
I0123 18:55:14.573647 140408815788032 ddar.py:60] Depth 14/1000 time = 25.564238786697388
I0123 18:55:40.261341 140408815788032 ddar.py:60] Depth 15/1000 time = 25.514283180236816
I0123 18:56:08.709925 140408815788032 ddar.py:60] Depth 16/1000 time = 28.4482684135437
I0123 18:56:45.463868 140408815788032 ddar.py:60] Depth 17/1000 time = 36.75360059738159
I0123 18:57:21.605012 140408815788032 ddar.py:60] Depth 18/1000 time = 36.140620946884155
I0123 18:57:58.843653 140408815788032 ddar.py:60] Depth 19/1000 time = 37.238182067871094
I0123 18:58:36.065804 140408815788032 ddar.py:60] Depth 20/1000 time = 37.019662618637085
I0123 18:58:36.383040 140408815788032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 18:58:36.383160 140408815788032 alphageometry.py:566] LM output (score=-1.307575): "o : C b e o 23 D b o e o 24 ;"
I0123 18:58:36.383198 140408815788032 alphageometry.py:567] Translation: "o = on_line o b e, on_bline o e b"

I0123 18:58:36.383244 140408815788032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a, on_bline e a c; f = on_circle f d c, on_bline f c b; g = midpoint g e f; h = on_circle h d b; i = incenter i c b a; j = incenter j c b h; k = incenter k c a h; l = circle l h j k; m = foot m h l d; n = mirror n h m; o = on_line o b e, on_bline o e b ? coll i g n"
I0123 18:58:36.383411 140408815788032 graph.py:498] 
I0123 18:58:36.383470 140408815788032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a, on_bline e a c; f = on_circle f d c, on_bline f c b; g = midpoint g e f; h = on_circle h d b; i = incenter i c b a; j = incenter j c b h; k = incenter k c a h; l = circle l h j k; m = foot m h l d; n = mirror n h m; o = on_line o b e, on_bline o e b ? coll i g n
I0123 18:58:39.885166 140408815788032 ddar.py:60] Depth 1/1000 time = 3.3250789642333984
I0123 18:58:51.112437 140408815788032 ddar.py:60] Depth 2/1000 time = 11.227105855941772
I0123 18:59:10.074554 140408815788032 ddar.py:60] Depth 3/1000 time = 18.96190047264099
I0123 18:59:28.644743 140408815788032 ddar.py:60] Depth 4/1000 time = 18.569916009902954
I0123 18:59:47.118121 140408815788032 ddar.py:60] Depth 5/1000 time = 18.472835779190063
I0123 19:00:06.245765 140408815788032 ddar.py:60] Depth 6/1000 time = 19.12691354751587
I0123 19:00:26.008297 140408815788032 ddar.py:60] Depth 7/1000 time = 19.762248277664185
I0123 19:00:47.035086 140408815788032 ddar.py:60] Depth 8/1000 time = 21.026376962661743
I0123 19:01:13.017403 140408815788032 ddar.py:60] Depth 9/1000 time = 25.981911182403564
I0123 19:01:37.724754 140408815788032 ddar.py:60] Depth 10/1000 time = 24.707037210464478
I0123 19:02:03.017184 140408815788032 ddar.py:60] Depth 11/1000 time = 25.292120933532715
I0123 19:02:28.407071 140408815788032 ddar.py:60] Depth 12/1000 time = 25.389550924301147
I0123 19:02:53.484754 140408815788032 ddar.py:60] Depth 13/1000 time = 25.077205657958984
I0123 19:03:19.099066 140408815788032 ddar.py:60] Depth 14/1000 time = 25.599862575531006
I0123 19:03:45.244445 140408815788032 ddar.py:60] Depth 15/1000 time = 26.144929885864258
I0123 19:04:12.842658 140408815788032 ddar.py:60] Depth 16/1000 time = 27.392958641052246
I0123 19:04:42.606355 140408815788032 ddar.py:60] Depth 17/1000 time = 29.763374090194702
I0123 19:05:15.909079 140408815788032 ddar.py:60] Depth 18/1000 time = 33.302345514297485
I0123 19:05:57.589869 140408815788032 ddar.py:60] Depth 19/1000 time = 41.68033957481384
I0123 19:06:37.721252 140408815788032 ddar.py:60] Depth 20/1000 time = 40.13101887702942
I0123 19:07:18.099693 140408815788032 ddar.py:60] Depth 21/1000 time = 40.37799882888794
I0123 19:07:58.504426 140408815788032 ddar.py:60] Depth 22/1000 time = 40.19235825538635
I0123 19:07:58.851965 140408815788032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 19:07:58.852107 140408815788032 alphageometry.py:566] LM output (score=-1.431791): "o : C b h o 23 T b h o k 24 ;"
I0123 19:07:58.852146 140408815788032 alphageometry.py:567] Translation: "o = on_line o b h, on_tline o k b h"

I0123 19:07:58.852202 140408815788032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a, on_bline e a c; f = on_circle f d c, on_bline f c b; g = midpoint g e f; h = on_circle h d b; i = incenter i c b a; j = incenter j c b h; k = incenter k c a h; l = circle l h j k; m = foot m h l d; n = mirror n h m; o = on_line o b h, on_tline o k b h ? coll i g n"
I0123 19:07:58.852396 140408815788032 graph.py:498] 
I0123 19:07:58.852454 140408815788032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a, on_bline e a c; f = on_circle f d c, on_bline f c b; g = midpoint g e f; h = on_circle h d b; i = incenter i c b a; j = incenter j c b h; k = incenter k c a h; l = circle l h j k; m = foot m h l d; n = mirror n h m; o = on_line o b h, on_tline o k b h ? coll i g n
I0123 19:08:02.327876 140408815788032 ddar.py:60] Depth 1/1000 time = 3.253692150115967
I0123 19:08:12.506551 140408815788032 ddar.py:60] Depth 2/1000 time = 10.178423643112183
I0123 19:08:32.426224 140408815788032 ddar.py:60] Depth 3/1000 time = 19.919288873672485
I0123 19:08:49.991506 140408815788032 ddar.py:60] Depth 4/1000 time = 17.56493043899536
I0123 19:09:07.542431 140408815788032 ddar.py:60] Depth 5/1000 time = 17.55038022994995
I0123 19:09:26.216631 140408815788032 ddar.py:60] Depth 6/1000 time = 18.669454336166382
I0123 19:09:46.712462 140408815788032 ddar.py:60] Depth 7/1000 time = 20.495567798614502
I0123 19:10:08.848068 140408815788032 ddar.py:60] Depth 8/1000 time = 22.135329723358154
I0123 19:10:36.946939 140408815788032 ddar.py:60] Depth 9/1000 time = 28.098565340042114
I0123 19:11:03.989249 140408815788032 ddar.py:60] Depth 10/1000 time = 27.042006731033325
I0123 19:11:30.770778 140408815788032 ddar.py:60] Depth 11/1000 time = 26.78113579750061
I0123 19:11:57.539686 140408815788032 ddar.py:60] Depth 12/1000 time = 26.768427848815918
I0123 19:12:24.331182 140408815788032 ddar.py:60] Depth 13/1000 time = 26.77746868133545
I0123 19:12:51.777941 140408815788032 ddar.py:60] Depth 14/1000 time = 27.446428537368774
I0123 19:13:19.096240 140408815788032 ddar.py:60] Depth 15/1000 time = 27.125622510910034
I0123 19:13:49.061538 140408815788032 ddar.py:60] Depth 16/1000 time = 29.9648380279541
I0123 19:14:28.756284 140408815788032 ddar.py:60] Depth 17/1000 time = 39.69433069229126
I0123 19:15:07.666663 140408815788032 ddar.py:60] Depth 18/1000 time = 38.90996479988098
I0123 19:15:46.880012 140408815788032 ddar.py:60] Depth 19/1000 time = 39.213037967681885
I0123 19:16:26.405715 140408815788032 ddar.py:60] Depth 20/1000 time = 39.3304078578949
I0123 19:16:26.742506 140408815788032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 19:16:26.742627 140408815788032 alphageometry.py:566] LM output (score=-1.622575): "o : C b f o 23 D b o f o 24 ;"
I0123 19:16:26.742665 140408815788032 alphageometry.py:567] Translation: "o = on_line o b f, on_bline o f b"

I0123 19:16:26.742713 140408815788032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a, on_bline e a c; f = on_circle f d c, on_bline f c b; g = midpoint g e f; h = on_circle h d b; i = incenter i c b a; j = incenter j c b h; k = incenter k c a h; l = circle l h j k; m = foot m h l d; n = mirror n h m; o = on_line o b f, on_bline o f b ? coll i g n"
I0123 19:16:26.742891 140408815788032 graph.py:498] 
I0123 19:16:26.742950 140408815788032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a, on_bline e a c; f = on_circle f d c, on_bline f c b; g = midpoint g e f; h = on_circle h d b; i = incenter i c b a; j = incenter j c b h; k = incenter k c a h; l = circle l h j k; m = foot m h l d; n = mirror n h m; o = on_line o b f, on_bline o f b ? coll i g n
I0123 19:16:30.614064 140408815788032 ddar.py:60] Depth 1/1000 time = 3.3499677181243896
I0123 19:16:42.154804 140408815788032 ddar.py:60] Depth 2/1000 time = 11.54055666923523
I0123 19:17:02.712397 140408815788032 ddar.py:60] Depth 3/1000 time = 20.557363986968994
I0123 19:17:20.987220 140408815788032 ddar.py:60] Depth 4/1000 time = 18.274558782577515
I0123 19:17:39.965711 140408815788032 ddar.py:60] Depth 5/1000 time = 18.977893352508545
I0123 19:17:59.721559 140408815788032 ddar.py:60] Depth 6/1000 time = 19.75098466873169
I0123 19:18:21.542512 140408815788032 ddar.py:60] Depth 7/1000 time = 21.820669651031494
I0123 19:18:45.157698 140408815788032 ddar.py:60] Depth 8/1000 time = 23.614883422851562
I0123 19:19:14.340320 140408815788032 ddar.py:60] Depth 9/1000 time = 29.18229651451111
I0123 19:19:42.569304 140408815788032 ddar.py:60] Depth 10/1000 time = 28.22851252555847
I0123 19:20:10.985767 140408815788032 ddar.py:60] Depth 11/1000 time = 28.416013479232788
I0123 19:20:39.171950 140408815788032 ddar.py:60] Depth 12/1000 time = 28.185826301574707
I0123 19:21:07.202667 140408815788032 ddar.py:60] Depth 13/1000 time = 28.030213832855225
I0123 19:21:35.755098 140408815788032 ddar.py:60] Depth 14/1000 time = 28.537516117095947
I0123 19:22:04.115182 140408815788032 ddar.py:60] Depth 15/1000 time = 28.359779834747314
I0123 19:22:33.301575 140408815788032 ddar.py:60] Depth 16/1000 time = 28.996812343597412
I0123 19:23:05.238781 140408815788032 ddar.py:60] Depth 17/1000 time = 31.93689203262329
I0123 19:23:45.914952 140408815788032 ddar.py:60] Depth 18/1000 time = 40.675846099853516
I0123 19:24:26.071778 140408815788032 ddar.py:60] Depth 19/1000 time = 40.15645933151245
I0123 19:25:07.186090 140408815788032 ddar.py:60] Depth 20/1000 time = 41.11388301849365
I0123 19:25:48.068736 140408815788032 ddar.py:60] Depth 21/1000 time = 40.88226389884949
I0123 19:26:29.694141 140408815788032 ddar.py:60] Depth 22/1000 time = 41.40929937362671
I0123 19:27:11.507730 140408815788032 ddar.py:60] Depth 23/1000 time = 41.813222885131836
I0123 19:27:11.856405 140408815788032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 19:27:11.856540 140408815788032 alphageometry.py:566] LM output (score=-1.666700): "o : C b c o 23 T b c o k 24 ;"
I0123 19:27:11.856579 140408815788032 alphageometry.py:567] Translation: "o = on_line o b c, on_tline o k b c"

I0123 19:27:11.856634 140408815788032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a, on_bline e a c; f = on_circle f d c, on_bline f c b; g = midpoint g e f; h = on_circle h d b; i = incenter i c b a; j = incenter j c b h; k = incenter k c a h; l = circle l h j k; m = foot m h l d; n = mirror n h m; o = on_line o b c, on_tline o k b c ? coll i g n"
I0123 19:27:11.856842 140408815788032 graph.py:498] 
I0123 19:27:11.856902 140408815788032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a, on_bline e a c; f = on_circle f d c, on_bline f c b; g = midpoint g e f; h = on_circle h d b; i = incenter i c b a; j = incenter j c b h; k = incenter k c a h; l = circle l h j k; m = foot m h l d; n = mirror n h m; o = on_line o b c, on_tline o k b c ? coll i g n
I0123 19:27:15.107755 140408815788032 ddar.py:60] Depth 1/1000 time = 2.9435181617736816
I0123 19:27:25.125736 140408815788032 ddar.py:60] Depth 2/1000 time = 10.01779818534851
I0123 19:27:41.541918 140408815788032 ddar.py:60] Depth 3/1000 time = 16.415967226028442
I0123 19:27:58.039698 140408815788032 ddar.py:60] Depth 4/1000 time = 16.49752640724182
I0123 19:28:14.499131 140408815788032 ddar.py:60] Depth 5/1000 time = 16.45886731147766
I0123 19:28:32.140822 140408815788032 ddar.py:60] Depth 6/1000 time = 17.63771629333496
I0123 19:28:51.520831 140408815788032 ddar.py:60] Depth 7/1000 time = 19.379698991775513
I0123 19:29:12.458939 140408815788032 ddar.py:60] Depth 8/1000 time = 20.937700510025024
I0123 19:29:38.552146 140408815788032 ddar.py:60] Depth 9/1000 time = 26.092792749404907
I0123 19:30:03.827320 140408815788032 ddar.py:60] Depth 10/1000 time = 25.274837493896484
I0123 19:30:29.072441 140408815788032 ddar.py:60] Depth 11/1000 time = 25.244757652282715
I0123 19:30:54.363858 140408815788032 ddar.py:60] Depth 12/1000 time = 25.290985107421875
I0123 19:31:19.696515 140408815788032 ddar.py:60] Depth 13/1000 time = 25.31880831718445
I0123 19:31:45.115942 140408815788032 ddar.py:60] Depth 14/1000 time = 25.419037103652954
I0123 19:32:11.074718 140408815788032 ddar.py:60] Depth 15/1000 time = 25.79319953918457
I0123 19:32:39.524640 140408815788032 ddar.py:60] Depth 16/1000 time = 28.44955086708069
I0123 19:33:14.875284 140408815788032 ddar.py:60] Depth 17/1000 time = 35.35018277168274
I0123 19:33:50.538212 140408815788032 ddar.py:60] Depth 18/1000 time = 35.66253042221069
I0123 19:34:27.129875 140408815788032 ddar.py:60] Depth 19/1000 time = 36.59117531776428
I0123 19:35:03.376484 140408815788032 ddar.py:60] Depth 20/1000 time = 36.2462739944458
I0123 19:35:40.122163 140408815788032 ddar.py:60] Depth 21/1000 time = 36.55551600456238
I0123 19:36:16.661156 140408815788032 ddar.py:60] Depth 22/1000 time = 36.53847599029541
I0123 19:36:16.974591 140408815788032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 19:36:16.974723 140408815788032 alphageometry.py:566] LM output (score=-1.758994): "o : C f h o 23 D f o h o 24 ;"
I0123 19:36:16.974760 140408815788032 alphageometry.py:567] Translation: "o = on_line o f h, on_bline o h f"

I0123 19:36:16.974814 140408815788032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a, on_bline e a c; f = on_circle f d c, on_bline f c b; g = midpoint g e f; h = on_circle h d b; i = incenter i c b a; j = incenter j c b h; k = incenter k c a h; l = circle l h j k; m = foot m h l d; n = mirror n h m; o = on_line o f h, on_bline o h f ? coll i g n"
I0123 19:36:16.975002 140408815788032 graph.py:498] 
I0123 19:36:16.975060 140408815788032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a, on_bline e a c; f = on_circle f d c, on_bline f c b; g = midpoint g e f; h = on_circle h d b; i = incenter i c b a; j = incenter j c b h; k = incenter k c a h; l = circle l h j k; m = foot m h l d; n = mirror n h m; o = on_line o f h, on_bline o h f ? coll i g n
I0123 19:36:20.930546 140408815788032 ddar.py:60] Depth 1/1000 time = 3.5462770462036133
I0123 19:36:32.845828 140408815788032 ddar.py:60] Depth 2/1000 time = 11.914988994598389
I0123 19:36:53.430173 140408815788032 ddar.py:60] Depth 3/1000 time = 20.584112644195557
I0123 19:37:11.798100 140408815788032 ddar.py:60] Depth 4/1000 time = 18.367592811584473
I0123 19:37:30.632108 140408815788032 ddar.py:60] Depth 5/1000 time = 18.83310079574585
I0123 19:37:52.251501 140408815788032 ddar.py:60] Depth 6/1000 time = 21.608964443206787
I0123 19:38:15.342485 140408815788032 ddar.py:60] Depth 7/1000 time = 23.090651035308838
I0123 19:38:40.611101 140408815788032 ddar.py:60] Depth 8/1000 time = 25.26813292503357
I0123 19:39:12.214759 140408815788032 ddar.py:60] Depth 9/1000 time = 31.603189945220947
I0123 19:39:41.516980 140408815788032 ddar.py:60] Depth 10/1000 time = 29.301791429519653
I0123 19:40:10.790415 140408815788032 ddar.py:60] Depth 11/1000 time = 29.273138999938965
I0123 19:40:39.632201 140408815788032 ddar.py:60] Depth 12/1000 time = 28.841408491134644
I0123 19:41:08.728223 140408815788032 ddar.py:60] Depth 13/1000 time = 29.081083059310913
I0123 19:41:37.810557 140408815788032 ddar.py:60] Depth 14/1000 time = 29.081880569458008
I0123 19:42:07.357164 140408815788032 ddar.py:60] Depth 15/1000 time = 29.546233892440796
I0123 19:42:36.540724 140408815788032 ddar.py:60] Depth 16/1000 time = 29.183061122894287
I0123 19:43:06.190853 140408815788032 ddar.py:60] Depth 17/1000 time = 29.449315071105957
I0123 19:43:38.898491 140408815788032 ddar.py:60] Depth 18/1000 time = 32.70732498168945
I0123 19:44:20.062718 140408815788032 ddar.py:60] Depth 19/1000 time = 41.16389226913452
I0123 19:45:01.282000 140408815788032 ddar.py:60] Depth 20/1000 time = 41.21889019012451
I0123 19:45:43.003729 140408815788032 ddar.py:60] Depth 21/1000 time = 41.721296072006226
I0123 19:46:24.853807 140408815788032 ddar.py:60] Depth 22/1000 time = 41.84974813461304
I0123 19:47:06.995461 140408815788032 ddar.py:60] Depth 23/1000 time = 41.93327450752258
I0123 19:47:07.350327 140408815788032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 19:47:07.350483 140408815788032 alphageometry.py:566] LM output (score=-1.780156): "o : C a e o 23 D a o e o 24 ;"
I0123 19:47:07.350522 140408815788032 alphageometry.py:567] Translation: "o = on_line o a e, on_bline o e a"

I0123 19:47:07.350579 140408815788032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a, on_bline e a c; f = on_circle f d c, on_bline f c b; g = midpoint g e f; h = on_circle h d b; i = incenter i c b a; j = incenter j c b h; k = incenter k c a h; l = circle l h j k; m = foot m h l d; n = mirror n h m; o = on_line o a e, on_bline o e a ? coll i g n"
I0123 19:47:07.350775 140408815788032 graph.py:498] 
I0123 19:47:07.350832 140408815788032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a, on_bline e a c; f = on_circle f d c, on_bline f c b; g = midpoint g e f; h = on_circle h d b; i = incenter i c b a; j = incenter j c b h; k = incenter k c a h; l = circle l h j k; m = foot m h l d; n = mirror n h m; o = on_line o a e, on_bline o e a ? coll i g n
I0123 19:47:10.489360 140408815788032 ddar.py:60] Depth 1/1000 time = 3.013355255126953
I0123 19:47:22.505702 140408815788032 ddar.py:60] Depth 2/1000 time = 12.016146183013916
I0123 19:47:40.884704 140408815788032 ddar.py:60] Depth 3/1000 time = 18.378777265548706
I0123 19:47:59.225779 140408815788032 ddar.py:60] Depth 4/1000 time = 18.340832948684692
I0123 19:48:17.158486 140408815788032 ddar.py:60] Depth 5/1000 time = 17.9321391582489
I0123 19:48:36.708909 140408815788032 ddar.py:60] Depth 6/1000 time = 19.54575800895691
I0123 19:48:58.352902 140408815788032 ddar.py:60] Depth 7/1000 time = 21.643569707870483
I0123 19:49:21.513129 140408815788032 ddar.py:60] Depth 8/1000 time = 23.15979766845703
I0123 19:49:50.292325 140408815788032 ddar.py:60] Depth 9/1000 time = 28.7787868976593
I0123 19:50:17.830674 140408815788032 ddar.py:60] Depth 10/1000 time = 27.537993907928467
I0123 19:50:46.010861 140408815788032 ddar.py:60] Depth 11/1000 time = 28.17969298362732
I0123 19:51:14.163902 140408815788032 ddar.py:60] Depth 12/1000 time = 28.15259861946106
I0123 19:51:42.276196 140408815788032 ddar.py:60] Depth 13/1000 time = 28.11193537712097
I0123 19:52:10.067521 140408815788032 ddar.py:60] Depth 14/1000 time = 27.77614164352417
I0123 19:52:38.542658 140408815788032 ddar.py:60] Depth 15/1000 time = 28.474634170532227
I0123 19:53:07.275907 140408815788032 ddar.py:60] Depth 16/1000 time = 28.545437812805176
I0123 19:53:38.794618 140408815788032 ddar.py:60] Depth 17/1000 time = 31.518234968185425
I0123 19:54:19.578935 140408815788032 ddar.py:60] Depth 18/1000 time = 40.78386688232422
I0123 19:55:00.325612 140408815788032 ddar.py:60] Depth 19/1000 time = 40.746342420578
I0123 19:55:41.141550 140408815788032 ddar.py:60] Depth 20/1000 time = 40.81559634208679
I0123 19:56:22.506744 140408815788032 ddar.py:60] Depth 21/1000 time = 41.158689737319946
I0123 19:56:22.861401 140408815788032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 19:56:22.861543 140408815788032 alphageometry.py:566] LM output (score=-1.820438): "o : C b c o 23 T b c o j 24 ;"
I0123 19:56:22.861581 140408815788032 alphageometry.py:567] Translation: "o = on_line o b c, on_tline o j b c"

I0123 19:56:22.861637 140408815788032 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d a, on_bline e a c; f = on_circle f d c, on_bline f c b; g = midpoint g e f; h = on_circle h d b; i = incenter i c b a; j = incenter j c b h; k = incenter k c a h; l = circle l h j k; m = foot m h l d; n = mirror n h m; o = on_line o b c, on_tline o j b c ? coll i g n"
I0123 19:56:22.861845 140408815788032 graph.py:498] 
I0123 19:56:22.861905 140408815788032 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d a, on_bline e a c; f = on_circle f d c, on_bline f c b; g = midpoint g e f; h = on_circle h d b; i = incenter i c b a; j = incenter j c b h; k = incenter k c a h; l = circle l h j k; m = foot m h l d; n = mirror n h m; o = on_line o b c, on_tline o j b c ? coll i g n
I0123 19:56:25.988284 140408815788032 ddar.py:60] Depth 1/1000 time = 2.9189295768737793
I0123 19:56:36.450946 140408815788032 ddar.py:60] Depth 2/1000 time = 10.462483882904053
I0123 19:56:54.483300 140408815788032 ddar.py:60] Depth 3/1000 time = 18.032122135162354
I0123 19:57:11.283684 140408815788032 ddar.py:60] Depth 4/1000 time = 16.800089359283447
I0123 19:57:27.533309 140408815788032 ddar.py:60] Depth 5/1000 time = 16.248894691467285
I0123 19:57:45.599605 140408815788032 ddar.py:60] Depth 6/1000 time = 18.061948776245117
I0123 19:58:05.552001 140408815788032 ddar.py:60] Depth 7/1000 time = 19.952033281326294
I0123 19:58:27.003402 140408815788032 ddar.py:60] Depth 8/1000 time = 21.45112657546997
I0123 19:58:53.951473 140408815788032 ddar.py:60] Depth 9/1000 time = 26.9477322101593
I0123 19:59:19.222091 140408815788032 ddar.py:60] Depth 10/1000 time = 25.27017307281494
I0123 19:59:44.582968 140408815788032 ddar.py:60] Depth 11/1000 time = 25.36057472229004
I0123 20:00:09.944851 140408815788032 ddar.py:60] Depth 12/1000 time = 25.361567735671997
I0123 20:00:34.818295 140408815788032 ddar.py:60] Depth 13/1000 time = 24.859286069869995
I0123 20:01:00.295600 140408815788032 ddar.py:60] Depth 14/1000 time = 25.47695279121399
I0123 20:01:26.477013 140408815788032 ddar.py:60] Depth 15/1000 time = 26.016809463500977
I0123 20:01:54.570409 140408815788032 ddar.py:60] Depth 16/1000 time = 28.093024492263794
I0123 20:02:30.765685 140408815788032 ddar.py:60] Depth 17/1000 time = 36.19478988647461
I0123 20:03:05.176939 140408815788032 ddar.py:60] Depth 18/1000 time = 34.41074752807617
I0123 20:03:42.031569 140408815788032 ddar.py:60] Depth 19/1000 time = 36.85413384437561
I0123 20:04:18.367283 140408815788032 ddar.py:60] Depth 20/1000 time = 36.33537936210632
I0123 20:04:18.573281 140408815788032 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 20:04:18.573339 140408815788032 alphageometry.py:585] Timeout.
