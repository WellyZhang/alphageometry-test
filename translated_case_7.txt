I0123 14:03:01.343186 140268056924160 inference_utils.py:69] Parsing gin configuration.
I0123 14:03:01.343323 140268056924160 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0123 14:03:01.343627 140268056924160 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0123 14:03:01.343660 140268056924160 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0123 14:03:01.343687 140268056924160 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0123 14:03:01.343713 140268056924160 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0123 14:03:01.343738 140268056924160 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0123 14:03:01.343763 140268056924160 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0123 14:03:01.343791 140268056924160 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0123 14:03:01.343818 140268056924160 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=32
I0123 14:03:01.343844 140268056924160 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0123 14:03:01.343869 140268056924160 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0123 14:03:01.343920 140268056924160 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0123 14:03:01.344098 140268056924160 resource_reader.py:55] Path not found: base_htrans.gin
I0123 14:03:01.344368 140268056924160 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0123 14:03:01.344463 140268056924160 resource_reader.py:55] Path not found: trainer_configuration.gin
I0123 14:03:01.350851 140268056924160 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0123 14:03:01.350970 140268056924160 resource_reader.py:55] Path not found: size/medium_150M.gin
I0123 14:03:01.351289 140268056924160 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0123 14:03:01.351393 140268056924160 resource_reader.py:55] Path not found: options/positions_t5.gin
I0123 14:03:01.351669 140268056924160 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0123 14:03:01.351768 140268056924160 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0123 14:03:01.352172 140268056924160 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0123 14:03:01.352270 140268056924160 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0123 14:03:01.356054 140268056924160 training_loop.py:334] ==== Training loop: initializing model ====
I0123 14:03:01.449522 140268056924160 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0123 14:03:01.450362 140268056924160 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0123 14:03:01.456764 140268056924160 training_loop.py:335] Process 0 of 1
I0123 14:03:01.456819 140268056924160 training_loop.py:336] Local device count = 1
I0123 14:03:01.456859 140268056924160 training_loop.py:337] Number of replicas = 1
I0123 14:03:01.456891 140268056924160 training_loop.py:339] Using random number seed 42
I0123 14:03:01.964087 140268056924160 training_loop.py:359] Initializing the model.
I0123 14:03:02.360351 140268056924160 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.361115 140268056924160 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 14:03:02.361228 140268056924160 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 14:03:02.361305 140268056924160 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 14:03:02.361380 140268056924160 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 14:03:02.361460 140268056924160 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 14:03:02.361531 140268056924160 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 14:03:02.361600 140268056924160 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 14:03:02.361676 140268056924160 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 14:03:02.361744 140268056924160 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 14:03:02.361810 140268056924160 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 14:03:02.361875 140268056924160 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 14:03:02.361940 140268056924160 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 14:03:02.362006 140268056924160 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 14:03:02.362047 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:02.362093 140268056924160 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 14:03:02.362209 140268056924160 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:03:02.362247 140268056924160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:03:02.362278 140268056924160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:03:02.364375 140268056924160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.369913 140268056924160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:03:02.380802 140268056924160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.381096 140268056924160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:03:02.385414 140268056924160 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:03:02.396311 140268056924160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:03:02.396373 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:02.396414 140268056924160 transformer_layer.py:389] tlayer: self-attention.
I0123 14:03:02.396448 140268056924160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.396510 140268056924160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.397808 140268056924160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.397891 140268056924160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.398594 140268056924160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.401029 140268056924160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.406712 140268056924160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.408410 140268056924160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.408490 140268056924160 transformer_layer.py:468] tlayer: End windows.
I0123 14:03:02.408526 140268056924160 transformer_layer.py:472] tlayer: final FFN.
I0123 14:03:02.408587 140268056924160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.408720 140268056924160 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:03:02.409061 140268056924160 nn_components.py:325] mlp: activation = None
I0123 14:03:02.409107 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:02.410983 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.411089 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:02.413923 140268056924160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.414003 140268056924160 transformer_base.py:443] tbase: final FFN
I0123 14:03:02.414505 140268056924160 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:03:02.424500 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:02.433062 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.433159 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:02.433452 140268056924160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.433533 140268056924160 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 14:03:02.433649 140268056924160 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:03:02.433689 140268056924160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:03:02.433720 140268056924160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:03:02.435546 140268056924160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.437981 140268056924160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:03:02.443429 140268056924160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.443687 140268056924160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:03:02.446242 140268056924160 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:03:02.449959 140268056924160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:03:02.450013 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:02.450049 140268056924160 transformer_layer.py:389] tlayer: self-attention.
I0123 14:03:02.450079 140268056924160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.450140 140268056924160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.450691 140268056924160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.450766 140268056924160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.451123 140268056924160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.451888 140268056924160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.454319 140268056924160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.454936 140268056924160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.455016 140268056924160 transformer_layer.py:468] tlayer: End windows.
I0123 14:03:02.455051 140268056924160 transformer_layer.py:472] tlayer: final FFN.
I0123 14:03:02.455109 140268056924160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.455234 140268056924160 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:03:02.455698 140268056924160 nn_components.py:325] mlp: activation = None
I0123 14:03:02.455740 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:02.457770 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.457863 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:02.460473 140268056924160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.460553 140268056924160 transformer_base.py:443] tbase: final FFN
I0123 14:03:02.460987 140268056924160 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:03:02.463264 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:02.465139 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.465234 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:02.465522 140268056924160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.465601 140268056924160 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 14:03:02.465717 140268056924160 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:03:02.465757 140268056924160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:03:02.465787 140268056924160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:03:02.467656 140268056924160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.469951 140268056924160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:03:02.475769 140268056924160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.476034 140268056924160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:03:02.478629 140268056924160 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:03:02.482405 140268056924160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:03:02.482460 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:02.482495 140268056924160 transformer_layer.py:389] tlayer: self-attention.
I0123 14:03:02.482525 140268056924160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.482585 140268056924160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.483138 140268056924160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.483218 140268056924160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.483569 140268056924160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.484318 140268056924160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.486783 140268056924160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.487443 140268056924160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.487519 140268056924160 transformer_layer.py:468] tlayer: End windows.
I0123 14:03:02.487553 140268056924160 transformer_layer.py:472] tlayer: final FFN.
I0123 14:03:02.487612 140268056924160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.487741 140268056924160 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:03:02.488072 140268056924160 nn_components.py:325] mlp: activation = None
I0123 14:03:02.488115 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:02.489989 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.490081 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:02.492522 140268056924160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.492604 140268056924160 transformer_base.py:443] tbase: final FFN
I0123 14:03:02.493087 140268056924160 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:03:02.495311 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:02.497166 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.497258 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:02.497542 140268056924160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.497622 140268056924160 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 14:03:02.497738 140268056924160 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:03:02.497776 140268056924160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:03:02.497806 140268056924160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:03:02.499670 140268056924160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.501991 140268056924160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:03:02.507462 140268056924160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.507717 140268056924160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:03:02.510309 140268056924160 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:03:02.514046 140268056924160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:03:02.514100 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:02.514135 140268056924160 transformer_layer.py:389] tlayer: self-attention.
I0123 14:03:02.514166 140268056924160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.514229 140268056924160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.514785 140268056924160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.514860 140268056924160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.515215 140268056924160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.515968 140268056924160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.518466 140268056924160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.519083 140268056924160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.519160 140268056924160 transformer_layer.py:468] tlayer: End windows.
I0123 14:03:02.519195 140268056924160 transformer_layer.py:472] tlayer: final FFN.
I0123 14:03:02.519254 140268056924160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.519380 140268056924160 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:03:02.519707 140268056924160 nn_components.py:325] mlp: activation = None
I0123 14:03:02.519750 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:02.521606 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.521705 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:02.524208 140268056924160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.524290 140268056924160 transformer_base.py:443] tbase: final FFN
I0123 14:03:02.524726 140268056924160 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:03:02.526944 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:02.528810 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.528906 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:02.529194 140268056924160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.529273 140268056924160 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 14:03:02.529381 140268056924160 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:03:02.529419 140268056924160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:03:02.529449 140268056924160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:03:02.531345 140268056924160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.533686 140268056924160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:03:02.539167 140268056924160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.539413 140268056924160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:03:02.542031 140268056924160 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:03:02.545697 140268056924160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:03:02.545752 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:02.545787 140268056924160 transformer_layer.py:389] tlayer: self-attention.
I0123 14:03:02.545817 140268056924160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.545879 140268056924160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.546433 140268056924160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.546508 140268056924160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.546861 140268056924160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.547607 140268056924160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.550390 140268056924160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.551000 140268056924160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.551078 140268056924160 transformer_layer.py:468] tlayer: End windows.
I0123 14:03:02.551112 140268056924160 transformer_layer.py:472] tlayer: final FFN.
I0123 14:03:02.551170 140268056924160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.551304 140268056924160 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:03:02.551632 140268056924160 nn_components.py:325] mlp: activation = None
I0123 14:03:02.551674 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:02.553521 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.553615 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:02.556094 140268056924160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.556171 140268056924160 transformer_base.py:443] tbase: final FFN
I0123 14:03:02.556599 140268056924160 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:03:02.558929 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:02.560888 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.560982 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:02.561269 140268056924160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.561348 140268056924160 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 14:03:02.561456 140268056924160 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:03:02.561495 140268056924160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:03:02.561525 140268056924160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:03:02.563367 140268056924160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.565695 140268056924160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:03:02.571168 140268056924160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.571422 140268056924160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:03:02.574027 140268056924160 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:03:02.577669 140268056924160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:03:02.577724 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:02.577759 140268056924160 transformer_layer.py:389] tlayer: self-attention.
I0123 14:03:02.577789 140268056924160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.577849 140268056924160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.578442 140268056924160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.578517 140268056924160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.578874 140268056924160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.579629 140268056924160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.582065 140268056924160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.582676 140268056924160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.582758 140268056924160 transformer_layer.py:468] tlayer: End windows.
I0123 14:03:02.582797 140268056924160 transformer_layer.py:472] tlayer: final FFN.
I0123 14:03:02.582856 140268056924160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.582984 140268056924160 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:03:02.583310 140268056924160 nn_components.py:325] mlp: activation = None
I0123 14:03:02.583355 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:02.585215 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.585307 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:02.587811 140268056924160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.587889 140268056924160 transformer_base.py:443] tbase: final FFN
I0123 14:03:02.588322 140268056924160 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:03:02.590582 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:02.592447 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.592540 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:02.592824 140268056924160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.592903 140268056924160 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 14:03:02.593011 140268056924160 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:03:02.593049 140268056924160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:03:02.593080 140268056924160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:03:02.594927 140268056924160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.597307 140268056924160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:03:02.602812 140268056924160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.603075 140268056924160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:03:02.605636 140268056924160 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:03:02.609349 140268056924160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:03:02.609402 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:02.609438 140268056924160 transformer_layer.py:389] tlayer: self-attention.
I0123 14:03:02.609469 140268056924160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.609532 140268056924160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.610100 140268056924160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.610176 140268056924160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.610533 140268056924160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.611283 140268056924160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.613696 140268056924160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.614304 140268056924160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.614381 140268056924160 transformer_layer.py:468] tlayer: End windows.
I0123 14:03:02.614415 140268056924160 transformer_layer.py:472] tlayer: final FFN.
I0123 14:03:02.614472 140268056924160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.614595 140268056924160 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:03:02.614915 140268056924160 nn_components.py:325] mlp: activation = None
I0123 14:03:02.614958 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:02.616887 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.616980 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:02.619426 140268056924160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.619504 140268056924160 transformer_base.py:443] tbase: final FFN
I0123 14:03:02.619930 140268056924160 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:03:02.622509 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:02.624381 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.624483 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:02.624771 140268056924160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.624852 140268056924160 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 14:03:02.624959 140268056924160 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:03:02.624998 140268056924160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:03:02.625028 140268056924160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:03:02.898387 140268056924160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.902446 140268056924160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:03:02.908546 140268056924160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.908903 140268056924160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:03:02.911625 140268056924160 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:03:02.916013 140268056924160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:03:02.916077 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:02.916117 140268056924160 transformer_layer.py:389] tlayer: self-attention.
I0123 14:03:02.916152 140268056924160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.916227 140268056924160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.916921 140268056924160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.916999 140268056924160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.917360 140268056924160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.918153 140268056924160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.920726 140268056924160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.921384 140268056924160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.921463 140268056924160 transformer_layer.py:468] tlayer: End windows.
I0123 14:03:02.921499 140268056924160 transformer_layer.py:472] tlayer: final FFN.
I0123 14:03:02.921562 140268056924160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.921704 140268056924160 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:03:02.922083 140268056924160 nn_components.py:325] mlp: activation = None
I0123 14:03:02.922127 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:02.924004 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.924104 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:02.926632 140268056924160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.926711 140268056924160 transformer_base.py:443] tbase: final FFN
I0123 14:03:02.927172 140268056924160 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:03:02.929513 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:02.931373 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.931485 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:02.931775 140268056924160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.931859 140268056924160 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 14:03:02.931970 140268056924160 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:03:02.932010 140268056924160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:03:02.932041 140268056924160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:03:02.934022 140268056924160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.936361 140268056924160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:03:02.941869 140268056924160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.942133 140268056924160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:03:02.944758 140268056924160 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:03:02.948532 140268056924160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:03:02.948588 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:02.948624 140268056924160 transformer_layer.py:389] tlayer: self-attention.
I0123 14:03:02.948656 140268056924160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.948718 140268056924160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.949287 140268056924160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.949363 140268056924160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.949718 140268056924160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.950484 140268056924160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.952992 140268056924160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.953612 140268056924160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.953695 140268056924160 transformer_layer.py:468] tlayer: End windows.
I0123 14:03:02.953730 140268056924160 transformer_layer.py:472] tlayer: final FFN.
I0123 14:03:02.953788 140268056924160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.953912 140268056924160 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:03:02.954232 140268056924160 nn_components.py:325] mlp: activation = None
I0123 14:03:02.954275 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:02.956135 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.956227 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:02.958732 140268056924160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.958815 140268056924160 transformer_base.py:443] tbase: final FFN
I0123 14:03:02.959247 140268056924160 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:03:02.961478 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:02.963405 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.963499 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:02.963783 140268056924160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.963870 140268056924160 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 14:03:02.963980 140268056924160 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:03:02.964019 140268056924160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:03:02.964050 140268056924160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:03:02.965872 140268056924160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.968248 140268056924160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:03:02.973618 140268056924160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.973875 140268056924160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:03:02.976811 140268056924160 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:03:02.980437 140268056924160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:03:02.980491 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:02.980527 140268056924160 transformer_layer.py:389] tlayer: self-attention.
I0123 14:03:02.980557 140268056924160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.980619 140268056924160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.981206 140268056924160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.981282 140268056924160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.981648 140268056924160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.982407 140268056924160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.984825 140268056924160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.985432 140268056924160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.985509 140268056924160 transformer_layer.py:468] tlayer: End windows.
I0123 14:03:02.985543 140268056924160 transformer_layer.py:472] tlayer: final FFN.
I0123 14:03:02.985604 140268056924160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.985742 140268056924160 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:03:02.986059 140268056924160 nn_components.py:325] mlp: activation = None
I0123 14:03:02.986100 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:02.987926 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.988016 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:02.990507 140268056924160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.990586 140268056924160 transformer_base.py:443] tbase: final FFN
I0123 14:03:02.991011 140268056924160 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:03:02.993255 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:02.995120 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.995215 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:02.995498 140268056924160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.995584 140268056924160 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 14:03:02.995695 140268056924160 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:03:02.995734 140268056924160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:03:02.995764 140268056924160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:03:02.997567 140268056924160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.999953 140268056924160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:03:03.005370 140268056924160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:03.005629 140268056924160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:03:03.008199 140268056924160 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:03:03.011922 140268056924160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:03:03.011976 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:03.012012 140268056924160 transformer_layer.py:389] tlayer: self-attention.
I0123 14:03:03.012044 140268056924160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:03.012106 140268056924160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:03.012670 140268056924160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:03.012747 140268056924160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:03.013103 140268056924160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:03.014067 140268056924160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:03.016656 140268056924160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:03.017273 140268056924160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:03.017350 140268056924160 transformer_layer.py:468] tlayer: End windows.
I0123 14:03:03.017385 140268056924160 transformer_layer.py:472] tlayer: final FFN.
I0123 14:03:03.017447 140268056924160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:03.017588 140268056924160 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:03:03.017922 140268056924160 nn_components.py:325] mlp: activation = None
I0123 14:03:03.017966 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.019886 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:03.019983 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.022847 140268056924160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:03.022928 140268056924160 transformer_base.py:443] tbase: final FFN
I0123 14:03:03.023361 140268056924160 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:03:03.025630 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.027507 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:03.027605 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.027891 140268056924160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:03.027972 140268056924160 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 14:03:03.028090 140268056924160 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:03:03.028129 140268056924160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:03:03.028160 140268056924160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:03:03.030054 140268056924160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:03.032372 140268056924160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:03:03.037867 140268056924160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:03.038120 140268056924160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:03:03.040692 140268056924160 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:03:03.044457 140268056924160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:03:03.044511 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:03.044547 140268056924160 transformer_layer.py:389] tlayer: self-attention.
I0123 14:03:03.044578 140268056924160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:03.044639 140268056924160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:03.045209 140268056924160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:03.045289 140268056924160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:03.045652 140268056924160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:03.046412 140268056924160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:03.048848 140268056924160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:03.049826 140268056924160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:03.049908 140268056924160 transformer_layer.py:468] tlayer: End windows.
I0123 14:03:03.049943 140268056924160 transformer_layer.py:472] tlayer: final FFN.
I0123 14:03:03.050005 140268056924160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:03.050134 140268056924160 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:03:03.050460 140268056924160 nn_components.py:325] mlp: activation = None
I0123 14:03:03.050503 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.052361 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:03.052454 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.054907 140268056924160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:03.054987 140268056924160 transformer_base.py:443] tbase: final FFN
I0123 14:03:03.055471 140268056924160 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:03:03.057687 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.059556 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:03.059654 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.059944 140268056924160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:03.060243 140268056924160 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 14:03:03.060311 140268056924160 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 14:03:03.060376 140268056924160 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 14:03:03.060433 140268056924160 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 14:03:03.060487 140268056924160 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 14:03:03.060539 140268056924160 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 14:03:03.060590 140268056924160 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 14:03:03.060642 140268056924160 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 14:03:03.060693 140268056924160 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 14:03:03.060743 140268056924160 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 14:03:03.060793 140268056924160 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 14:03:03.060842 140268056924160 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 14:03:03.060879 140268056924160 decoder_stack.py:344] dstack: Final layernorm.
I0123 14:03:03.064380 140268056924160 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:03.111737 140268056924160 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.111824 140268056924160 decoder_stack.py:333] dstack: autoregressive generator.
I0123 14:03:03.111878 140268056924160 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 14:03:03.111980 140268056924160 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:03:03.112018 140268056924160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:03:03.112047 140268056924160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:03:03.112108 140268056924160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.114511 140268056924160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:03:03.119981 140268056924160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.120236 140268056924160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:03:03.122828 140268056924160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:03:03.139239 140268056924160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:03:03.139294 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:03.139329 140268056924160 transformer_layer.py:389] tlayer: self-attention.
I0123 14:03:03.139360 140268056924160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.139423 140268056924160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.140561 140268056924160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.140640 140268056924160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.141339 140268056924160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.143347 140268056924160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.148047 140268056924160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.149346 140268056924160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.149431 140268056924160 transformer_layer.py:468] tlayer: End windows.
I0123 14:03:03.149468 140268056924160 transformer_layer.py:472] tlayer: final FFN.
I0123 14:03:03.149528 140268056924160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.149665 140268056924160 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:03:03.149776 140268056924160 nn_components.py:325] mlp: activation = None
I0123 14:03:03.149816 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.151833 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.151925 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.154324 140268056924160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.154403 140268056924160 transformer_base.py:443] tbase: final FFN
I0123 14:03:03.154511 140268056924160 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:03:03.156697 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.158618 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.158714 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.159000 140268056924160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.159082 140268056924160 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 14:03:03.159190 140268056924160 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:03:03.159228 140268056924160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:03:03.159258 140268056924160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:03:03.159321 140268056924160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.161526 140268056924160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:03:03.166943 140268056924160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.167196 140268056924160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:03:03.169843 140268056924160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:03:03.182722 140268056924160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:03:03.182777 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:03.182812 140268056924160 transformer_layer.py:389] tlayer: self-attention.
I0123 14:03:03.182842 140268056924160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.182904 140268056924160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.183458 140268056924160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.183533 140268056924160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.183881 140268056924160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.184557 140268056924160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.187019 140268056924160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.187631 140268056924160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.187708 140268056924160 transformer_layer.py:468] tlayer: End windows.
I0123 14:03:03.187748 140268056924160 transformer_layer.py:472] tlayer: final FFN.
I0123 14:03:03.187807 140268056924160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.187936 140268056924160 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:03:03.188044 140268056924160 nn_components.py:325] mlp: activation = None
I0123 14:03:03.188082 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.189972 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.190065 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.192407 140268056924160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.192483 140268056924160 transformer_base.py:443] tbase: final FFN
I0123 14:03:03.192588 140268056924160 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:03:03.194781 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.196656 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.196751 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.197036 140268056924160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.197116 140268056924160 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 14:03:03.197224 140268056924160 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:03:03.197262 140268056924160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:03:03.197291 140268056924160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:03:03.197353 140268056924160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.199559 140268056924160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:03:03.204869 140268056924160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.205121 140268056924160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:03:03.207747 140268056924160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:03:03.220211 140268056924160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:03:03.220266 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:03.220301 140268056924160 transformer_layer.py:389] tlayer: self-attention.
I0123 14:03:03.220331 140268056924160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.220396 140268056924160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.220944 140268056924160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.221020 140268056924160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.221370 140268056924160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.222065 140268056924160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.224483 140268056924160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.225098 140268056924160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.225174 140268056924160 transformer_layer.py:468] tlayer: End windows.
I0123 14:03:03.225207 140268056924160 transformer_layer.py:472] tlayer: final FFN.
I0123 14:03:03.225269 140268056924160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.225394 140268056924160 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:03:03.225503 140268056924160 nn_components.py:325] mlp: activation = None
I0123 14:03:03.225542 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.227450 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.227542 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.229943 140268056924160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.230022 140268056924160 transformer_base.py:443] tbase: final FFN
I0123 14:03:03.230129 140268056924160 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:03:03.232300 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.234202 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.234298 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.234581 140268056924160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.234661 140268056924160 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 14:03:03.234770 140268056924160 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:03:03.234808 140268056924160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:03:03.234838 140268056924160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:03:03.234899 140268056924160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.237107 140268056924160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:03:03.242503 140268056924160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.242756 140268056924160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:03:03.245399 140268056924160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:03:03.257871 140268056924160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:03:03.257925 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:03.257959 140268056924160 transformer_layer.py:389] tlayer: self-attention.
I0123 14:03:03.257989 140268056924160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.258051 140268056924160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.258602 140268056924160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.258677 140268056924160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.259027 140268056924160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.259710 140268056924160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.262116 140268056924160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.262720 140268056924160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.262798 140268056924160 transformer_layer.py:468] tlayer: End windows.
I0123 14:03:03.262832 140268056924160 transformer_layer.py:472] tlayer: final FFN.
I0123 14:03:03.262889 140268056924160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.263026 140268056924160 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:03:03.263136 140268056924160 nn_components.py:325] mlp: activation = None
I0123 14:03:03.263174 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.265078 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.265170 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.267621 140268056924160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.267699 140268056924160 transformer_base.py:443] tbase: final FFN
I0123 14:03:03.267807 140268056924160 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:03:03.269984 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.271802 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.271896 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.272176 140268056924160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.272255 140268056924160 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 14:03:03.272362 140268056924160 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:03:03.272401 140268056924160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:03:03.272432 140268056924160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:03:03.272493 140268056924160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.275027 140268056924160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:03:03.280365 140268056924160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.280625 140268056924160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:03:03.283182 140268056924160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:03:03.295615 140268056924160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:03:03.295670 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:03.295705 140268056924160 transformer_layer.py:389] tlayer: self-attention.
I0123 14:03:03.295736 140268056924160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.295797 140268056924160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.296350 140268056924160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.296426 140268056924160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.296780 140268056924160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.297456 140268056924160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.300106 140268056924160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.300723 140268056924160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.300804 140268056924160 transformer_layer.py:468] tlayer: End windows.
I0123 14:03:03.300839 140268056924160 transformer_layer.py:472] tlayer: final FFN.
I0123 14:03:03.300897 140268056924160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.301029 140268056924160 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:03:03.301139 140268056924160 nn_components.py:325] mlp: activation = None
I0123 14:03:03.301177 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.303026 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.303118 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.305460 140268056924160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.305537 140268056924160 transformer_base.py:443] tbase: final FFN
I0123 14:03:03.305656 140268056924160 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:03:03.307896 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.309711 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.309805 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.310084 140268056924160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.310162 140268056924160 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 14:03:03.310270 140268056924160 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:03:03.310307 140268056924160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:03:03.310337 140268056924160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:03:03.310398 140268056924160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.312579 140268056924160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:03:03.317887 140268056924160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.318136 140268056924160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:03:03.320743 140268056924160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:03:03.333140 140268056924160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:03:03.333195 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:03.333229 140268056924160 transformer_layer.py:389] tlayer: self-attention.
I0123 14:03:03.333260 140268056924160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.333322 140268056924160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.333885 140268056924160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.333961 140268056924160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.334316 140268056924160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.334991 140268056924160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.337399 140268056924160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.338020 140268056924160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.338097 140268056924160 transformer_layer.py:468] tlayer: End windows.
I0123 14:03:03.338131 140268056924160 transformer_layer.py:472] tlayer: final FFN.
I0123 14:03:03.338188 140268056924160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.338325 140268056924160 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:03:03.338443 140268056924160 nn_components.py:325] mlp: activation = None
I0123 14:03:03.338483 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.340389 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.340482 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.342849 140268056924160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.342928 140268056924160 transformer_base.py:443] tbase: final FFN
I0123 14:03:03.343035 140268056924160 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:03:03.345213 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.347034 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.347130 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.347414 140268056924160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.347495 140268056924160 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 14:03:03.347602 140268056924160 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:03:03.347640 140268056924160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:03:03.347670 140268056924160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:03:03.347731 140268056924160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.349923 140268056924160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:03:03.355324 140268056924160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.355577 140268056924160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:03:03.358120 140268056924160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:03:03.370541 140268056924160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:03:03.370594 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:03.370630 140268056924160 transformer_layer.py:389] tlayer: self-attention.
I0123 14:03:03.370660 140268056924160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.370721 140268056924160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.371269 140268056924160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.371344 140268056924160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.371694 140268056924160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.372368 140268056924160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.374781 140268056924160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.375752 140268056924160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.375829 140268056924160 transformer_layer.py:468] tlayer: End windows.
I0123 14:03:03.375863 140268056924160 transformer_layer.py:472] tlayer: final FFN.
I0123 14:03:03.375924 140268056924160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.376050 140268056924160 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:03:03.376158 140268056924160 nn_components.py:325] mlp: activation = None
I0123 14:03:03.376203 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.378071 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.378163 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.380527 140268056924160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.380605 140268056924160 transformer_base.py:443] tbase: final FFN
I0123 14:03:03.380712 140268056924160 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:03:03.382915 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.384785 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.384880 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.385164 140268056924160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.385246 140268056924160 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 14:03:03.385354 140268056924160 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:03:03.385393 140268056924160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:03:03.385422 140268056924160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:03:03.385483 140268056924160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.387682 140268056924160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:03:03.392987 140268056924160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.393253 140268056924160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:03:03.395865 140268056924160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:03:03.408389 140268056924160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:03:03.408444 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:03.408479 140268056924160 transformer_layer.py:389] tlayer: self-attention.
I0123 14:03:03.408509 140268056924160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.408571 140268056924160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.409166 140268056924160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.409244 140268056924160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.409595 140268056924160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.410276 140268056924160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.412669 140268056924160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.413278 140268056924160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.413356 140268056924160 transformer_layer.py:468] tlayer: End windows.
I0123 14:03:03.413391 140268056924160 transformer_layer.py:472] tlayer: final FFN.
I0123 14:03:03.413450 140268056924160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.413581 140268056924160 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:03:03.413697 140268056924160 nn_components.py:325] mlp: activation = None
I0123 14:03:03.413742 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.415589 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.415681 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.418078 140268056924160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.418158 140268056924160 transformer_base.py:443] tbase: final FFN
I0123 14:03:03.418265 140268056924160 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:03:03.420430 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.422257 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.422353 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.422785 140268056924160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.422866 140268056924160 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 14:03:03.422974 140268056924160 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:03:03.423013 140268056924160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:03:03.423044 140268056924160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:03:03.423105 140268056924160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.425278 140268056924160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:03:03.430641 140268056924160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.430895 140268056924160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:03:03.433428 140268056924160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:03:03.445852 140268056924160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:03:03.445907 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:03.445942 140268056924160 transformer_layer.py:389] tlayer: self-attention.
I0123 14:03:03.445972 140268056924160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.446034 140268056924160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.446595 140268056924160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.446672 140268056924160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.447026 140268056924160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.447705 140268056924160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.450134 140268056924160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.450797 140268056924160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.450876 140268056924160 transformer_layer.py:468] tlayer: End windows.
I0123 14:03:03.450911 140268056924160 transformer_layer.py:472] tlayer: final FFN.
I0123 14:03:03.450973 140268056924160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.451102 140268056924160 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:03:03.451210 140268056924160 nn_components.py:325] mlp: activation = None
I0123 14:03:03.451248 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.453096 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.453188 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.455527 140268056924160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.455606 140268056924160 transformer_base.py:443] tbase: final FFN
I0123 14:03:03.455718 140268056924160 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:03:03.457886 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.459758 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.459851 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.460131 140268056924160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.460209 140268056924160 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 14:03:03.460318 140268056924160 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:03:03.460356 140268056924160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:03:03.460386 140268056924160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:03:03.460446 140268056924160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.462636 140268056924160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:03:03.467877 140268056924160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.468125 140268056924160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:03:03.470737 140268056924160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:03:03.483407 140268056924160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:03:03.483462 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:03.483496 140268056924160 transformer_layer.py:389] tlayer: self-attention.
I0123 14:03:03.483527 140268056924160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.483587 140268056924160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.484189 140268056924160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.484265 140268056924160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.484615 140268056924160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.485295 140268056924160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.487696 140268056924160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.488317 140268056924160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.488394 140268056924160 transformer_layer.py:468] tlayer: End windows.
I0123 14:03:03.488428 140268056924160 transformer_layer.py:472] tlayer: final FFN.
I0123 14:03:03.488487 140268056924160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.488614 140268056924160 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:03:03.488723 140268056924160 nn_components.py:325] mlp: activation = None
I0123 14:03:03.488761 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.490609 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.490707 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.493097 140268056924160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.493175 140268056924160 transformer_base.py:443] tbase: final FFN
I0123 14:03:03.493282 140268056924160 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:03:03.495461 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.497256 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.497349 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.497629 140268056924160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.497717 140268056924160 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 14:03:03.497828 140268056924160 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:03:03.497866 140268056924160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:03:03.497896 140268056924160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:03:03.497957 140268056924160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.500149 140268056924160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:03:03.505523 140268056924160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.505783 140268056924160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:03:03.508346 140268056924160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:03:03.520734 140268056924160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:03:03.520788 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:03.520823 140268056924160 transformer_layer.py:389] tlayer: self-attention.
I0123 14:03:03.520854 140268056924160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.520919 140268056924160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.521466 140268056924160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.521543 140268056924160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.521903 140268056924160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.522686 140268056924160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.525099 140268056924160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.525755 140268056924160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.525832 140268056924160 transformer_layer.py:468] tlayer: End windows.
I0123 14:03:03.525866 140268056924160 transformer_layer.py:472] tlayer: final FFN.
I0123 14:03:03.525923 140268056924160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.526050 140268056924160 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:03:03.526161 140268056924160 nn_components.py:325] mlp: activation = None
I0123 14:03:03.526200 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.528031 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.528129 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.530666 140268056924160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.530746 140268056924160 transformer_base.py:443] tbase: final FFN
I0123 14:03:03.530851 140268056924160 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:03:03.532997 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.534871 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.534969 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.535248 140268056924160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.535329 140268056924160 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 14:03:03.535435 140268056924160 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:03:03.535474 140268056924160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:03:03.535504 140268056924160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:03:03.535565 140268056924160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.537756 140268056924160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:03:03.543094 140268056924160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.543348 140268056924160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:03:03.545969 140268056924160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:03:03.558437 140268056924160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:03:03.558493 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:03.558527 140268056924160 transformer_layer.py:389] tlayer: self-attention.
I0123 14:03:03.558557 140268056924160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.558618 140268056924160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.559170 140268056924160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.559247 140268056924160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.559597 140268056924160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.560328 140268056924160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.562748 140268056924160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.563363 140268056924160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.563440 140268056924160 transformer_layer.py:468] tlayer: End windows.
I0123 14:03:03.563475 140268056924160 transformer_layer.py:472] tlayer: final FFN.
I0123 14:03:03.563532 140268056924160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.563662 140268056924160 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:03:03.563774 140268056924160 nn_components.py:325] mlp: activation = None
I0123 14:03:03.563813 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.565660 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.565752 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.568102 140268056924160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.568181 140268056924160 transformer_base.py:443] tbase: final FFN
I0123 14:03:03.568287 140268056924160 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:03:03.570524 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.572339 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.572432 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.572712 140268056924160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.572798 140268056924160 decoder_stack.py:344] dstack: Final layernorm.
I0123 14:03:03.575630 140268056924160 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.629886 140268056924160 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.629974 140268056924160 decoder_stack.py:333] dstack: autoregressive generator.
I0123 14:03:03.630028 140268056924160 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 14:03:03.630130 140268056924160 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:03:03.630167 140268056924160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:03:03.630197 140268056924160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:03:03.630258 140268056924160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.632845 140268056924160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:03:03.638138 140268056924160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.638390 140268056924160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:03:03.640890 140268056924160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:03:03.653131 140268056924160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:03:03.653185 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:03.653220 140268056924160 transformer_layer.py:389] tlayer: self-attention.
I0123 14:03:03.653251 140268056924160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.653313 140268056924160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.653866 140268056924160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.653942 140268056924160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.654291 140268056924160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.654954 140268056924160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.657387 140268056924160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.658001 140268056924160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.658081 140268056924160 transformer_layer.py:468] tlayer: End windows.
I0123 14:03:03.658115 140268056924160 transformer_layer.py:472] tlayer: final FFN.
I0123 14:03:03.658175 140268056924160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.658303 140268056924160 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:03:03.658419 140268056924160 nn_components.py:325] mlp: activation = None
I0123 14:03:03.658458 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.660254 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.660346 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.662673 140268056924160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.662752 140268056924160 transformer_base.py:443] tbase: final FFN
I0123 14:03:03.662860 140268056924160 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:03:03.665044 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.666859 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.666955 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.667237 140268056924160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.667318 140268056924160 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 14:03:03.667426 140268056924160 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:03:03.667465 140268056924160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:03:03.667495 140268056924160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:03:03.667556 140268056924160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.669725 140268056924160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:03:03.674957 140268056924160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.675209 140268056924160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:03:03.677799 140268056924160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:03:03.689770 140268056924160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:03:03.689825 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:03.689860 140268056924160 transformer_layer.py:389] tlayer: self-attention.
I0123 14:03:03.689891 140268056924160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.689954 140268056924160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.690498 140268056924160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.690573 140268056924160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.690921 140268056924160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.691585 140268056924160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.694029 140268056924160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.694646 140268056924160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.694723 140268056924160 transformer_layer.py:468] tlayer: End windows.
I0123 14:03:03.694757 140268056924160 transformer_layer.py:472] tlayer: final FFN.
I0123 14:03:03.694815 140268056924160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.694941 140268056924160 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:03:03.695047 140268056924160 nn_components.py:325] mlp: activation = None
I0123 14:03:03.695091 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.696881 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.696974 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.699292 140268056924160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.699371 140268056924160 transformer_base.py:443] tbase: final FFN
I0123 14:03:03.699481 140268056924160 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:03:03.701695 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.703480 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.703574 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.703856 140268056924160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.703937 140268056924160 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 14:03:03.704043 140268056924160 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:03:03.704081 140268056924160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:03:03.704111 140268056924160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:03:03.704171 140268056924160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.706342 140268056924160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:03:03.711538 140268056924160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.711793 140268056924160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:03:03.714383 140268056924160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:03:03.726347 140268056924160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:03:03.726401 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:03.726436 140268056924160 transformer_layer.py:389] tlayer: self-attention.
I0123 14:03:03.726466 140268056924160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.726527 140268056924160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.727073 140268056924160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.727149 140268056924160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.727499 140268056924160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.728162 140268056924160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.730593 140268056924160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.731198 140268056924160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.731276 140268056924160 transformer_layer.py:468] tlayer: End windows.
I0123 14:03:03.731311 140268056924160 transformer_layer.py:472] tlayer: final FFN.
I0123 14:03:03.731369 140268056924160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.731495 140268056924160 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:03:03.731601 140268056924160 nn_components.py:325] mlp: activation = None
I0123 14:03:03.731639 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.733421 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.733514 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.735823 140268056924160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.735902 140268056924160 transformer_base.py:443] tbase: final FFN
I0123 14:03:03.736008 140268056924160 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:03:03.738633 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.740420 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.740514 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.740793 140268056924160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.740874 140268056924160 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 14:03:03.740981 140268056924160 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:03:03.741020 140268056924160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:03:03.741050 140268056924160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:03:03.741111 140268056924160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.743298 140268056924160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:03:03.748471 140268056924160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.748722 140268056924160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:03:03.751425 140268056924160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:03:03.763905 140268056924160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:03:03.763960 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:03.763997 140268056924160 transformer_layer.py:389] tlayer: self-attention.
I0123 14:03:03.764033 140268056924160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.764096 140268056924160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.764655 140268056924160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.764730 140268056924160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.765083 140268056924160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.765761 140268056924160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.768221 140268056924160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.768829 140268056924160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.768904 140268056924160 transformer_layer.py:468] tlayer: End windows.
I0123 14:03:03.768936 140268056924160 transformer_layer.py:472] tlayer: final FFN.
I0123 14:03:03.768992 140268056924160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.769117 140268056924160 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:03:03.769223 140268056924160 nn_components.py:325] mlp: activation = None
I0123 14:03:03.769262 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.771090 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.771182 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.773494 140268056924160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.773572 140268056924160 transformer_base.py:443] tbase: final FFN
I0123 14:03:03.773686 140268056924160 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:03:03.775895 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.777697 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.777791 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.778072 140268056924160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.778151 140268056924160 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 14:03:03.778258 140268056924160 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:03:03.778295 140268056924160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:03:03.778323 140268056924160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:03:03.778383 140268056924160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.780538 140268056924160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:03:03.785795 140268056924160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.786046 140268056924160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:03:03.788634 140268056924160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:03:03.800812 140268056924160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:03:03.800865 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:03.800899 140268056924160 transformer_layer.py:389] tlayer: self-attention.
I0123 14:03:03.800928 140268056924160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.800988 140268056924160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.801532 140268056924160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.801606 140268056924160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.801966 140268056924160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.802636 140268056924160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.805100 140268056924160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.805720 140268056924160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.805796 140268056924160 transformer_layer.py:468] tlayer: End windows.
I0123 14:03:03.805830 140268056924160 transformer_layer.py:472] tlayer: final FFN.
I0123 14:03:03.805886 140268056924160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.806011 140268056924160 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:03:03.806116 140268056924160 nn_components.py:325] mlp: activation = None
I0123 14:03:03.806153 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.807976 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.808073 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.810406 140268056924160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.810484 140268056924160 transformer_base.py:443] tbase: final FFN
I0123 14:03:03.810590 140268056924160 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:03:03.812813 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.814631 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.814726 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.815014 140268056924160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.815093 140268056924160 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 14:03:03.815199 140268056924160 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:03:03.815235 140268056924160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:03:03.815264 140268056924160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:03:03.815325 140268056924160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.817500 140268056924160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:03:03.822765 140268056924160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.823013 140268056924160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:03:03.825626 140268056924160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:03:03.837835 140268056924160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:03:03.837893 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:03.837927 140268056924160 transformer_layer.py:389] tlayer: self-attention.
I0123 14:03:03.837956 140268056924160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.838018 140268056924160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.838572 140268056924160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.838649 140268056924160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.839013 140268056924160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.839683 140268056924160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.842161 140268056924160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.842763 140268056924160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.842839 140268056924160 transformer_layer.py:468] tlayer: End windows.
I0123 14:03:03.842872 140268056924160 transformer_layer.py:472] tlayer: final FFN.
I0123 14:03:03.842929 140268056924160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.843052 140268056924160 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:03:03.843158 140268056924160 nn_components.py:325] mlp: activation = None
I0123 14:03:03.843195 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.845016 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.845115 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.847440 140268056924160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.847517 140268056924160 transformer_base.py:443] tbase: final FFN
I0123 14:03:03.847626 140268056924160 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:03:03.850257 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.852060 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.852153 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.852433 140268056924160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.852511 140268056924160 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 14:03:03.852616 140268056924160 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:03:03.852653 140268056924160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:03:03.852682 140268056924160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:03:03.852743 140268056924160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.854913 140268056924160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:03:03.860337 140268056924160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.860593 140268056924160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:03:03.863271 140268056924160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:03:03.875519 140268056924160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:03:03.875571 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:03.875604 140268056924160 transformer_layer.py:389] tlayer: self-attention.
I0123 14:03:03.875633 140268056924160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.875698 140268056924160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.876258 140268056924160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.876332 140268056924160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.876677 140268056924160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.877353 140268056924160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.879817 140268056924160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.880429 140268056924160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.880504 140268056924160 transformer_layer.py:468] tlayer: End windows.
I0123 14:03:03.880538 140268056924160 transformer_layer.py:472] tlayer: final FFN.
I0123 14:03:03.880595 140268056924160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.880719 140268056924160 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:03:03.880826 140268056924160 nn_components.py:325] mlp: activation = None
I0123 14:03:03.880863 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.882700 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.882792 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.885145 140268056924160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.885224 140268056924160 transformer_base.py:443] tbase: final FFN
I0123 14:03:03.885330 140268056924160 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:03:03.887586 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.889402 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.889496 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.889786 140268056924160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.889866 140268056924160 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 14:03:03.889971 140268056924160 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:03:03.890008 140268056924160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:03:03.890036 140268056924160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:03:03.890095 140268056924160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.892271 140268056924160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:03:03.897549 140268056924160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.897808 140268056924160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:03:03.900416 140268056924160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:03:03.912611 140268056924160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:03:03.912664 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:03.912701 140268056924160 transformer_layer.py:389] tlayer: self-attention.
I0123 14:03:03.912731 140268056924160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.912791 140268056924160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.913338 140268056924160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.913412 140268056924160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.913767 140268056924160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.914437 140268056924160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.916884 140268056924160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.917494 140268056924160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.917569 140268056924160 transformer_layer.py:468] tlayer: End windows.
I0123 14:03:03.917602 140268056924160 transformer_layer.py:472] tlayer: final FFN.
I0123 14:03:03.917670 140268056924160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.917797 140268056924160 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:03:03.917904 140268056924160 nn_components.py:325] mlp: activation = None
I0123 14:03:03.917940 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.919738 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.919828 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.922142 140268056924160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.922226 140268056924160 transformer_base.py:443] tbase: final FFN
I0123 14:03:03.922333 140268056924160 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:03:03.924548 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.926356 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.926448 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.926727 140268056924160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.926806 140268056924160 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 14:03:03.926912 140268056924160 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:03:03.926949 140268056924160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:03:03.926978 140268056924160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:03:03.927037 140268056924160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.929188 140268056924160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:03:03.934422 140268056924160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.934679 140268056924160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:03:03.937293 140268056924160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:03:03.950039 140268056924160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:03:03.950093 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:03.950127 140268056924160 transformer_layer.py:389] tlayer: self-attention.
I0123 14:03:03.950156 140268056924160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.950218 140268056924160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.950768 140268056924160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.950842 140268056924160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.951188 140268056924160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.951856 140268056924160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.954334 140268056924160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.954943 140268056924160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.955017 140268056924160 transformer_layer.py:468] tlayer: End windows.
I0123 14:03:03.955050 140268056924160 transformer_layer.py:472] tlayer: final FFN.
I0123 14:03:03.955107 140268056924160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.955233 140268056924160 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:03:03.955338 140268056924160 nn_components.py:325] mlp: activation = None
I0123 14:03:03.955376 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.957194 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.957285 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.959733 140268056924160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.959827 140268056924160 transformer_base.py:443] tbase: final FFN
I0123 14:03:03.959934 140268056924160 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:03:03.962544 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.964352 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.964444 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.964725 140268056924160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.964804 140268056924160 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 14:03:03.964908 140268056924160 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:03:03.964944 140268056924160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:03:03.964972 140268056924160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:03:03.965030 140268056924160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.967204 140268056924160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:03:03.972473 140268056924160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.972726 140268056924160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:03:03.975340 140268056924160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:03:03.987546 140268056924160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:03:03.987599 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:03.987633 140268056924160 transformer_layer.py:389] tlayer: self-attention.
I0123 14:03:03.987662 140268056924160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.987720 140268056924160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.988268 140268056924160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.988342 140268056924160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.988692 140268056924160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.989354 140268056924160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.991880 140268056924160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.992490 140268056924160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.992566 140268056924160 transformer_layer.py:468] tlayer: End windows.
I0123 14:03:03.992599 140268056924160 transformer_layer.py:472] tlayer: final FFN.
I0123 14:03:03.992654 140268056924160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.992776 140268056924160 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:03:03.992880 140268056924160 nn_components.py:325] mlp: activation = None
I0123 14:03:03.992916 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.995086 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.995180 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.997470 140268056924160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.997546 140268056924160 transformer_base.py:443] tbase: final FFN
I0123 14:03:03.997665 140268056924160 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:03:03.999859 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:04.001649 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:04.001744 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:04.002023 140268056924160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:04.002102 140268056924160 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 14:03:04.002207 140268056924160 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:03:04.002244 140268056924160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:03:04.002273 140268056924160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:03:04.002331 140268056924160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:04.004498 140268056924160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:03:04.009772 140268056924160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:04.010022 140268056924160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:03:04.012604 140268056924160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:03:04.024713 140268056924160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:03:04.024766 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:04.024800 140268056924160 transformer_layer.py:389] tlayer: self-attention.
I0123 14:03:04.024829 140268056924160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:04.024888 140268056924160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:04.025435 140268056924160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:04.025509 140268056924160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:04.025863 140268056924160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:04.026528 140268056924160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:04.028959 140268056924160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:04.029564 140268056924160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:04.029644 140268056924160 transformer_layer.py:468] tlayer: End windows.
I0123 14:03:04.029678 140268056924160 transformer_layer.py:472] tlayer: final FFN.
I0123 14:03:04.029734 140268056924160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:04.029859 140268056924160 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:03:04.029965 140268056924160 nn_components.py:325] mlp: activation = None
I0123 14:03:04.030001 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:04.031805 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:04.031897 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:04.034213 140268056924160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:04.034290 140268056924160 transformer_base.py:443] tbase: final FFN
I0123 14:03:04.034395 140268056924160 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:03:04.036601 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:04.038422 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:04.038516 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:04.038795 140268056924160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:04.038875 140268056924160 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 14:03:04.038981 140268056924160 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:03:04.039019 140268056924160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:03:04.039047 140268056924160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:03:04.039105 140268056924160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:04.041262 140268056924160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:03:04.046521 140268056924160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:04.046768 140268056924160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:03:04.049372 140268056924160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:03:04.061755 140268056924160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:03:04.061808 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:04.061841 140268056924160 transformer_layer.py:389] tlayer: self-attention.
I0123 14:03:04.061870 140268056924160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:04.061930 140268056924160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:04.062484 140268056924160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:04.062567 140268056924160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:04.062916 140268056924160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:04.063587 140268056924160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:04.066047 140268056924160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:04.066659 140268056924160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:04.066738 140268056924160 transformer_layer.py:468] tlayer: End windows.
I0123 14:03:04.066772 140268056924160 transformer_layer.py:472] tlayer: final FFN.
I0123 14:03:04.066829 140268056924160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:04.066954 140268056924160 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:03:04.067066 140268056924160 nn_components.py:325] mlp: activation = None
I0123 14:03:04.067104 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:04.068906 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:04.068997 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:04.071317 140268056924160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:04.071394 140268056924160 transformer_base.py:443] tbase: final FFN
I0123 14:03:04.071499 140268056924160 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:03:04.074095 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:04.075922 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:04.076016 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:04.076292 140268056924160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:04.076378 140268056924160 decoder_stack.py:344] dstack: Final layernorm.
I0123 14:03:04.079132 140268056924160 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:08.829138 140268056924160 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
I0123 14:03:09.421212 140268056924160 training_loop.py:409] No working directory specified.
I0123 14:03:09.421409 140268056924160 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0123 14:03:09.422506 140268056924160 checkpoints.py:1062] Restoring legacy Flax checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0123 14:03:12.556052 140268056924160 training_loop.py:447] Only restoring trainable parameters.
I0123 14:03:12.557091 140268056924160 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0123 14:03:12.557151 140268056924160 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0123 14:03:12.557196 140268056924160 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 14:03:12.557238 140268056924160 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 14:03:12.557278 140268056924160 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.557316 140268056924160 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 14:03:12.557353 140268056924160 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.557390 140268056924160 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.557425 140268056924160 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 14:03:12.557462 140268056924160 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 14:03:12.557497 140268056924160 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.557533 140268056924160 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 14:03:12.557570 140268056924160 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 14:03:12.557606 140268056924160 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 14:03:12.557657 140268056924160 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.557696 140268056924160 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 14:03:12.557732 140268056924160 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.557768 140268056924160 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.557804 140268056924160 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 14:03:12.557839 140268056924160 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 14:03:12.557888 140268056924160 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.557925 140268056924160 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 14:03:12.557961 140268056924160 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 14:03:12.557997 140268056924160 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 14:03:12.558032 140268056924160 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.558067 140268056924160 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 14:03:12.558102 140268056924160 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.558137 140268056924160 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.558172 140268056924160 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 14:03:12.558207 140268056924160 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 14:03:12.558242 140268056924160 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.558277 140268056924160 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 14:03:12.558310 140268056924160 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 14:03:12.558345 140268056924160 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 14:03:12.558380 140268056924160 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.558414 140268056924160 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 14:03:12.558448 140268056924160 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.558482 140268056924160 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.558517 140268056924160 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 14:03:12.558552 140268056924160 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 14:03:12.558587 140268056924160 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.558622 140268056924160 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 14:03:12.558656 140268056924160 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 14:03:12.558691 140268056924160 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 14:03:12.558727 140268056924160 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.558761 140268056924160 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 14:03:12.558802 140268056924160 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.558838 140268056924160 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.558874 140268056924160 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 14:03:12.558910 140268056924160 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 14:03:12.558944 140268056924160 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.558980 140268056924160 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 14:03:12.559015 140268056924160 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 14:03:12.559051 140268056924160 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 14:03:12.559087 140268056924160 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.559122 140268056924160 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 14:03:12.559157 140268056924160 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.559191 140268056924160 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.559227 140268056924160 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 14:03:12.559262 140268056924160 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 14:03:12.559295 140268056924160 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.559331 140268056924160 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 14:03:12.559365 140268056924160 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 14:03:12.559401 140268056924160 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 14:03:12.559437 140268056924160 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.559472 140268056924160 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 14:03:12.559508 140268056924160 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.559543 140268056924160 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.559578 140268056924160 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 14:03:12.559614 140268056924160 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 14:03:12.559649 140268056924160 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.559684 140268056924160 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 14:03:12.559720 140268056924160 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 14:03:12.559760 140268056924160 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 14:03:12.559797 140268056924160 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.559832 140268056924160 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 14:03:12.559868 140268056924160 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.559904 140268056924160 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.559939 140268056924160 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 14:03:12.559975 140268056924160 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 14:03:12.560011 140268056924160 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.560047 140268056924160 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 14:03:12.560081 140268056924160 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 14:03:12.560116 140268056924160 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 14:03:12.560151 140268056924160 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.560187 140268056924160 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 14:03:12.560222 140268056924160 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.560256 140268056924160 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.560292 140268056924160 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 14:03:12.560327 140268056924160 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 14:03:12.560362 140268056924160 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.560397 140268056924160 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 14:03:12.560431 140268056924160 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 14:03:12.560465 140268056924160 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 14:03:12.560500 140268056924160 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.560534 140268056924160 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 14:03:12.560569 140268056924160 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.560604 140268056924160 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.560639 140268056924160 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 14:03:12.560675 140268056924160 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 14:03:12.560714 140268056924160 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.560752 140268056924160 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 14:03:12.560787 140268056924160 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 14:03:12.560822 140268056924160 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 14:03:12.560857 140268056924160 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.560892 140268056924160 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 14:03:12.560927 140268056924160 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.560962 140268056924160 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.560997 140268056924160 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 14:03:12.561032 140268056924160 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 14:03:12.561066 140268056924160 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.561101 140268056924160 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 14:03:12.561136 140268056924160 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 14:03:12.561172 140268056924160 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 14:03:12.561208 140268056924160 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.561243 140268056924160 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 14:03:12.561278 140268056924160 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.561312 140268056924160 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.561348 140268056924160 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 14:03:12.561383 140268056924160 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 14:03:12.561417 140268056924160 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.561451 140268056924160 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 14:03:12.561478 140268056924160 training_loop.py:725] Total parameters: 152072288
I0123 14:03:12.561943 140268056924160 training_loop.py:739] Total state size: 0
I0123 14:03:12.618884 140268056924160 training_loop.py:492] Training loop: creating task for mode beam_search
I0123 14:03:12.619323 140268056924160 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0123 14:03:12.620050 140268056924160 training_loop.py:652] Compiling mode beam_search with jit.
I0123 14:03:12.620481 140268056924160 training_loop.py:89] registering functions: dict_keys([])
I0123 14:03:12.638913 140268056924160 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = midpoint e b a; f = midpoint f c a; g = on_line g c e, on_line g b f; h = foot h c b a; i = on_circle i e b, on_line i c h; j = on_circle j e b, on_line j c h; k = on_line k j g, on_line k d e; l = on_pline l k d b, on_line l b a; m = on_pline m k d a, on_line m a b; n = lc_tangent n l b, on_line n c b; o = lc_tangent o m a, on_line o c a; p = lc_tangent p b d, on_line p e d; q = circle q c n o; r = circle r b e n; s = foot s n r q; t = mirror t n s ? coll t q p
I0123 14:03:28.500306 140268056924160 ddar.py:60] Depth 1/1000 time = 15.735305786132812
I0123 14:03:59.150009 140268056924160 ddar.py:60] Depth 2/1000 time = 30.649014234542847
I0123 14:04:44.846199 140268056924160 ddar.py:60] Depth 3/1000 time = 45.695746660232544
I0123 14:05:42.811341 140268056924160 ddar.py:60] Depth 4/1000 time = 57.964688301086426
I0123 14:06:48.733868 140268056924160 ddar.py:60] Depth 5/1000 time = 65.92208218574524
I0123 14:07:54.063442 140268056924160 ddar.py:60] Depth 6/1000 time = 65.32910418510437
I0123 14:08:59.558415 140268056924160 ddar.py:60] Depth 7/1000 time = 65.49459338188171
I0123 14:10:05.338720 140268056924160 ddar.py:60] Depth 8/1000 time = 65.7786078453064
I0123 14:11:12.853513 140268056924160 ddar.py:60] Depth 9/1000 time = 67.32647180557251
I0123 14:12:18.637287 140268056924160 ddar.py:60] Depth 10/1000 time = 65.78328323364258
I0123 14:13:26.472293 140268056924160 ddar.py:60] Depth 11/1000 time = 67.52458453178406
I0123 14:14:37.208371 140268056924160 ddar.py:60] Depth 12/1000 time = 70.73562693595886
I0123 14:15:48.602149 140268056924160 ddar.py:60] Depth 13/1000 time = 71.39341998100281
I0123 14:17:00.460817 140268056924160 ddar.py:60] Depth 14/1000 time = 71.85815572738647
I0123 14:18:15.003939 140268056924160 ddar.py:60] Depth 15/1000 time = 74.26563167572021
I0123 14:19:29.176731 140268056924160 ddar.py:60] Depth 16/1000 time = 74.1354169845581
I0123 14:20:44.014913 140268056924160 ddar.py:60] Depth 17/1000 time = 74.79886889457703
I0123 14:21:58.798824 140268056924160 ddar.py:60] Depth 18/1000 time = 74.78355813026428
I0123 14:23:15.782031 140268056924160 ddar.py:60] Depth 19/1000 time = 76.98271203041077
I0123 14:24:37.130128 140268056924160 ddar.py:60] Depth 20/1000 time = 81.34762787818909
I0123 14:26:02.228141 140268056924160 ddar.py:60] Depth 21/1000 time = 85.097323179245
I0123 14:27:37.242638 140268056924160 ddar.py:60] Depth 22/1000 time = 95.01400351524353
I0123 14:29:12.565151 140268056924160 ddar.py:60] Depth 23/1000 time = 95.32197070121765
I0123 14:30:50.511122 140268056924160 ddar.py:60] Depth 24/1000 time = 97.94540405273438
I0123 14:32:30.580911 140268056924160 ddar.py:60] Depth 25/1000 time = 100.06925368309021
I0123 14:34:11.255799 140268056924160 ddar.py:60] Depth 26/1000 time = 100.59659600257874
I0123 14:35:51.471040 140268056924160 ddar.py:60] Depth 27/1000 time = 100.17413592338562
I0123 14:37:32.184306 140268056924160 ddar.py:60] Depth 28/1000 time = 100.71245527267456
I0123 14:39:10.488051 140268056924160 ddar.py:60] Depth 29/1000 time = 98.27051091194153
I0123 14:40:50.231255 140268056924160 ddar.py:60] Depth 30/1000 time = 99.62519097328186
I0123 14:42:33.362498 140268056924160 ddar.py:60] Depth 31/1000 time = 103.13039994239807
I0123 14:44:17.764958 140268056924160 ddar.py:60] Depth 32/1000 time = 104.40166401863098
I0123 14:46:02.788200 140268056924160 ddar.py:60] Depth 33/1000 time = 104.7107298374176
I0123 14:48:12.885032 140268056924160 ddar.py:60] Depth 34/1000 time = 130.09597301483154
I0123 14:50:27.035619 140268056924160 ddar.py:60] Depth 35/1000 time = 134.15004181861877
I0123 14:52:46.225190 140268056924160 ddar.py:60] Depth 36/1000 time = 139.18904972076416
I0123 14:52:50.444336 140268056924160 alphageometry.py:191] 
==========================
 * From theorem premises:
A B C D E F G H J K L M N O P Q R S T : Points
DC = DB [00]
DB = DA [01]
EB = EA [02]
A,B,E are collinear [03]
A,C,F are collinear [04]
FC = FA [05]
C,G,E are collinear [06]
G,B,F are collinear [07]
CH ⟂ AB [08]
A,B,H are collinear [09]
∠(AB-CH) = ∠(AB-CH) [10]
EJ = EB [11]
C,J,H are collinear [12]
D,K,E are collinear [13]
G,K,J are collinear [14]
A,B,L are collinear [15]
LK ∥ DB [16]
A,B,M are collinear [17]
MK ∥ DA [18]
LB ⟂ LN [19]
C,B,N are collinear [20]
MA ⟂ MO [21]
A,C,O are collinear [22]
BP ⟂ BD [23]
D,P,E are collinear [24]
QN = QO [25]
QC = QN [26]
RE = RN [27]
RB = RE [28]
S,Q,R are collinear [29]
NS ⟂ QR [30]
S,N,T are collinear [31]
SN = ST [32]

 * Auxiliary Constructions:
I : Points
C,I,H are collinear [33]
EI = EB [34]
∠BAI = ∠BAI [35]

 * Proof steps:
001. LB ⟂ LN [19] & CH ⟂ AB [08] & A,B,L are collinear [15] ⇒  LN ∥ CH [36]
002. A,B,L are collinear [15] & A,B,H are collinear [09] ⇒  B,H,L are collinear [37]
003. LN ∥ CH [36] & C,B,N are collinear [20] & B,H,L are collinear [37] ⇒  CB:BN = BH:BL [38]
004. LN ∥ CH [36] & C,B,N are collinear [20] & B,H,L are collinear [37] ⇒  CB:BN = CH:NL [39]
005. MA ⟂ MO [21] & CH ⟂ AB [08] & A,B,M are collinear [17] ⇒  MO ∥ CH [40]
006. A,B,M are collinear [17] & A,B,H are collinear [09] ⇒  A,H,M are collinear [41]
007. MO ∥ CH [40] & A,C,O are collinear [22] & A,H,M are collinear [41] ⇒  AC:AO = AH:AM [42]
008. MO ∥ CH [40] & A,C,O are collinear [22] & A,H,M are collinear [41] ⇒  AC:AO = CH:OM [43]
009. A,B,L are collinear [15] & A,B,E are collinear [03] ⇒  E,B,L are collinear [44]
010. KL ∥ BD [16] & D,K,E are collinear [13] & E,B,L are collinear [44] ⇒  ED:EK = EB:EL [45]
011. KL ∥ BD [16] & D,K,E are collinear [13] & E,B,L are collinear [44] ⇒  DE:DK = BE:BL [46]
012. KL ∥ BD [16] & D,K,E are collinear [13] & E,B,L are collinear [44] ⇒  DE:BE = DK:BL [47]
013. KL ∥ BD [16] & D,K,E are collinear [13] & E,B,L are collinear [44] ⇒  KE:DK = LE:BL [48]
014. ED:EK = EB:EL [45] & EJ = EB [11] ⇒  DE:KE = JE:LE [49]
015. DE:DK = BE:BL [46] & EJ = EB [11] ⇒  DE:DK = JE:BL [50]
016. A,B,M are collinear [17] & A,B,E are collinear [03] ⇒  E,A,M are collinear [51]
017. KM ∥ AD [18] & E,A,M are collinear [51] & D,K,E are collinear [13] ⇒  AE:AM = DE:DK [52]
018. KM ∥ AD [18] & E,A,M are collinear [51] & D,K,E are collinear [13] ⇒  AE:DE = AM:DK [53]
019. KM ∥ AD [18] & E,A,M are collinear [51] & D,K,E are collinear [13] ⇒  ME:AM = KE:DK [54]
020. AE:AM = DE:DK [52] & EJ = EB [11] & EB = EA [02] ⇒  JE:AM = DE:DK [55]
021. A,B,M are collinear [17] & A,B,L are collinear [15] & KM ∥ AD [18] ⇒  ∠KML = ∠DAB [56]
022. A,B,M are collinear [17] & A,B,L are collinear [15] & KL ∥ BD [16] ⇒  ∠KLM = ∠DBA [57]
023. ∠KML = ∠DAB [56] & ∠KLM = ∠DBA [57] (Similar Triangles)⇒  MK:ML = AD:AB [58]
024. MK:ML = AD:AB [58] & DB = DA [01] ⇒  KM:LM = DB:AB [59]
025. DB = DA [01] & EB = EA [02] ⇒  BA ⟂ DE [60]
026. DB = DA [01] & EB = EA [02] ⇒  EA:EB = DA:DB [61]
027. C,J,H are collinear [12] & D,K,E are collinear [13] & BA ⟂ DE [60] & CH ⟂ AB [08] ⇒  CJ ∥ EK [62]
028. CJ ∥ EK [62] & C,G,E are collinear [06] & G,K,J are collinear [14] ⇒  CG:GE = GJ:GK [63]
029. CJ ∥ EK [62] & C,G,E are collinear [06] & G,K,J are collinear [14] ⇒  CG:GE = CJ:KE [64]
030. CJ ∥ EK [62] & C,G,E are collinear [06] & G,K,J are collinear [14] ⇒  GE:CE = GK:KJ [65]
031. EJ = EB [11] & EB = EA [02] ⇒  E is the circumcenter of \Delta BJA [66]
032. E is the circumcenter of \Delta BJA [66] & A,B,E are collinear [03] ⇒  BJ ⟂ AJ [67]
033. A,B,H are collinear [09] & C,I,H are collinear [33] & BJ ⟂ AJ [67] & CH ⟂ AB [08] ⇒  ∠BHI = ∠BJA [68]
034. EI = EB [34] & EJ = EB [11] & EB = EA [02] ⇒  A,B,I,J are concyclic [69]
035. A,B,I,J are concyclic [69] ⇒  ∠ABI = ∠AJI [70]
036. A,B,I,J are concyclic [69] ⇒  ∠ABJ = ∠AIJ [71]
037. A,B,I,J are concyclic [69] ⇒  ∠AIB = ∠AJB [72]
038. ∠ABI = ∠AJI [70] & C,J,H are collinear [12] & C,I,H are collinear [33] ⇒  ∠ABI = ∠(AJ-CH) [73]
039. C,I,H are collinear [33] & ∠ABI = ∠AJI [70] & C,J,H are collinear [12] ⇒  ∠BIH = ∠BAJ [74]
040. ∠BHI = ∠BJA [68] & ∠BIH = ∠BAJ [74] (Similar Triangles)⇒  BH:IH = BJ:AJ [75]
041. A,B,H are collinear [09] & C,I,H are collinear [33] & BJ ⟂ AJ [67] & CH ⟂ AB [08] ⇒  ∠AHI = ∠AJB [76]
042. C,I,H are collinear [33] & ∠ABJ = ∠AIJ [71] & C,J,H are collinear [12] ⇒  ∠AIH = ∠ABJ [77]
043. ∠AHI = ∠AJB [76] & ∠AIH = ∠ABJ [77] (Similar Triangles)⇒  AH:AI = AJ:AB [78]
044. ∠AHI = ∠AJB [76] & ∠AIH = ∠ABJ [77] (Similar Triangles)⇒  AH:IH = AJ:BJ [79]
045. EI = EB [34] & EB = EA [02] ⇒  E is the circumcenter of \Delta BIA [80]
046. EI = EB [34] & EB = EA [02] ⇒  EI = EA [81]
047. E is the circumcenter of \Delta BIA [80] & A,B,E are collinear [03] ⇒  BI ⟂ AI [82]
048. A,B,H are collinear [09] & C,I,H are collinear [33] & CH ⟂ AB [08] & BI ⟂ AI [82] ⇒  ∠BIA = ∠AHI [83]
049. A,B,H are collinear [09] & ∠BAI = ∠BAI [35] ⇒  ∠BAI = ∠HAI [84]
050. ∠BIA = ∠AHI [83] & ∠BAI = ∠HAI [84] (Similar Triangles)⇒  BI:AB = IH:AI [85]
051. A,B,E are collinear [03] & EB = EA [02] ⇒  E is midpoint of AB [86]
052. A,C,F are collinear [04] & FC = FA [05] ⇒  F is midpoint of AC [87]
053. E is midpoint of AB [86] & F is midpoint of AC [87] ⇒  EF ∥ BC [88]
054. EF ∥ BC [88] & G,B,F are collinear [07] & C,G,E are collinear [06] ⇒  GF:BF = GE:CE [89]
055. EF ∥ BC [88] & G,B,F are collinear [07] & C,G,E are collinear [06] ⇒  GF:GB = GE:CG [90]
056. EF ∥ BC [88] & G,B,F are collinear [07] & C,G,E are collinear [06] ⇒  GF:GB = FE:CB [91]
057. GE:CE = GK:KJ [65] & GF:BF = GE:CE [89] ⇒  GF:BF = GK:KJ [92]
058. GF:BF = GK:KJ [92] & G,K,J are collinear [14] & G,B,F are collinear [07] ⇒  KF ∥ JB [93]
059. FK ∥ BJ [93] & G,K,J are collinear [14] & G,B,F are collinear [07] ⇒  GK:GJ = KF:JB [94]
060. A,B,E are collinear [03] & C,I,H are collinear [33] & CH ⟂ AB [08] & BI ⟂ AI [82] ⇒  ∠AIB = ∠(EA-CI) [95]
061. A,B,E are collinear [03] & C,I,H are collinear [33] & ∠ABI = ∠(AJ-CH) [73] ⇒  ∠(EA-BI) = ∠(AJ-CI) [96]
062. ∠AIB = ∠(EA-CI) [95] & ∠(EA-BI) = ∠(AJ-CI) [96] ⇒  ∠IAE = ∠EAJ [97]
063. ∠IAE = ∠EAJ [97] & A,B,E are collinear [03] ⇒  ∠IAB = ∠BAJ [98]
064. A,B,I,J are concyclic [69] & ∠IAB = ∠BAJ [98] ⇒  BI = JB [99]
065. GK:GJ = KF:JB [94] & BI = JB [99] ⇒  GK:GJ = KF:BI [100]
066. DB = DA [01] & DC = DB [00] ⇒  DC = DA [101]
067. DB = DA [01] & DC = DB [00] ⇒  D is the circumcenter of \Delta BCA [102]
068. DC = DA [101] ⇒  ∠DCA = ∠CAD [103]
069. DC = DA [101] & FC = FA [05] ⇒  AC ⟂ DF [104]
070. S,Q,R are collinear [29] & QR ⟂ NS [30] ⇒  SQ ⟂ SN [105]
071. AC ⟂ DF [104] & SQ ⟂ SN [105] ⇒  ∠(DF-SN) = ∠(AC-SQ) [106]
072. S,Q,R are collinear [29] & ∠(DF-SN) = ∠(AC-SQ) [106] ⇒  ∠QSN = ∠(AC-DF) [107]
073. D is the circumcenter of \Delta BCA [102] & F is midpoint of AC [87] ⇒  ∠CBA = ∠FDA [108]
074. D is the circumcenter of \Delta BCA [102] & F is midpoint of AC [87] ⇒  ∠BCD = ∠(AB-DF) [109]
075. S,N,T are collinear [31] & SN = ST [32] ⇒  S is midpoint of NT [110]
076. S,Q,R are collinear [29] & T,S,N are collinear [31] & QR ⟂ NS [30] ⇒  RS ⟂ NT [111]
077. S is midpoint of NT [110] & RS ⟂ NT [111] ⇒  RN = RT [112]
078. RB = RE [28] & RE = RN [27] & RN = RT [112] ⇒  T,B,N,E are concyclic [113]
079. T,B,N,E are concyclic [113] ⇒  ∠TNB = ∠TEB [114]
080. T,B,N,E are concyclic [113] ⇒  ∠TBN = ∠TEN [115]
081. ∠CBA = ∠FDA [108] & ∠TNB = ∠TEB [114] & S,N,T are collinear [31] & C,B,N are collinear [20] & A,B,E are collinear [03] ⇒  ∠(SN-TE) = ∠FDA [116]
082. ∠QSN = ∠(AC-DF) [107] & ∠(SN-TE) = ∠FDA [116] ⇒  ∠(SQ-TE) = ∠CAD [117]
083. RN = RT [112] & RE = RN [27] ⇒  R is the circumcenter of \Delta ETN [118]
084. RN = RT [112] & RE = RN [27] ⇒  RT = RE [119]
085. R is the circumcenter of \Delta ETN [118] & S is midpoint of NT [110] ⇒  ∠NET = ∠NRS [120]
086. ∠DCA = ∠CAD [103] & ∠(SQ-TE) = ∠CAD [117] & S,Q,R are collinear [29] & ∠NET = ∠NRS [120] ⇒  ∠RNE = ∠DCA [121]
087. RE = RN [27] ⇒  ∠NER = ∠RNE [122]
088. ∠(SQ-TE) = ∠CAD [117] & S,Q,R are collinear [29] & ∠NET = ∠NRS [120] & ∠NER = ∠RNE [122] ⇒  ∠NER = ∠CAD [123]
089. ∠RNE = ∠DCA [121] & ∠NER = ∠CAD [123] (Similar Triangles)⇒  CD:CA = NR:NE [124]
090. CD:CA = NR:NE [124] & DC = DB [00] & RN = RT [112] ⇒  DB:AC = TR:NE [125]
091. C,I,H are collinear [33] & A,B,E are collinear [03] & CH ⟂ AB [08] ⇒  CI ⟂ EA [126]
092. CI ⟂ EA [126] & SQ ⟂ SN [105] ⇒  ∠(CI-SN) = ∠(EA-SQ) [127]
093. S,Q,R are collinear [29] & A,B,E are collinear [03] & C,I,H are collinear [33] & ∠(CI-SN) = ∠(EA-SQ) [127] ⇒  ∠QSN = ∠(EA-CI) [128]
094. D is the circumcenter of \Delta BCA [102] & E is midpoint of AB [86] ⇒  ∠BCA = ∠EDA [129]
095. D is the circumcenter of \Delta BCA [102] & E is midpoint of AB [86] ⇒  ∠CAD = ∠(BC-DE) [130]
096. D is the circumcenter of \Delta BCA [102] & E is midpoint of AB [86] ⇒  ∠BCA = ∠BDE [131]
097. S,Q,R are collinear [29] & T,S,N are collinear [31] & QR ⟂ NS [30] ⇒  QS ⟂ NT [132]
098. S is midpoint of NT [110] & QS ⟂ NT [132] ⇒  QN = QT [133]
099. QN = QO [25] & QC = QN [26] & QN = QT [133] ⇒  C,O,N,T are concyclic [134]
100. C,O,N,T are concyclic [134] ⇒  ∠COT = ∠CNT [135]
101. C,I,H are collinear [33] & ∠BCA = ∠EDA [129] & ∠COT = ∠CNT [135] & A,C,O are collinear [22] & C,B,N are collinear [20] & S,N,T are collinear [31] & BA ⟂ DE [60] & CH ⟂ AB [08] ⇒  ∠(SN-TO) = ∠(CI-AD) [136]
102. ∠QSN = ∠(EA-CI) [128] & ∠(SN-TO) = ∠(CI-AD) [136] ⇒  ∠(SQ-TO) = ∠EAD [137]
103. QN = QT [133] & QN = QO [25] ⇒  Q is the circumcenter of \Delta OTN [138]
104. QN = QT [133] & QN = QO [25] ⇒  QT = QO [139]
105. Q is the circumcenter of \Delta OTN [138] & S is midpoint of NT [110] ⇒  ∠NOT = ∠NQS [140]
106. Q is the circumcenter of \Delta OTN [138] & S is midpoint of NT [110] ⇒  ∠TON = ∠TQS [141]
107. QN = QO [25] ⇒  ∠QON = ∠ONQ [142]
108. A,B,M are collinear [17] & A,B,L are collinear [15] & ∠(SQ-TO) = ∠EAD [137] & S,Q,R are collinear [29] & A,B,E are collinear [03] & ∠NOT = ∠NQS [140] & ∠QON = ∠ONQ [142] & AD ∥ KM [18] ⇒  ∠QON = ∠KML [143]
109. SQ ⟂ SN [105] & BP ⟂ BD [23] ⇒  ∠(SQ-DB) = ∠(SN-BP) [144]
110. SQ ⟂ SN [105] & BP ⟂ BD [23] ⇒  ∠(SQ-BP) = ∠(SN-DB) [145]
111. S,Q,R are collinear [29] & ∠(SQ-DB) = ∠(SN-BP) [144] ⇒  ∠QSN = ∠DBP [146]
112. D is the circumcenter of \Delta BCA [102] & BP ⟂ BD [23] ⇒  ∠PBA = ∠BCA [147]
113. A,B,E are collinear [03] & ∠PBA = ∠BCA [147] & ∠COT = ∠CNT [135] & A,C,O are collinear [22] & C,B,N are collinear [20] & S,N,T are collinear [31] ⇒  ∠(SN-TO) = ∠(BP-EA) [148]
114. ∠QSN = ∠DBP [146] & ∠(SN-TO) = ∠(BP-EA) [148] ⇒  ∠(SQ-TO) = ∠(DB-EA) [149]
115. A,B,M are collinear [17] & A,B,L are collinear [15] & ∠(SQ-TO) = ∠(DB-EA) [149] & S,Q,R are collinear [29] & A,B,E are collinear [03] & ∠NOT = ∠NQS [140] & BD ∥ KL [16] ⇒  ∠QNO = ∠KLM [150]
116. ∠QON = ∠KML [143] & ∠QNO = ∠KLM [150] (Similar Triangles)⇒  OQ:ON = MK:ML [151]
117. OQ:ON = MK:ML [151] & QN = QO [25] ⇒  QN:ON = KM:LM [152]
118. A,B,L are collinear [15] & C,A,F are collinear [04] & AC ⟂ DF [104] & CH ⟂ AB [08] & CH ∥ LN [36] ⇒  ∠BLN = ∠DFC [153]
119. C,B,N are collinear [20] & C,A,F are collinear [04] & ∠DCA = ∠CAD [103] & ∠CAD = ∠(BC-DE) [130] & BA ⟂ DE [60] & CH ⟂ AB [08] & LN ∥ CH [36] ⇒  ∠BNL = ∠DCF [154]
120. ∠BLN = ∠DFC [153] & ∠BNL = ∠DCF [154] (Similar Triangles)⇒  BL:BN = DF:DC [155]
121. BE = JE [11] ⇒  DE:BE = DE:BE [156]
122. DE:DK = JE:BL [50] & JE:AM = DE:DK [55] & BE = JE [11] ⇒  BE:AM = BE:BL [157]
123. DE:BE = DE:BE [156] & BE:AM = BE:BL [157] ⇒  AM = BL [158]
124. BL:BN = DF:DC [155] & AM = BL [158] & DC = DB [00] ⇒  BL:BN = DF:DB [159]
125. CI ⟂ EA [126] & BP ⟂ BD [23] ⇒  ∠(CI-DB) = ∠(EA-BP) [160]
126. CI ⟂ EA [126] & BP ⟂ BD [23] ⇒  ∠(CI-BP) = ∠(EA-DB) [161]
127. A,B,E are collinear [03] & ∠(CI-DB) = ∠(EA-BP) [160] & C,I,H are collinear [33] & BA ⟂ DE [60] & CH ⟂ AB [08] ⇒  ∠AED = ∠PBD [162]
128. EA:EB = DA:DB [61] & A,B,E are collinear [03] ⇒  ∠ADE = ∠EDB [163]
129. D,P,E are collinear [24] & ∠ADE = ∠EDB [163] ⇒  ∠ADE = ∠PDB [164]
130. ∠AED = ∠PBD [162] & ∠ADE = ∠PDB [164] (Similar Triangles)⇒  EA:ED = BP:BD [165]
131. EA:ED = BP:BD [165] & EJ = EB [11] & EB = EA [02] ⇒  JE:DE = BP:DB [166]
132. DC = DB [00] & EB = EA [02] & DE:DK = JE:BL [50] & BL:BN = DF:DB [159] & JE:DE = BP:DB [166] (Ratio chase)⇒  BN:BP = DK:DF [167]
133. A,B,E are collinear [03] & C,I,H are collinear [33] & ∠(CI-BP) = ∠(EA-DB) [161] ⇒  ∠DBP = ∠(EA-CI) [168]
134. DC = DB [00] ⇒  ∠DBC = ∠BCD [169]
135. A,B,E are collinear [03] & ∠DBC = ∠BCD [169] & ∠BCD = ∠(AB-DF) [109] & BC ∥ EF [88] ⇒  ∠(EA-DF) = ∠(DB-FE) [170]
136. ∠DBP = ∠(EA-CI) [168] & ∠(EA-DF) = ∠(DB-FE) [170] ⇒  ∠(CI-DF) = ∠(BP-FE) [171]
137. C,B,N are collinear [20] & D,K,E are collinear [13] & ∠(CI-DF) = ∠(BP-FE) [171] & C,I,H are collinear [33] & BA ⟂ DE [60] & CH ⟂ AB [08] & EF ∥ BC [88] ⇒  ∠NBP = ∠FDK [172]
138. BN:BP = DK:DF [167] & ∠NBP = ∠FDK [172] (Similar Triangles)⇒  BN:PN = DK:KF [173]
139. BN:BP = DK:DF [167] & ∠NBP = ∠FDK [172] (Similar Triangles)⇒  ∠BNP = ∠FKD [174]
140. BN:BP = DK:DF [167] & ∠NBP = ∠FDK [172] (Similar Triangles)⇒  ∠(NP-DF) = ∠(BP-FK) [175]
141. E is midpoint of AB [86] & S is midpoint of NT [110] ⇒  EA:AB = SN:NT [176]
142. EA:AB = SN:NT [176] & EJ = EB [11] & EB = EA [02] & SN = ST [32] ⇒  JE:AB = ST:TN [177]
143. F is midpoint of AC [87] & S is midpoint of NT [110] ⇒  FA:AC = SN:NT [178]
144. A,B,H are collinear [09] & CH ⟂ AB [08] ⇒  CH ⟂ HA [179]
145. CH ⟂ HA [179] & F is midpoint of AC [87] ⇒  AF = HF [180]
146. CH ⟂ HA [179] & F is midpoint of AC [87] ⇒  CF = HF [181]
147. FA:AC = SN:NT [178] & AF = HF [180] & SN = ST [32] ⇒  FH:AC = ST:TN [182]
148. EF ∥ BC [88] & A,C,F are collinear [04] & A,B,E are collinear [03] ⇒  AF:AC = FE:CB [183]
149. AF:AC = FE:CB [183] & AF = HF [180] ⇒  FH:AC = FE:CB [184]
150. A,B,E are collinear [03] & D,P,E are collinear [24] & CH ⟂ AB [08] & BA ⟂ DE [60] ⇒  ∠BED = ∠PEB [185]
151. A,B,E are collinear [03] & ∠(CI-DB) = ∠(EA-BP) [160] & C,I,H are collinear [33] & BA ⟂ DE [60] & CH ⟂ AB [08] ⇒  ∠BDE = ∠PBE [186]
152. ∠BED = ∠PEB [185] & ∠BDE = ∠PBE [186] (Similar Triangles)⇒  BE:BD = PE:PB [187]
153. BE:BD = PE:PB [187] & EJ = EB [11] ⇒  JE:DB = PE:BP [188]
154. A,B,H are collinear [09] & C,J,H are collinear [12] & CH ⟂ AB [08] & BI ⟂ AI [82] ⇒  ∠BIA = ∠BHJ [189]
155. C,J,H are collinear [12] & ∠ABJ = ∠AIJ [71] & C,I,H are collinear [33] ⇒  ∠BAI = ∠BJH [190]
156. ∠BIA = ∠BHJ [189] & ∠BAI = ∠BJH [190] (Similar Triangles)⇒  BI:AB = BH:BJ [191]
157. CB:BN = BH:BL [38] & DE:DK = JE:BL [50] & JE:AB = ST:TN [177] & FH:AC = ST:TN [182] & FH:AC = FE:CB [184] & GF:GB = GE:CG [90] & GF:GB = FE:CB [91] & CG:GE = GJ:GK [63] & JE:DE = BP:DB [166] & JE:DB = PE:BP [188] & BI:AB = BH:BJ [191] & AH:AI = AJ:AB [78] & AH:IH = AJ:BJ [79] & BI:AB = IH:AI [85] & GK:GJ = KF:BI [100] & BN:PN = DK:KF [173] (Ratio chase)⇒  CB:BJ = PN:PE [192]
158. D,P,E are collinear [24] & ∠BNP = ∠FKD [174] & C,B,N are collinear [20] & D,K,E are collinear [13] & FK ∥ BJ [93] ⇒  ∠NPE = ∠CBJ [193]
159. PN:PE = BC:BJ [192] & ∠NPE = ∠CBJ [193] (Similar Triangles)⇒  PN:NE = CB:CJ [194]
160. PN:PE = BC:BJ [192] & ∠NPE = ∠CBJ [193] (Similar Triangles)⇒  ∠PNE = ∠BCJ [195]
161. A,B,L are collinear [15] & A,B,E are collinear [03] & A,B,H are collinear [09] & C,I,H are collinear [33] & CH ∥ LN [36] ⇒  ∠ELN = ∠BHI [196]
162. C,I,H are collinear [33] & A,B,E are collinear [03] & BJ ⟂ AJ [67] & CH ⟂ AB [08] ⇒  ∠(CI-EA) = ∠BJA [197]
163. ∠(CI-EA) = ∠BJA [197] & ∠(EA-BI) = ∠(AJ-CI) [96] ⇒  ∠CIB = ∠(BJ-CI) [198]
164. C,I,H are collinear [33] & ∠CIB = ∠(BJ-CI) [198] & BJ ∥ FK [93] ⇒  ∠BIC = ∠(CI-KF) [199]
165. C,I,H are collinear [33] & ∠BNP = ∠FKD [174] & C,B,N are collinear [20] & D,K,E are collinear [13] & FK ∥ BJ [93] & BA ⟂ DE [60] & CH ⟂ AB [08] & BC ∥ EF [88] ⇒  ∠(CI-PN) = ∠KFE [200]
166. ∠BIC = ∠(CI-KF) [199] & ∠(CI-PN) = ∠KFE [200] ⇒  ∠(BI-PN) = ∠(CI-FE) [201]
167. C,I,H are collinear [33] & ∠(BI-PN) = ∠(CI-FE) [201] & BA ⟂ DE [60] & CH ⟂ AB [08] & EF ∥ BC [88] & ∠PNE = ∠BCJ [195] & C,J,H are collinear [12] & LN ∥ CH [36] ⇒  ∠ENL = ∠BIH [202]
168. ∠ELN = ∠BHI [196] & ∠ENL = ∠BIH [202] (Similar Triangles)⇒  LE:LN = HB:HI [203]
169. ∠ELN = ∠BHI [196] & ∠ENL = ∠BIH [202] (Similar Triangles)⇒  LE:BH = NL:IH [204]
170. A,B,H are collinear [09] & A,B,E are collinear [03] & C,I,H are collinear [33] & C,J,H are collinear [12] & CH ⟂ AB [08] ⇒  ∠EHI = ∠JHE [205]
171. EJ = EB [11] & EI = EB [34] ⇒  EI = EJ [206]
172. EI = EJ [206] ⇒  ∠EIJ = ∠IJE [207]
173. C,I,H are collinear [33] & C,J,H are collinear [12] & ∠EIJ = ∠IJE [207] ⇒  ∠EIH = ∠HJE [208]
174. ∠EHI = ∠JHE [205] & ∠EIH = ∠HJE [208] (Similar Triangles)⇒  HI = HJ [209]
175. LE:LN = HB:HI [203] & HI = HJ [209] ⇒  LE:NL = BH:JH [210]
176. A,B,L are collinear [15] & A,B,E are collinear [03] & C,I,H are collinear [33] & A,B,H are collinear [09] & CH ⟂ AB [08] & CH ∥ LN [36] ⇒  ∠ELN = ∠IHA [211]
177. A,B,E are collinear [03] & C,I,H are collinear [33] & ∠ABJ = ∠AIJ [71] & C,J,H are collinear [12] & BJ ∥ FK [93] ⇒  ∠(EA-KF) = ∠AIC [212]
178. ∠(EA-KF) = ∠AIC [212] & ∠(CI-PN) = ∠KFE [200] ⇒  ∠(AI-PN) = ∠AEF [213]
179. A,B,H are collinear [09] & ∠(AI-PN) = ∠AEF [213] & A,B,E are collinear [03] & EF ∥ BC [88] & ∠PNE = ∠BCJ [195] & C,J,H are collinear [12] & BA ⟂ DE [60] & CH ⟂ AB [08] & LN ∥ CH [36] ⇒  ∠ENL = ∠IAH [214]
180. ∠ELN = ∠IHA [211] & ∠ENL = ∠IAH [214] (Similar Triangles)⇒  LE:NL = IH:AH [215]
181. ∠ELN = ∠IHA [211] & ∠ENL = ∠IAH [214] (Similar Triangles)⇒  IH:LE = AH:NL [216]
182. EB = EA [02] & CB:BN = BH:BL [38] & CB:BN = CH:NL [39] & AC:AO = AH:AM [42] & AC:AO = CH:OM [43] & DE:DK = JE:BL [50] & JE:AM = DE:DK [55] (Ratio chase)⇒  BH:AH = OM:NL [217]
183. EB = EA [02] & CB:BN = BH:BL [38] & CB:BN = CH:NL [39] & AC:AO = AH:AM [42] & AC:AO = CH:OM [43] & DE:DK = JE:BL [50] & JE:AM = DE:DK [55] (Ratio chase)⇒  BH:OM = AH:NL [218]
184. LE:NL = IH:AH [215] & BH:AH = OM:NL [217] ⇒  IH:BH = LE:OM [219]
185. AE:DE = AM:DK [53] & EJ = EB [11] & EB = EA [02] & DE:BE = DK:BL [47] ⇒  DK:BL = DK:AM [220]
186. ME:AM = KE:DK [54] & KE:DK = LE:BL [48] ⇒  LE:BL = ME:AM [221]
187. DK:BL = DK:AM [220] & LE:BL = ME:AM [221] ⇒  LE = ME [222]
188. IH:BH = LE:OM [219] & HI = HJ [209] & ME = LE [222] ⇒  HB:HI = MO:ME [223]
189. A,B,H are collinear [09] & C,I,H are collinear [33] & A,B,M are collinear [17] & A,B,E are collinear [03] & CH ⟂ AB [08] & CH ∥ MO [40] ⇒  ∠BHI = ∠OME [224]
190. HB:HI = MO:ME [223] & ∠BHI = ∠OME [224] (Similar Triangles)⇒  BH:BI = OM:OE [225]
191. HB:HI = MO:ME [223] & ∠BHI = ∠OME [224] (Similar Triangles)⇒  ∠BIH = ∠OEM [226]
192. HB:HI = MO:ME [223] & ∠BHI = ∠OME [224] (Similar Triangles)⇒  ∠HBI = ∠MOE [227]
193. BH:BI = OM:OE [225] & BI = JB [99] ⇒  BH:BJ = OM:OE [228]
194. BH:OM = AH:NL [218] & IH:LE = AH:NL [216] ⇒  IH:LE = BH:OM [229]
195. IH:LE = BH:OM [229] & LE:BH = NL:IH [204] ⇒  LE:NL = OM:LE [230]
196. LE:NL = OM:LE [230] & ME = LE [222] ⇒  LE:LN = MO:ME [231]
197. A,B,L are collinear [15] & A,B,E are collinear [03] & A,B,M are collinear [17] & CH ⟂ AB [08] & CH ∥ LN [36] & CH ∥ MO [40] ⇒  ∠ELN = ∠OME [232]
198. LE:LN = MO:ME [231] & ∠ELN = ∠OME [232] (Similar Triangles)⇒  NE:EO = NL:EM [233]
199. NE:EO = NL:EM [233] & LE = ME [222] & LE:NL = BH:JH [210] ⇒  BH:JH = OE:NE [234]
200. BH:JH = OE:NE [234] & IH = JH [209] ⇒  HB:HI = EO:EN [235]
201. ∠PNE = ∠BCJ [195] & C,J,H are collinear [12] & BA ⟂ DE [60] & CH ⟂ AB [08] & ∠(BI-PN) = ∠(CI-FE) [201] & C,I,H are collinear [33] & EF ∥ BC [88] ⇒  ∠(BI-PN) = ∠ENP [236]
202. ∠(BI-PN) = ∠ENP [236] ⇒  BI ∥ NE [237]
203. A,B,H are collinear [09] & C,I,H are collinear [33] & ∠BIH = ∠OEM [226] & A,B,M are collinear [17] & A,B,E are collinear [03] & BI ∥ EN [237] ⇒  ∠BHI = ∠OEN [238]
204. HB:HI = EO:EN [235] & ∠BHI = ∠OEN [238] (Similar Triangles)⇒  BH:BI = OE:ON [239]
205. HB:HI = EO:EN [235] & ∠BHI = ∠OEN [238] (Similar Triangles)⇒  ∠BIH = ∠ONE [240]
206. BH:BI = OE:ON [239] & BI = JB [99] ⇒  BH:BJ = OE:ON [241]
207. DC = DB [00] & DB = DA [01] & EB = EA [02] & QN = QO [25] & RE = RN [27] & CB:BN = BH:BL [38] & CB:BN = CH:NL [39] & AC:AO = AH:AM [42] & AC:AO = CH:OM [43] & DE:KE = JE:LE [49] & DE:DK = JE:BL [50] & JE:AM = DE:DK [55] & KM:LM = DB:AB [59] & CG:GE = GJ:GK [63] & CG:GE = CJ:KE [64] & BH:IH = BJ:AJ [75] & AH:AI = AJ:AB [78] & AH:IH = AJ:BJ [79] & BI:AB = IH:AI [85] & GK:GJ = KF:BI [100] & DB:AC = TR:NE [125] & QN:ON = KM:LM [152] & BN:PN = DK:KF [173] & PN:NE = CB:CJ [194] & LE:NL = BH:JH [210] & BH:BJ = OM:OE [228] & BH:BJ = OE:ON [241] (Ratio chase)⇒  QN:RE = AC:AI [242]
208. DC = DB [00] & DB = DA [01] & EB = EA [02] & QN = QO [25] & RE = RN [27] & CB:BN = BH:BL [38] & CB:BN = CH:NL [39] & AC:AO = AH:AM [42] & AC:AO = CH:OM [43] & DE:KE = JE:LE [49] & DE:DK = JE:BL [50] & JE:AM = DE:DK [55] & KM:LM = DB:AB [59] & CG:GE = GJ:GK [63] & CG:GE = CJ:KE [64] & BH:IH = BJ:AJ [75] & AH:AI = AJ:AB [78] & AH:IH = AJ:BJ [79] & BI:AB = IH:AI [85] & GK:GJ = KF:BI [100] & DB:AC = TR:NE [125] & QN:ON = KM:LM [152] & BN:PN = DK:KF [173] & PN:NE = CB:CJ [194] & LE:NL = BH:JH [210] & BH:BJ = OM:OE [228] & BH:BJ = OE:ON [241] (Ratio chase)⇒  QN:AC = RE:AI [243]
209. QN:RE = AC:AI [242] & ER = TR [119] & TQ = QN [133] ⇒  TQ:TR = AC:AI [244]
210. A,B,E are collinear [03] & A,C,O are collinear [22] & ∠COT = ∠CNT [135] & C,B,N are collinear [20] & S,N,T are collinear [31] & ∠TNB = ∠TEB [114] ⇒  ∠AET = ∠AOT [245]
211. ∠AET = ∠AOT [245] ⇒  A,O,T,E are concyclic [246]
212. A,O,T,E are concyclic [246] ⇒  ∠AOE = ∠ATE [247]
213. A,O,T,E are concyclic [246] ⇒  ∠ATO = ∠AEO [248]
214. C,J,H are collinear [12] & A,B,H are collinear [09] & ∠(AJ-CH) = ∠ABI [73] ⇒  ∠AJH = ∠HBI [249]
215. A,B,H are collinear [09] & C,J,H are collinear [12] & C,I,H are collinear [33] & ∠(AB-CH) = ∠(AB-CH) [10] ⇒  ∠AHJ = ∠BHI [250]
216. ∠AJH = ∠HBI [249] & ∠AHJ = ∠BHI [250] (Similar Triangles)⇒  HA:HJ = HI:HB [251]
217. HA:HJ = HI:HB [251] & HI = HJ [209] ⇒  IH:AH = BH:IH [252]
218. IH:AH = BH:IH [252] & IH:LE = BH:OM [229] ⇒  IH:AH = OM:LE [253]
219. OM:LE = IH:AH [253] & ME = LE [222] ⇒  MO:ME = HI:HA [254]
220. A,B,M are collinear [17] & A,B,E are collinear [03] & C,I,H are collinear [33] & A,B,H are collinear [09] & CH ∥ MO [40] ⇒  ∠OME = ∠IHA [255]
221. MO:ME = HI:HA [254] & ∠OME = ∠IHA [255] (Similar Triangles)⇒  ∠MOE = ∠HIA [256]
222. ∠AOE = ∠ATE [247] & A,C,O are collinear [22] & ∠MOE = ∠HIA [256] & C,I,H are collinear [33] & MO ∥ CH [40] ⇒  ∠(AI-TE) = ∠CAT [257]
223. C,I,H are collinear [33] & ∠ADE = ∠ACB [129] & BA ⟂ DE [60] & CH ⟂ AB [08] & BC ∥ EF [88] ⇒  ∠(AD-CI) = ∠(AC-FE) [258]
224. S,Q,R are collinear [29] & ∠(SQ-TE) = ∠CAD [117] ⇒  ∠(AD-TE) = ∠(AC-SQ) [259]
225. ∠(AD-CI) = ∠(AC-FE) [258] & ∠(AD-TE) = ∠(AC-SQ) [259] ⇒  ∠(FE-SQ) = ∠(CI-TE) [260]
226. C,I,H are collinear [33] & ∠(BI-PN) = ∠(CI-FE) [201] ⇒  ∠CIB = ∠(FE-PN) [261]
227. C,I,H are collinear [33] & ∠REN = ∠ENR [122] & ∠NET = ∠NRS [120] & S,Q,R are collinear [29] & ∠(SQ-TE) = ∠CAD [117] & ∠DAC = ∠(DE-BC) [130] & BA ⟂ DE [60] & CH ⟂ AB [08] & BC ∥ EF [88] ⇒  ∠(CI-FE) = ∠REN [262]
228. ∠TBN = ∠TEN [115] & C,B,N are collinear [20] & BC ∥ EF [88] ⇒  ∠(FE-TB) = ∠NET [263]
229. ∠(CI-FE) = ∠REN [262] & ∠(FE-TB) = ∠NET [263] ⇒  ∠(CI-TB) = ∠RET [264]
230. C,I,H are collinear [33] & ∠TBN = ∠TEN [115] & C,B,N are collinear [20] & ∠(CI-TB) = ∠RET [264] & BA ⟂ DE [60] & CH ⟂ AB [08] & BC ∥ EF [88] ⇒  ∠(CI-RE) = ∠FEN [265]
231. ∠CIB = ∠(FE-PN) [261] & ∠(CI-RE) = ∠FEN [265] ⇒  ∠PNE = ∠(BI-ER) [266]
232. S,Q,R are collinear [29] & ∠(FE-SQ) = ∠(CI-TE) [260] & C,I,H are collinear [33] & EF ∥ BC [88] & BA ⟂ DE [60] & CH ⟂ AB [08] & ∠PNE = ∠BCJ [195] & C,J,H are collinear [12] & ∠PNE = ∠(BI-ER) [266] ⇒  ∠(BI-RE) = ∠(SQ-TE) [267]
233. RE = RT [119] ⇒  ∠RET = ∠ETR [268]
234. ∠(BI-RE) = ∠(SQ-TE) [267] & ∠RET = ∠ETR [268] ⇒  ∠(BI-TE) = ∠(SQ-TR) [269]
235. A,B,E are collinear [03] & ∠ATO = ∠AEO [248] & ∠MOE = ∠HIA [256] & C,I,H are collinear [33] & MO ∥ CH [40] ⇒  ∠EAT = ∠(AI-TO) [270]
236. ∠AIB = ∠(EA-CI) [95] & ∠EAT = ∠(AI-TO) [270] ⇒  ∠(CI-AT) = ∠(BI-TO) [271]
237. ∠(CI-AT) = ∠(BI-TO) [271] & C,I,H are collinear [33] & ∠BIH = ∠ONE [240] & BA ⟂ DE [60] & CH ⟂ AB [08] & BI ∥ EN [237] ⇒  ∠(BI-ON) = ∠ATO [272]
238. S,Q,R are collinear [29] & ∠TON = ∠TQS [141] ⇒  ∠(ON-SQ) = ∠OTQ [273]
239. ∠(BI-ON) = ∠ATO [272] & ∠(ON-SQ) = ∠OTQ [273] ⇒  ∠(BI-SQ) = ∠ATQ [274]
240. ∠(BI-TE) = ∠(SQ-TR) [269] & S,Q,R are collinear [29] & ∠(BI-SQ) = ∠ATQ [274] ⇒  ∠ATQ = ∠ETR [275]
241. ∠(AI-TE) = ∠CAT [257] & ∠ATQ = ∠ETR [275] ⇒  ∠CAI = ∠QTR [276]
242. TQ:TR = AC:AI [244] & ∠CAI = ∠QTR [276] (Similar Triangles)⇒  ∠TQR = ∠ACI [277]
243. EB = EA [02] & CB:BN = BH:BL [38] & CB:BN = CH:NL [39] & AC:AO = AH:AM [42] & AC:AO = CH:OM [43] & DE:KE = JE:LE [49] & DE:DK = JE:BL [50] & JE:AM = DE:DK [55] & JE:AB = ST:TN [177] & FH:AC = ST:TN [182] & FH:AC = FE:CB [184] & GF:GB = GE:CG [90] & GF:GB = FE:CB [91] & CG:GE = CJ:KE [64] & JE:DE = BP:DB [166] & JE:DB = PE:BP [188] & BH:IH = BJ:AJ [75] & AH:IH = AJ:BJ [79] & BI:AB = IH:AI [85] & LE:NL = BH:JH [210] & BH:BJ = OM:OE [228] (Ratio chase)⇒  CJ:AI = OE:PE [278]
244. A,B,E are collinear [03] & C,I,H are collinear [33] & BJ ⟂ AJ [67] & CH ⟂ AB [08] ⇒  ∠(EA-CI) = ∠BJA [279]
245. A,B,E are collinear [03] & C,I,H are collinear [33] & ∠ABI = ∠AJI [70] & C,J,H are collinear [12] ⇒  ∠EAJ = ∠BIC [280]
246. ∠(EA-CI) = ∠BJA [279] & ∠EAJ = ∠BIC [280] ⇒  ∠(EA-BI) = ∠(BJ-EA) [281]
247. ∠(EA-BI) = ∠(BJ-EA) [281] & A,B,E are collinear [03] ⇒  ∠ABI = ∠JBA [282]
248. A,B,I,J are concyclic [69] & ∠ABI = ∠JBA [282] ⇒  AI = JA [283]
249. OE:PE = CJ:AI [278] & AJ = AI [283] ⇒  EO:EP = JC:JA [284]
250. IH:AH = OM:LE [253] & JH = IH [209] & ME = LE [222] ⇒  HJ:HA = MO:ME [285]
251. C,J,H are collinear [12] & A,B,H are collinear [09] & A,B,M are collinear [17] & A,B,E are collinear [03] & CH ⟂ AB [08] & CH ∥ MO [40] ⇒  ∠JHA = ∠EMO [286]
252. HJ:HA = MO:ME [285] & ∠JHA = ∠EMO [286] (Similar Triangles)⇒  ∠HJA = ∠EOM [287]
253. DB = DA [01] ⇒  ∠DBA = ∠BAD [288]
254. CH ⟂ AB [08] & MO ∥ CH [40] & ∠ADE = ∠EDB [163] & ∠DBA = ∠BAD [288] (Angle chase)⇒  MO ∥ DE [289]
255. D,P,E are collinear [24] & C,J,H are collinear [12] & ∠HJA = ∠EOM [287] & BA ⟂ DE [60] & CH ⟂ AB [08] & MO ∥ DE [289] ⇒  ∠OEP = ∠CJA [290]
256. EO:EP = JC:JA [284] & ∠OEP = ∠CJA [290] (Similar Triangles)⇒  ∠EOP = ∠JCA [291]
257. EO:EP = JC:JA [284] & ∠OEP = ∠CJA [290] (Similar Triangles)⇒  PO:AC = PE:AJ [292]
258. S,Q,R are collinear [29] & ∠TQR = ∠ACI [277] & C,I,H are collinear [33] & BA ⟂ DE [60] & CH ⟂ AB [08] & ∠EOP = ∠JCA [291] & C,J,H are collinear [12] & ∠MOE = ∠HIA [256] & MO ∥ CH [40] ⇒  ∠(AI-PO) = ∠SQT [293]
259. RB = RE [28] ⇒  ∠RBE = ∠BER [294]
260. ∠RBE = ∠BER [294] & A,B,E are collinear [03] ⇒  ∠RBA = ∠(AB-ER) [295]
261. RE = RN [27] & RB = RE [28] ⇒  RB = RN [296]
262. RB = RN [296] ⇒  ∠RBN = ∠BNR [297]
263. ∠RBN = ∠BNR [297] & C,B,N are collinear [20] ⇒  ∠RBC = ∠(BC-NR) [298]
264. EF ∥ BC [88] & ∠ADE = ∠EDB [163] & ∠DBA = ∠BAD [288] & ∠RBA = ∠(AB-ER) [295] & ∠RBC = ∠(BC-NR) [298] & ∠REN = ∠ENR [122] (Angle chase)⇒  ∠FED = ∠RNE [299]
265. ∠PNE = ∠BCJ [195] & C,J,H are collinear [12] & BA ⟂ DE [60] & CH ⟂ AB [08] & ∠FED = ∠RNE [299] & EF ∥ BC [88] ⇒  ∠RNE = ∠PNE [300]
266. ∠RNE = ∠PNE [300] ⇒  RN ∥ PN [301]
267. NR ∥ NP [301] ⇒  R,P,N are collinear [302]
268. QN:RE = AC:AI [242] & ER = TR [119] & RN = TR [112] ⇒  NQ:NR = AC:AI [303]
269. AC ⟂ DF [104] & BJ ⟂ AJ [67] ⇒  ∠(AC-DF) = ∠AJB [304]
270. ∠(AC-DF) = ∠AJB [304] & BJ ∥ FK [93] ⇒  ∠(AC-DF) = ∠(AJ-KF) [305]
271. ∠(AC-DF) = ∠(AJ-KF) [305] & ∠(DF-PN) = ∠(KF-BP) [175] ⇒  ∠JAC = ∠BPN [306]
272. P,R,N are collinear [302] & ∠JAC = ∠BPN [306] ⇒  ∠JAC = ∠BPR [307]
273. S,Q,R are collinear [29] & ∠(SQ-BP) = ∠(SN-DB) [145] ⇒  ∠QSN = ∠PBD [308]
274. C,I,H are collinear [33] & ∠BCA = ∠BDE [131] & ∠COT = ∠CNT [135] & A,C,O are collinear [22] & C,B,N are collinear [20] & S,N,T are collinear [31] & BA ⟂ DE [60] & CH ⟂ AB [08] ⇒  ∠(SN-TO) = ∠(DB-CI) [309]
275. ∠QSN = ∠PBD [308] & ∠(SN-TO) = ∠(DB-CI) [309] ⇒  ∠(SQ-TO) = ∠(BP-CI) [310]
276. C,I,H are collinear [33] & S,Q,R are collinear [29] & ∠(SQ-TO) = ∠(BP-CI) [310] ⇒  ∠(CI-TO) = ∠(BP-SQ) [311]
277. ∠NOT = ∠NQS [140] & ∠(CI-TO) = ∠(BP-SQ) [311] ⇒  ∠(QN-BP) = ∠(ON-CI) [312]
278. NE:EO = NL:EM [233] & LE = ME [222] & LE:NL = IH:AH [215] & JH = IH [209] ⇒  HJ:HA = EO:EN [313]
279. C,J,H are collinear [12] & A,B,H are collinear [09] & ∠BIH = ∠OEM [226] & C,I,H are collinear [33] & A,B,M are collinear [17] & A,B,E are collinear [03] & BI ∥ EN [237] ⇒  ∠JHA = ∠NEO [314]
280. HJ:HA = EO:EN [313] & ∠JHA = ∠NEO [314] (Similar Triangles)⇒  ∠HJA = ∠NOE [315]
281. ∠(QN-BP) = ∠(ON-CI) [312] & C,I,H are collinear [33] & BA ⟂ DE [60] & CH ⟂ AB [08] & ∠HJA = ∠NOE [315] & C,J,H are collinear [12] & ∠MOE = ∠HIA [256] & MO ∥ CH [40] ⇒  ∠JAI = ∠(BP-QN) [316]
282. ∠JAC = ∠BPR [307] & ∠JAI = ∠(BP-QN) [316] ⇒  ∠(RP-QN) = ∠CAI [317]
283. ∠(RP-QN) = ∠CAI [317] & R,P,N are collinear [302] & NR ∥ NP [301] & ∠MOE = ∠HIA [256] & C,I,H are collinear [33] & MO ∥ CH [40] ⇒  ∠RNQ = ∠CAI [318]
284. NQ:NR = AC:AI [303] & ∠RNQ = ∠CAI [318] (Similar Triangles)⇒  ∠QRN = ∠AIC [319]
285. C,I,H are collinear [33] & S,Q,R are collinear [29] & P,R,N are collinear [302] & ∠QRN = ∠AIC [319] ⇒  ∠AIC = ∠(SQ-RP) [320]
286. ∠(AI-PO) = ∠SQT [293] & ∠AIC = ∠(SQ-RP) [320] ⇒  ∠(PO-CI) = ∠(TQ-RP) [321]
287. PO:AC = PE:AJ [292] & AI = JA [283] ⇒  PO:AC = PE:AI [322]
288. QN:AC = RE:AI [243] & ER = TR [119] & CQ = QN [26] & RB = RE [28] & RE = RN [27] & RN = RT [112] ⇒  CQ:AC = BR:AI [323]
289. PO:AC = PE:AI [322] & CQ:AC = BR:AI [323] ⇒  PO:CQ = PE:BR [324]
290. PO:CQ = PE:BR [324] & QC = QN [26] & RB = RE [28] & RE = RN [27] & RN = RT [112] & RE = TR [119] & QO = QN [25] ⇒  EP:ER = OP:OQ [325]
291. CF = HF [181] ⇒  ∠FCH = ∠CHF [326]
292. C,I,H are collinear [33] & ∠EOP = ∠JCA [291] & C,J,H are collinear [12] & BA ⟂ DE [60] & CH ⟂ AB [08] & ∠FCH = ∠CHF [326] & A,C,F are collinear [04] & ∠MOE = ∠HIA [256] & MO ∥ CH [40] ⇒  ∠(CI-FH) = ∠(PO-AI) [327]
293. A,B,H are collinear [09] & C,J,H are collinear [12] & ∠HBI = ∠MOE [227] & MO ∥ CH [40] & BA ⟂ DE [60] & CH ⟂ AB [08] & BI ∥ EN [237] ⇒  ∠BHJ = ∠NEO [328]
294. HB:HJ = EO:EN [234] & ∠BHJ = ∠NEO [328] (Similar Triangles)⇒  ∠HBJ = ∠NOE [329]
295. A,B,E are collinear [03] & S,Q,R are collinear [29] & ∠(SQ-TO) = ∠EAD [137] ⇒  ∠(EA-SQ) = ∠(AD-TO) [330]
296. QT = QO [139] ⇒  ∠OTQ = ∠QOT [331]
297. S,Q,R are collinear [29] & ∠TON = ∠TQS [141] & ∠OTQ = ∠QOT [331] ⇒  ∠QOT = ∠(ON-SQ) [332]
298. ∠(EA-SQ) = ∠(AD-TO) [330] & ∠QOT = ∠(ON-SQ) [332] ⇒  ∠(EA-ON) = ∠(AD-QO) [333]
299. ∠HBJ = ∠NOE [329] & A,B,H are collinear [09] & ∠MOE = ∠HIA [256] & C,I,H are collinear [33] & MO ∥ CH [40] & ∠(EA-ON) = ∠(AD-QO) [333] & A,B,E are collinear [03] & BJ ∥ FK [93] ⇒  ∠(AD-QO) = ∠(KF-AI) [334]
300. C,I,H are collinear [33] & ∠ACB = ∠EDB [131] & BC ∥ EF [88] & BA ⟂ DE [60] & CH ⟂ AB [08] ⇒  ∠(AC-FE) = ∠(CI-DB) [335]
301. C,I,H are collinear [33] & ∠FCH = ∠CHF [326] & A,C,F are collinear [04] ⇒  ∠ACI = ∠(CI-FH) [336]
302. ∠(AC-FE) = ∠(CI-DB) [335] & ∠ACI = ∠(CI-FH) [336] ⇒  ∠(DB-FH) = ∠(FE-CI) [337]
303. ∠(DB-FH) = ∠(FE-CI) [337] & C,I,H are collinear [33] & EF ∥ BC [88] & BA ⟂ DE [60] & CH ⟂ AB [08] & ∠PNE = ∠BCJ [195] & C,J,H are collinear [12] & ∠PNE = ∠(BI-ER) [266] ⇒  ∠(BI-RE) = ∠(DB-FH) [338]
304. EI = EB [34] ⇒  ∠EBI = ∠BIE [339]
305. ∠EBI = ∠BIE [339] & A,B,E are collinear [03] ⇒  ∠ABI = ∠BIE [340]
306. EI = EA [81] ⇒  ∠EIA = ∠IAE [341]
307. ∠EIA = ∠IAE [341] & A,B,E are collinear [03] ⇒  ∠EIA = ∠IAB [342]
308. CH ⟂ AB [08] & KL ∥ BD [16] & ∠AIB = ∠AJB [72] & ∠ABI = ∠(AJ-CH) [73] & ∠DBA = ∠BAD [288] & ∠ABI = ∠BIE [340] & ∠EIA = ∠IAB [342] (Angle chase)⇒  ∠(KL-BJ) = ∠(BI-AD) [343]
309. ∠(KL-BJ) = ∠(BI-AD) [343] & KL ∥ BD [16] & BJ ∥ FK [93] ⇒  ∠(BI-AD) = ∠(DB-KF) [344]
310. ∠(BI-RE) = ∠(DB-FH) [338] & ∠(BI-AD) = ∠(DB-KF) [344] ⇒  ∠HFK = ∠(ER-AD) [345]
311. ∠(AD-QO) = ∠(KF-AI) [334] & ∠(AD-RE) = ∠KFH [345] ⇒  ∠(OQ-AI) = ∠(ER-FH) [346]
312. ∠(CI-FH) = ∠(PO-AI) [327] & ∠(QO-AI) = ∠(RE-FH) [346] ⇒  ∠(CI-RE) = ∠POQ [347]
313. D,P,E are collinear [24] & ∠(CI-RE) = ∠POQ [347] & C,I,H are collinear [33] & BA ⟂ DE [60] & CH ⟂ AB [08] ⇒  ∠PER = ∠POQ [348]
314. EP:ER = OP:OQ [325] & ∠PER = ∠POQ [348] (Similar Triangles)⇒  ∠EPR = ∠OPQ [349]
315. ∠(PO-CI) = ∠(TQ-RP) [321] & C,I,H are collinear [33] & R,P,N are collinear [302] & BA ⟂ DE [60] & CH ⟂ AB [08] & NR ∥ NP [301] & ∠EPR = ∠OPQ [349] & D,P,E are collinear [24] ⇒  ∠QPO = ∠(TQ-PO) [350]
316. ∠QPO = ∠(TQ-PO) [350] ⇒  QP ∥ TQ [351]
317. PQ ∥ QT [351] ⇒  T,Q,P are collinear
==========================

