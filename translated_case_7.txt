I0123 14:03:01.343186 140268056924160 inference_utils.py:69] Parsing gin configuration.
I0123 14:03:01.343323 140268056924160 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0123 14:03:01.343627 140268056924160 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0123 14:03:01.343660 140268056924160 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0123 14:03:01.343687 140268056924160 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0123 14:03:01.343713 140268056924160 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0123 14:03:01.343738 140268056924160 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0123 14:03:01.343763 140268056924160 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0123 14:03:01.343791 140268056924160 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0123 14:03:01.343818 140268056924160 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=32
I0123 14:03:01.343844 140268056924160 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0123 14:03:01.343869 140268056924160 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0123 14:03:01.343920 140268056924160 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0123 14:03:01.344098 140268056924160 resource_reader.py:55] Path not found: base_htrans.gin
I0123 14:03:01.344368 140268056924160 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0123 14:03:01.344463 140268056924160 resource_reader.py:55] Path not found: trainer_configuration.gin
I0123 14:03:01.350851 140268056924160 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0123 14:03:01.350970 140268056924160 resource_reader.py:55] Path not found: size/medium_150M.gin
I0123 14:03:01.351289 140268056924160 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0123 14:03:01.351393 140268056924160 resource_reader.py:55] Path not found: options/positions_t5.gin
I0123 14:03:01.351669 140268056924160 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0123 14:03:01.351768 140268056924160 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0123 14:03:01.352172 140268056924160 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0123 14:03:01.352270 140268056924160 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0123 14:03:01.356054 140268056924160 training_loop.py:334] ==== Training loop: initializing model ====
I0123 14:03:01.449522 140268056924160 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0123 14:03:01.450362 140268056924160 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0123 14:03:01.456764 140268056924160 training_loop.py:335] Process 0 of 1
I0123 14:03:01.456819 140268056924160 training_loop.py:336] Local device count = 1
I0123 14:03:01.456859 140268056924160 training_loop.py:337] Number of replicas = 1
I0123 14:03:01.456891 140268056924160 training_loop.py:339] Using random number seed 42
I0123 14:03:01.964087 140268056924160 training_loop.py:359] Initializing the model.
I0123 14:03:02.360351 140268056924160 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.361115 140268056924160 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 14:03:02.361228 140268056924160 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 14:03:02.361305 140268056924160 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 14:03:02.361380 140268056924160 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 14:03:02.361460 140268056924160 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 14:03:02.361531 140268056924160 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 14:03:02.361600 140268056924160 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 14:03:02.361676 140268056924160 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 14:03:02.361744 140268056924160 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 14:03:02.361810 140268056924160 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 14:03:02.361875 140268056924160 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 14:03:02.361940 140268056924160 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 14:03:02.362006 140268056924160 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 14:03:02.362047 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:02.362093 140268056924160 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 14:03:02.362209 140268056924160 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:03:02.362247 140268056924160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:03:02.362278 140268056924160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:03:02.364375 140268056924160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.369913 140268056924160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:03:02.380802 140268056924160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.381096 140268056924160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:03:02.385414 140268056924160 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:03:02.396311 140268056924160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:03:02.396373 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:02.396414 140268056924160 transformer_layer.py:389] tlayer: self-attention.
I0123 14:03:02.396448 140268056924160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.396510 140268056924160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.397808 140268056924160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.397891 140268056924160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.398594 140268056924160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.401029 140268056924160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.406712 140268056924160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.408410 140268056924160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.408490 140268056924160 transformer_layer.py:468] tlayer: End windows.
I0123 14:03:02.408526 140268056924160 transformer_layer.py:472] tlayer: final FFN.
I0123 14:03:02.408587 140268056924160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.408720 140268056924160 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:03:02.409061 140268056924160 nn_components.py:325] mlp: activation = None
I0123 14:03:02.409107 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:02.410983 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.411089 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:02.413923 140268056924160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.414003 140268056924160 transformer_base.py:443] tbase: final FFN
I0123 14:03:02.414505 140268056924160 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:03:02.424500 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:02.433062 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.433159 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:02.433452 140268056924160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.433533 140268056924160 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 14:03:02.433649 140268056924160 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:03:02.433689 140268056924160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:03:02.433720 140268056924160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:03:02.435546 140268056924160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.437981 140268056924160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:03:02.443429 140268056924160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.443687 140268056924160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:03:02.446242 140268056924160 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:03:02.449959 140268056924160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:03:02.450013 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:02.450049 140268056924160 transformer_layer.py:389] tlayer: self-attention.
I0123 14:03:02.450079 140268056924160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.450140 140268056924160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.450691 140268056924160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.450766 140268056924160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.451123 140268056924160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.451888 140268056924160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.454319 140268056924160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.454936 140268056924160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.455016 140268056924160 transformer_layer.py:468] tlayer: End windows.
I0123 14:03:02.455051 140268056924160 transformer_layer.py:472] tlayer: final FFN.
I0123 14:03:02.455109 140268056924160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.455234 140268056924160 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:03:02.455698 140268056924160 nn_components.py:325] mlp: activation = None
I0123 14:03:02.455740 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:02.457770 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.457863 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:02.460473 140268056924160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.460553 140268056924160 transformer_base.py:443] tbase: final FFN
I0123 14:03:02.460987 140268056924160 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:03:02.463264 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:02.465139 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.465234 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:02.465522 140268056924160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.465601 140268056924160 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 14:03:02.465717 140268056924160 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:03:02.465757 140268056924160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:03:02.465787 140268056924160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:03:02.467656 140268056924160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.469951 140268056924160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:03:02.475769 140268056924160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.476034 140268056924160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:03:02.478629 140268056924160 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:03:02.482405 140268056924160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:03:02.482460 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:02.482495 140268056924160 transformer_layer.py:389] tlayer: self-attention.
I0123 14:03:02.482525 140268056924160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.482585 140268056924160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.483138 140268056924160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.483218 140268056924160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.483569 140268056924160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.484318 140268056924160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.486783 140268056924160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.487443 140268056924160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.487519 140268056924160 transformer_layer.py:468] tlayer: End windows.
I0123 14:03:02.487553 140268056924160 transformer_layer.py:472] tlayer: final FFN.
I0123 14:03:02.487612 140268056924160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.487741 140268056924160 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:03:02.488072 140268056924160 nn_components.py:325] mlp: activation = None
I0123 14:03:02.488115 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:02.489989 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.490081 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:02.492522 140268056924160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.492604 140268056924160 transformer_base.py:443] tbase: final FFN
I0123 14:03:02.493087 140268056924160 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:03:02.495311 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:02.497166 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.497258 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:02.497542 140268056924160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.497622 140268056924160 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 14:03:02.497738 140268056924160 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:03:02.497776 140268056924160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:03:02.497806 140268056924160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:03:02.499670 140268056924160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.501991 140268056924160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:03:02.507462 140268056924160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.507717 140268056924160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:03:02.510309 140268056924160 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:03:02.514046 140268056924160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:03:02.514100 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:02.514135 140268056924160 transformer_layer.py:389] tlayer: self-attention.
I0123 14:03:02.514166 140268056924160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.514229 140268056924160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.514785 140268056924160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.514860 140268056924160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.515215 140268056924160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.515968 140268056924160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.518466 140268056924160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.519083 140268056924160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.519160 140268056924160 transformer_layer.py:468] tlayer: End windows.
I0123 14:03:02.519195 140268056924160 transformer_layer.py:472] tlayer: final FFN.
I0123 14:03:02.519254 140268056924160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.519380 140268056924160 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:03:02.519707 140268056924160 nn_components.py:325] mlp: activation = None
I0123 14:03:02.519750 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:02.521606 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.521705 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:02.524208 140268056924160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.524290 140268056924160 transformer_base.py:443] tbase: final FFN
I0123 14:03:02.524726 140268056924160 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:03:02.526944 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:02.528810 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.528906 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:02.529194 140268056924160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.529273 140268056924160 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 14:03:02.529381 140268056924160 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:03:02.529419 140268056924160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:03:02.529449 140268056924160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:03:02.531345 140268056924160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.533686 140268056924160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:03:02.539167 140268056924160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.539413 140268056924160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:03:02.542031 140268056924160 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:03:02.545697 140268056924160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:03:02.545752 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:02.545787 140268056924160 transformer_layer.py:389] tlayer: self-attention.
I0123 14:03:02.545817 140268056924160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.545879 140268056924160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.546433 140268056924160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.546508 140268056924160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.546861 140268056924160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.547607 140268056924160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.550390 140268056924160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.551000 140268056924160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.551078 140268056924160 transformer_layer.py:468] tlayer: End windows.
I0123 14:03:02.551112 140268056924160 transformer_layer.py:472] tlayer: final FFN.
I0123 14:03:02.551170 140268056924160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.551304 140268056924160 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:03:02.551632 140268056924160 nn_components.py:325] mlp: activation = None
I0123 14:03:02.551674 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:02.553521 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.553615 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:02.556094 140268056924160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.556171 140268056924160 transformer_base.py:443] tbase: final FFN
I0123 14:03:02.556599 140268056924160 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:03:02.558929 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:02.560888 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.560982 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:02.561269 140268056924160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.561348 140268056924160 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 14:03:02.561456 140268056924160 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:03:02.561495 140268056924160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:03:02.561525 140268056924160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:03:02.563367 140268056924160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.565695 140268056924160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:03:02.571168 140268056924160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.571422 140268056924160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:03:02.574027 140268056924160 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:03:02.577669 140268056924160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:03:02.577724 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:02.577759 140268056924160 transformer_layer.py:389] tlayer: self-attention.
I0123 14:03:02.577789 140268056924160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.577849 140268056924160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.578442 140268056924160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.578517 140268056924160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.578874 140268056924160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.579629 140268056924160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.582065 140268056924160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.582676 140268056924160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.582758 140268056924160 transformer_layer.py:468] tlayer: End windows.
I0123 14:03:02.582797 140268056924160 transformer_layer.py:472] tlayer: final FFN.
I0123 14:03:02.582856 140268056924160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.582984 140268056924160 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:03:02.583310 140268056924160 nn_components.py:325] mlp: activation = None
I0123 14:03:02.583355 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:02.585215 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.585307 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:02.587811 140268056924160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.587889 140268056924160 transformer_base.py:443] tbase: final FFN
I0123 14:03:02.588322 140268056924160 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:03:02.590582 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:02.592447 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.592540 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:02.592824 140268056924160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.592903 140268056924160 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 14:03:02.593011 140268056924160 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:03:02.593049 140268056924160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:03:02.593080 140268056924160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:03:02.594927 140268056924160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.597307 140268056924160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:03:02.602812 140268056924160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.603075 140268056924160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:03:02.605636 140268056924160 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:03:02.609349 140268056924160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:03:02.609402 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:02.609438 140268056924160 transformer_layer.py:389] tlayer: self-attention.
I0123 14:03:02.609469 140268056924160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.609532 140268056924160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.610100 140268056924160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.610176 140268056924160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.610533 140268056924160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.611283 140268056924160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.613696 140268056924160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.614304 140268056924160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.614381 140268056924160 transformer_layer.py:468] tlayer: End windows.
I0123 14:03:02.614415 140268056924160 transformer_layer.py:472] tlayer: final FFN.
I0123 14:03:02.614472 140268056924160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.614595 140268056924160 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:03:02.614915 140268056924160 nn_components.py:325] mlp: activation = None
I0123 14:03:02.614958 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:02.616887 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.616980 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:02.619426 140268056924160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.619504 140268056924160 transformer_base.py:443] tbase: final FFN
I0123 14:03:02.619930 140268056924160 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:03:02.622509 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:02.624381 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.624483 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:02.624771 140268056924160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.624852 140268056924160 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 14:03:02.624959 140268056924160 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:03:02.624998 140268056924160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:03:02.625028 140268056924160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:03:02.898387 140268056924160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.902446 140268056924160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:03:02.908546 140268056924160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.908903 140268056924160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:03:02.911625 140268056924160 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:03:02.916013 140268056924160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:03:02.916077 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:02.916117 140268056924160 transformer_layer.py:389] tlayer: self-attention.
I0123 14:03:02.916152 140268056924160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.916227 140268056924160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.916921 140268056924160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.916999 140268056924160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.917360 140268056924160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.918153 140268056924160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.920726 140268056924160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.921384 140268056924160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.921463 140268056924160 transformer_layer.py:468] tlayer: End windows.
I0123 14:03:02.921499 140268056924160 transformer_layer.py:472] tlayer: final FFN.
I0123 14:03:02.921562 140268056924160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.921704 140268056924160 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:03:02.922083 140268056924160 nn_components.py:325] mlp: activation = None
I0123 14:03:02.922127 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:02.924004 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.924104 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:02.926632 140268056924160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.926711 140268056924160 transformer_base.py:443] tbase: final FFN
I0123 14:03:02.927172 140268056924160 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:03:02.929513 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:02.931373 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.931485 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:02.931775 140268056924160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.931859 140268056924160 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 14:03:02.931970 140268056924160 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:03:02.932010 140268056924160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:03:02.932041 140268056924160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:03:02.934022 140268056924160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.936361 140268056924160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:03:02.941869 140268056924160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.942133 140268056924160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:03:02.944758 140268056924160 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:03:02.948532 140268056924160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:03:02.948588 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:02.948624 140268056924160 transformer_layer.py:389] tlayer: self-attention.
I0123 14:03:02.948656 140268056924160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.948718 140268056924160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.949287 140268056924160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.949363 140268056924160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.949718 140268056924160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.950484 140268056924160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.952992 140268056924160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.953612 140268056924160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.953695 140268056924160 transformer_layer.py:468] tlayer: End windows.
I0123 14:03:02.953730 140268056924160 transformer_layer.py:472] tlayer: final FFN.
I0123 14:03:02.953788 140268056924160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.953912 140268056924160 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:03:02.954232 140268056924160 nn_components.py:325] mlp: activation = None
I0123 14:03:02.954275 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:02.956135 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.956227 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:02.958732 140268056924160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.958815 140268056924160 transformer_base.py:443] tbase: final FFN
I0123 14:03:02.959247 140268056924160 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:03:02.961478 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:02.963405 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.963499 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:02.963783 140268056924160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.963870 140268056924160 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 14:03:02.963980 140268056924160 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:03:02.964019 140268056924160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:03:02.964050 140268056924160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:03:02.965872 140268056924160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.968248 140268056924160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:03:02.973618 140268056924160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.973875 140268056924160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:03:02.976811 140268056924160 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:03:02.980437 140268056924160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:03:02.980491 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:02.980527 140268056924160 transformer_layer.py:389] tlayer: self-attention.
I0123 14:03:02.980557 140268056924160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.980619 140268056924160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.981206 140268056924160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.981282 140268056924160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.981648 140268056924160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.982407 140268056924160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.984825 140268056924160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.985432 140268056924160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.985509 140268056924160 transformer_layer.py:468] tlayer: End windows.
I0123 14:03:02.985543 140268056924160 transformer_layer.py:472] tlayer: final FFN.
I0123 14:03:02.985604 140268056924160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.985742 140268056924160 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:03:02.986059 140268056924160 nn_components.py:325] mlp: activation = None
I0123 14:03:02.986100 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:02.987926 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.988016 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:02.990507 140268056924160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.990586 140268056924160 transformer_base.py:443] tbase: final FFN
I0123 14:03:02.991011 140268056924160 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:03:02.993255 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:02.995120 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.995215 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:02.995498 140268056924160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.995584 140268056924160 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 14:03:02.995695 140268056924160 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:03:02.995734 140268056924160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:03:02.995764 140268056924160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:03:02.997567 140268056924160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:02.999953 140268056924160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:03:03.005370 140268056924160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:03.005629 140268056924160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:03:03.008199 140268056924160 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:03:03.011922 140268056924160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:03:03.011976 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:03.012012 140268056924160 transformer_layer.py:389] tlayer: self-attention.
I0123 14:03:03.012044 140268056924160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:03.012106 140268056924160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:03.012670 140268056924160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:03.012747 140268056924160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:03.013103 140268056924160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:03.014067 140268056924160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:03.016656 140268056924160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:03.017273 140268056924160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:03.017350 140268056924160 transformer_layer.py:468] tlayer: End windows.
I0123 14:03:03.017385 140268056924160 transformer_layer.py:472] tlayer: final FFN.
I0123 14:03:03.017447 140268056924160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:03.017588 140268056924160 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:03:03.017922 140268056924160 nn_components.py:325] mlp: activation = None
I0123 14:03:03.017966 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.019886 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:03.019983 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.022847 140268056924160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:03.022928 140268056924160 transformer_base.py:443] tbase: final FFN
I0123 14:03:03.023361 140268056924160 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:03:03.025630 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.027507 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:03.027605 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.027891 140268056924160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:03.027972 140268056924160 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 14:03:03.028090 140268056924160 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:03:03.028129 140268056924160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:03:03.028160 140268056924160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:03:03.030054 140268056924160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:03.032372 140268056924160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:03:03.037867 140268056924160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:03.038120 140268056924160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:03:03.040692 140268056924160 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:03:03.044457 140268056924160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:03:03.044511 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:03.044547 140268056924160 transformer_layer.py:389] tlayer: self-attention.
I0123 14:03:03.044578 140268056924160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:03.044639 140268056924160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:03.045209 140268056924160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:03.045289 140268056924160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:03.045652 140268056924160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:03.046412 140268056924160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:03.048848 140268056924160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:03.049826 140268056924160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:03.049908 140268056924160 transformer_layer.py:468] tlayer: End windows.
I0123 14:03:03.049943 140268056924160 transformer_layer.py:472] tlayer: final FFN.
I0123 14:03:03.050005 140268056924160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:03.050134 140268056924160 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:03:03.050460 140268056924160 nn_components.py:325] mlp: activation = None
I0123 14:03:03.050503 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.052361 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:03.052454 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.054907 140268056924160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:03.054987 140268056924160 transformer_base.py:443] tbase: final FFN
I0123 14:03:03.055471 140268056924160 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:03:03.057687 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.059556 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:03.059654 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.059944 140268056924160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:03.060243 140268056924160 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 14:03:03.060311 140268056924160 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 14:03:03.060376 140268056924160 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 14:03:03.060433 140268056924160 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 14:03:03.060487 140268056924160 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 14:03:03.060539 140268056924160 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 14:03:03.060590 140268056924160 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 14:03:03.060642 140268056924160 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 14:03:03.060693 140268056924160 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 14:03:03.060743 140268056924160 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 14:03:03.060793 140268056924160 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 14:03:03.060842 140268056924160 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 14:03:03.060879 140268056924160 decoder_stack.py:344] dstack: Final layernorm.
I0123 14:03:03.064380 140268056924160 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:03:03.111737 140268056924160 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.111824 140268056924160 decoder_stack.py:333] dstack: autoregressive generator.
I0123 14:03:03.111878 140268056924160 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 14:03:03.111980 140268056924160 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:03:03.112018 140268056924160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:03:03.112047 140268056924160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:03:03.112108 140268056924160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.114511 140268056924160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:03:03.119981 140268056924160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.120236 140268056924160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:03:03.122828 140268056924160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:03:03.139239 140268056924160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:03:03.139294 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:03.139329 140268056924160 transformer_layer.py:389] tlayer: self-attention.
I0123 14:03:03.139360 140268056924160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.139423 140268056924160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.140561 140268056924160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.140640 140268056924160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.141339 140268056924160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.143347 140268056924160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.148047 140268056924160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.149346 140268056924160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.149431 140268056924160 transformer_layer.py:468] tlayer: End windows.
I0123 14:03:03.149468 140268056924160 transformer_layer.py:472] tlayer: final FFN.
I0123 14:03:03.149528 140268056924160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.149665 140268056924160 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:03:03.149776 140268056924160 nn_components.py:325] mlp: activation = None
I0123 14:03:03.149816 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.151833 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.151925 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.154324 140268056924160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.154403 140268056924160 transformer_base.py:443] tbase: final FFN
I0123 14:03:03.154511 140268056924160 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:03:03.156697 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.158618 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.158714 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.159000 140268056924160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.159082 140268056924160 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 14:03:03.159190 140268056924160 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:03:03.159228 140268056924160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:03:03.159258 140268056924160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:03:03.159321 140268056924160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.161526 140268056924160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:03:03.166943 140268056924160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.167196 140268056924160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:03:03.169843 140268056924160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:03:03.182722 140268056924160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:03:03.182777 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:03.182812 140268056924160 transformer_layer.py:389] tlayer: self-attention.
I0123 14:03:03.182842 140268056924160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.182904 140268056924160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.183458 140268056924160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.183533 140268056924160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.183881 140268056924160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.184557 140268056924160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.187019 140268056924160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.187631 140268056924160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.187708 140268056924160 transformer_layer.py:468] tlayer: End windows.
I0123 14:03:03.187748 140268056924160 transformer_layer.py:472] tlayer: final FFN.
I0123 14:03:03.187807 140268056924160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.187936 140268056924160 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:03:03.188044 140268056924160 nn_components.py:325] mlp: activation = None
I0123 14:03:03.188082 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.189972 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.190065 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.192407 140268056924160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.192483 140268056924160 transformer_base.py:443] tbase: final FFN
I0123 14:03:03.192588 140268056924160 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:03:03.194781 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.196656 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.196751 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.197036 140268056924160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.197116 140268056924160 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 14:03:03.197224 140268056924160 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:03:03.197262 140268056924160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:03:03.197291 140268056924160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:03:03.197353 140268056924160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.199559 140268056924160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:03:03.204869 140268056924160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.205121 140268056924160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:03:03.207747 140268056924160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:03:03.220211 140268056924160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:03:03.220266 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:03.220301 140268056924160 transformer_layer.py:389] tlayer: self-attention.
I0123 14:03:03.220331 140268056924160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.220396 140268056924160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.220944 140268056924160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.221020 140268056924160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.221370 140268056924160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.222065 140268056924160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.224483 140268056924160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.225098 140268056924160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.225174 140268056924160 transformer_layer.py:468] tlayer: End windows.
I0123 14:03:03.225207 140268056924160 transformer_layer.py:472] tlayer: final FFN.
I0123 14:03:03.225269 140268056924160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.225394 140268056924160 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:03:03.225503 140268056924160 nn_components.py:325] mlp: activation = None
I0123 14:03:03.225542 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.227450 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.227542 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.229943 140268056924160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.230022 140268056924160 transformer_base.py:443] tbase: final FFN
I0123 14:03:03.230129 140268056924160 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:03:03.232300 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.234202 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.234298 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.234581 140268056924160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.234661 140268056924160 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 14:03:03.234770 140268056924160 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:03:03.234808 140268056924160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:03:03.234838 140268056924160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:03:03.234899 140268056924160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.237107 140268056924160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:03:03.242503 140268056924160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.242756 140268056924160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:03:03.245399 140268056924160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:03:03.257871 140268056924160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:03:03.257925 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:03.257959 140268056924160 transformer_layer.py:389] tlayer: self-attention.
I0123 14:03:03.257989 140268056924160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.258051 140268056924160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.258602 140268056924160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.258677 140268056924160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.259027 140268056924160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.259710 140268056924160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.262116 140268056924160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.262720 140268056924160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.262798 140268056924160 transformer_layer.py:468] tlayer: End windows.
I0123 14:03:03.262832 140268056924160 transformer_layer.py:472] tlayer: final FFN.
I0123 14:03:03.262889 140268056924160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.263026 140268056924160 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:03:03.263136 140268056924160 nn_components.py:325] mlp: activation = None
I0123 14:03:03.263174 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.265078 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.265170 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.267621 140268056924160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.267699 140268056924160 transformer_base.py:443] tbase: final FFN
I0123 14:03:03.267807 140268056924160 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:03:03.269984 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.271802 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.271896 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.272176 140268056924160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.272255 140268056924160 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 14:03:03.272362 140268056924160 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:03:03.272401 140268056924160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:03:03.272432 140268056924160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:03:03.272493 140268056924160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.275027 140268056924160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:03:03.280365 140268056924160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.280625 140268056924160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:03:03.283182 140268056924160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:03:03.295615 140268056924160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:03:03.295670 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:03.295705 140268056924160 transformer_layer.py:389] tlayer: self-attention.
I0123 14:03:03.295736 140268056924160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.295797 140268056924160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.296350 140268056924160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.296426 140268056924160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.296780 140268056924160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.297456 140268056924160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.300106 140268056924160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.300723 140268056924160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.300804 140268056924160 transformer_layer.py:468] tlayer: End windows.
I0123 14:03:03.300839 140268056924160 transformer_layer.py:472] tlayer: final FFN.
I0123 14:03:03.300897 140268056924160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.301029 140268056924160 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:03:03.301139 140268056924160 nn_components.py:325] mlp: activation = None
I0123 14:03:03.301177 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.303026 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.303118 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.305460 140268056924160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.305537 140268056924160 transformer_base.py:443] tbase: final FFN
I0123 14:03:03.305656 140268056924160 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:03:03.307896 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.309711 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.309805 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.310084 140268056924160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.310162 140268056924160 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 14:03:03.310270 140268056924160 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:03:03.310307 140268056924160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:03:03.310337 140268056924160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:03:03.310398 140268056924160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.312579 140268056924160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:03:03.317887 140268056924160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.318136 140268056924160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:03:03.320743 140268056924160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:03:03.333140 140268056924160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:03:03.333195 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:03.333229 140268056924160 transformer_layer.py:389] tlayer: self-attention.
I0123 14:03:03.333260 140268056924160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.333322 140268056924160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.333885 140268056924160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.333961 140268056924160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.334316 140268056924160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.334991 140268056924160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.337399 140268056924160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.338020 140268056924160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.338097 140268056924160 transformer_layer.py:468] tlayer: End windows.
I0123 14:03:03.338131 140268056924160 transformer_layer.py:472] tlayer: final FFN.
I0123 14:03:03.338188 140268056924160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.338325 140268056924160 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:03:03.338443 140268056924160 nn_components.py:325] mlp: activation = None
I0123 14:03:03.338483 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.340389 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.340482 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.342849 140268056924160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.342928 140268056924160 transformer_base.py:443] tbase: final FFN
I0123 14:03:03.343035 140268056924160 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:03:03.345213 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.347034 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.347130 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.347414 140268056924160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.347495 140268056924160 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 14:03:03.347602 140268056924160 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:03:03.347640 140268056924160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:03:03.347670 140268056924160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:03:03.347731 140268056924160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.349923 140268056924160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:03:03.355324 140268056924160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.355577 140268056924160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:03:03.358120 140268056924160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:03:03.370541 140268056924160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:03:03.370594 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:03.370630 140268056924160 transformer_layer.py:389] tlayer: self-attention.
I0123 14:03:03.370660 140268056924160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.370721 140268056924160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.371269 140268056924160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.371344 140268056924160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.371694 140268056924160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.372368 140268056924160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.374781 140268056924160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.375752 140268056924160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.375829 140268056924160 transformer_layer.py:468] tlayer: End windows.
I0123 14:03:03.375863 140268056924160 transformer_layer.py:472] tlayer: final FFN.
I0123 14:03:03.375924 140268056924160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.376050 140268056924160 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:03:03.376158 140268056924160 nn_components.py:325] mlp: activation = None
I0123 14:03:03.376203 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.378071 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.378163 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.380527 140268056924160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.380605 140268056924160 transformer_base.py:443] tbase: final FFN
I0123 14:03:03.380712 140268056924160 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:03:03.382915 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.384785 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.384880 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.385164 140268056924160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.385246 140268056924160 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 14:03:03.385354 140268056924160 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:03:03.385393 140268056924160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:03:03.385422 140268056924160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:03:03.385483 140268056924160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.387682 140268056924160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:03:03.392987 140268056924160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.393253 140268056924160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:03:03.395865 140268056924160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:03:03.408389 140268056924160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:03:03.408444 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:03.408479 140268056924160 transformer_layer.py:389] tlayer: self-attention.
I0123 14:03:03.408509 140268056924160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.408571 140268056924160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.409166 140268056924160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.409244 140268056924160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.409595 140268056924160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.410276 140268056924160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.412669 140268056924160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.413278 140268056924160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.413356 140268056924160 transformer_layer.py:468] tlayer: End windows.
I0123 14:03:03.413391 140268056924160 transformer_layer.py:472] tlayer: final FFN.
I0123 14:03:03.413450 140268056924160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.413581 140268056924160 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:03:03.413697 140268056924160 nn_components.py:325] mlp: activation = None
I0123 14:03:03.413742 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.415589 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.415681 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.418078 140268056924160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.418158 140268056924160 transformer_base.py:443] tbase: final FFN
I0123 14:03:03.418265 140268056924160 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:03:03.420430 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.422257 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.422353 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.422785 140268056924160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.422866 140268056924160 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 14:03:03.422974 140268056924160 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:03:03.423013 140268056924160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:03:03.423044 140268056924160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:03:03.423105 140268056924160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.425278 140268056924160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:03:03.430641 140268056924160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.430895 140268056924160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:03:03.433428 140268056924160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:03:03.445852 140268056924160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:03:03.445907 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:03.445942 140268056924160 transformer_layer.py:389] tlayer: self-attention.
I0123 14:03:03.445972 140268056924160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.446034 140268056924160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.446595 140268056924160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.446672 140268056924160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.447026 140268056924160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.447705 140268056924160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.450134 140268056924160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.450797 140268056924160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.450876 140268056924160 transformer_layer.py:468] tlayer: End windows.
I0123 14:03:03.450911 140268056924160 transformer_layer.py:472] tlayer: final FFN.
I0123 14:03:03.450973 140268056924160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.451102 140268056924160 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:03:03.451210 140268056924160 nn_components.py:325] mlp: activation = None
I0123 14:03:03.451248 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.453096 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.453188 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.455527 140268056924160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.455606 140268056924160 transformer_base.py:443] tbase: final FFN
I0123 14:03:03.455718 140268056924160 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:03:03.457886 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.459758 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.459851 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.460131 140268056924160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.460209 140268056924160 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 14:03:03.460318 140268056924160 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:03:03.460356 140268056924160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:03:03.460386 140268056924160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:03:03.460446 140268056924160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.462636 140268056924160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:03:03.467877 140268056924160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.468125 140268056924160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:03:03.470737 140268056924160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:03:03.483407 140268056924160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:03:03.483462 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:03.483496 140268056924160 transformer_layer.py:389] tlayer: self-attention.
I0123 14:03:03.483527 140268056924160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.483587 140268056924160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.484189 140268056924160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.484265 140268056924160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.484615 140268056924160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.485295 140268056924160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.487696 140268056924160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.488317 140268056924160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.488394 140268056924160 transformer_layer.py:468] tlayer: End windows.
I0123 14:03:03.488428 140268056924160 transformer_layer.py:472] tlayer: final FFN.
I0123 14:03:03.488487 140268056924160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.488614 140268056924160 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:03:03.488723 140268056924160 nn_components.py:325] mlp: activation = None
I0123 14:03:03.488761 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.490609 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.490707 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.493097 140268056924160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.493175 140268056924160 transformer_base.py:443] tbase: final FFN
I0123 14:03:03.493282 140268056924160 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:03:03.495461 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.497256 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.497349 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.497629 140268056924160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.497717 140268056924160 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 14:03:03.497828 140268056924160 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:03:03.497866 140268056924160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:03:03.497896 140268056924160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:03:03.497957 140268056924160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.500149 140268056924160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:03:03.505523 140268056924160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.505783 140268056924160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:03:03.508346 140268056924160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:03:03.520734 140268056924160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:03:03.520788 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:03.520823 140268056924160 transformer_layer.py:389] tlayer: self-attention.
I0123 14:03:03.520854 140268056924160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.520919 140268056924160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.521466 140268056924160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.521543 140268056924160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.521903 140268056924160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.522686 140268056924160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.525099 140268056924160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.525755 140268056924160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.525832 140268056924160 transformer_layer.py:468] tlayer: End windows.
I0123 14:03:03.525866 140268056924160 transformer_layer.py:472] tlayer: final FFN.
I0123 14:03:03.525923 140268056924160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.526050 140268056924160 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:03:03.526161 140268056924160 nn_components.py:325] mlp: activation = None
I0123 14:03:03.526200 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.528031 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.528129 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.530666 140268056924160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.530746 140268056924160 transformer_base.py:443] tbase: final FFN
I0123 14:03:03.530851 140268056924160 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:03:03.532997 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.534871 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.534969 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.535248 140268056924160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.535329 140268056924160 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 14:03:03.535435 140268056924160 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:03:03.535474 140268056924160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:03:03.535504 140268056924160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:03:03.535565 140268056924160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.537756 140268056924160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:03:03.543094 140268056924160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.543348 140268056924160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:03:03.545969 140268056924160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:03:03.558437 140268056924160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:03:03.558493 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:03.558527 140268056924160 transformer_layer.py:389] tlayer: self-attention.
I0123 14:03:03.558557 140268056924160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.558618 140268056924160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.559170 140268056924160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.559247 140268056924160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.559597 140268056924160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.560328 140268056924160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.562748 140268056924160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.563363 140268056924160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.563440 140268056924160 transformer_layer.py:468] tlayer: End windows.
I0123 14:03:03.563475 140268056924160 transformer_layer.py:472] tlayer: final FFN.
I0123 14:03:03.563532 140268056924160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.563662 140268056924160 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:03:03.563774 140268056924160 nn_components.py:325] mlp: activation = None
I0123 14:03:03.563813 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.565660 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.565752 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.568102 140268056924160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.568181 140268056924160 transformer_base.py:443] tbase: final FFN
I0123 14:03:03.568287 140268056924160 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:03:03.570524 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.572339 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.572432 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.572712 140268056924160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.572798 140268056924160 decoder_stack.py:344] dstack: Final layernorm.
I0123 14:03:03.575630 140268056924160 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:03:03.629886 140268056924160 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.629974 140268056924160 decoder_stack.py:333] dstack: autoregressive generator.
I0123 14:03:03.630028 140268056924160 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 14:03:03.630130 140268056924160 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:03:03.630167 140268056924160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:03:03.630197 140268056924160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:03:03.630258 140268056924160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.632845 140268056924160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:03:03.638138 140268056924160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.638390 140268056924160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:03:03.640890 140268056924160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:03:03.653131 140268056924160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:03:03.653185 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:03.653220 140268056924160 transformer_layer.py:389] tlayer: self-attention.
I0123 14:03:03.653251 140268056924160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.653313 140268056924160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.653866 140268056924160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.653942 140268056924160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.654291 140268056924160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.654954 140268056924160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.657387 140268056924160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.658001 140268056924160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.658081 140268056924160 transformer_layer.py:468] tlayer: End windows.
I0123 14:03:03.658115 140268056924160 transformer_layer.py:472] tlayer: final FFN.
I0123 14:03:03.658175 140268056924160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.658303 140268056924160 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:03:03.658419 140268056924160 nn_components.py:325] mlp: activation = None
I0123 14:03:03.658458 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.660254 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.660346 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.662673 140268056924160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.662752 140268056924160 transformer_base.py:443] tbase: final FFN
I0123 14:03:03.662860 140268056924160 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:03:03.665044 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.666859 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.666955 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.667237 140268056924160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.667318 140268056924160 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 14:03:03.667426 140268056924160 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:03:03.667465 140268056924160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:03:03.667495 140268056924160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:03:03.667556 140268056924160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.669725 140268056924160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:03:03.674957 140268056924160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.675209 140268056924160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:03:03.677799 140268056924160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:03:03.689770 140268056924160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:03:03.689825 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:03.689860 140268056924160 transformer_layer.py:389] tlayer: self-attention.
I0123 14:03:03.689891 140268056924160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.689954 140268056924160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.690498 140268056924160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.690573 140268056924160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.690921 140268056924160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.691585 140268056924160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.694029 140268056924160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.694646 140268056924160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.694723 140268056924160 transformer_layer.py:468] tlayer: End windows.
I0123 14:03:03.694757 140268056924160 transformer_layer.py:472] tlayer: final FFN.
I0123 14:03:03.694815 140268056924160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.694941 140268056924160 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:03:03.695047 140268056924160 nn_components.py:325] mlp: activation = None
I0123 14:03:03.695091 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.696881 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.696974 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.699292 140268056924160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.699371 140268056924160 transformer_base.py:443] tbase: final FFN
I0123 14:03:03.699481 140268056924160 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:03:03.701695 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.703480 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.703574 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.703856 140268056924160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.703937 140268056924160 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 14:03:03.704043 140268056924160 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:03:03.704081 140268056924160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:03:03.704111 140268056924160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:03:03.704171 140268056924160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.706342 140268056924160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:03:03.711538 140268056924160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.711793 140268056924160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:03:03.714383 140268056924160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:03:03.726347 140268056924160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:03:03.726401 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:03.726436 140268056924160 transformer_layer.py:389] tlayer: self-attention.
I0123 14:03:03.726466 140268056924160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.726527 140268056924160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.727073 140268056924160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.727149 140268056924160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.727499 140268056924160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.728162 140268056924160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.730593 140268056924160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.731198 140268056924160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.731276 140268056924160 transformer_layer.py:468] tlayer: End windows.
I0123 14:03:03.731311 140268056924160 transformer_layer.py:472] tlayer: final FFN.
I0123 14:03:03.731369 140268056924160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.731495 140268056924160 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:03:03.731601 140268056924160 nn_components.py:325] mlp: activation = None
I0123 14:03:03.731639 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.733421 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.733514 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.735823 140268056924160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.735902 140268056924160 transformer_base.py:443] tbase: final FFN
I0123 14:03:03.736008 140268056924160 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:03:03.738633 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.740420 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.740514 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.740793 140268056924160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.740874 140268056924160 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 14:03:03.740981 140268056924160 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:03:03.741020 140268056924160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:03:03.741050 140268056924160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:03:03.741111 140268056924160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.743298 140268056924160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:03:03.748471 140268056924160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.748722 140268056924160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:03:03.751425 140268056924160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:03:03.763905 140268056924160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:03:03.763960 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:03.763997 140268056924160 transformer_layer.py:389] tlayer: self-attention.
I0123 14:03:03.764033 140268056924160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.764096 140268056924160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.764655 140268056924160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.764730 140268056924160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.765083 140268056924160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.765761 140268056924160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.768221 140268056924160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.768829 140268056924160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.768904 140268056924160 transformer_layer.py:468] tlayer: End windows.
I0123 14:03:03.768936 140268056924160 transformer_layer.py:472] tlayer: final FFN.
I0123 14:03:03.768992 140268056924160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.769117 140268056924160 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:03:03.769223 140268056924160 nn_components.py:325] mlp: activation = None
I0123 14:03:03.769262 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.771090 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.771182 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.773494 140268056924160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.773572 140268056924160 transformer_base.py:443] tbase: final FFN
I0123 14:03:03.773686 140268056924160 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:03:03.775895 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.777697 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.777791 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.778072 140268056924160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.778151 140268056924160 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 14:03:03.778258 140268056924160 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:03:03.778295 140268056924160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:03:03.778323 140268056924160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:03:03.778383 140268056924160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.780538 140268056924160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:03:03.785795 140268056924160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.786046 140268056924160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:03:03.788634 140268056924160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:03:03.800812 140268056924160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:03:03.800865 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:03.800899 140268056924160 transformer_layer.py:389] tlayer: self-attention.
I0123 14:03:03.800928 140268056924160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.800988 140268056924160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.801532 140268056924160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.801606 140268056924160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.801966 140268056924160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.802636 140268056924160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.805100 140268056924160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.805720 140268056924160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.805796 140268056924160 transformer_layer.py:468] tlayer: End windows.
I0123 14:03:03.805830 140268056924160 transformer_layer.py:472] tlayer: final FFN.
I0123 14:03:03.805886 140268056924160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.806011 140268056924160 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:03:03.806116 140268056924160 nn_components.py:325] mlp: activation = None
I0123 14:03:03.806153 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.807976 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.808073 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.810406 140268056924160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.810484 140268056924160 transformer_base.py:443] tbase: final FFN
I0123 14:03:03.810590 140268056924160 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:03:03.812813 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.814631 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.814726 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.815014 140268056924160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.815093 140268056924160 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 14:03:03.815199 140268056924160 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:03:03.815235 140268056924160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:03:03.815264 140268056924160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:03:03.815325 140268056924160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.817500 140268056924160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:03:03.822765 140268056924160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.823013 140268056924160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:03:03.825626 140268056924160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:03:03.837835 140268056924160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:03:03.837893 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:03.837927 140268056924160 transformer_layer.py:389] tlayer: self-attention.
I0123 14:03:03.837956 140268056924160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.838018 140268056924160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.838572 140268056924160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.838649 140268056924160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.839013 140268056924160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.839683 140268056924160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.842161 140268056924160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.842763 140268056924160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.842839 140268056924160 transformer_layer.py:468] tlayer: End windows.
I0123 14:03:03.842872 140268056924160 transformer_layer.py:472] tlayer: final FFN.
I0123 14:03:03.842929 140268056924160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.843052 140268056924160 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:03:03.843158 140268056924160 nn_components.py:325] mlp: activation = None
I0123 14:03:03.843195 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.845016 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.845115 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.847440 140268056924160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.847517 140268056924160 transformer_base.py:443] tbase: final FFN
I0123 14:03:03.847626 140268056924160 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:03:03.850257 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.852060 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.852153 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.852433 140268056924160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.852511 140268056924160 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 14:03:03.852616 140268056924160 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:03:03.852653 140268056924160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:03:03.852682 140268056924160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:03:03.852743 140268056924160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.854913 140268056924160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:03:03.860337 140268056924160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.860593 140268056924160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:03:03.863271 140268056924160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:03:03.875519 140268056924160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:03:03.875571 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:03.875604 140268056924160 transformer_layer.py:389] tlayer: self-attention.
I0123 14:03:03.875633 140268056924160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.875698 140268056924160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.876258 140268056924160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.876332 140268056924160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.876677 140268056924160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.877353 140268056924160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.879817 140268056924160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.880429 140268056924160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.880504 140268056924160 transformer_layer.py:468] tlayer: End windows.
I0123 14:03:03.880538 140268056924160 transformer_layer.py:472] tlayer: final FFN.
I0123 14:03:03.880595 140268056924160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.880719 140268056924160 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:03:03.880826 140268056924160 nn_components.py:325] mlp: activation = None
I0123 14:03:03.880863 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.882700 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.882792 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.885145 140268056924160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.885224 140268056924160 transformer_base.py:443] tbase: final FFN
I0123 14:03:03.885330 140268056924160 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:03:03.887586 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.889402 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.889496 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.889786 140268056924160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.889866 140268056924160 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 14:03:03.889971 140268056924160 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:03:03.890008 140268056924160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:03:03.890036 140268056924160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:03:03.890095 140268056924160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.892271 140268056924160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:03:03.897549 140268056924160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.897808 140268056924160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:03:03.900416 140268056924160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:03:03.912611 140268056924160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:03:03.912664 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:03.912701 140268056924160 transformer_layer.py:389] tlayer: self-attention.
I0123 14:03:03.912731 140268056924160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.912791 140268056924160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.913338 140268056924160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.913412 140268056924160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.913767 140268056924160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.914437 140268056924160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.916884 140268056924160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.917494 140268056924160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.917569 140268056924160 transformer_layer.py:468] tlayer: End windows.
I0123 14:03:03.917602 140268056924160 transformer_layer.py:472] tlayer: final FFN.
I0123 14:03:03.917670 140268056924160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.917797 140268056924160 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:03:03.917904 140268056924160 nn_components.py:325] mlp: activation = None
I0123 14:03:03.917940 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.919738 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.919828 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.922142 140268056924160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.922226 140268056924160 transformer_base.py:443] tbase: final FFN
I0123 14:03:03.922333 140268056924160 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:03:03.924548 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.926356 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.926448 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.926727 140268056924160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.926806 140268056924160 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 14:03:03.926912 140268056924160 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:03:03.926949 140268056924160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:03:03.926978 140268056924160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:03:03.927037 140268056924160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.929188 140268056924160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:03:03.934422 140268056924160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.934679 140268056924160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:03:03.937293 140268056924160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:03:03.950039 140268056924160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:03:03.950093 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:03.950127 140268056924160 transformer_layer.py:389] tlayer: self-attention.
I0123 14:03:03.950156 140268056924160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.950218 140268056924160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.950768 140268056924160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.950842 140268056924160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.951188 140268056924160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.951856 140268056924160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.954334 140268056924160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.954943 140268056924160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.955017 140268056924160 transformer_layer.py:468] tlayer: End windows.
I0123 14:03:03.955050 140268056924160 transformer_layer.py:472] tlayer: final FFN.
I0123 14:03:03.955107 140268056924160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.955233 140268056924160 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:03:03.955338 140268056924160 nn_components.py:325] mlp: activation = None
I0123 14:03:03.955376 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.957194 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.957285 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.959733 140268056924160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.959827 140268056924160 transformer_base.py:443] tbase: final FFN
I0123 14:03:03.959934 140268056924160 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:03:03.962544 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.964352 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.964444 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.964725 140268056924160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.964804 140268056924160 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 14:03:03.964908 140268056924160 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:03:03.964944 140268056924160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:03:03.964972 140268056924160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:03:03.965030 140268056924160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.967204 140268056924160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:03:03.972473 140268056924160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.972726 140268056924160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:03:03.975340 140268056924160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:03:03.987546 140268056924160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:03:03.987599 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:03.987633 140268056924160 transformer_layer.py:389] tlayer: self-attention.
I0123 14:03:03.987662 140268056924160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.987720 140268056924160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.988268 140268056924160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.988342 140268056924160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.988692 140268056924160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.989354 140268056924160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.991880 140268056924160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.992490 140268056924160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.992566 140268056924160 transformer_layer.py:468] tlayer: End windows.
I0123 14:03:03.992599 140268056924160 transformer_layer.py:472] tlayer: final FFN.
I0123 14:03:03.992654 140268056924160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.992776 140268056924160 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:03:03.992880 140268056924160 nn_components.py:325] mlp: activation = None
I0123 14:03:03.992916 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:03.995086 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.995180 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:03.997470 140268056924160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:03.997546 140268056924160 transformer_base.py:443] tbase: final FFN
I0123 14:03:03.997665 140268056924160 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:03:03.999859 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:04.001649 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:04.001744 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:04.002023 140268056924160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:04.002102 140268056924160 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 14:03:04.002207 140268056924160 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:03:04.002244 140268056924160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:03:04.002273 140268056924160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:03:04.002331 140268056924160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:04.004498 140268056924160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:03:04.009772 140268056924160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:04.010022 140268056924160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:03:04.012604 140268056924160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:03:04.024713 140268056924160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:03:04.024766 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:04.024800 140268056924160 transformer_layer.py:389] tlayer: self-attention.
I0123 14:03:04.024829 140268056924160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:04.024888 140268056924160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:04.025435 140268056924160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:04.025509 140268056924160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:04.025863 140268056924160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:04.026528 140268056924160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:04.028959 140268056924160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:04.029564 140268056924160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:04.029644 140268056924160 transformer_layer.py:468] tlayer: End windows.
I0123 14:03:04.029678 140268056924160 transformer_layer.py:472] tlayer: final FFN.
I0123 14:03:04.029734 140268056924160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:04.029859 140268056924160 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:03:04.029965 140268056924160 nn_components.py:325] mlp: activation = None
I0123 14:03:04.030001 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:04.031805 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:04.031897 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:04.034213 140268056924160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:04.034290 140268056924160 transformer_base.py:443] tbase: final FFN
I0123 14:03:04.034395 140268056924160 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:03:04.036601 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:04.038422 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:04.038516 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:04.038795 140268056924160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:04.038875 140268056924160 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 14:03:04.038981 140268056924160 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:03:04.039019 140268056924160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:03:04.039047 140268056924160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:03:04.039105 140268056924160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:04.041262 140268056924160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:03:04.046521 140268056924160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:04.046768 140268056924160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:03:04.049372 140268056924160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:03:04.061755 140268056924160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:03:04.061808 140268056924160 attention.py:418] Single window, no scan.
I0123 14:03:04.061841 140268056924160 transformer_layer.py:389] tlayer: self-attention.
I0123 14:03:04.061870 140268056924160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:04.061930 140268056924160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:04.062484 140268056924160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:04.062567 140268056924160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:04.062916 140268056924160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:04.063587 140268056924160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:04.066047 140268056924160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:04.066659 140268056924160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:04.066738 140268056924160 transformer_layer.py:468] tlayer: End windows.
I0123 14:03:04.066772 140268056924160 transformer_layer.py:472] tlayer: final FFN.
I0123 14:03:04.066829 140268056924160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:04.066954 140268056924160 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:03:04.067066 140268056924160 nn_components.py:325] mlp: activation = None
I0123 14:03:04.067104 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:04.068906 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:04.068997 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:04.071317 140268056924160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:04.071394 140268056924160 transformer_base.py:443] tbase: final FFN
I0123 14:03:04.071499 140268056924160 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:03:04.074095 140268056924160 nn_components.py:329] mlp: final activation = None
I0123 14:03:04.075922 140268056924160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:04.076016 140268056924160 nn_components.py:261] mlp: residual
I0123 14:03:04.076292 140268056924160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:04.076378 140268056924160 decoder_stack.py:344] dstack: Final layernorm.
I0123 14:03:04.079132 140268056924160 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:03:08.829138 140268056924160 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
I0123 14:03:09.421212 140268056924160 training_loop.py:409] No working directory specified.
I0123 14:03:09.421409 140268056924160 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0123 14:03:09.422506 140268056924160 checkpoints.py:1062] Restoring legacy Flax checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0123 14:03:12.556052 140268056924160 training_loop.py:447] Only restoring trainable parameters.
I0123 14:03:12.557091 140268056924160 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0123 14:03:12.557151 140268056924160 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0123 14:03:12.557196 140268056924160 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 14:03:12.557238 140268056924160 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 14:03:12.557278 140268056924160 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.557316 140268056924160 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 14:03:12.557353 140268056924160 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.557390 140268056924160 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.557425 140268056924160 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 14:03:12.557462 140268056924160 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 14:03:12.557497 140268056924160 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.557533 140268056924160 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 14:03:12.557570 140268056924160 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 14:03:12.557606 140268056924160 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 14:03:12.557657 140268056924160 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.557696 140268056924160 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 14:03:12.557732 140268056924160 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.557768 140268056924160 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.557804 140268056924160 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 14:03:12.557839 140268056924160 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 14:03:12.557888 140268056924160 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.557925 140268056924160 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 14:03:12.557961 140268056924160 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 14:03:12.557997 140268056924160 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 14:03:12.558032 140268056924160 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.558067 140268056924160 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 14:03:12.558102 140268056924160 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.558137 140268056924160 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.558172 140268056924160 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 14:03:12.558207 140268056924160 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 14:03:12.558242 140268056924160 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.558277 140268056924160 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 14:03:12.558310 140268056924160 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 14:03:12.558345 140268056924160 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 14:03:12.558380 140268056924160 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.558414 140268056924160 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 14:03:12.558448 140268056924160 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.558482 140268056924160 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.558517 140268056924160 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 14:03:12.558552 140268056924160 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 14:03:12.558587 140268056924160 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.558622 140268056924160 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 14:03:12.558656 140268056924160 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 14:03:12.558691 140268056924160 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 14:03:12.558727 140268056924160 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.558761 140268056924160 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 14:03:12.558802 140268056924160 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.558838 140268056924160 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.558874 140268056924160 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 14:03:12.558910 140268056924160 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 14:03:12.558944 140268056924160 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.558980 140268056924160 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 14:03:12.559015 140268056924160 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 14:03:12.559051 140268056924160 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 14:03:12.559087 140268056924160 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.559122 140268056924160 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 14:03:12.559157 140268056924160 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.559191 140268056924160 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.559227 140268056924160 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 14:03:12.559262 140268056924160 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 14:03:12.559295 140268056924160 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.559331 140268056924160 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 14:03:12.559365 140268056924160 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 14:03:12.559401 140268056924160 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 14:03:12.559437 140268056924160 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.559472 140268056924160 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 14:03:12.559508 140268056924160 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.559543 140268056924160 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.559578 140268056924160 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 14:03:12.559614 140268056924160 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 14:03:12.559649 140268056924160 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.559684 140268056924160 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 14:03:12.559720 140268056924160 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 14:03:12.559760 140268056924160 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 14:03:12.559797 140268056924160 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.559832 140268056924160 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 14:03:12.559868 140268056924160 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.559904 140268056924160 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.559939 140268056924160 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 14:03:12.559975 140268056924160 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 14:03:12.560011 140268056924160 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.560047 140268056924160 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 14:03:12.560081 140268056924160 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 14:03:12.560116 140268056924160 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 14:03:12.560151 140268056924160 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.560187 140268056924160 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 14:03:12.560222 140268056924160 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.560256 140268056924160 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.560292 140268056924160 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 14:03:12.560327 140268056924160 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 14:03:12.560362 140268056924160 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.560397 140268056924160 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 14:03:12.560431 140268056924160 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 14:03:12.560465 140268056924160 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 14:03:12.560500 140268056924160 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.560534 140268056924160 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 14:03:12.560569 140268056924160 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.560604 140268056924160 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.560639 140268056924160 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 14:03:12.560675 140268056924160 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 14:03:12.560714 140268056924160 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.560752 140268056924160 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 14:03:12.560787 140268056924160 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 14:03:12.560822 140268056924160 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 14:03:12.560857 140268056924160 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.560892 140268056924160 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 14:03:12.560927 140268056924160 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.560962 140268056924160 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.560997 140268056924160 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 14:03:12.561032 140268056924160 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 14:03:12.561066 140268056924160 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.561101 140268056924160 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 14:03:12.561136 140268056924160 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 14:03:12.561172 140268056924160 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 14:03:12.561208 140268056924160 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.561243 140268056924160 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 14:03:12.561278 140268056924160 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.561312 140268056924160 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.561348 140268056924160 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 14:03:12.561383 140268056924160 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 14:03:12.561417 140268056924160 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:03:12.561451 140268056924160 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 14:03:12.561478 140268056924160 training_loop.py:725] Total parameters: 152072288
I0123 14:03:12.561943 140268056924160 training_loop.py:739] Total state size: 0
I0123 14:03:12.618884 140268056924160 training_loop.py:492] Training loop: creating task for mode beam_search
I0123 14:03:12.619323 140268056924160 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0123 14:03:12.620050 140268056924160 training_loop.py:652] Compiling mode beam_search with jit.
I0123 14:03:12.620481 140268056924160 training_loop.py:89] registering functions: dict_keys([])
I0123 14:03:12.638913 140268056924160 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = midpoint e b a; f = midpoint f c a; g = on_line g c e, on_line g b f; h = foot h c b a; i = on_circle i e b, on_line i c h; j = on_circle j e b, on_line j c h; k = on_line k j g, on_line k d e; l = on_pline l k d b, on_line l b a; m = on_pline m k d a, on_line m a b; n = lc_tangent n l b, on_line n c b; o = lc_tangent o m a, on_line o c a; p = lc_tangent p b d, on_line p e d; q = circle q c n o; r = circle r b e n; s = foot s n r q; t = mirror t n s ? coll t q p
I0123 14:03:28.500306 140268056924160 ddar.py:60] Depth 1/1000 time = 15.735305786132812
I0123 14:03:59.150009 140268056924160 ddar.py:60] Depth 2/1000 time = 30.649014234542847
I0123 14:04:44.846199 140268056924160 ddar.py:60] Depth 3/1000 time = 45.695746660232544
I0123 14:05:42.811341 140268056924160 ddar.py:60] Depth 4/1000 time = 57.964688301086426
I0123 14:06:48.733868 140268056924160 ddar.py:60] Depth 5/1000 time = 65.92208218574524
I0123 14:07:54.063442 140268056924160 ddar.py:60] Depth 6/1000 time = 65.32910418510437
I0123 14:08:59.558415 140268056924160 ddar.py:60] Depth 7/1000 time = 65.49459338188171
I0123 14:10:05.338720 140268056924160 ddar.py:60] Depth 8/1000 time = 65.7786078453064
I0123 14:11:12.853513 140268056924160 ddar.py:60] Depth 9/1000 time = 67.32647180557251
I0123 14:12:18.637287 140268056924160 ddar.py:60] Depth 10/1000 time = 65.78328323364258
I0123 14:13:26.472293 140268056924160 ddar.py:60] Depth 11/1000 time = 67.52458453178406
I0123 14:14:37.208371 140268056924160 ddar.py:60] Depth 12/1000 time = 70.73562693595886
I0123 14:15:48.602149 140268056924160 ddar.py:60] Depth 13/1000 time = 71.39341998100281
I0123 14:17:00.460817 140268056924160 ddar.py:60] Depth 14/1000 time = 71.85815572738647
I0123 14:18:15.003939 140268056924160 ddar.py:60] Depth 15/1000 time = 74.26563167572021
I0123 14:19:29.176731 140268056924160 ddar.py:60] Depth 16/1000 time = 74.1354169845581
I0123 14:20:44.014913 140268056924160 ddar.py:60] Depth 17/1000 time = 74.79886889457703
I0123 14:21:58.798824 140268056924160 ddar.py:60] Depth 18/1000 time = 74.78355813026428
I0123 14:23:15.782031 140268056924160 ddar.py:60] Depth 19/1000 time = 76.98271203041077
I0123 14:24:37.130128 140268056924160 ddar.py:60] Depth 20/1000 time = 81.34762787818909
I0123 14:26:02.228141 140268056924160 ddar.py:60] Depth 21/1000 time = 85.097323179245
I0123 14:27:37.242638 140268056924160 ddar.py:60] Depth 22/1000 time = 95.01400351524353
I0123 14:29:12.565151 140268056924160 ddar.py:60] Depth 23/1000 time = 95.32197070121765
I0123 14:30:50.511122 140268056924160 ddar.py:60] Depth 24/1000 time = 97.94540405273438
I0123 14:32:30.580911 140268056924160 ddar.py:60] Depth 25/1000 time = 100.06925368309021
I0123 14:34:11.255799 140268056924160 ddar.py:60] Depth 26/1000 time = 100.59659600257874
I0123 14:35:51.471040 140268056924160 ddar.py:60] Depth 27/1000 time = 100.17413592338562
I0123 14:37:32.184306 140268056924160 ddar.py:60] Depth 28/1000 time = 100.71245527267456
I0123 14:39:10.488051 140268056924160 ddar.py:60] Depth 29/1000 time = 98.27051091194153
I0123 14:40:50.231255 140268056924160 ddar.py:60] Depth 30/1000 time = 99.62519097328186
I0123 14:42:33.362498 140268056924160 ddar.py:60] Depth 31/1000 time = 103.13039994239807
I0123 14:44:17.764958 140268056924160 ddar.py:60] Depth 32/1000 time = 104.40166401863098
I0123 14:46:02.788200 140268056924160 ddar.py:60] Depth 33/1000 time = 104.7107298374176
I0123 14:48:12.885032 140268056924160 ddar.py:60] Depth 34/1000 time = 130.09597301483154
I0123 14:50:27.035619 140268056924160 ddar.py:60] Depth 35/1000 time = 134.15004181861877
I0123 14:52:46.225190 140268056924160 ddar.py:60] Depth 36/1000 time = 139.18904972076416
I0123 14:52:50.444336 140268056924160 alphageometry.py:191] 
==========================
 * From theorem premises:
A B C D E F G H J K L M N O P Q R S T : Points
DC = DB [00]
DB = DA [01]
EB = EA [02]
A,B,E are collinear [03]
A,C,F are collinear [04]
FC = FA [05]
C,G,E are collinear [06]
G,B,F are collinear [07]
CH  AB [08]
A,B,H are collinear [09]
(AB-CH) = (AB-CH) [10]
EJ = EB [11]
C,J,H are collinear [12]
D,K,E are collinear [13]
G,K,J are collinear [14]
A,B,L are collinear [15]
LK  DB [16]
A,B,M are collinear [17]
MK  DA [18]
LB  LN [19]
C,B,N are collinear [20]
MA  MO [21]
A,C,O are collinear [22]
BP  BD [23]
D,P,E are collinear [24]
QN = QO [25]
QC = QN [26]
RE = RN [27]
RB = RE [28]
S,Q,R are collinear [29]
NS  QR [30]
S,N,T are collinear [31]
SN = ST [32]

 * Auxiliary Constructions:
I : Points
C,I,H are collinear [33]
EI = EB [34]
BAI = BAI [35]

 * Proof steps:
001. LB  LN [19] & CH  AB [08] & A,B,L are collinear [15]   LN  CH [36]
002. A,B,L are collinear [15] & A,B,H are collinear [09]   B,H,L are collinear [37]
003. LN  CH [36] & C,B,N are collinear [20] & B,H,L are collinear [37]   CB:BN = BH:BL [38]
004. LN  CH [36] & C,B,N are collinear [20] & B,H,L are collinear [37]   CB:BN = CH:NL [39]
005. MA  MO [21] & CH  AB [08] & A,B,M are collinear [17]   MO  CH [40]
006. A,B,M are collinear [17] & A,B,H are collinear [09]   A,H,M are collinear [41]
007. MO  CH [40] & A,C,O are collinear [22] & A,H,M are collinear [41]   AC:AO = AH:AM [42]
008. MO  CH [40] & A,C,O are collinear [22] & A,H,M are collinear [41]   AC:AO = CH:OM [43]
009. A,B,L are collinear [15] & A,B,E are collinear [03]   E,B,L are collinear [44]
010. KL  BD [16] & D,K,E are collinear [13] & E,B,L are collinear [44]   ED:EK = EB:EL [45]
011. KL  BD [16] & D,K,E are collinear [13] & E,B,L are collinear [44]   DE:DK = BE:BL [46]
012. KL  BD [16] & D,K,E are collinear [13] & E,B,L are collinear [44]   DE:BE = DK:BL [47]
013. KL  BD [16] & D,K,E are collinear [13] & E,B,L are collinear [44]   KE:DK = LE:BL [48]
014. ED:EK = EB:EL [45] & EJ = EB [11]   DE:KE = JE:LE [49]
015. DE:DK = BE:BL [46] & EJ = EB [11]   DE:DK = JE:BL [50]
016. A,B,M are collinear [17] & A,B,E are collinear [03]   E,A,M are collinear [51]
017. KM  AD [18] & E,A,M are collinear [51] & D,K,E are collinear [13]   AE:AM = DE:DK [52]
018. KM  AD [18] & E,A,M are collinear [51] & D,K,E are collinear [13]   AE:DE = AM:DK [53]
019. KM  AD [18] & E,A,M are collinear [51] & D,K,E are collinear [13]   ME:AM = KE:DK [54]
020. AE:AM = DE:DK [52] & EJ = EB [11] & EB = EA [02]   JE:AM = DE:DK [55]
021. A,B,M are collinear [17] & A,B,L are collinear [15] & KM  AD [18]   KML = DAB [56]
022. A,B,M are collinear [17] & A,B,L are collinear [15] & KL  BD [16]   KLM = DBA [57]
023. KML = DAB [56] & KLM = DBA [57] (Similar Triangles)  MK:ML = AD:AB [58]
024. MK:ML = AD:AB [58] & DB = DA [01]   KM:LM = DB:AB [59]
025. DB = DA [01] & EB = EA [02]   BA  DE [60]
026. DB = DA [01] & EB = EA [02]   EA:EB = DA:DB [61]
027. C,J,H are collinear [12] & D,K,E are collinear [13] & BA  DE [60] & CH  AB [08]   CJ  EK [62]
028. CJ  EK [62] & C,G,E are collinear [06] & G,K,J are collinear [14]   CG:GE = GJ:GK [63]
029. CJ  EK [62] & C,G,E are collinear [06] & G,K,J are collinear [14]   CG:GE = CJ:KE [64]
030. CJ  EK [62] & C,G,E are collinear [06] & G,K,J are collinear [14]   GE:CE = GK:KJ [65]
031. EJ = EB [11] & EB = EA [02]   E is the circumcenter of \Delta BJA [66]
032. E is the circumcenter of \Delta BJA [66] & A,B,E are collinear [03]   BJ  AJ [67]
033. A,B,H are collinear [09] & C,I,H are collinear [33] & BJ  AJ [67] & CH  AB [08]   BHI = BJA [68]
034. EI = EB [34] & EJ = EB [11] & EB = EA [02]   A,B,I,J are concyclic [69]
035. A,B,I,J are concyclic [69]   ABI = AJI [70]
036. A,B,I,J are concyclic [69]   ABJ = AIJ [71]
037. A,B,I,J are concyclic [69]   AIB = AJB [72]
038. ABI = AJI [70] & C,J,H are collinear [12] & C,I,H are collinear [33]   ABI = (AJ-CH) [73]
039. C,I,H are collinear [33] & ABI = AJI [70] & C,J,H are collinear [12]   BIH = BAJ [74]
040. BHI = BJA [68] & BIH = BAJ [74] (Similar Triangles)  BH:IH = BJ:AJ [75]
041. A,B,H are collinear [09] & C,I,H are collinear [33] & BJ  AJ [67] & CH  AB [08]   AHI = AJB [76]
042. C,I,H are collinear [33] & ABJ = AIJ [71] & C,J,H are collinear [12]   AIH = ABJ [77]
043. AHI = AJB [76] & AIH = ABJ [77] (Similar Triangles)  AH:AI = AJ:AB [78]
044. AHI = AJB [76] & AIH = ABJ [77] (Similar Triangles)  AH:IH = AJ:BJ [79]
045. EI = EB [34] & EB = EA [02]   E is the circumcenter of \Delta BIA [80]
046. EI = EB [34] & EB = EA [02]   EI = EA [81]
047. E is the circumcenter of \Delta BIA [80] & A,B,E are collinear [03]   BI  AI [82]
048. A,B,H are collinear [09] & C,I,H are collinear [33] & CH  AB [08] & BI  AI [82]   BIA = AHI [83]
049. A,B,H are collinear [09] & BAI = BAI [35]   BAI = HAI [84]
050. BIA = AHI [83] & BAI = HAI [84] (Similar Triangles)  BI:AB = IH:AI [85]
051. A,B,E are collinear [03] & EB = EA [02]   E is midpoint of AB [86]
052. A,C,F are collinear [04] & FC = FA [05]   F is midpoint of AC [87]
053. E is midpoint of AB [86] & F is midpoint of AC [87]   EF  BC [88]
054. EF  BC [88] & G,B,F are collinear [07] & C,G,E are collinear [06]   GF:BF = GE:CE [89]
055. EF  BC [88] & G,B,F are collinear [07] & C,G,E are collinear [06]   GF:GB = GE:CG [90]
056. EF  BC [88] & G,B,F are collinear [07] & C,G,E are collinear [06]   GF:GB = FE:CB [91]
057. GE:CE = GK:KJ [65] & GF:BF = GE:CE [89]   GF:BF = GK:KJ [92]
058. GF:BF = GK:KJ [92] & G,K,J are collinear [14] & G,B,F are collinear [07]   KF  JB [93]
059. FK  BJ [93] & G,K,J are collinear [14] & G,B,F are collinear [07]   GK:GJ = KF:JB [94]
060. A,B,E are collinear [03] & C,I,H are collinear [33] & CH  AB [08] & BI  AI [82]   AIB = (EA-CI) [95]
061. A,B,E are collinear [03] & C,I,H are collinear [33] & ABI = (AJ-CH) [73]   (EA-BI) = (AJ-CI) [96]
062. AIB = (EA-CI) [95] & (EA-BI) = (AJ-CI) [96]   IAE = EAJ [97]
063. IAE = EAJ [97] & A,B,E are collinear [03]   IAB = BAJ [98]
064. A,B,I,J are concyclic [69] & IAB = BAJ [98]   BI = JB [99]
065. GK:GJ = KF:JB [94] & BI = JB [99]   GK:GJ = KF:BI [100]
066. DB = DA [01] & DC = DB [00]   DC = DA [101]
067. DB = DA [01] & DC = DB [00]   D is the circumcenter of \Delta BCA [102]
068. DC = DA [101]   DCA = CAD [103]
069. DC = DA [101] & FC = FA [05]   AC  DF [104]
070. S,Q,R are collinear [29] & QR  NS [30]   SQ  SN [105]
071. AC  DF [104] & SQ  SN [105]   (DF-SN) = (AC-SQ) [106]
072. S,Q,R are collinear [29] & (DF-SN) = (AC-SQ) [106]   QSN = (AC-DF) [107]
073. D is the circumcenter of \Delta BCA [102] & F is midpoint of AC [87]   CBA = FDA [108]
074. D is the circumcenter of \Delta BCA [102] & F is midpoint of AC [87]   BCD = (AB-DF) [109]
075. S,N,T are collinear [31] & SN = ST [32]   S is midpoint of NT [110]
076. S,Q,R are collinear [29] & T,S,N are collinear [31] & QR  NS [30]   RS  NT [111]
077. S is midpoint of NT [110] & RS  NT [111]   RN = RT [112]
078. RB = RE [28] & RE = RN [27] & RN = RT [112]   T,B,N,E are concyclic [113]
079. T,B,N,E are concyclic [113]   TNB = TEB [114]
080. T,B,N,E are concyclic [113]   TBN = TEN [115]
081. CBA = FDA [108] & TNB = TEB [114] & S,N,T are collinear [31] & C,B,N are collinear [20] & A,B,E are collinear [03]   (SN-TE) = FDA [116]
082. QSN = (AC-DF) [107] & (SN-TE) = FDA [116]   (SQ-TE) = CAD [117]
083. RN = RT [112] & RE = RN [27]   R is the circumcenter of \Delta ETN [118]
084. RN = RT [112] & RE = RN [27]   RT = RE [119]
085. R is the circumcenter of \Delta ETN [118] & S is midpoint of NT [110]   NET = NRS [120]
086. DCA = CAD [103] & (SQ-TE) = CAD [117] & S,Q,R are collinear [29] & NET = NRS [120]   RNE = DCA [121]
087. RE = RN [27]   NER = RNE [122]
088. (SQ-TE) = CAD [117] & S,Q,R are collinear [29] & NET = NRS [120] & NER = RNE [122]   NER = CAD [123]
089. RNE = DCA [121] & NER = CAD [123] (Similar Triangles)  CD:CA = NR:NE [124]
090. CD:CA = NR:NE [124] & DC = DB [00] & RN = RT [112]   DB:AC = TR:NE [125]
091. C,I,H are collinear [33] & A,B,E are collinear [03] & CH  AB [08]   CI  EA [126]
092. CI  EA [126] & SQ  SN [105]   (CI-SN) = (EA-SQ) [127]
093. S,Q,R are collinear [29] & A,B,E are collinear [03] & C,I,H are collinear [33] & (CI-SN) = (EA-SQ) [127]   QSN = (EA-CI) [128]
094. D is the circumcenter of \Delta BCA [102] & E is midpoint of AB [86]   BCA = EDA [129]
095. D is the circumcenter of \Delta BCA [102] & E is midpoint of AB [86]   CAD = (BC-DE) [130]
096. D is the circumcenter of \Delta BCA [102] & E is midpoint of AB [86]   BCA = BDE [131]
097. S,Q,R are collinear [29] & T,S,N are collinear [31] & QR  NS [30]   QS  NT [132]
098. S is midpoint of NT [110] & QS  NT [132]   QN = QT [133]
099. QN = QO [25] & QC = QN [26] & QN = QT [133]   C,O,N,T are concyclic [134]
100. C,O,N,T are concyclic [134]   COT = CNT [135]
101. C,I,H are collinear [33] & BCA = EDA [129] & COT = CNT [135] & A,C,O are collinear [22] & C,B,N are collinear [20] & S,N,T are collinear [31] & BA  DE [60] & CH  AB [08]   (SN-TO) = (CI-AD) [136]
102. QSN = (EA-CI) [128] & (SN-TO) = (CI-AD) [136]   (SQ-TO) = EAD [137]
103. QN = QT [133] & QN = QO [25]   Q is the circumcenter of \Delta OTN [138]
104. QN = QT [133] & QN = QO [25]   QT = QO [139]
105. Q is the circumcenter of \Delta OTN [138] & S is midpoint of NT [110]   NOT = NQS [140]
106. Q is the circumcenter of \Delta OTN [138] & S is midpoint of NT [110]   TON = TQS [141]
107. QN = QO [25]   QON = ONQ [142]
108. A,B,M are collinear [17] & A,B,L are collinear [15] & (SQ-TO) = EAD [137] & S,Q,R are collinear [29] & A,B,E are collinear [03] & NOT = NQS [140] & QON = ONQ [142] & AD  KM [18]   QON = KML [143]
109. SQ  SN [105] & BP  BD [23]   (SQ-DB) = (SN-BP) [144]
110. SQ  SN [105] & BP  BD [23]   (SQ-BP) = (SN-DB) [145]
111. S,Q,R are collinear [29] & (SQ-DB) = (SN-BP) [144]   QSN = DBP [146]
112. D is the circumcenter of \Delta BCA [102] & BP  BD [23]   PBA = BCA [147]
113. A,B,E are collinear [03] & PBA = BCA [147] & COT = CNT [135] & A,C,O are collinear [22] & C,B,N are collinear [20] & S,N,T are collinear [31]   (SN-TO) = (BP-EA) [148]
114. QSN = DBP [146] & (SN-TO) = (BP-EA) [148]   (SQ-TO) = (DB-EA) [149]
115. A,B,M are collinear [17] & A,B,L are collinear [15] & (SQ-TO) = (DB-EA) [149] & S,Q,R are collinear [29] & A,B,E are collinear [03] & NOT = NQS [140] & BD  KL [16]   QNO = KLM [150]
116. QON = KML [143] & QNO = KLM [150] (Similar Triangles)  OQ:ON = MK:ML [151]
117. OQ:ON = MK:ML [151] & QN = QO [25]   QN:ON = KM:LM [152]
118. A,B,L are collinear [15] & C,A,F are collinear [04] & AC  DF [104] & CH  AB [08] & CH  LN [36]   BLN = DFC [153]
119. C,B,N are collinear [20] & C,A,F are collinear [04] & DCA = CAD [103] & CAD = (BC-DE) [130] & BA  DE [60] & CH  AB [08] & LN  CH [36]   BNL = DCF [154]
120. BLN = DFC [153] & BNL = DCF [154] (Similar Triangles)  BL:BN = DF:DC [155]
121. BE = JE [11]   DE:BE = DE:BE [156]
122. DE:DK = JE:BL [50] & JE:AM = DE:DK [55] & BE = JE [11]   BE:AM = BE:BL [157]
123. DE:BE = DE:BE [156] & BE:AM = BE:BL [157]   AM = BL [158]
124. BL:BN = DF:DC [155] & AM = BL [158] & DC = DB [00]   BL:BN = DF:DB [159]
125. CI  EA [126] & BP  BD [23]   (CI-DB) = (EA-BP) [160]
126. CI  EA [126] & BP  BD [23]   (CI-BP) = (EA-DB) [161]
127. A,B,E are collinear [03] & (CI-DB) = (EA-BP) [160] & C,I,H are collinear [33] & BA  DE [60] & CH  AB [08]   AED = PBD [162]
128. EA:EB = DA:DB [61] & A,B,E are collinear [03]   ADE = EDB [163]
129. D,P,E are collinear [24] & ADE = EDB [163]   ADE = PDB [164]
130. AED = PBD [162] & ADE = PDB [164] (Similar Triangles)  EA:ED = BP:BD [165]
131. EA:ED = BP:BD [165] & EJ = EB [11] & EB = EA [02]   JE:DE = BP:DB [166]
132. DC = DB [00] & EB = EA [02] & DE:DK = JE:BL [50] & BL:BN = DF:DB [159] & JE:DE = BP:DB [166] (Ratio chase)  BN:BP = DK:DF [167]
133. A,B,E are collinear [03] & C,I,H are collinear [33] & (CI-BP) = (EA-DB) [161]   DBP = (EA-CI) [168]
134. DC = DB [00]   DBC = BCD [169]
135. A,B,E are collinear [03] & DBC = BCD [169] & BCD = (AB-DF) [109] & BC  EF [88]   (EA-DF) = (DB-FE) [170]
136. DBP = (EA-CI) [168] & (EA-DF) = (DB-FE) [170]   (CI-DF) = (BP-FE) [171]
137. C,B,N are collinear [20] & D,K,E are collinear [13] & (CI-DF) = (BP-FE) [171] & C,I,H are collinear [33] & BA  DE [60] & CH  AB [08] & EF  BC [88]   NBP = FDK [172]
138. BN:BP = DK:DF [167] & NBP = FDK [172] (Similar Triangles)  BN:PN = DK:KF [173]
139. BN:BP = DK:DF [167] & NBP = FDK [172] (Similar Triangles)  BNP = FKD [174]
140. BN:BP = DK:DF [167] & NBP = FDK [172] (Similar Triangles)  (NP-DF) = (BP-FK) [175]
141. E is midpoint of AB [86] & S is midpoint of NT [110]   EA:AB = SN:NT [176]
142. EA:AB = SN:NT [176] & EJ = EB [11] & EB = EA [02] & SN = ST [32]   JE:AB = ST:TN [177]
143. F is midpoint of AC [87] & S is midpoint of NT [110]   FA:AC = SN:NT [178]
144. A,B,H are collinear [09] & CH  AB [08]   CH  HA [179]
145. CH  HA [179] & F is midpoint of AC [87]   AF = HF [180]
146. CH  HA [179] & F is midpoint of AC [87]   CF = HF [181]
147. FA:AC = SN:NT [178] & AF = HF [180] & SN = ST [32]   FH:AC = ST:TN [182]
148. EF  BC [88] & A,C,F are collinear [04] & A,B,E are collinear [03]   AF:AC = FE:CB [183]
149. AF:AC = FE:CB [183] & AF = HF [180]   FH:AC = FE:CB [184]
150. A,B,E are collinear [03] & D,P,E are collinear [24] & CH  AB [08] & BA  DE [60]   BED = PEB [185]
151. A,B,E are collinear [03] & (CI-DB) = (EA-BP) [160] & C,I,H are collinear [33] & BA  DE [60] & CH  AB [08]   BDE = PBE [186]
152. BED = PEB [185] & BDE = PBE [186] (Similar Triangles)  BE:BD = PE:PB [187]
153. BE:BD = PE:PB [187] & EJ = EB [11]   JE:DB = PE:BP [188]
154. A,B,H are collinear [09] & C,J,H are collinear [12] & CH  AB [08] & BI  AI [82]   BIA = BHJ [189]
155. C,J,H are collinear [12] & ABJ = AIJ [71] & C,I,H are collinear [33]   BAI = BJH [190]
156. BIA = BHJ [189] & BAI = BJH [190] (Similar Triangles)  BI:AB = BH:BJ [191]
157. CB:BN = BH:BL [38] & DE:DK = JE:BL [50] & JE:AB = ST:TN [177] & FH:AC = ST:TN [182] & FH:AC = FE:CB [184] & GF:GB = GE:CG [90] & GF:GB = FE:CB [91] & CG:GE = GJ:GK [63] & JE:DE = BP:DB [166] & JE:DB = PE:BP [188] & BI:AB = BH:BJ [191] & AH:AI = AJ:AB [78] & AH:IH = AJ:BJ [79] & BI:AB = IH:AI [85] & GK:GJ = KF:BI [100] & BN:PN = DK:KF [173] (Ratio chase)  CB:BJ = PN:PE [192]
158. D,P,E are collinear [24] & BNP = FKD [174] & C,B,N are collinear [20] & D,K,E are collinear [13] & FK  BJ [93]   NPE = CBJ [193]
159. PN:PE = BC:BJ [192] & NPE = CBJ [193] (Similar Triangles)  PN:NE = CB:CJ [194]
160. PN:PE = BC:BJ [192] & NPE = CBJ [193] (Similar Triangles)  PNE = BCJ [195]
161. A,B,L are collinear [15] & A,B,E are collinear [03] & A,B,H are collinear [09] & C,I,H are collinear [33] & CH  LN [36]   ELN = BHI [196]
162. C,I,H are collinear [33] & A,B,E are collinear [03] & BJ  AJ [67] & CH  AB [08]   (CI-EA) = BJA [197]
163. (CI-EA) = BJA [197] & (EA-BI) = (AJ-CI) [96]   CIB = (BJ-CI) [198]
164. C,I,H are collinear [33] & CIB = (BJ-CI) [198] & BJ  FK [93]   BIC = (CI-KF) [199]
165. C,I,H are collinear [33] & BNP = FKD [174] & C,B,N are collinear [20] & D,K,E are collinear [13] & FK  BJ [93] & BA  DE [60] & CH  AB [08] & BC  EF [88]   (CI-PN) = KFE [200]
166. BIC = (CI-KF) [199] & (CI-PN) = KFE [200]   (BI-PN) = (CI-FE) [201]
167. C,I,H are collinear [33] & (BI-PN) = (CI-FE) [201] & BA  DE [60] & CH  AB [08] & EF  BC [88] & PNE = BCJ [195] & C,J,H are collinear [12] & LN  CH [36]   ENL = BIH [202]
168. ELN = BHI [196] & ENL = BIH [202] (Similar Triangles)  LE:LN = HB:HI [203]
169. ELN = BHI [196] & ENL = BIH [202] (Similar Triangles)  LE:BH = NL:IH [204]
170. A,B,H are collinear [09] & A,B,E are collinear [03] & C,I,H are collinear [33] & C,J,H are collinear [12] & CH  AB [08]   EHI = JHE [205]
171. EJ = EB [11] & EI = EB [34]   EI = EJ [206]
172. EI = EJ [206]   EIJ = IJE [207]
173. C,I,H are collinear [33] & C,J,H are collinear [12] & EIJ = IJE [207]   EIH = HJE [208]
174. EHI = JHE [205] & EIH = HJE [208] (Similar Triangles)  HI = HJ [209]
175. LE:LN = HB:HI [203] & HI = HJ [209]   LE:NL = BH:JH [210]
176. A,B,L are collinear [15] & A,B,E are collinear [03] & C,I,H are collinear [33] & A,B,H are collinear [09] & CH  AB [08] & CH  LN [36]   ELN = IHA [211]
177. A,B,E are collinear [03] & C,I,H are collinear [33] & ABJ = AIJ [71] & C,J,H are collinear [12] & BJ  FK [93]   (EA-KF) = AIC [212]
178. (EA-KF) = AIC [212] & (CI-PN) = KFE [200]   (AI-PN) = AEF [213]
179. A,B,H are collinear [09] & (AI-PN) = AEF [213] & A,B,E are collinear [03] & EF  BC [88] & PNE = BCJ [195] & C,J,H are collinear [12] & BA  DE [60] & CH  AB [08] & LN  CH [36]   ENL = IAH [214]
180. ELN = IHA [211] & ENL = IAH [214] (Similar Triangles)  LE:NL = IH:AH [215]
181. ELN = IHA [211] & ENL = IAH [214] (Similar Triangles)  IH:LE = AH:NL [216]
182. EB = EA [02] & CB:BN = BH:BL [38] & CB:BN = CH:NL [39] & AC:AO = AH:AM [42] & AC:AO = CH:OM [43] & DE:DK = JE:BL [50] & JE:AM = DE:DK [55] (Ratio chase)  BH:AH = OM:NL [217]
183. EB = EA [02] & CB:BN = BH:BL [38] & CB:BN = CH:NL [39] & AC:AO = AH:AM [42] & AC:AO = CH:OM [43] & DE:DK = JE:BL [50] & JE:AM = DE:DK [55] (Ratio chase)  BH:OM = AH:NL [218]
184. LE:NL = IH:AH [215] & BH:AH = OM:NL [217]   IH:BH = LE:OM [219]
185. AE:DE = AM:DK [53] & EJ = EB [11] & EB = EA [02] & DE:BE = DK:BL [47]   DK:BL = DK:AM [220]
186. ME:AM = KE:DK [54] & KE:DK = LE:BL [48]   LE:BL = ME:AM [221]
187. DK:BL = DK:AM [220] & LE:BL = ME:AM [221]   LE = ME [222]
188. IH:BH = LE:OM [219] & HI = HJ [209] & ME = LE [222]   HB:HI = MO:ME [223]
189. A,B,H are collinear [09] & C,I,H are collinear [33] & A,B,M are collinear [17] & A,B,E are collinear [03] & CH  AB [08] & CH  MO [40]   BHI = OME [224]
190. HB:HI = MO:ME [223] & BHI = OME [224] (Similar Triangles)  BH:BI = OM:OE [225]
191. HB:HI = MO:ME [223] & BHI = OME [224] (Similar Triangles)  BIH = OEM [226]
192. HB:HI = MO:ME [223] & BHI = OME [224] (Similar Triangles)  HBI = MOE [227]
193. BH:BI = OM:OE [225] & BI = JB [99]   BH:BJ = OM:OE [228]
194. BH:OM = AH:NL [218] & IH:LE = AH:NL [216]   IH:LE = BH:OM [229]
195. IH:LE = BH:OM [229] & LE:BH = NL:IH [204]   LE:NL = OM:LE [230]
196. LE:NL = OM:LE [230] & ME = LE [222]   LE:LN = MO:ME [231]
197. A,B,L are collinear [15] & A,B,E are collinear [03] & A,B,M are collinear [17] & CH  AB [08] & CH  LN [36] & CH  MO [40]   ELN = OME [232]
198. LE:LN = MO:ME [231] & ELN = OME [232] (Similar Triangles)  NE:EO = NL:EM [233]
199. NE:EO = NL:EM [233] & LE = ME [222] & LE:NL = BH:JH [210]   BH:JH = OE:NE [234]
200. BH:JH = OE:NE [234] & IH = JH [209]   HB:HI = EO:EN [235]
201. PNE = BCJ [195] & C,J,H are collinear [12] & BA  DE [60] & CH  AB [08] & (BI-PN) = (CI-FE) [201] & C,I,H are collinear [33] & EF  BC [88]   (BI-PN) = ENP [236]
202. (BI-PN) = ENP [236]   BI  NE [237]
203. A,B,H are collinear [09] & C,I,H are collinear [33] & BIH = OEM [226] & A,B,M are collinear [17] & A,B,E are collinear [03] & BI  EN [237]   BHI = OEN [238]
204. HB:HI = EO:EN [235] & BHI = OEN [238] (Similar Triangles)  BH:BI = OE:ON [239]
205. HB:HI = EO:EN [235] & BHI = OEN [238] (Similar Triangles)  BIH = ONE [240]
206. BH:BI = OE:ON [239] & BI = JB [99]   BH:BJ = OE:ON [241]
207. DC = DB [00] & DB = DA [01] & EB = EA [02] & QN = QO [25] & RE = RN [27] & CB:BN = BH:BL [38] & CB:BN = CH:NL [39] & AC:AO = AH:AM [42] & AC:AO = CH:OM [43] & DE:KE = JE:LE [49] & DE:DK = JE:BL [50] & JE:AM = DE:DK [55] & KM:LM = DB:AB [59] & CG:GE = GJ:GK [63] & CG:GE = CJ:KE [64] & BH:IH = BJ:AJ [75] & AH:AI = AJ:AB [78] & AH:IH = AJ:BJ [79] & BI:AB = IH:AI [85] & GK:GJ = KF:BI [100] & DB:AC = TR:NE [125] & QN:ON = KM:LM [152] & BN:PN = DK:KF [173] & PN:NE = CB:CJ [194] & LE:NL = BH:JH [210] & BH:BJ = OM:OE [228] & BH:BJ = OE:ON [241] (Ratio chase)  QN:RE = AC:AI [242]
208. DC = DB [00] & DB = DA [01] & EB = EA [02] & QN = QO [25] & RE = RN [27] & CB:BN = BH:BL [38] & CB:BN = CH:NL [39] & AC:AO = AH:AM [42] & AC:AO = CH:OM [43] & DE:KE = JE:LE [49] & DE:DK = JE:BL [50] & JE:AM = DE:DK [55] & KM:LM = DB:AB [59] & CG:GE = GJ:GK [63] & CG:GE = CJ:KE [64] & BH:IH = BJ:AJ [75] & AH:AI = AJ:AB [78] & AH:IH = AJ:BJ [79] & BI:AB = IH:AI [85] & GK:GJ = KF:BI [100] & DB:AC = TR:NE [125] & QN:ON = KM:LM [152] & BN:PN = DK:KF [173] & PN:NE = CB:CJ [194] & LE:NL = BH:JH [210] & BH:BJ = OM:OE [228] & BH:BJ = OE:ON [241] (Ratio chase)  QN:AC = RE:AI [243]
209. QN:RE = AC:AI [242] & ER = TR [119] & TQ = QN [133]   TQ:TR = AC:AI [244]
210. A,B,E are collinear [03] & A,C,O are collinear [22] & COT = CNT [135] & C,B,N are collinear [20] & S,N,T are collinear [31] & TNB = TEB [114]   AET = AOT [245]
211. AET = AOT [245]   A,O,T,E are concyclic [246]
212. A,O,T,E are concyclic [246]   AOE = ATE [247]
213. A,O,T,E are concyclic [246]   ATO = AEO [248]
214. C,J,H are collinear [12] & A,B,H are collinear [09] & (AJ-CH) = ABI [73]   AJH = HBI [249]
215. A,B,H are collinear [09] & C,J,H are collinear [12] & C,I,H are collinear [33] & (AB-CH) = (AB-CH) [10]   AHJ = BHI [250]
216. AJH = HBI [249] & AHJ = BHI [250] (Similar Triangles)  HA:HJ = HI:HB [251]
217. HA:HJ = HI:HB [251] & HI = HJ [209]   IH:AH = BH:IH [252]
218. IH:AH = BH:IH [252] & IH:LE = BH:OM [229]   IH:AH = OM:LE [253]
219. OM:LE = IH:AH [253] & ME = LE [222]   MO:ME = HI:HA [254]
220. A,B,M are collinear [17] & A,B,E are collinear [03] & C,I,H are collinear [33] & A,B,H are collinear [09] & CH  MO [40]   OME = IHA [255]
221. MO:ME = HI:HA [254] & OME = IHA [255] (Similar Triangles)  MOE = HIA [256]
222. AOE = ATE [247] & A,C,O are collinear [22] & MOE = HIA [256] & C,I,H are collinear [33] & MO  CH [40]   (AI-TE) = CAT [257]
223. C,I,H are collinear [33] & ADE = ACB [129] & BA  DE [60] & CH  AB [08] & BC  EF [88]   (AD-CI) = (AC-FE) [258]
224. S,Q,R are collinear [29] & (SQ-TE) = CAD [117]   (AD-TE) = (AC-SQ) [259]
225. (AD-CI) = (AC-FE) [258] & (AD-TE) = (AC-SQ) [259]   (FE-SQ) = (CI-TE) [260]
226. C,I,H are collinear [33] & (BI-PN) = (CI-FE) [201]   CIB = (FE-PN) [261]
227. C,I,H are collinear [33] & REN = ENR [122] & NET = NRS [120] & S,Q,R are collinear [29] & (SQ-TE) = CAD [117] & DAC = (DE-BC) [130] & BA  DE [60] & CH  AB [08] & BC  EF [88]   (CI-FE) = REN [262]
228. TBN = TEN [115] & C,B,N are collinear [20] & BC  EF [88]   (FE-TB) = NET [263]
229. (CI-FE) = REN [262] & (FE-TB) = NET [263]   (CI-TB) = RET [264]
230. C,I,H are collinear [33] & TBN = TEN [115] & C,B,N are collinear [20] & (CI-TB) = RET [264] & BA  DE [60] & CH  AB [08] & BC  EF [88]   (CI-RE) = FEN [265]
231. CIB = (FE-PN) [261] & (CI-RE) = FEN [265]   PNE = (BI-ER) [266]
232. S,Q,R are collinear [29] & (FE-SQ) = (CI-TE) [260] & C,I,H are collinear [33] & EF  BC [88] & BA  DE [60] & CH  AB [08] & PNE = BCJ [195] & C,J,H are collinear [12] & PNE = (BI-ER) [266]   (BI-RE) = (SQ-TE) [267]
233. RE = RT [119]   RET = ETR [268]
234. (BI-RE) = (SQ-TE) [267] & RET = ETR [268]   (BI-TE) = (SQ-TR) [269]
235. A,B,E are collinear [03] & ATO = AEO [248] & MOE = HIA [256] & C,I,H are collinear [33] & MO  CH [40]   EAT = (AI-TO) [270]
236. AIB = (EA-CI) [95] & EAT = (AI-TO) [270]   (CI-AT) = (BI-TO) [271]
237. (CI-AT) = (BI-TO) [271] & C,I,H are collinear [33] & BIH = ONE [240] & BA  DE [60] & CH  AB [08] & BI  EN [237]   (BI-ON) = ATO [272]
238. S,Q,R are collinear [29] & TON = TQS [141]   (ON-SQ) = OTQ [273]
239. (BI-ON) = ATO [272] & (ON-SQ) = OTQ [273]   (BI-SQ) = ATQ [274]
240. (BI-TE) = (SQ-TR) [269] & S,Q,R are collinear [29] & (BI-SQ) = ATQ [274]   ATQ = ETR [275]
241. (AI-TE) = CAT [257] & ATQ = ETR [275]   CAI = QTR [276]
242. TQ:TR = AC:AI [244] & CAI = QTR [276] (Similar Triangles)  TQR = ACI [277]
243. EB = EA [02] & CB:BN = BH:BL [38] & CB:BN = CH:NL [39] & AC:AO = AH:AM [42] & AC:AO = CH:OM [43] & DE:KE = JE:LE [49] & DE:DK = JE:BL [50] & JE:AM = DE:DK [55] & JE:AB = ST:TN [177] & FH:AC = ST:TN [182] & FH:AC = FE:CB [184] & GF:GB = GE:CG [90] & GF:GB = FE:CB [91] & CG:GE = CJ:KE [64] & JE:DE = BP:DB [166] & JE:DB = PE:BP [188] & BH:IH = BJ:AJ [75] & AH:IH = AJ:BJ [79] & BI:AB = IH:AI [85] & LE:NL = BH:JH [210] & BH:BJ = OM:OE [228] (Ratio chase)  CJ:AI = OE:PE [278]
244. A,B,E are collinear [03] & C,I,H are collinear [33] & BJ  AJ [67] & CH  AB [08]   (EA-CI) = BJA [279]
245. A,B,E are collinear [03] & C,I,H are collinear [33] & ABI = AJI [70] & C,J,H are collinear [12]   EAJ = BIC [280]
246. (EA-CI) = BJA [279] & EAJ = BIC [280]   (EA-BI) = (BJ-EA) [281]
247. (EA-BI) = (BJ-EA) [281] & A,B,E are collinear [03]   ABI = JBA [282]
248. A,B,I,J are concyclic [69] & ABI = JBA [282]   AI = JA [283]
249. OE:PE = CJ:AI [278] & AJ = AI [283]   EO:EP = JC:JA [284]
250. IH:AH = OM:LE [253] & JH = IH [209] & ME = LE [222]   HJ:HA = MO:ME [285]
251. C,J,H are collinear [12] & A,B,H are collinear [09] & A,B,M are collinear [17] & A,B,E are collinear [03] & CH  AB [08] & CH  MO [40]   JHA = EMO [286]
252. HJ:HA = MO:ME [285] & JHA = EMO [286] (Similar Triangles)  HJA = EOM [287]
253. DB = DA [01]   DBA = BAD [288]
254. CH  AB [08] & MO  CH [40] & ADE = EDB [163] & DBA = BAD [288] (Angle chase)  MO  DE [289]
255. D,P,E are collinear [24] & C,J,H are collinear [12] & HJA = EOM [287] & BA  DE [60] & CH  AB [08] & MO  DE [289]   OEP = CJA [290]
256. EO:EP = JC:JA [284] & OEP = CJA [290] (Similar Triangles)  EOP = JCA [291]
257. EO:EP = JC:JA [284] & OEP = CJA [290] (Similar Triangles)  PO:AC = PE:AJ [292]
258. S,Q,R are collinear [29] & TQR = ACI [277] & C,I,H are collinear [33] & BA  DE [60] & CH  AB [08] & EOP = JCA [291] & C,J,H are collinear [12] & MOE = HIA [256] & MO  CH [40]   (AI-PO) = SQT [293]
259. RB = RE [28]   RBE = BER [294]
260. RBE = BER [294] & A,B,E are collinear [03]   RBA = (AB-ER) [295]
261. RE = RN [27] & RB = RE [28]   RB = RN [296]
262. RB = RN [296]   RBN = BNR [297]
263. RBN = BNR [297] & C,B,N are collinear [20]   RBC = (BC-NR) [298]
264. EF  BC [88] & ADE = EDB [163] & DBA = BAD [288] & RBA = (AB-ER) [295] & RBC = (BC-NR) [298] & REN = ENR [122] (Angle chase)  FED = RNE [299]
265. PNE = BCJ [195] & C,J,H are collinear [12] & BA  DE [60] & CH  AB [08] & FED = RNE [299] & EF  BC [88]   RNE = PNE [300]
266. RNE = PNE [300]   RN  PN [301]
267. NR  NP [301]   R,P,N are collinear [302]
268. QN:RE = AC:AI [242] & ER = TR [119] & RN = TR [112]   NQ:NR = AC:AI [303]
269. AC  DF [104] & BJ  AJ [67]   (AC-DF) = AJB [304]
270. (AC-DF) = AJB [304] & BJ  FK [93]   (AC-DF) = (AJ-KF) [305]
271. (AC-DF) = (AJ-KF) [305] & (DF-PN) = (KF-BP) [175]   JAC = BPN [306]
272. P,R,N are collinear [302] & JAC = BPN [306]   JAC = BPR [307]
273. S,Q,R are collinear [29] & (SQ-BP) = (SN-DB) [145]   QSN = PBD [308]
274. C,I,H are collinear [33] & BCA = BDE [131] & COT = CNT [135] & A,C,O are collinear [22] & C,B,N are collinear [20] & S,N,T are collinear [31] & BA  DE [60] & CH  AB [08]   (SN-TO) = (DB-CI) [309]
275. QSN = PBD [308] & (SN-TO) = (DB-CI) [309]   (SQ-TO) = (BP-CI) [310]
276. C,I,H are collinear [33] & S,Q,R are collinear [29] & (SQ-TO) = (BP-CI) [310]   (CI-TO) = (BP-SQ) [311]
277. NOT = NQS [140] & (CI-TO) = (BP-SQ) [311]   (QN-BP) = (ON-CI) [312]
278. NE:EO = NL:EM [233] & LE = ME [222] & LE:NL = IH:AH [215] & JH = IH [209]   HJ:HA = EO:EN [313]
279. C,J,H are collinear [12] & A,B,H are collinear [09] & BIH = OEM [226] & C,I,H are collinear [33] & A,B,M are collinear [17] & A,B,E are collinear [03] & BI  EN [237]   JHA = NEO [314]
280. HJ:HA = EO:EN [313] & JHA = NEO [314] (Similar Triangles)  HJA = NOE [315]
281. (QN-BP) = (ON-CI) [312] & C,I,H are collinear [33] & BA  DE [60] & CH  AB [08] & HJA = NOE [315] & C,J,H are collinear [12] & MOE = HIA [256] & MO  CH [40]   JAI = (BP-QN) [316]
282. JAC = BPR [307] & JAI = (BP-QN) [316]   (RP-QN) = CAI [317]
283. (RP-QN) = CAI [317] & R,P,N are collinear [302] & NR  NP [301] & MOE = HIA [256] & C,I,H are collinear [33] & MO  CH [40]   RNQ = CAI [318]
284. NQ:NR = AC:AI [303] & RNQ = CAI [318] (Similar Triangles)  QRN = AIC [319]
285. C,I,H are collinear [33] & S,Q,R are collinear [29] & P,R,N are collinear [302] & QRN = AIC [319]   AIC = (SQ-RP) [320]
286. (AI-PO) = SQT [293] & AIC = (SQ-RP) [320]   (PO-CI) = (TQ-RP) [321]
287. PO:AC = PE:AJ [292] & AI = JA [283]   PO:AC = PE:AI [322]
288. QN:AC = RE:AI [243] & ER = TR [119] & CQ = QN [26] & RB = RE [28] & RE = RN [27] & RN = RT [112]   CQ:AC = BR:AI [323]
289. PO:AC = PE:AI [322] & CQ:AC = BR:AI [323]   PO:CQ = PE:BR [324]
290. PO:CQ = PE:BR [324] & QC = QN [26] & RB = RE [28] & RE = RN [27] & RN = RT [112] & RE = TR [119] & QO = QN [25]   EP:ER = OP:OQ [325]
291. CF = HF [181]   FCH = CHF [326]
292. C,I,H are collinear [33] & EOP = JCA [291] & C,J,H are collinear [12] & BA  DE [60] & CH  AB [08] & FCH = CHF [326] & A,C,F are collinear [04] & MOE = HIA [256] & MO  CH [40]   (CI-FH) = (PO-AI) [327]
293. A,B,H are collinear [09] & C,J,H are collinear [12] & HBI = MOE [227] & MO  CH [40] & BA  DE [60] & CH  AB [08] & BI  EN [237]   BHJ = NEO [328]
294. HB:HJ = EO:EN [234] & BHJ = NEO [328] (Similar Triangles)  HBJ = NOE [329]
295. A,B,E are collinear [03] & S,Q,R are collinear [29] & (SQ-TO) = EAD [137]   (EA-SQ) = (AD-TO) [330]
296. QT = QO [139]   OTQ = QOT [331]
297. S,Q,R are collinear [29] & TON = TQS [141] & OTQ = QOT [331]   QOT = (ON-SQ) [332]
298. (EA-SQ) = (AD-TO) [330] & QOT = (ON-SQ) [332]   (EA-ON) = (AD-QO) [333]
299. HBJ = NOE [329] & A,B,H are collinear [09] & MOE = HIA [256] & C,I,H are collinear [33] & MO  CH [40] & (EA-ON) = (AD-QO) [333] & A,B,E are collinear [03] & BJ  FK [93]   (AD-QO) = (KF-AI) [334]
300. C,I,H are collinear [33] & ACB = EDB [131] & BC  EF [88] & BA  DE [60] & CH  AB [08]   (AC-FE) = (CI-DB) [335]
301. C,I,H are collinear [33] & FCH = CHF [326] & A,C,F are collinear [04]   ACI = (CI-FH) [336]
302. (AC-FE) = (CI-DB) [335] & ACI = (CI-FH) [336]   (DB-FH) = (FE-CI) [337]
303. (DB-FH) = (FE-CI) [337] & C,I,H are collinear [33] & EF  BC [88] & BA  DE [60] & CH  AB [08] & PNE = BCJ [195] & C,J,H are collinear [12] & PNE = (BI-ER) [266]   (BI-RE) = (DB-FH) [338]
304. EI = EB [34]   EBI = BIE [339]
305. EBI = BIE [339] & A,B,E are collinear [03]   ABI = BIE [340]
306. EI = EA [81]   EIA = IAE [341]
307. EIA = IAE [341] & A,B,E are collinear [03]   EIA = IAB [342]
308. CH  AB [08] & KL  BD [16] & AIB = AJB [72] & ABI = (AJ-CH) [73] & DBA = BAD [288] & ABI = BIE [340] & EIA = IAB [342] (Angle chase)  (KL-BJ) = (BI-AD) [343]
309. (KL-BJ) = (BI-AD) [343] & KL  BD [16] & BJ  FK [93]   (BI-AD) = (DB-KF) [344]
310. (BI-RE) = (DB-FH) [338] & (BI-AD) = (DB-KF) [344]   HFK = (ER-AD) [345]
311. (AD-QO) = (KF-AI) [334] & (AD-RE) = KFH [345]   (OQ-AI) = (ER-FH) [346]
312. (CI-FH) = (PO-AI) [327] & (QO-AI) = (RE-FH) [346]   (CI-RE) = POQ [347]
313. D,P,E are collinear [24] & (CI-RE) = POQ [347] & C,I,H are collinear [33] & BA  DE [60] & CH  AB [08]   PER = POQ [348]
314. EP:ER = OP:OQ [325] & PER = POQ [348] (Similar Triangles)  EPR = OPQ [349]
315. (PO-CI) = (TQ-RP) [321] & C,I,H are collinear [33] & R,P,N are collinear [302] & BA  DE [60] & CH  AB [08] & NR  NP [301] & EPR = OPQ [349] & D,P,E are collinear [24]   QPO = (TQ-PO) [350]
316. QPO = (TQ-PO) [350]   QP  TQ [351]
317. PQ  QT [351]   T,Q,P are collinear
==========================

