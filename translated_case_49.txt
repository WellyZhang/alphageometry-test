I0123 16:32:32.348598 139719371399168 inference_utils.py:69] Parsing gin configuration.
I0123 16:32:32.348699 139719371399168 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0123 16:32:32.348906 139719371399168 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0123 16:32:32.348940 139719371399168 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0123 16:32:32.348968 139719371399168 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0123 16:32:32.348997 139719371399168 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0123 16:32:32.349024 139719371399168 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0123 16:32:32.349051 139719371399168 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0123 16:32:32.349077 139719371399168 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0123 16:32:32.349103 139719371399168 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=32
I0123 16:32:32.349128 139719371399168 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0123 16:32:32.349153 139719371399168 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0123 16:32:32.349198 139719371399168 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0123 16:32:32.349331 139719371399168 resource_reader.py:55] Path not found: base_htrans.gin
I0123 16:32:32.349544 139719371399168 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0123 16:32:32.349660 139719371399168 resource_reader.py:55] Path not found: trainer_configuration.gin
I0123 16:32:32.355969 139719371399168 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0123 16:32:32.356090 139719371399168 resource_reader.py:55] Path not found: size/medium_150M.gin
I0123 16:32:32.356408 139719371399168 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0123 16:32:32.356512 139719371399168 resource_reader.py:55] Path not found: options/positions_t5.gin
I0123 16:32:32.356787 139719371399168 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0123 16:32:32.356886 139719371399168 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0123 16:32:32.357287 139719371399168 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0123 16:32:32.357387 139719371399168 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0123 16:32:32.361044 139719371399168 training_loop.py:334] ==== Training loop: initializing model ====
I0123 16:32:32.469964 139719371399168 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0123 16:32:32.470673 139719371399168 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0123 16:32:32.477170 139719371399168 training_loop.py:335] Process 0 of 1
I0123 16:32:32.477225 139719371399168 training_loop.py:336] Local device count = 1
I0123 16:32:32.477264 139719371399168 training_loop.py:337] Number of replicas = 1
I0123 16:32:32.477295 139719371399168 training_loop.py:339] Using random number seed 42
I0123 16:32:32.979070 139719371399168 training_loop.py:359] Initializing the model.
I0123 16:32:33.348014 139719371399168 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.348304 139719371399168 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 16:32:33.348411 139719371399168 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 16:32:33.348487 139719371399168 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 16:32:33.348561 139719371399168 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 16:32:33.348640 139719371399168 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 16:32:33.348710 139719371399168 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 16:32:33.348777 139719371399168 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 16:32:33.348843 139719371399168 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 16:32:33.348909 139719371399168 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 16:32:33.348976 139719371399168 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 16:32:33.349043 139719371399168 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 16:32:33.349109 139719371399168 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 16:32:33.349175 139719371399168 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 16:32:33.349213 139719371399168 attention.py:418] Single window, no scan.
I0123 16:32:33.349257 139719371399168 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 16:32:33.349372 139719371399168 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:32:33.349412 139719371399168 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:32:33.349441 139719371399168 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:32:33.351461 139719371399168 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.356733 139719371399168 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:32:33.367251 139719371399168 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.367528 139719371399168 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:32:33.371887 139719371399168 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:32:33.382481 139719371399168 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:32:33.382539 139719371399168 attention.py:418] Single window, no scan.
I0123 16:32:33.382577 139719371399168 transformer_layer.py:389] tlayer: self-attention.
I0123 16:32:33.382609 139719371399168 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.382672 139719371399168 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.383855 139719371399168 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.383934 139719371399168 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.384640 139719371399168 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.387090 139719371399168 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.392715 139719371399168 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.394440 139719371399168 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.394520 139719371399168 transformer_layer.py:468] tlayer: End windows.
I0123 16:32:33.394556 139719371399168 transformer_layer.py:472] tlayer: final FFN.
I0123 16:32:33.394615 139719371399168 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.394741 139719371399168 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:32:33.395075 139719371399168 nn_components.py:325] mlp: activation = None
I0123 16:32:33.395123 139719371399168 nn_components.py:329] mlp: final activation = None
I0123 16:32:33.397008 139719371399168 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.397108 139719371399168 nn_components.py:261] mlp: residual
I0123 16:32:33.399909 139719371399168 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.399989 139719371399168 transformer_base.py:443] tbase: final FFN
I0123 16:32:33.400475 139719371399168 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:32:33.410423 139719371399168 nn_components.py:329] mlp: final activation = None
I0123 16:32:33.419003 139719371399168 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.419101 139719371399168 nn_components.py:261] mlp: residual
I0123 16:32:33.419391 139719371399168 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.419471 139719371399168 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 16:32:33.419581 139719371399168 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:32:33.419622 139719371399168 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:32:33.419652 139719371399168 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:32:33.421481 139719371399168 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.423902 139719371399168 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:32:33.429367 139719371399168 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.429632 139719371399168 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:32:33.432220 139719371399168 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:32:33.435993 139719371399168 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:32:33.436048 139719371399168 attention.py:418] Single window, no scan.
I0123 16:32:33.436084 139719371399168 transformer_layer.py:389] tlayer: self-attention.
I0123 16:32:33.436115 139719371399168 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.436176 139719371399168 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.436742 139719371399168 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.436817 139719371399168 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.437167 139719371399168 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.437928 139719371399168 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.440347 139719371399168 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.440960 139719371399168 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.441035 139719371399168 transformer_layer.py:468] tlayer: End windows.
I0123 16:32:33.441069 139719371399168 transformer_layer.py:472] tlayer: final FFN.
I0123 16:32:33.441125 139719371399168 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.441249 139719371399168 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:32:33.441578 139719371399168 nn_components.py:325] mlp: activation = None
I0123 16:32:33.441621 139719371399168 nn_components.py:329] mlp: final activation = None
I0123 16:32:33.443536 139719371399168 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.443627 139719371399168 nn_components.py:261] mlp: residual
I0123 16:32:33.446066 139719371399168 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.446151 139719371399168 transformer_base.py:443] tbase: final FFN
I0123 16:32:33.446574 139719371399168 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:32:33.448862 139719371399168 nn_components.py:329] mlp: final activation = None
I0123 16:32:33.450724 139719371399168 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.450818 139719371399168 nn_components.py:261] mlp: residual
I0123 16:32:33.451100 139719371399168 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.451179 139719371399168 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 16:32:33.451288 139719371399168 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:32:33.451327 139719371399168 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:32:33.451356 139719371399168 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:32:33.453234 139719371399168 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.455558 139719371399168 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:32:33.461410 139719371399168 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.461678 139719371399168 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:32:33.464266 139719371399168 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:32:33.468069 139719371399168 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:32:33.468124 139719371399168 attention.py:418] Single window, no scan.
I0123 16:32:33.468161 139719371399168 transformer_layer.py:389] tlayer: self-attention.
I0123 16:32:33.468190 139719371399168 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.468252 139719371399168 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.468806 139719371399168 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.468882 139719371399168 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.469236 139719371399168 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.469998 139719371399168 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.472422 139719371399168 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.473084 139719371399168 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.473161 139719371399168 transformer_layer.py:468] tlayer: End windows.
I0123 16:32:33.473196 139719371399168 transformer_layer.py:472] tlayer: final FFN.
I0123 16:32:33.473254 139719371399168 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.473383 139719371399168 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:32:33.473714 139719371399168 nn_components.py:325] mlp: activation = None
I0123 16:32:33.473760 139719371399168 nn_components.py:329] mlp: final activation = None
I0123 16:32:33.475635 139719371399168 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.475728 139719371399168 nn_components.py:261] mlp: residual
I0123 16:32:33.478208 139719371399168 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.478294 139719371399168 transformer_base.py:443] tbase: final FFN
I0123 16:32:33.478776 139719371399168 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:32:33.481036 139719371399168 nn_components.py:329] mlp: final activation = None
I0123 16:32:33.482947 139719371399168 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.483042 139719371399168 nn_components.py:261] mlp: residual
I0123 16:32:33.483329 139719371399168 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.483409 139719371399168 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 16:32:33.483518 139719371399168 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:32:33.483557 139719371399168 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:32:33.483587 139719371399168 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:32:33.485483 139719371399168 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.487814 139719371399168 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:32:33.493379 139719371399168 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.493634 139719371399168 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:32:33.496234 139719371399168 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:32:33.499989 139719371399168 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:32:33.500045 139719371399168 attention.py:418] Single window, no scan.
I0123 16:32:33.500081 139719371399168 transformer_layer.py:389] tlayer: self-attention.
I0123 16:32:33.500112 139719371399168 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.500172 139719371399168 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.500733 139719371399168 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.500808 139719371399168 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.501162 139719371399168 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.501919 139719371399168 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.504561 139719371399168 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.505182 139719371399168 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.505259 139719371399168 transformer_layer.py:468] tlayer: End windows.
I0123 16:32:33.505294 139719371399168 transformer_layer.py:472] tlayer: final FFN.
I0123 16:32:33.505352 139719371399168 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.505476 139719371399168 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:32:33.505805 139719371399168 nn_components.py:325] mlp: activation = None
I0123 16:32:33.505849 139719371399168 nn_components.py:329] mlp: final activation = None
I0123 16:32:33.507723 139719371399168 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.507815 139719371399168 nn_components.py:261] mlp: residual
I0123 16:32:33.510344 139719371399168 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.510429 139719371399168 transformer_base.py:443] tbase: final FFN
I0123 16:32:33.510862 139719371399168 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:32:33.513068 139719371399168 nn_components.py:329] mlp: final activation = None
I0123 16:32:33.514956 139719371399168 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.515052 139719371399168 nn_components.py:261] mlp: residual
I0123 16:32:33.515340 139719371399168 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.515419 139719371399168 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 16:32:33.515526 139719371399168 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:32:33.515565 139719371399168 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:32:33.515594 139719371399168 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:32:33.517472 139719371399168 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.519816 139719371399168 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:32:33.525352 139719371399168 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.525604 139719371399168 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:32:33.528239 139719371399168 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:32:33.532751 139719371399168 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:32:33.532861 139719371399168 attention.py:418] Single window, no scan.
I0123 16:32:33.532905 139719371399168 transformer_layer.py:389] tlayer: self-attention.
I0123 16:32:33.532938 139719371399168 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.533010 139719371399168 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.533587 139719371399168 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.533676 139719371399168 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.534038 139719371399168 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.534803 139719371399168 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.537646 139719371399168 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.538261 139719371399168 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.538342 139719371399168 transformer_layer.py:468] tlayer: End windows.
I0123 16:32:33.538376 139719371399168 transformer_layer.py:472] tlayer: final FFN.
I0123 16:32:33.538435 139719371399168 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.538571 139719371399168 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:32:33.538914 139719371399168 nn_components.py:325] mlp: activation = None
I0123 16:32:33.538958 139719371399168 nn_components.py:329] mlp: final activation = None
I0123 16:32:33.540869 139719371399168 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.540963 139719371399168 nn_components.py:261] mlp: residual
I0123 16:32:33.543484 139719371399168 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.543569 139719371399168 transformer_base.py:443] tbase: final FFN
I0123 16:32:33.544004 139719371399168 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:32:33.546268 139719371399168 nn_components.py:329] mlp: final activation = None
I0123 16:32:33.548192 139719371399168 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.548285 139719371399168 nn_components.py:261] mlp: residual
I0123 16:32:33.548570 139719371399168 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.548651 139719371399168 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 16:32:33.548761 139719371399168 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:32:33.548799 139719371399168 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:32:33.548829 139719371399168 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:32:33.550679 139719371399168 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.553034 139719371399168 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:32:33.558613 139719371399168 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.558874 139719371399168 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:32:33.561506 139719371399168 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:32:33.565270 139719371399168 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:32:33.565326 139719371399168 attention.py:418] Single window, no scan.
I0123 16:32:33.565362 139719371399168 transformer_layer.py:389] tlayer: self-attention.
I0123 16:32:33.565392 139719371399168 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.565453 139719371399168 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.566066 139719371399168 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.566146 139719371399168 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.566504 139719371399168 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.567270 139719371399168 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.569708 139719371399168 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.570326 139719371399168 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.570403 139719371399168 transformer_layer.py:468] tlayer: End windows.
I0123 16:32:33.570438 139719371399168 transformer_layer.py:472] tlayer: final FFN.
I0123 16:32:33.570495 139719371399168 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.570625 139719371399168 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:32:33.570950 139719371399168 nn_components.py:325] mlp: activation = None
I0123 16:32:33.570993 139719371399168 nn_components.py:329] mlp: final activation = None
I0123 16:32:33.572885 139719371399168 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.572977 139719371399168 nn_components.py:261] mlp: residual
I0123 16:32:33.575518 139719371399168 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.575598 139719371399168 transformer_base.py:443] tbase: final FFN
I0123 16:32:33.576028 139719371399168 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:32:33.578333 139719371399168 nn_components.py:329] mlp: final activation = None
I0123 16:32:33.580263 139719371399168 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.580363 139719371399168 nn_components.py:261] mlp: residual
I0123 16:32:33.580654 139719371399168 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.580734 139719371399168 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 16:32:33.580843 139719371399168 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:32:33.580883 139719371399168 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:32:33.580912 139719371399168 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:32:33.582742 139719371399168 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.585161 139719371399168 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:32:33.590719 139719371399168 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.590979 139719371399168 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:32:33.593600 139719371399168 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:32:33.597362 139719371399168 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:32:33.597417 139719371399168 attention.py:418] Single window, no scan.
I0123 16:32:33.597452 139719371399168 transformer_layer.py:389] tlayer: self-attention.
I0123 16:32:33.597483 139719371399168 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.597545 139719371399168 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.598110 139719371399168 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.598187 139719371399168 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.598539 139719371399168 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.599299 139719371399168 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.601714 139719371399168 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.602335 139719371399168 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.602412 139719371399168 transformer_layer.py:468] tlayer: End windows.
I0123 16:32:33.602447 139719371399168 transformer_layer.py:472] tlayer: final FFN.
I0123 16:32:33.602505 139719371399168 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.602630 139719371399168 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:32:33.602952 139719371399168 nn_components.py:325] mlp: activation = None
I0123 16:32:33.602997 139719371399168 nn_components.py:329] mlp: final activation = None
I0123 16:32:33.604965 139719371399168 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.605061 139719371399168 nn_components.py:261] mlp: residual
I0123 16:32:33.607536 139719371399168 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.607615 139719371399168 transformer_base.py:443] tbase: final FFN
I0123 16:32:33.608050 139719371399168 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:32:33.610678 139719371399168 nn_components.py:329] mlp: final activation = None
I0123 16:32:33.612581 139719371399168 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.612681 139719371399168 nn_components.py:261] mlp: residual
I0123 16:32:33.612975 139719371399168 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.613057 139719371399168 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 16:32:33.613165 139719371399168 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:32:33.613204 139719371399168 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:32:33.613234 139719371399168 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:32:33.751600 139719371399168 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.754784 139719371399168 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:32:33.760751 139719371399168 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.761068 139719371399168 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:32:33.763784 139719371399168 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:32:33.767806 139719371399168 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:32:33.767867 139719371399168 attention.py:418] Single window, no scan.
I0123 16:32:33.767906 139719371399168 transformer_layer.py:389] tlayer: self-attention.
I0123 16:32:33.767938 139719371399168 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.768006 139719371399168 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.768623 139719371399168 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.768700 139719371399168 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.769064 139719371399168 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.769857 139719371399168 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.772414 139719371399168 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.773050 139719371399168 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.773129 139719371399168 transformer_layer.py:468] tlayer: End windows.
I0123 16:32:33.773164 139719371399168 transformer_layer.py:472] tlayer: final FFN.
I0123 16:32:33.773223 139719371399168 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.773350 139719371399168 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:32:33.773699 139719371399168 nn_components.py:325] mlp: activation = None
I0123 16:32:33.773744 139719371399168 nn_components.py:329] mlp: final activation = None
I0123 16:32:33.775647 139719371399168 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.775742 139719371399168 nn_components.py:261] mlp: residual
I0123 16:32:33.778282 139719371399168 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.778364 139719371399168 transformer_base.py:443] tbase: final FFN
I0123 16:32:33.778807 139719371399168 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:32:33.781154 139719371399168 nn_components.py:329] mlp: final activation = None
I0123 16:32:33.783058 139719371399168 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.783165 139719371399168 nn_components.py:261] mlp: residual
I0123 16:32:33.783455 139719371399168 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.783539 139719371399168 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 16:32:33.783648 139719371399168 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:32:33.783687 139719371399168 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:32:33.783717 139719371399168 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:32:33.785696 139719371399168 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.788055 139719371399168 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:32:33.793729 139719371399168 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.793999 139719371399168 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:32:33.796673 139719371399168 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:32:33.800485 139719371399168 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:32:33.800542 139719371399168 attention.py:418] Single window, no scan.
I0123 16:32:33.800579 139719371399168 transformer_layer.py:389] tlayer: self-attention.
I0123 16:32:33.800610 139719371399168 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.800671 139719371399168 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.801244 139719371399168 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.801320 139719371399168 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.801679 139719371399168 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.802454 139719371399168 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.804970 139719371399168 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.805604 139719371399168 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.805690 139719371399168 transformer_layer.py:468] tlayer: End windows.
I0123 16:32:33.805726 139719371399168 transformer_layer.py:472] tlayer: final FFN.
I0123 16:32:33.805786 139719371399168 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.805913 139719371399168 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:32:33.806245 139719371399168 nn_components.py:325] mlp: activation = None
I0123 16:32:33.806290 139719371399168 nn_components.py:329] mlp: final activation = None
I0123 16:32:33.808179 139719371399168 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.808271 139719371399168 nn_components.py:261] mlp: residual
I0123 16:32:33.810795 139719371399168 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.810875 139719371399168 transformer_base.py:443] tbase: final FFN
I0123 16:32:33.811305 139719371399168 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:32:33.813592 139719371399168 nn_components.py:329] mlp: final activation = None
I0123 16:32:33.815560 139719371399168 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.815656 139719371399168 nn_components.py:261] mlp: residual
I0123 16:32:33.815946 139719371399168 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.816034 139719371399168 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 16:32:33.816145 139719371399168 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:32:33.816184 139719371399168 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:32:33.816215 139719371399168 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:32:33.818044 139719371399168 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.820646 139719371399168 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:32:33.826346 139719371399168 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.826613 139719371399168 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:32:33.829625 139719371399168 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:32:33.833400 139719371399168 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:32:33.833455 139719371399168 attention.py:418] Single window, no scan.
I0123 16:32:33.833492 139719371399168 transformer_layer.py:389] tlayer: self-attention.
I0123 16:32:33.833524 139719371399168 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.833592 139719371399168 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.834207 139719371399168 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.834284 139719371399168 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.834645 139719371399168 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.835418 139719371399168 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.837884 139719371399168 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.838509 139719371399168 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.838585 139719371399168 transformer_layer.py:468] tlayer: End windows.
I0123 16:32:33.838620 139719371399168 transformer_layer.py:472] tlayer: final FFN.
I0123 16:32:33.838678 139719371399168 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.838804 139719371399168 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:32:33.839131 139719371399168 nn_components.py:325] mlp: activation = None
I0123 16:32:33.839175 139719371399168 nn_components.py:329] mlp: final activation = None
I0123 16:32:33.841076 139719371399168 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.841169 139719371399168 nn_components.py:261] mlp: residual
I0123 16:32:33.843708 139719371399168 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.843788 139719371399168 transformer_base.py:443] tbase: final FFN
I0123 16:32:33.844223 139719371399168 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:32:33.846563 139719371399168 nn_components.py:329] mlp: final activation = None
I0123 16:32:33.848457 139719371399168 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.848551 139719371399168 nn_components.py:261] mlp: residual
I0123 16:32:33.848835 139719371399168 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.848924 139719371399168 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 16:32:33.849037 139719371399168 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:32:33.849076 139719371399168 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:32:33.849107 139719371399168 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:32:33.850951 139719371399168 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.853383 139719371399168 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:32:33.858958 139719371399168 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.859240 139719371399168 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:32:33.861972 139719371399168 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:32:33.865893 139719371399168 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:32:33.865952 139719371399168 attention.py:418] Single window, no scan.
I0123 16:32:33.865988 139719371399168 transformer_layer.py:389] tlayer: self-attention.
I0123 16:32:33.866021 139719371399168 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.866084 139719371399168 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.866671 139719371399168 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.866750 139719371399168 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.867123 139719371399168 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.867919 139719371399168 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.870489 139719371399168 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.871135 139719371399168 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.871214 139719371399168 transformer_layer.py:468] tlayer: End windows.
I0123 16:32:33.871250 139719371399168 transformer_layer.py:472] tlayer: final FFN.
I0123 16:32:33.871311 139719371399168 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.871441 139719371399168 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:32:33.871775 139719371399168 nn_components.py:325] mlp: activation = None
I0123 16:32:33.871821 139719371399168 nn_components.py:329] mlp: final activation = None
I0123 16:32:33.873796 139719371399168 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.873891 139719371399168 nn_components.py:261] mlp: residual
I0123 16:32:33.876712 139719371399168 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.876791 139719371399168 transformer_base.py:443] tbase: final FFN
I0123 16:32:33.877218 139719371399168 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:32:33.879591 139719371399168 nn_components.py:329] mlp: final activation = None
I0123 16:32:33.881483 139719371399168 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.881577 139719371399168 nn_components.py:261] mlp: residual
I0123 16:32:33.881875 139719371399168 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.881960 139719371399168 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 16:32:33.882081 139719371399168 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:32:33.882122 139719371399168 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:32:33.882154 139719371399168 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:32:33.884099 139719371399168 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.886582 139719371399168 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:32:33.892293 139719371399168 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.892559 139719371399168 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:32:33.895210 139719371399168 transformer_layer.py:213] tlayer: windowed attention.
I0123 16:32:33.899044 139719371399168 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:32:33.899101 139719371399168 attention.py:418] Single window, no scan.
I0123 16:32:33.899137 139719371399168 transformer_layer.py:389] tlayer: self-attention.
I0123 16:32:33.899170 139719371399168 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.899233 139719371399168 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.899810 139719371399168 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.899893 139719371399168 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.900249 139719371399168 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.900999 139719371399168 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.903492 139719371399168 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.904476 139719371399168 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.904554 139719371399168 transformer_layer.py:468] tlayer: End windows.
I0123 16:32:33.904590 139719371399168 transformer_layer.py:472] tlayer: final FFN.
I0123 16:32:33.904653 139719371399168 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.904794 139719371399168 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:32:33.905112 139719371399168 nn_components.py:325] mlp: activation = None
I0123 16:32:33.905156 139719371399168 nn_components.py:329] mlp: final activation = None
I0123 16:32:33.907080 139719371399168 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.907177 139719371399168 nn_components.py:261] mlp: residual
I0123 16:32:33.909648 139719371399168 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.909732 139719371399168 transformer_base.py:443] tbase: final FFN
I0123 16:32:33.910220 139719371399168 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:32:33.912455 139719371399168 nn_components.py:329] mlp: final activation = None
I0123 16:32:33.914348 139719371399168 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.914446 139719371399168 nn_components.py:261] mlp: residual
I0123 16:32:33.914738 139719371399168 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.915024 139719371399168 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 16:32:33.915103 139719371399168 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 16:32:33.915182 139719371399168 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 16:32:33.915242 139719371399168 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 16:32:33.915299 139719371399168 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 16:32:33.915354 139719371399168 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 16:32:33.915409 139719371399168 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 16:32:33.915462 139719371399168 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 16:32:33.915515 139719371399168 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 16:32:33.915568 139719371399168 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 16:32:33.915622 139719371399168 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 16:32:33.915675 139719371399168 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 16:32:33.915713 139719371399168 decoder_stack.py:344] dstack: Final layernorm.
I0123 16:32:33.919326 139719371399168 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 16:32:33.967429 139719371399168 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:33.967514 139719371399168 decoder_stack.py:333] dstack: autoregressive generator.
I0123 16:32:33.967568 139719371399168 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 16:32:33.967671 139719371399168 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:32:33.967710 139719371399168 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:32:33.967741 139719371399168 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:32:33.967804 139719371399168 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:33.970222 139719371399168 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:32:33.975670 139719371399168 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:33.975929 139719371399168 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:32:33.978507 139719371399168 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:32:33.995048 139719371399168 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:32:33.995105 139719371399168 attention.py:418] Single window, no scan.
I0123 16:32:33.995143 139719371399168 transformer_layer.py:389] tlayer: self-attention.
I0123 16:32:33.995174 139719371399168 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:33.995236 139719371399168 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:33.996358 139719371399168 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:33.996437 139719371399168 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:33.997145 139719371399168 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:33.999129 139719371399168 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.003823 139719371399168 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.005119 139719371399168 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.005207 139719371399168 transformer_layer.py:468] tlayer: End windows.
I0123 16:32:34.005243 139719371399168 transformer_layer.py:472] tlayer: final FFN.
I0123 16:32:34.005302 139719371399168 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.005434 139719371399168 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:32:34.005544 139719371399168 nn_components.py:325] mlp: activation = None
I0123 16:32:34.005583 139719371399168 nn_components.py:329] mlp: final activation = None
I0123 16:32:34.007492 139719371399168 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.007588 139719371399168 nn_components.py:261] mlp: residual
I0123 16:32:34.009979 139719371399168 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.010059 139719371399168 transformer_base.py:443] tbase: final FFN
I0123 16:32:34.010173 139719371399168 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:32:34.012365 139719371399168 nn_components.py:329] mlp: final activation = None
I0123 16:32:34.014302 139719371399168 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.014397 139719371399168 nn_components.py:261] mlp: residual
I0123 16:32:34.014682 139719371399168 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.014762 139719371399168 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 16:32:34.014868 139719371399168 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:32:34.014908 139719371399168 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:32:34.014938 139719371399168 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:32:34.015004 139719371399168 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.017220 139719371399168 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:32:34.022621 139719371399168 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.022878 139719371399168 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:32:34.025799 139719371399168 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:32:34.038792 139719371399168 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:32:34.038849 139719371399168 attention.py:418] Single window, no scan.
I0123 16:32:34.038885 139719371399168 transformer_layer.py:389] tlayer: self-attention.
I0123 16:32:34.038917 139719371399168 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.038978 139719371399168 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.039534 139719371399168 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.039610 139719371399168 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.039965 139719371399168 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.040653 139719371399168 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.043109 139719371399168 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.043726 139719371399168 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.043802 139719371399168 transformer_layer.py:468] tlayer: End windows.
I0123 16:32:34.043842 139719371399168 transformer_layer.py:472] tlayer: final FFN.
I0123 16:32:34.043901 139719371399168 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.044032 139719371399168 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:32:34.044139 139719371399168 nn_components.py:325] mlp: activation = None
I0123 16:32:34.044177 139719371399168 nn_components.py:329] mlp: final activation = None
I0123 16:32:34.046081 139719371399168 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.046175 139719371399168 nn_components.py:261] mlp: residual
I0123 16:32:34.048522 139719371399168 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.048601 139719371399168 transformer_base.py:443] tbase: final FFN
I0123 16:32:34.048709 139719371399168 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:32:34.050894 139719371399168 nn_components.py:329] mlp: final activation = None
I0123 16:32:34.052791 139719371399168 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.052886 139719371399168 nn_components.py:261] mlp: residual
I0123 16:32:34.053169 139719371399168 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.053248 139719371399168 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 16:32:34.053355 139719371399168 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:32:34.053394 139719371399168 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:32:34.053425 139719371399168 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:32:34.053486 139719371399168 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.055691 139719371399168 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:32:34.061039 139719371399168 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.061295 139719371399168 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:32:34.063918 139719371399168 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:32:34.076472 139719371399168 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:32:34.076529 139719371399168 attention.py:418] Single window, no scan.
I0123 16:32:34.076565 139719371399168 transformer_layer.py:389] tlayer: self-attention.
I0123 16:32:34.076597 139719371399168 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.076658 139719371399168 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.077206 139719371399168 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.077281 139719371399168 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.077630 139719371399168 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.078315 139719371399168 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.080751 139719371399168 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.081366 139719371399168 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.081442 139719371399168 transformer_layer.py:468] tlayer: End windows.
I0123 16:32:34.081476 139719371399168 transformer_layer.py:472] tlayer: final FFN.
I0123 16:32:34.081537 139719371399168 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.081671 139719371399168 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:32:34.081781 139719371399168 nn_components.py:325] mlp: activation = None
I0123 16:32:34.081819 139719371399168 nn_components.py:329] mlp: final activation = None
I0123 16:32:34.083716 139719371399168 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.083809 139719371399168 nn_components.py:261] mlp: residual
I0123 16:32:34.086192 139719371399168 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.086276 139719371399168 transformer_base.py:443] tbase: final FFN
I0123 16:32:34.086385 139719371399168 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:32:34.088588 139719371399168 nn_components.py:329] mlp: final activation = None
I0123 16:32:34.090493 139719371399168 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.090589 139719371399168 nn_components.py:261] mlp: residual
I0123 16:32:34.090871 139719371399168 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.090952 139719371399168 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 16:32:34.091062 139719371399168 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:32:34.091101 139719371399168 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:32:34.091132 139719371399168 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:32:34.091195 139719371399168 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.093390 139719371399168 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:32:34.098773 139719371399168 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.099031 139719371399168 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:32:34.101671 139719371399168 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:32:34.114318 139719371399168 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:32:34.114372 139719371399168 attention.py:418] Single window, no scan.
I0123 16:32:34.114412 139719371399168 transformer_layer.py:389] tlayer: self-attention.
I0123 16:32:34.114443 139719371399168 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.114505 139719371399168 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.115055 139719371399168 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.115131 139719371399168 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.115479 139719371399168 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.116162 139719371399168 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.118619 139719371399168 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.119233 139719371399168 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.119310 139719371399168 transformer_layer.py:468] tlayer: End windows.
I0123 16:32:34.119346 139719371399168 transformer_layer.py:472] tlayer: final FFN.
I0123 16:32:34.119404 139719371399168 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.119542 139719371399168 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:32:34.119658 139719371399168 nn_components.py:325] mlp: activation = None
I0123 16:32:34.119698 139719371399168 nn_components.py:329] mlp: final activation = None
I0123 16:32:34.121616 139719371399168 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.121716 139719371399168 nn_components.py:261] mlp: residual
I0123 16:32:34.124083 139719371399168 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.124335 139719371399168 transformer_base.py:443] tbase: final FFN
I0123 16:32:34.124441 139719371399168 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:32:34.126777 139719371399168 nn_components.py:329] mlp: final activation = None
I0123 16:32:34.128617 139719371399168 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.128710 139719371399168 nn_components.py:261] mlp: residual
I0123 16:32:34.128989 139719371399168 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.129069 139719371399168 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 16:32:34.129178 139719371399168 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:32:34.129217 139719371399168 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:32:34.129247 139719371399168 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:32:34.129310 139719371399168 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.131860 139719371399168 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:32:34.137241 139719371399168 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.137505 139719371399168 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:32:34.140096 139719371399168 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:32:34.152727 139719371399168 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:32:34.152782 139719371399168 attention.py:418] Single window, no scan.
I0123 16:32:34.152818 139719371399168 transformer_layer.py:389] tlayer: self-attention.
I0123 16:32:34.152849 139719371399168 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.152910 139719371399168 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.153467 139719371399168 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.153542 139719371399168 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.153907 139719371399168 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.154590 139719371399168 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.157104 139719371399168 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.157735 139719371399168 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.157817 139719371399168 transformer_layer.py:468] tlayer: End windows.
I0123 16:32:34.157853 139719371399168 transformer_layer.py:472] tlayer: final FFN.
I0123 16:32:34.157912 139719371399168 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.158050 139719371399168 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:32:34.158160 139719371399168 nn_components.py:325] mlp: activation = None
I0123 16:32:34.158199 139719371399168 nn_components.py:329] mlp: final activation = None
I0123 16:32:34.160044 139719371399168 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.160136 139719371399168 nn_components.py:261] mlp: residual
I0123 16:32:34.162508 139719371399168 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.162589 139719371399168 transformer_base.py:443] tbase: final FFN
I0123 16:32:34.162695 139719371399168 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:32:34.164934 139719371399168 nn_components.py:329] mlp: final activation = None
I0123 16:32:34.166767 139719371399168 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.166863 139719371399168 nn_components.py:261] mlp: residual
I0123 16:32:34.167145 139719371399168 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.167227 139719371399168 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 16:32:34.167336 139719371399168 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:32:34.167375 139719371399168 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:32:34.167406 139719371399168 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:32:34.167471 139719371399168 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.169685 139719371399168 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:32:34.175084 139719371399168 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.175343 139719371399168 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:32:34.177981 139719371399168 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:32:34.195870 139719371399168 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:32:34.195953 139719371399168 attention.py:418] Single window, no scan.
I0123 16:32:34.195992 139719371399168 transformer_layer.py:389] tlayer: self-attention.
I0123 16:32:34.196024 139719371399168 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.196099 139719371399168 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.196725 139719371399168 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.196801 139719371399168 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.197173 139719371399168 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.197886 139719371399168 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.200400 139719371399168 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.201025 139719371399168 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.201102 139719371399168 transformer_layer.py:468] tlayer: End windows.
I0123 16:32:34.201137 139719371399168 transformer_layer.py:472] tlayer: final FFN.
I0123 16:32:34.201200 139719371399168 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.201331 139719371399168 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:32:34.201449 139719371399168 nn_components.py:325] mlp: activation = None
I0123 16:32:34.201490 139719371399168 nn_components.py:329] mlp: final activation = None
I0123 16:32:34.203516 139719371399168 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.203611 139719371399168 nn_components.py:261] mlp: residual
I0123 16:32:34.206042 139719371399168 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.206121 139719371399168 transformer_base.py:443] tbase: final FFN
I0123 16:32:34.206235 139719371399168 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:32:34.208479 139719371399168 nn_components.py:329] mlp: final activation = None
I0123 16:32:34.210332 139719371399168 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.210427 139719371399168 nn_components.py:261] mlp: residual
I0123 16:32:34.210710 139719371399168 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.210793 139719371399168 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 16:32:34.210907 139719371399168 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:32:34.210950 139719371399168 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:32:34.210981 139719371399168 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:32:34.211050 139719371399168 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.213266 139719371399168 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:32:34.218753 139719371399168 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.219010 139719371399168 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:32:34.221609 139719371399168 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:32:34.234484 139719371399168 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:32:34.234539 139719371399168 attention.py:418] Single window, no scan.
I0123 16:32:34.234574 139719371399168 transformer_layer.py:389] tlayer: self-attention.
I0123 16:32:34.234605 139719371399168 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.234666 139719371399168 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.235224 139719371399168 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.235300 139719371399168 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.235649 139719371399168 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.236326 139719371399168 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.238746 139719371399168 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.239732 139719371399168 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.239811 139719371399168 transformer_layer.py:468] tlayer: End windows.
I0123 16:32:34.239846 139719371399168 transformer_layer.py:472] tlayer: final FFN.
I0123 16:32:34.239904 139719371399168 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.240032 139719371399168 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:32:34.240139 139719371399168 nn_components.py:325] mlp: activation = None
I0123 16:32:34.240181 139719371399168 nn_components.py:329] mlp: final activation = None
I0123 16:32:34.242073 139719371399168 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.242167 139719371399168 nn_components.py:261] mlp: residual
I0123 16:32:34.244539 139719371399168 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.244621 139719371399168 transformer_base.py:443] tbase: final FFN
I0123 16:32:34.244729 139719371399168 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:32:34.246921 139719371399168 nn_components.py:329] mlp: final activation = None
I0123 16:32:34.248835 139719371399168 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.248930 139719371399168 nn_components.py:261] mlp: residual
I0123 16:32:34.249212 139719371399168 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.249294 139719371399168 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 16:32:34.249403 139719371399168 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:32:34.249443 139719371399168 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:32:34.249474 139719371399168 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:32:34.249537 139719371399168 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.251735 139719371399168 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:32:34.257081 139719371399168 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.257360 139719371399168 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:32:34.260010 139719371399168 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:32:34.272558 139719371399168 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:32:34.272614 139719371399168 attention.py:418] Single window, no scan.
I0123 16:32:34.272650 139719371399168 transformer_layer.py:389] tlayer: self-attention.
I0123 16:32:34.272681 139719371399168 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.272746 139719371399168 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.273348 139719371399168 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.273425 139719371399168 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.273786 139719371399168 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.274466 139719371399168 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.276887 139719371399168 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.277507 139719371399168 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.277585 139719371399168 transformer_layer.py:468] tlayer: End windows.
I0123 16:32:34.277620 139719371399168 transformer_layer.py:472] tlayer: final FFN.
I0123 16:32:34.277686 139719371399168 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.277821 139719371399168 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:32:34.277931 139719371399168 nn_components.py:325] mlp: activation = None
I0123 16:32:34.277975 139719371399168 nn_components.py:329] mlp: final activation = None
I0123 16:32:34.279843 139719371399168 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.279937 139719371399168 nn_components.py:261] mlp: residual
I0123 16:32:34.282374 139719371399168 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.282454 139719371399168 transformer_base.py:443] tbase: final FFN
I0123 16:32:34.282562 139719371399168 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:32:34.284744 139719371399168 nn_components.py:329] mlp: final activation = None
I0123 16:32:34.286593 139719371399168 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.286689 139719371399168 nn_components.py:261] mlp: residual
I0123 16:32:34.286969 139719371399168 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.287050 139719371399168 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 16:32:34.287159 139719371399168 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:32:34.287198 139719371399168 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:32:34.287228 139719371399168 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:32:34.287292 139719371399168 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.289480 139719371399168 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:32:34.294924 139719371399168 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.295180 139719371399168 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:32:34.297748 139719371399168 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:32:34.310340 139719371399168 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:32:34.310396 139719371399168 attention.py:418] Single window, no scan.
I0123 16:32:34.310432 139719371399168 transformer_layer.py:389] tlayer: self-attention.
I0123 16:32:34.310464 139719371399168 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.310529 139719371399168 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.311091 139719371399168 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.311167 139719371399168 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.311519 139719371399168 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.312199 139719371399168 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.314638 139719371399168 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.315307 139719371399168 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.315385 139719371399168 transformer_layer.py:468] tlayer: End windows.
I0123 16:32:34.315420 139719371399168 transformer_layer.py:472] tlayer: final FFN.
I0123 16:32:34.315483 139719371399168 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.315610 139719371399168 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:32:34.315718 139719371399168 nn_components.py:325] mlp: activation = None
I0123 16:32:34.315757 139719371399168 nn_components.py:329] mlp: final activation = None
I0123 16:32:34.317608 139719371399168 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.317707 139719371399168 nn_components.py:261] mlp: residual
I0123 16:32:34.320073 139719371399168 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.320153 139719371399168 transformer_base.py:443] tbase: final FFN
I0123 16:32:34.320261 139719371399168 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:32:34.322461 139719371399168 nn_components.py:329] mlp: final activation = None
I0123 16:32:34.324366 139719371399168 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.324460 139719371399168 nn_components.py:261] mlp: residual
I0123 16:32:34.324741 139719371399168 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.324823 139719371399168 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 16:32:34.324931 139719371399168 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:32:34.324969 139719371399168 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:32:34.325000 139719371399168 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:32:34.325063 139719371399168 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.327244 139719371399168 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:32:34.332571 139719371399168 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.332827 139719371399168 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:32:34.335545 139719371399168 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:32:34.348353 139719371399168 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:32:34.348408 139719371399168 attention.py:418] Single window, no scan.
I0123 16:32:34.348444 139719371399168 transformer_layer.py:389] tlayer: self-attention.
I0123 16:32:34.348475 139719371399168 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.348540 139719371399168 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.349140 139719371399168 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.349218 139719371399168 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.349562 139719371399168 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.350257 139719371399168 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.352681 139719371399168 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.353303 139719371399168 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.353381 139719371399168 transformer_layer.py:468] tlayer: End windows.
I0123 16:32:34.353415 139719371399168 transformer_layer.py:472] tlayer: final FFN.
I0123 16:32:34.353471 139719371399168 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.353595 139719371399168 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:32:34.353709 139719371399168 nn_components.py:325] mlp: activation = None
I0123 16:32:34.353749 139719371399168 nn_components.py:329] mlp: final activation = None
I0123 16:32:34.355623 139719371399168 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.355721 139719371399168 nn_components.py:261] mlp: residual
I0123 16:32:34.358150 139719371399168 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.358230 139719371399168 transformer_base.py:443] tbase: final FFN
I0123 16:32:34.358337 139719371399168 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:32:34.360521 139719371399168 nn_components.py:329] mlp: final activation = None
I0123 16:32:34.362357 139719371399168 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.362452 139719371399168 nn_components.py:261] mlp: residual
I0123 16:32:34.362734 139719371399168 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.362814 139719371399168 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 16:32:34.362923 139719371399168 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:32:34.362962 139719371399168 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:32:34.362993 139719371399168 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:32:34.363054 139719371399168 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.365229 139719371399168 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:32:34.370631 139719371399168 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.370887 139719371399168 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:32:34.373464 139719371399168 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:32:34.386052 139719371399168 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:32:34.386107 139719371399168 attention.py:418] Single window, no scan.
I0123 16:32:34.386143 139719371399168 transformer_layer.py:389] tlayer: self-attention.
I0123 16:32:34.386173 139719371399168 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.386236 139719371399168 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.386786 139719371399168 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.386863 139719371399168 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.387214 139719371399168 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.387892 139719371399168 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.390337 139719371399168 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.391007 139719371399168 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.391085 139719371399168 transformer_layer.py:468] tlayer: End windows.
I0123 16:32:34.391121 139719371399168 transformer_layer.py:472] tlayer: final FFN.
I0123 16:32:34.391177 139719371399168 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.391305 139719371399168 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:32:34.391416 139719371399168 nn_components.py:325] mlp: activation = None
I0123 16:32:34.391455 139719371399168 nn_components.py:329] mlp: final activation = None
I0123 16:32:34.393311 139719371399168 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.393409 139719371399168 nn_components.py:261] mlp: residual
I0123 16:32:34.395790 139719371399168 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.395870 139719371399168 transformer_base.py:443] tbase: final FFN
I0123 16:32:34.395975 139719371399168 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:32:34.398158 139719371399168 nn_components.py:329] mlp: final activation = None
I0123 16:32:34.400052 139719371399168 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.400146 139719371399168 nn_components.py:261] mlp: residual
I0123 16:32:34.400426 139719371399168 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.400507 139719371399168 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 16:32:34.400616 139719371399168 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:32:34.400656 139719371399168 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:32:34.400687 139719371399168 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:32:34.400750 139719371399168 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.402938 139719371399168 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:32:34.408278 139719371399168 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.408534 139719371399168 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:32:34.411182 139719371399168 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:32:34.423649 139719371399168 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:32:34.423705 139719371399168 attention.py:418] Single window, no scan.
I0123 16:32:34.423742 139719371399168 transformer_layer.py:389] tlayer: self-attention.
I0123 16:32:34.423773 139719371399168 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.423836 139719371399168 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.424388 139719371399168 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.424464 139719371399168 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.424808 139719371399168 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.425554 139719371399168 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.427998 139719371399168 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.428621 139719371399168 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.428699 139719371399168 transformer_layer.py:468] tlayer: End windows.
I0123 16:32:34.428734 139719371399168 transformer_layer.py:472] tlayer: final FFN.
I0123 16:32:34.428791 139719371399168 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.428916 139719371399168 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:32:34.429025 139719371399168 nn_components.py:325] mlp: activation = None
I0123 16:32:34.429064 139719371399168 nn_components.py:329] mlp: final activation = None
I0123 16:32:34.430912 139719371399168 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.431003 139719371399168 nn_components.py:261] mlp: residual
I0123 16:32:34.433369 139719371399168 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.433448 139719371399168 transformer_base.py:443] tbase: final FFN
I0123 16:32:34.433554 139719371399168 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:32:34.435788 139719371399168 nn_components.py:329] mlp: final activation = None
I0123 16:32:34.437614 139719371399168 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.437716 139719371399168 nn_components.py:261] mlp: residual
I0123 16:32:34.437999 139719371399168 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.438088 139719371399168 decoder_stack.py:344] dstack: Final layernorm.
I0123 16:32:34.440914 139719371399168 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 16:32:34.496562 139719371399168 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.496645 139719371399168 decoder_stack.py:333] dstack: autoregressive generator.
I0123 16:32:34.496699 139719371399168 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 16:32:34.496800 139719371399168 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:32:34.496838 139719371399168 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:32:34.496867 139719371399168 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:32:34.496930 139719371399168 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.499533 139719371399168 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:32:34.504783 139719371399168 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.505039 139719371399168 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:32:34.507537 139719371399168 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:32:34.519669 139719371399168 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:32:34.519726 139719371399168 attention.py:418] Single window, no scan.
I0123 16:32:34.519762 139719371399168 transformer_layer.py:389] tlayer: self-attention.
I0123 16:32:34.519793 139719371399168 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.519854 139719371399168 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.520410 139719371399168 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.520485 139719371399168 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.520832 139719371399168 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.521491 139719371399168 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.523949 139719371399168 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.524554 139719371399168 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.524631 139719371399168 transformer_layer.py:468] tlayer: End windows.
I0123 16:32:34.524666 139719371399168 transformer_layer.py:472] tlayer: final FFN.
I0123 16:32:34.524724 139719371399168 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.524849 139719371399168 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:32:34.524964 139719371399168 nn_components.py:325] mlp: activation = None
I0123 16:32:34.525004 139719371399168 nn_components.py:329] mlp: final activation = None
I0123 16:32:34.526818 139719371399168 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.526912 139719371399168 nn_components.py:261] mlp: residual
I0123 16:32:34.529221 139719371399168 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.529300 139719371399168 transformer_base.py:443] tbase: final FFN
I0123 16:32:34.529406 139719371399168 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:32:34.531602 139719371399168 nn_components.py:329] mlp: final activation = None
I0123 16:32:34.533407 139719371399168 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.533501 139719371399168 nn_components.py:261] mlp: residual
I0123 16:32:34.533787 139719371399168 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.533868 139719371399168 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 16:32:34.533973 139719371399168 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:32:34.534013 139719371399168 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:32:34.534042 139719371399168 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:32:34.534105 139719371399168 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.536271 139719371399168 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:32:34.541523 139719371399168 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.541781 139719371399168 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:32:34.544355 139719371399168 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:32:34.556597 139719371399168 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:32:34.556653 139719371399168 attention.py:418] Single window, no scan.
I0123 16:32:34.556689 139719371399168 transformer_layer.py:389] tlayer: self-attention.
I0123 16:32:34.556721 139719371399168 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.556783 139719371399168 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.557327 139719371399168 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.557403 139719371399168 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.557751 139719371399168 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.558413 139719371399168 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.560842 139719371399168 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.561453 139719371399168 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.561530 139719371399168 transformer_layer.py:468] tlayer: End windows.
I0123 16:32:34.561565 139719371399168 transformer_layer.py:472] tlayer: final FFN.
I0123 16:32:34.561624 139719371399168 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.561760 139719371399168 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:32:34.561869 139719371399168 nn_components.py:325] mlp: activation = None
I0123 16:32:34.561914 139719371399168 nn_components.py:329] mlp: final activation = None
I0123 16:32:34.563717 139719371399168 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.563811 139719371399168 nn_components.py:261] mlp: residual
I0123 16:32:34.566150 139719371399168 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.566229 139719371399168 transformer_base.py:443] tbase: final FFN
I0123 16:32:34.566335 139719371399168 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:32:34.568535 139719371399168 nn_components.py:329] mlp: final activation = None
I0123 16:32:34.570344 139719371399168 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.570439 139719371399168 nn_components.py:261] mlp: residual
I0123 16:32:34.570719 139719371399168 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.570800 139719371399168 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 16:32:34.570907 139719371399168 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:32:34.570946 139719371399168 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:32:34.570976 139719371399168 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:32:34.571039 139719371399168 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.573201 139719371399168 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:32:34.578478 139719371399168 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.578731 139719371399168 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:32:34.581317 139719371399168 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:32:34.593520 139719371399168 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:32:34.593575 139719371399168 attention.py:418] Single window, no scan.
I0123 16:32:34.593609 139719371399168 transformer_layer.py:389] tlayer: self-attention.
I0123 16:32:34.593645 139719371399168 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.593708 139719371399168 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.594251 139719371399168 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.594327 139719371399168 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.594672 139719371399168 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.595331 139719371399168 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.597777 139719371399168 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.598377 139719371399168 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.598455 139719371399168 transformer_layer.py:468] tlayer: End windows.
I0123 16:32:34.598489 139719371399168 transformer_layer.py:472] tlayer: final FFN.
I0123 16:32:34.598548 139719371399168 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.598674 139719371399168 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:32:34.598783 139719371399168 nn_components.py:325] mlp: activation = None
I0123 16:32:34.598822 139719371399168 nn_components.py:329] mlp: final activation = None
I0123 16:32:34.600626 139719371399168 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.600718 139719371399168 nn_components.py:261] mlp: residual
I0123 16:32:34.603049 139719371399168 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.603129 139719371399168 transformer_base.py:443] tbase: final FFN
I0123 16:32:34.603236 139719371399168 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:32:34.605879 139719371399168 nn_components.py:329] mlp: final activation = None
I0123 16:32:34.607706 139719371399168 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.607800 139719371399168 nn_components.py:261] mlp: residual
I0123 16:32:34.608083 139719371399168 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.608163 139719371399168 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 16:32:34.608268 139719371399168 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:32:34.608306 139719371399168 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:32:34.608336 139719371399168 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:32:34.608399 139719371399168 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.610594 139719371399168 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:32:34.615911 139719371399168 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.616165 139719371399168 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:32:34.618728 139719371399168 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:32:34.630948 139719371399168 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:32:34.631005 139719371399168 attention.py:418] Single window, no scan.
I0123 16:32:34.631043 139719371399168 transformer_layer.py:389] tlayer: self-attention.
I0123 16:32:34.631087 139719371399168 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.631150 139719371399168 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.631697 139719371399168 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.631771 139719371399168 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.632120 139719371399168 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.632781 139719371399168 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.635265 139719371399168 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.635881 139719371399168 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.635956 139719371399168 transformer_layer.py:468] tlayer: End windows.
I0123 16:32:34.635991 139719371399168 transformer_layer.py:472] tlayer: final FFN.
I0123 16:32:34.636047 139719371399168 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.636172 139719371399168 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:32:34.636277 139719371399168 nn_components.py:325] mlp: activation = None
I0123 16:32:34.636320 139719371399168 nn_components.py:329] mlp: final activation = None
I0123 16:32:34.638135 139719371399168 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.638225 139719371399168 nn_components.py:261] mlp: residual
I0123 16:32:34.640564 139719371399168 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.640642 139719371399168 transformer_base.py:443] tbase: final FFN
I0123 16:32:34.640747 139719371399168 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:32:34.642982 139719371399168 nn_components.py:329] mlp: final activation = None
I0123 16:32:34.644793 139719371399168 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.644885 139719371399168 nn_components.py:261] mlp: residual
I0123 16:32:34.645159 139719371399168 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.645236 139719371399168 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 16:32:34.645341 139719371399168 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:32:34.645378 139719371399168 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:32:34.645406 139719371399168 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:32:34.645468 139719371399168 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.647651 139719371399168 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:32:34.652951 139719371399168 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.653203 139719371399168 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:32:34.655803 139719371399168 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:32:34.668134 139719371399168 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:32:34.668187 139719371399168 attention.py:418] Single window, no scan.
I0123 16:32:34.668221 139719371399168 transformer_layer.py:389] tlayer: self-attention.
I0123 16:32:34.668250 139719371399168 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.668309 139719371399168 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.668853 139719371399168 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.668927 139719371399168 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.669267 139719371399168 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.669946 139719371399168 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.672408 139719371399168 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.673018 139719371399168 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.673095 139719371399168 transformer_layer.py:468] tlayer: End windows.
I0123 16:32:34.673129 139719371399168 transformer_layer.py:472] tlayer: final FFN.
I0123 16:32:34.673186 139719371399168 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.673310 139719371399168 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:32:34.673417 139719371399168 nn_components.py:325] mlp: activation = None
I0123 16:32:34.673455 139719371399168 nn_components.py:329] mlp: final activation = None
I0123 16:32:34.675295 139719371399168 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.675394 139719371399168 nn_components.py:261] mlp: residual
I0123 16:32:34.677722 139719371399168 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.677799 139719371399168 transformer_base.py:443] tbase: final FFN
I0123 16:32:34.677906 139719371399168 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:32:34.680142 139719371399168 nn_components.py:329] mlp: final activation = None
I0123 16:32:34.681959 139719371399168 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.682056 139719371399168 nn_components.py:261] mlp: residual
I0123 16:32:34.682332 139719371399168 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.682411 139719371399168 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 16:32:34.682515 139719371399168 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:32:34.682553 139719371399168 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:32:34.682582 139719371399168 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:32:34.682644 139719371399168 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.684814 139719371399168 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:32:34.690076 139719371399168 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.690329 139719371399168 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:32:34.692929 139719371399168 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:32:34.705259 139719371399168 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:32:34.705314 139719371399168 attention.py:418] Single window, no scan.
I0123 16:32:34.705348 139719371399168 transformer_layer.py:389] tlayer: self-attention.
I0123 16:32:34.705378 139719371399168 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.705438 139719371399168 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.705984 139719371399168 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.706060 139719371399168 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.706405 139719371399168 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.707068 139719371399168 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.709542 139719371399168 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.710153 139719371399168 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.710229 139719371399168 transformer_layer.py:468] tlayer: End windows.
I0123 16:32:34.710263 139719371399168 transformer_layer.py:472] tlayer: final FFN.
I0123 16:32:34.710319 139719371399168 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.710443 139719371399168 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:32:34.710553 139719371399168 nn_components.py:325] mlp: activation = None
I0123 16:32:34.710591 139719371399168 nn_components.py:329] mlp: final activation = None
I0123 16:32:34.712407 139719371399168 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.712503 139719371399168 nn_components.py:261] mlp: residual
I0123 16:32:34.714859 139719371399168 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.714941 139719371399168 transformer_base.py:443] tbase: final FFN
I0123 16:32:34.715049 139719371399168 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:32:34.717696 139719371399168 nn_components.py:329] mlp: final activation = None
I0123 16:32:34.719525 139719371399168 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.719617 139719371399168 nn_components.py:261] mlp: residual
I0123 16:32:34.719895 139719371399168 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.719974 139719371399168 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 16:32:34.720078 139719371399168 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:32:34.720115 139719371399168 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:32:34.720144 139719371399168 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:32:34.720205 139719371399168 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.722370 139719371399168 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:32:34.727651 139719371399168 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.727904 139719371399168 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:32:34.730520 139719371399168 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:32:34.742902 139719371399168 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:32:34.742957 139719371399168 attention.py:418] Single window, no scan.
I0123 16:32:34.742991 139719371399168 transformer_layer.py:389] tlayer: self-attention.
I0123 16:32:34.743020 139719371399168 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.743079 139719371399168 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.743628 139719371399168 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.743702 139719371399168 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.744048 139719371399168 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.744716 139719371399168 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.747174 139719371399168 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.747790 139719371399168 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.747865 139719371399168 transformer_layer.py:468] tlayer: End windows.
I0123 16:32:34.747899 139719371399168 transformer_layer.py:472] tlayer: final FFN.
I0123 16:32:34.747955 139719371399168 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.748082 139719371399168 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:32:34.748188 139719371399168 nn_components.py:325] mlp: activation = None
I0123 16:32:34.748226 139719371399168 nn_components.py:329] mlp: final activation = None
I0123 16:32:34.750052 139719371399168 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.750143 139719371399168 nn_components.py:261] mlp: residual
I0123 16:32:34.752489 139719371399168 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.752566 139719371399168 transformer_base.py:443] tbase: final FFN
I0123 16:32:34.752671 139719371399168 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:32:34.754911 139719371399168 nn_components.py:329] mlp: final activation = None
I0123 16:32:34.756736 139719371399168 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.756829 139719371399168 nn_components.py:261] mlp: residual
I0123 16:32:34.757104 139719371399168 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.757182 139719371399168 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 16:32:34.757287 139719371399168 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:32:34.757324 139719371399168 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:32:34.757353 139719371399168 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:32:34.757413 139719371399168 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.759601 139719371399168 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:32:34.764897 139719371399168 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.765150 139719371399168 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:32:34.767779 139719371399168 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:32:34.780189 139719371399168 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:32:34.780248 139719371399168 attention.py:418] Single window, no scan.
I0123 16:32:34.780282 139719371399168 transformer_layer.py:389] tlayer: self-attention.
I0123 16:32:34.780311 139719371399168 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.780372 139719371399168 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.780922 139719371399168 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.780996 139719371399168 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.781345 139719371399168 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.782031 139719371399168 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.784484 139719371399168 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.785100 139719371399168 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.785176 139719371399168 transformer_layer.py:468] tlayer: End windows.
I0123 16:32:34.785209 139719371399168 transformer_layer.py:472] tlayer: final FFN.
I0123 16:32:34.785267 139719371399168 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.785390 139719371399168 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:32:34.785495 139719371399168 nn_components.py:325] mlp: activation = None
I0123 16:32:34.785534 139719371399168 nn_components.py:329] mlp: final activation = None
I0123 16:32:34.787357 139719371399168 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.787448 139719371399168 nn_components.py:261] mlp: residual
I0123 16:32:34.789787 139719371399168 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.789871 139719371399168 transformer_base.py:443] tbase: final FFN
I0123 16:32:34.789978 139719371399168 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:32:34.792201 139719371399168 nn_components.py:329] mlp: final activation = None
I0123 16:32:34.794017 139719371399168 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.794110 139719371399168 nn_components.py:261] mlp: residual
I0123 16:32:34.794386 139719371399168 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.794465 139719371399168 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 16:32:34.794569 139719371399168 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:32:34.794607 139719371399168 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:32:34.794636 139719371399168 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:32:34.794698 139719371399168 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.796853 139719371399168 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:32:34.802116 139719371399168 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.802371 139719371399168 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:32:34.804986 139719371399168 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:32:34.817306 139719371399168 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:32:34.817360 139719371399168 attention.py:418] Single window, no scan.
I0123 16:32:34.817394 139719371399168 transformer_layer.py:389] tlayer: self-attention.
I0123 16:32:34.817423 139719371399168 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.817483 139719371399168 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.818037 139719371399168 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.818111 139719371399168 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.818457 139719371399168 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.819123 139719371399168 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.821593 139719371399168 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.822202 139719371399168 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.822278 139719371399168 transformer_layer.py:468] tlayer: End windows.
I0123 16:32:34.822311 139719371399168 transformer_layer.py:472] tlayer: final FFN.
I0123 16:32:34.822367 139719371399168 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.822491 139719371399168 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:32:34.822596 139719371399168 nn_components.py:325] mlp: activation = None
I0123 16:32:34.822633 139719371399168 nn_components.py:329] mlp: final activation = None
I0123 16:32:34.824473 139719371399168 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.824563 139719371399168 nn_components.py:261] mlp: residual
I0123 16:32:34.826878 139719371399168 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.826960 139719371399168 transformer_base.py:443] tbase: final FFN
I0123 16:32:34.827068 139719371399168 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:32:34.829680 139719371399168 nn_components.py:329] mlp: final activation = None
I0123 16:32:34.831518 139719371399168 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.831609 139719371399168 nn_components.py:261] mlp: residual
I0123 16:32:34.831885 139719371399168 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.831964 139719371399168 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 16:32:34.832068 139719371399168 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:32:34.832106 139719371399168 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:32:34.832135 139719371399168 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:32:34.832195 139719371399168 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.834365 139719371399168 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:32:34.839658 139719371399168 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.839909 139719371399168 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:32:34.842516 139719371399168 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:32:34.854894 139719371399168 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:32:34.854947 139719371399168 attention.py:418] Single window, no scan.
I0123 16:32:34.854980 139719371399168 transformer_layer.py:389] tlayer: self-attention.
I0123 16:32:34.855010 139719371399168 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.855069 139719371399168 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.855614 139719371399168 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.855687 139719371399168 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.856033 139719371399168 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.856697 139719371399168 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.859172 139719371399168 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.859812 139719371399168 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.859890 139719371399168 transformer_layer.py:468] tlayer: End windows.
I0123 16:32:34.859926 139719371399168 transformer_layer.py:472] tlayer: final FFN.
I0123 16:32:34.859983 139719371399168 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.860114 139719371399168 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:32:34.860223 139719371399168 nn_components.py:325] mlp: activation = None
I0123 16:32:34.860262 139719371399168 nn_components.py:329] mlp: final activation = None
I0123 16:32:34.862728 139719371399168 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.862825 139719371399168 nn_components.py:261] mlp: residual
I0123 16:32:34.865177 139719371399168 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.865254 139719371399168 transformer_base.py:443] tbase: final FFN
I0123 16:32:34.865365 139719371399168 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:32:34.867652 139719371399168 nn_components.py:329] mlp: final activation = None
I0123 16:32:34.869519 139719371399168 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.869614 139719371399168 nn_components.py:261] mlp: residual
I0123 16:32:34.869930 139719371399168 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.870012 139719371399168 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 16:32:34.870122 139719371399168 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:32:34.870162 139719371399168 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:32:34.870192 139719371399168 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:32:34.870256 139719371399168 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.872491 139719371399168 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:32:34.877848 139719371399168 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.878112 139719371399168 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:32:34.880807 139719371399168 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:32:34.893433 139719371399168 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:32:34.893487 139719371399168 attention.py:418] Single window, no scan.
I0123 16:32:34.893521 139719371399168 transformer_layer.py:389] tlayer: self-attention.
I0123 16:32:34.893550 139719371399168 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.893611 139719371399168 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.894184 139719371399168 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.894260 139719371399168 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.894622 139719371399168 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.895311 139719371399168 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.897801 139719371399168 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.898437 139719371399168 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.898516 139719371399168 transformer_layer.py:468] tlayer: End windows.
I0123 16:32:34.898552 139719371399168 transformer_layer.py:472] tlayer: final FFN.
I0123 16:32:34.898609 139719371399168 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.898738 139719371399168 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:32:34.898853 139719371399168 nn_components.py:325] mlp: activation = None
I0123 16:32:34.898893 139719371399168 nn_components.py:329] mlp: final activation = None
I0123 16:32:34.900738 139719371399168 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.900827 139719371399168 nn_components.py:261] mlp: residual
I0123 16:32:34.903194 139719371399168 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.903279 139719371399168 transformer_base.py:443] tbase: final FFN
I0123 16:32:34.903388 139719371399168 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:32:34.905630 139719371399168 nn_components.py:329] mlp: final activation = None
I0123 16:32:34.907506 139719371399168 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.907600 139719371399168 nn_components.py:261] mlp: residual
I0123 16:32:34.907886 139719371399168 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.907968 139719371399168 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 16:32:34.908073 139719371399168 transformer_layer.py:154] tlayer: recurrent = False
I0123 16:32:34.908110 139719371399168 transformer_layer.py:155] tlayer: compute_importance = False
I0123 16:32:34.908138 139719371399168 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 16:32:34.908199 139719371399168 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.910382 139719371399168 transformer_base.py:161] kvq: pre_attn dropout.
I0123 16:32:34.915775 139719371399168 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.916036 139719371399168 transformer_base.py:194] kvq: normalize keys, queries.
I0123 16:32:34.918697 139719371399168 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 16:32:34.931316 139719371399168 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 16:32:34.931373 139719371399168 attention.py:418] Single window, no scan.
I0123 16:32:34.931409 139719371399168 transformer_layer.py:389] tlayer: self-attention.
I0123 16:32:34.931440 139719371399168 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.931502 139719371399168 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.932071 139719371399168 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.932145 139719371399168 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.932489 139719371399168 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.933160 139719371399168 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.935669 139719371399168 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.936278 139719371399168 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.936353 139719371399168 transformer_layer.py:468] tlayer: End windows.
I0123 16:32:34.936386 139719371399168 transformer_layer.py:472] tlayer: final FFN.
I0123 16:32:34.936441 139719371399168 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.936566 139719371399168 transformer_base.py:410] tbase: post-attention MLP.
I0123 16:32:34.936676 139719371399168 nn_components.py:325] mlp: activation = None
I0123 16:32:34.936714 139719371399168 nn_components.py:329] mlp: final activation = None
I0123 16:32:34.938600 139719371399168 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.938694 139719371399168 nn_components.py:261] mlp: residual
I0123 16:32:34.941012 139719371399168 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.941089 139719371399168 transformer_base.py:443] tbase: final FFN
I0123 16:32:34.941194 139719371399168 nn_components.py:320] mlp: hidden 4096, relu
I0123 16:32:34.943881 139719371399168 nn_components.py:329] mlp: final activation = None
I0123 16:32:34.945714 139719371399168 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.945807 139719371399168 nn_components.py:261] mlp: residual
I0123 16:32:34.946095 139719371399168 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:34.946181 139719371399168 decoder_stack.py:344] dstack: Final layernorm.
I0123 16:32:34.948933 139719371399168 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 16:32:39.371181 139719371399168 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
I0123 16:32:39.892589 139719371399168 training_loop.py:409] No working directory specified.
I0123 16:32:39.892731 139719371399168 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0123 16:32:39.893550 139719371399168 checkpoints.py:1062] Restoring legacy Flax checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0123 16:32:43.007590 139719371399168 training_loop.py:447] Only restoring trainable parameters.
I0123 16:32:43.008248 139719371399168 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0123 16:32:43.008308 139719371399168 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0123 16:32:43.008354 139719371399168 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 16:32:43.008397 139719371399168 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 16:32:43.008439 139719371399168 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:43.008480 139719371399168 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 16:32:43.008518 139719371399168 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:43.008556 139719371399168 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:43.008593 139719371399168 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 16:32:43.008630 139719371399168 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 16:32:43.008668 139719371399168 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:43.008705 139719371399168 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 16:32:43.008742 139719371399168 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 16:32:43.008781 139719371399168 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 16:32:43.008820 139719371399168 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:43.008859 139719371399168 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 16:32:43.008897 139719371399168 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:43.008934 139719371399168 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:43.008971 139719371399168 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 16:32:43.009007 139719371399168 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 16:32:43.009057 139719371399168 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:43.009096 139719371399168 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 16:32:43.009134 139719371399168 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 16:32:43.009171 139719371399168 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 16:32:43.009209 139719371399168 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:43.009245 139719371399168 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 16:32:43.009282 139719371399168 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:43.009318 139719371399168 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:43.009355 139719371399168 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 16:32:43.009392 139719371399168 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 16:32:43.009429 139719371399168 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:43.009466 139719371399168 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 16:32:43.009503 139719371399168 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 16:32:43.009539 139719371399168 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 16:32:43.009575 139719371399168 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:43.009612 139719371399168 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 16:32:43.009661 139719371399168 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:43.009703 139719371399168 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:43.009740 139719371399168 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 16:32:43.009778 139719371399168 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 16:32:43.009814 139719371399168 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:43.009851 139719371399168 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 16:32:43.009887 139719371399168 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 16:32:43.009923 139719371399168 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 16:32:43.009959 139719371399168 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:43.009995 139719371399168 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 16:32:43.010037 139719371399168 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:43.010074 139719371399168 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:43.010110 139719371399168 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 16:32:43.010146 139719371399168 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 16:32:43.010180 139719371399168 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:43.010216 139719371399168 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 16:32:43.010251 139719371399168 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 16:32:43.010287 139719371399168 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 16:32:43.010322 139719371399168 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:43.010359 139719371399168 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 16:32:43.010396 139719371399168 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:43.010432 139719371399168 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:43.010468 139719371399168 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 16:32:43.010503 139719371399168 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 16:32:43.010540 139719371399168 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:43.010576 139719371399168 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 16:32:43.010612 139719371399168 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 16:32:43.010648 139719371399168 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 16:32:43.010683 139719371399168 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:43.010720 139719371399168 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 16:32:43.010756 139719371399168 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:43.010791 139719371399168 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:43.010827 139719371399168 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 16:32:43.010863 139719371399168 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 16:32:43.010899 139719371399168 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:43.010936 139719371399168 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 16:32:43.010972 139719371399168 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 16:32:43.011014 139719371399168 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 16:32:43.011052 139719371399168 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:43.011088 139719371399168 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 16:32:43.011125 139719371399168 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:43.011161 139719371399168 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:43.011197 139719371399168 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 16:32:43.011234 139719371399168 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 16:32:43.011270 139719371399168 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:43.011306 139719371399168 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 16:32:43.011342 139719371399168 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 16:32:43.011377 139719371399168 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 16:32:43.011413 139719371399168 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:43.011448 139719371399168 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 16:32:43.011484 139719371399168 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:43.011519 139719371399168 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:43.011554 139719371399168 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 16:32:43.011590 139719371399168 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 16:32:43.011625 139719371399168 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:43.011661 139719371399168 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 16:32:43.011698 139719371399168 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 16:32:43.011734 139719371399168 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 16:32:43.011770 139719371399168 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:43.011806 139719371399168 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 16:32:43.011841 139719371399168 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:43.011876 139719371399168 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:43.011912 139719371399168 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 16:32:43.011948 139719371399168 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 16:32:43.011989 139719371399168 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:43.012027 139719371399168 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 16:32:43.012063 139719371399168 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 16:32:43.012098 139719371399168 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 16:32:43.012134 139719371399168 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:43.012169 139719371399168 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 16:32:43.012207 139719371399168 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:43.012243 139719371399168 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:43.012279 139719371399168 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 16:32:43.012315 139719371399168 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 16:32:43.012350 139719371399168 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:43.012386 139719371399168 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 16:32:43.012421 139719371399168 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 16:32:43.012456 139719371399168 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 16:32:43.012492 139719371399168 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:43.012528 139719371399168 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 16:32:43.012564 139719371399168 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:43.012599 139719371399168 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:43.012635 139719371399168 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 16:32:43.012671 139719371399168 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 16:32:43.012707 139719371399168 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 16:32:43.012743 139719371399168 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 16:32:43.012771 139719371399168 training_loop.py:725] Total parameters: 152072288
I0123 16:32:43.012997 139719371399168 training_loop.py:739] Total state size: 0
I0123 16:32:43.038213 139719371399168 training_loop.py:492] Training loop: creating task for mode beam_search
I0123 16:32:43.038444 139719371399168 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0123 16:32:43.038833 139719371399168 training_loop.py:652] Compiling mode beam_search with jit.
I0123 16:32:43.039173 139719371399168 training_loop.py:89] registering functions: dict_keys([])
I0123 16:32:43.055717 139719371399168 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = lc_tangent e b d, on_line e a d; f = midpoint f b a; g = on_line g b e, on_line g d f; h = on_circle h d c, on_line h g c; i = on_line i b a, on_line i c h; j = midpoint j c h ? eqratio c i i h c g h g
I0123 16:32:44.494367 139719371399168 ddar.py:60] Depth 1/1000 time = 1.412930965423584
I0123 16:32:46.842557 139719371399168 ddar.py:60] Depth 2/1000 time = 2.348034381866455
I0123 16:32:49.756333 139719371399168 ddar.py:60] Depth 3/1000 time = 2.913592576980591
I0123 16:32:54.908815 139719371399168 ddar.py:60] Depth 4/1000 time = 5.152247428894043
I0123 16:33:00.602508 139719371399168 ddar.py:60] Depth 5/1000 time = 5.693403244018555
I0123 16:33:00.616686 139719371399168 alphageometry.py:191] 
==========================
 * From theorem premises:
A B C D E F G H I : Points
DB = DA [00]
DC = DB [01]
BE  BD [02]
A,F,B are collinear [03]
FB = FA [04]
E,B,G are collinear [05]
F,G,D are collinear [06]
DH = DC [07]
H,C,G are collinear [08]
I,A,B are collinear [09]
I,C,H are collinear [10]

 * Auxiliary Constructions:
J : Points
JC = JH [11]
C,J,H are collinear [12]

 * Proof steps:
001. DB = DA [00] & FB = FA [04]   AB  DF [13]
002. DB = DA [00] & FB = FA [04]   FA:FB = DA:DB [14]
003. E,B,G are collinear [05] & A,F,B are collinear [03] & AB  DF [13] & BE  BD [02]   GBD = AFD [15]
004. FA:FB = DA:DB [14] & A,F,B are collinear [03]   ADF = FDB [16]
005. F,G,D are collinear [06] & FDB = ADF [16]   GDB = ADF [17]
006. GBD = AFD [15] & GDB = ADF [17] (Similar Triangles)  DG:DB = DA:DF [18]
007. DG:DB = DA:DF [18] & DH = DC [07] & DC = DB [01] & DB = DA [00]   GD:HD = HD:FD [19]
008. F,G,D are collinear [06]   HDF = HDG [20]
009. F,G,D are collinear [06]   GDC = FDC [21]
010. GD:HD = HD:FD [19] & HDF = HDG [20] (Similar Triangles)  HD:FH = GD:HG [22]
011. GD:HD = HD:FD [19] & HDF = HDG [20] (Similar Triangles)  DHF = HGD [23]
012. HD:FH = GD:HG [22] & DH = DC [07] & DC = DB [01]   BD:FH = GD:HG [24]
013. GD:HD = HD:FD [19] & CD = HD [07]   DG:DC = DC:DF [25]
014. DG:DC = DC:DF [25] & GDC = FDC [21] (Similar Triangles)  GD:GC = CD:CF [26]
015. DG:DC = DC:DF [25] & GDC = FDC [21] (Similar Triangles)  DGC = FCD [27]
016. GD:GC = CD:CF [26] & DH = DC [07] & DC = DB [01]   BD:CF = GD:CG [28]
017. BD:FH = GD:HG [24] & BD:CF = GD:CG [28]   CF:FH = CG:HG [29]
018. JC = JH [11] & DC = DH [07]   JC:JH = DC:DH [30]
019. JC = JH [11] & DC = DH [07]   CH  DJ [31]
020. JC:JH = DC:DH [30] & J,C,H are collinear [12]   CDJ = JDH [32]
021. D,F,G are collinear [06] & I,A,B are collinear [09] & DF  AB [13]   FG  IA [33]
022. C,J,H are collinear [12] & H,C,G are collinear [08] & CH  DJ [31]   JD  CJ [34]
023. FG  IA [33] & JD  CJ [34]   (FG-CJ) = (IA-JD) [35]
024. I,A,B are collinear [09] & (FG-CJ) = (IA-JD) [35] & F,G,D are collinear [06] & C,J,H are collinear [12] & H,C,G are collinear [08] & DHF = HGD [23]   DHF = (JD-IA) [36]
025. CDJ = JDH [32] & DHF = (JD-IA) [36]   (JD-FH) = (CD-IA) [37]
026. I,A,B are collinear [09] & D,F,G are collinear [06] & C,J,H are collinear [12] & H,C,G are collinear [08] & (FG-CJ) = (IA-JD) [35]   (IA-FG) = DJC [38]
027. D,F,G are collinear [06] & C,J,H are collinear [12] & H,C,G are collinear [08] & DGC = FCD [27]   GFC = JCD [39]
028. (IA-FG) = DJC [38] & GFC = JCD [39]   (IA-CF) = JDC [40]
029. I,A,B are collinear [09] & A,F,B are collinear [03] & (JD-FH) = (CD-IA) [37] & (IA-CF) = JDC [40]   CFI = IFH [41]
030. CFI = IFH [41] & I,C,H are collinear [10]   IC:IH = CF:FH [42]
031. CF:FH = CG:HG [29] & IC:IH = CF:FH [42]   CI:IH = CG:HG
==========================

