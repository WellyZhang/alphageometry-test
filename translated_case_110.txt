I0123 12:50:29.840383 140456679309312 inference_utils.py:69] Parsing gin configuration.
I0123 12:50:29.840506 140456679309312 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0123 12:50:29.840714 140456679309312 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0123 12:50:29.840747 140456679309312 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0123 12:50:29.840775 140456679309312 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0123 12:50:29.840801 140456679309312 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0123 12:50:29.840826 140456679309312 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0123 12:50:29.840851 140456679309312 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0123 12:50:29.840876 140456679309312 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0123 12:50:29.840902 140456679309312 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=32
I0123 12:50:29.840926 140456679309312 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0123 12:50:29.840951 140456679309312 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0123 12:50:29.840997 140456679309312 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0123 12:50:29.841140 140456679309312 resource_reader.py:55] Path not found: base_htrans.gin
I0123 12:50:29.841387 140456679309312 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0123 12:50:29.841492 140456679309312 resource_reader.py:55] Path not found: trainer_configuration.gin
I0123 12:50:29.847933 140456679309312 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0123 12:50:29.848059 140456679309312 resource_reader.py:55] Path not found: size/medium_150M.gin
I0123 12:50:29.848380 140456679309312 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0123 12:50:29.848483 140456679309312 resource_reader.py:55] Path not found: options/positions_t5.gin
I0123 12:50:29.848762 140456679309312 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0123 12:50:29.848861 140456679309312 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0123 12:50:29.849267 140456679309312 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0123 12:50:29.849367 140456679309312 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0123 12:50:29.853105 140456679309312 training_loop.py:334] ==== Training loop: initializing model ====
I0123 12:50:29.955337 140456679309312 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0123 12:50:29.956127 140456679309312 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0123 12:50:29.962662 140456679309312 training_loop.py:335] Process 0 of 1
I0123 12:50:29.962718 140456679309312 training_loop.py:336] Local device count = 1
I0123 12:50:29.962760 140456679309312 training_loop.py:337] Number of replicas = 1
I0123 12:50:29.962793 140456679309312 training_loop.py:339] Using random number seed 42
I0123 12:50:30.477484 140456679309312 training_loop.py:359] Initializing the model.
I0123 12:50:30.883247 140456679309312 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:30.883611 140456679309312 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 12:50:30.883717 140456679309312 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:50:30.883796 140456679309312 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:50:30.883875 140456679309312 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:50:30.883959 140456679309312 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:50:30.884031 140456679309312 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:50:30.884102 140456679309312 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:50:30.884171 140456679309312 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:50:30.884240 140456679309312 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:50:30.884308 140456679309312 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:50:30.884377 140456679309312 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:50:30.884446 140456679309312 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:50:30.884515 140456679309312 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:50:30.884554 140456679309312 attention.py:418] Single window, no scan.
I0123 12:50:30.884599 140456679309312 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 12:50:30.884714 140456679309312 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:50:30.884753 140456679309312 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:50:30.884783 140456679309312 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:50:30.886881 140456679309312 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:30.892346 140456679309312 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:50:30.903268 140456679309312 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:30.903549 140456679309312 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:50:30.907970 140456679309312 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:50:30.918823 140456679309312 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:50:30.918884 140456679309312 attention.py:418] Single window, no scan.
I0123 12:50:30.918924 140456679309312 transformer_layer.py:389] tlayer: self-attention.
I0123 12:50:30.918956 140456679309312 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:30.919021 140456679309312 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:30.920220 140456679309312 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:30.920298 140456679309312 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:30.921010 140456679309312 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:30.923507 140456679309312 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:30.929768 140456679309312 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:30.931084 140456679309312 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:30.931167 140456679309312 transformer_layer.py:468] tlayer: End windows.
I0123 12:50:30.931202 140456679309312 transformer_layer.py:472] tlayer: final FFN.
I0123 12:50:30.931263 140456679309312 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:30.931390 140456679309312 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:50:30.931725 140456679309312 nn_components.py:325] mlp: activation = None
I0123 12:50:30.931775 140456679309312 nn_components.py:329] mlp: final activation = None
I0123 12:50:30.933698 140456679309312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:30.933803 140456679309312 nn_components.py:261] mlp: residual
I0123 12:50:30.936733 140456679309312 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:30.936823 140456679309312 transformer_base.py:443] tbase: final FFN
I0123 12:50:30.937322 140456679309312 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:50:30.947620 140456679309312 nn_components.py:329] mlp: final activation = None
I0123 12:50:30.956502 140456679309312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:30.956599 140456679309312 nn_components.py:261] mlp: residual
I0123 12:50:30.956896 140456679309312 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:30.956977 140456679309312 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 12:50:30.957087 140456679309312 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:50:30.957126 140456679309312 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:50:30.957156 140456679309312 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:50:30.959023 140456679309312 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:30.961518 140456679309312 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:50:30.967373 140456679309312 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:30.967631 140456679309312 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:50:30.970291 140456679309312 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:50:30.974120 140456679309312 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:50:30.974176 140456679309312 attention.py:418] Single window, no scan.
I0123 12:50:30.974212 140456679309312 transformer_layer.py:389] tlayer: self-attention.
I0123 12:50:30.974243 140456679309312 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:30.974304 140456679309312 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:30.974873 140456679309312 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:30.974949 140456679309312 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:30.975310 140456679309312 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:30.976077 140456679309312 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:30.978594 140456679309312 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:30.979217 140456679309312 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:30.979293 140456679309312 transformer_layer.py:468] tlayer: End windows.
I0123 12:50:30.979328 140456679309312 transformer_layer.py:472] tlayer: final FFN.
I0123 12:50:30.979387 140456679309312 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:30.979513 140456679309312 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:50:30.979843 140456679309312 nn_components.py:325] mlp: activation = None
I0123 12:50:30.979886 140456679309312 nn_components.py:329] mlp: final activation = None
I0123 12:50:30.981838 140456679309312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:30.981932 140456679309312 nn_components.py:261] mlp: residual
I0123 12:50:30.984428 140456679309312 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:30.984506 140456679309312 transformer_base.py:443] tbase: final FFN
I0123 12:50:30.984938 140456679309312 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:50:30.987265 140456679309312 nn_components.py:329] mlp: final activation = None
I0123 12:50:30.989166 140456679309312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:30.989259 140456679309312 nn_components.py:261] mlp: residual
I0123 12:50:30.989549 140456679309312 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:30.989628 140456679309312 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 12:50:30.989747 140456679309312 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:50:30.989785 140456679309312 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:50:30.989814 140456679309312 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:50:30.992060 140456679309312 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:30.994462 140456679309312 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:50:31.000031 140456679309312 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.000298 140456679309312 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:50:31.003025 140456679309312 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:50:31.006884 140456679309312 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:50:31.006939 140456679309312 attention.py:418] Single window, no scan.
I0123 12:50:31.006974 140456679309312 transformer_layer.py:389] tlayer: self-attention.
I0123 12:50:31.007003 140456679309312 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.007064 140456679309312 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.007616 140456679309312 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.007691 140456679309312 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.008052 140456679309312 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.008818 140456679309312 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.011364 140456679309312 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.012043 140456679309312 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.012121 140456679309312 transformer_layer.py:468] tlayer: End windows.
I0123 12:50:31.012156 140456679309312 transformer_layer.py:472] tlayer: final FFN.
I0123 12:50:31.012216 140456679309312 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.012350 140456679309312 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:50:31.012669 140456679309312 nn_components.py:325] mlp: activation = None
I0123 12:50:31.012712 140456679309312 nn_components.py:329] mlp: final activation = None
I0123 12:50:31.014660 140456679309312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.014755 140456679309312 nn_components.py:261] mlp: residual
I0123 12:50:31.017302 140456679309312 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.017386 140456679309312 transformer_base.py:443] tbase: final FFN
I0123 12:50:31.017878 140456679309312 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:50:31.020193 140456679309312 nn_components.py:329] mlp: final activation = None
I0123 12:50:31.022148 140456679309312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.022244 140456679309312 nn_components.py:261] mlp: residual
I0123 12:50:31.022546 140456679309312 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.022626 140456679309312 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 12:50:31.022737 140456679309312 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:50:31.022775 140456679309312 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:50:31.022806 140456679309312 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:50:31.025058 140456679309312 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.027503 140456679309312 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:50:31.033219 140456679309312 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.033490 140456679309312 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:50:31.036210 140456679309312 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:50:31.040095 140456679309312 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:50:31.040152 140456679309312 attention.py:418] Single window, no scan.
I0123 12:50:31.040189 140456679309312 transformer_layer.py:389] tlayer: self-attention.
I0123 12:50:31.040220 140456679309312 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.040283 140456679309312 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.040844 140456679309312 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.040920 140456679309312 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.041285 140456679309312 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.042088 140456679309312 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.044686 140456679309312 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.045312 140456679309312 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.045394 140456679309312 transformer_layer.py:468] tlayer: End windows.
I0123 12:50:31.045431 140456679309312 transformer_layer.py:472] tlayer: final FFN.
I0123 12:50:31.045490 140456679309312 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.045619 140456679309312 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:50:31.045949 140456679309312 nn_components.py:325] mlp: activation = None
I0123 12:50:31.045993 140456679309312 nn_components.py:329] mlp: final activation = None
I0123 12:50:31.047917 140456679309312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.048009 140456679309312 nn_components.py:261] mlp: residual
I0123 12:50:31.050649 140456679309312 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.050734 140456679309312 transformer_base.py:443] tbase: final FFN
I0123 12:50:31.051163 140456679309312 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:50:31.053444 140456679309312 nn_components.py:329] mlp: final activation = None
I0123 12:50:31.055385 140456679309312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.055483 140456679309312 nn_components.py:261] mlp: residual
I0123 12:50:31.055781 140456679309312 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.055862 140456679309312 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 12:50:31.055974 140456679309312 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:50:31.056013 140456679309312 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:50:31.056044 140456679309312 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:50:31.057963 140456679309312 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.060397 140456679309312 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:50:31.066067 140456679309312 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.066338 140456679309312 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:50:31.069387 140456679309312 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:50:31.073169 140456679309312 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:50:31.073225 140456679309312 attention.py:418] Single window, no scan.
I0123 12:50:31.073261 140456679309312 transformer_layer.py:389] tlayer: self-attention.
I0123 12:50:31.073292 140456679309312 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.073353 140456679309312 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.073929 140456679309312 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.074006 140456679309312 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.074378 140456679309312 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.075157 140456679309312 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.077772 140456679309312 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.078408 140456679309312 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.078485 140456679309312 transformer_layer.py:468] tlayer: End windows.
I0123 12:50:31.078520 140456679309312 transformer_layer.py:472] tlayer: final FFN.
I0123 12:50:31.078581 140456679309312 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.078713 140456679309312 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:50:31.079041 140456679309312 nn_components.py:325] mlp: activation = None
I0123 12:50:31.079085 140456679309312 nn_components.py:329] mlp: final activation = None
I0123 12:50:31.081021 140456679309312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.081115 140456679309312 nn_components.py:261] mlp: residual
I0123 12:50:31.083755 140456679309312 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.083835 140456679309312 transformer_base.py:443] tbase: final FFN
I0123 12:50:31.084265 140456679309312 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:50:31.086574 140456679309312 nn_components.py:329] mlp: final activation = None
I0123 12:50:31.088545 140456679309312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.088639 140456679309312 nn_components.py:261] mlp: residual
I0123 12:50:31.088938 140456679309312 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.089020 140456679309312 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 12:50:31.089129 140456679309312 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:50:31.089168 140456679309312 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:50:31.089199 140456679309312 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:50:31.091100 140456679309312 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.093558 140456679309312 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:50:31.099379 140456679309312 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.099642 140456679309312 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:50:31.102400 140456679309312 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:50:31.106189 140456679309312 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:50:31.106245 140456679309312 attention.py:418] Single window, no scan.
I0123 12:50:31.106282 140456679309312 transformer_layer.py:389] tlayer: self-attention.
I0123 12:50:31.106314 140456679309312 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.106377 140456679309312 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.106986 140456679309312 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.107065 140456679309312 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.107433 140456679309312 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.108225 140456679309312 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.110783 140456679309312 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.111412 140456679309312 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.111489 140456679309312 transformer_layer.py:468] tlayer: End windows.
I0123 12:50:31.111524 140456679309312 transformer_layer.py:472] tlayer: final FFN.
I0123 12:50:31.111583 140456679309312 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.111710 140456679309312 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:50:31.112037 140456679309312 nn_components.py:325] mlp: activation = None
I0123 12:50:31.112080 140456679309312 nn_components.py:329] mlp: final activation = None
I0123 12:50:31.114036 140456679309312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.114130 140456679309312 nn_components.py:261] mlp: residual
I0123 12:50:31.116742 140456679309312 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.116822 140456679309312 transformer_base.py:443] tbase: final FFN
I0123 12:50:31.117249 140456679309312 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:50:31.119596 140456679309312 nn_components.py:329] mlp: final activation = None
I0123 12:50:31.122478 140456679309312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.122573 140456679309312 nn_components.py:261] mlp: residual
I0123 12:50:31.122871 140456679309312 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.122952 140456679309312 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 12:50:31.123061 140456679309312 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:50:31.123100 140456679309312 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:50:31.123132 140456679309312 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:50:31.125198 140456679309312 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.127898 140456679309312 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:50:31.133746 140456679309312 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.134008 140456679309312 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:50:31.136696 140456679309312 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:50:31.140535 140456679309312 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:50:31.140592 140456679309312 attention.py:418] Single window, no scan.
I0123 12:50:31.140628 140456679309312 transformer_layer.py:389] tlayer: self-attention.
I0123 12:50:31.140660 140456679309312 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.140721 140456679309312 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.141288 140456679309312 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.141367 140456679309312 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.141739 140456679309312 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.142522 140456679309312 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.145041 140456679309312 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.145667 140456679309312 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.145749 140456679309312 transformer_layer.py:468] tlayer: End windows.
I0123 12:50:31.145783 140456679309312 transformer_layer.py:472] tlayer: final FFN.
I0123 12:50:31.145842 140456679309312 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.145969 140456679309312 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:50:31.146299 140456679309312 nn_components.py:325] mlp: activation = None
I0123 12:50:31.146343 140456679309312 nn_components.py:329] mlp: final activation = None
I0123 12:50:31.148665 140456679309312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.148758 140456679309312 nn_components.py:261] mlp: residual
I0123 12:50:31.151326 140456679309312 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.151407 140456679309312 transformer_base.py:443] tbase: final FFN
I0123 12:50:31.151839 140456679309312 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:50:31.293677 140456679309312 nn_components.py:329] mlp: final activation = None
I0123 12:50:31.295981 140456679309312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.296145 140456679309312 nn_components.py:261] mlp: residual
I0123 12:50:31.296455 140456679309312 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.296545 140456679309312 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 12:50:31.296659 140456679309312 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:50:31.296699 140456679309312 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:50:31.296729 140456679309312 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:50:31.298801 140456679309312 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.301386 140456679309312 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:50:31.307199 140456679309312 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.307480 140456679309312 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:50:31.310209 140456679309312 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:50:31.314231 140456679309312 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:50:31.314288 140456679309312 attention.py:418] Single window, no scan.
I0123 12:50:31.314325 140456679309312 transformer_layer.py:389] tlayer: self-attention.
I0123 12:50:31.314357 140456679309312 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.314420 140456679309312 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.315034 140456679309312 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.315110 140456679309312 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.315478 140456679309312 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.316275 140456679309312 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.318953 140456679309312 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.319599 140456679309312 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.319678 140456679309312 transformer_layer.py:468] tlayer: End windows.
I0123 12:50:31.319714 140456679309312 transformer_layer.py:472] tlayer: final FFN.
I0123 12:50:31.319774 140456679309312 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.319902 140456679309312 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:50:31.320233 140456679309312 nn_components.py:325] mlp: activation = None
I0123 12:50:31.320276 140456679309312 nn_components.py:329] mlp: final activation = None
I0123 12:50:31.322216 140456679309312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.322315 140456679309312 nn_components.py:261] mlp: residual
I0123 12:50:31.324872 140456679309312 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.324952 140456679309312 transformer_base.py:443] tbase: final FFN
I0123 12:50:31.325463 140456679309312 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:50:31.327788 140456679309312 nn_components.py:329] mlp: final activation = None
I0123 12:50:31.329742 140456679309312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.329849 140456679309312 nn_components.py:261] mlp: residual
I0123 12:50:31.330147 140456679309312 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.330230 140456679309312 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 12:50:31.330341 140456679309312 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:50:31.330381 140456679309312 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:50:31.330412 140456679309312 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:50:31.332325 140456679309312 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.334756 140456679309312 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:50:31.340666 140456679309312 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.340934 140456679309312 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:50:31.343697 140456679309312 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:50:31.347572 140456679309312 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:50:31.347629 140456679309312 attention.py:418] Single window, no scan.
I0123 12:50:31.347666 140456679309312 transformer_layer.py:389] tlayer: self-attention.
I0123 12:50:31.347696 140456679309312 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.347759 140456679309312 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.348336 140456679309312 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.348412 140456679309312 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.348778 140456679309312 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.349573 140456679309312 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.352206 140456679309312 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.352833 140456679309312 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.352909 140456679309312 transformer_layer.py:468] tlayer: End windows.
I0123 12:50:31.352944 140456679309312 transformer_layer.py:472] tlayer: final FFN.
I0123 12:50:31.353004 140456679309312 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.353136 140456679309312 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:50:31.353460 140456679309312 nn_components.py:325] mlp: activation = None
I0123 12:50:31.353503 140456679309312 nn_components.py:329] mlp: final activation = None
I0123 12:50:31.355437 140456679309312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.355531 140456679309312 nn_components.py:261] mlp: residual
I0123 12:50:31.358124 140456679309312 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.358204 140456679309312 transformer_base.py:443] tbase: final FFN
I0123 12:50:31.358641 140456679309312 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:50:31.360974 140456679309312 nn_components.py:329] mlp: final activation = None
I0123 12:50:31.362900 140456679309312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.362997 140456679309312 nn_components.py:261] mlp: residual
I0123 12:50:31.363290 140456679309312 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.363375 140456679309312 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 12:50:31.363486 140456679309312 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:50:31.363524 140456679309312 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:50:31.363554 140456679309312 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:50:31.365448 140456679309312 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.367838 140456679309312 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:50:31.373813 140456679309312 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.374071 140456679309312 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:50:31.376888 140456679309312 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:50:31.380625 140456679309312 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:50:31.380681 140456679309312 attention.py:418] Single window, no scan.
I0123 12:50:31.380716 140456679309312 transformer_layer.py:389] tlayer: self-attention.
I0123 12:50:31.380747 140456679309312 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.380807 140456679309312 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.381364 140456679309312 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.381444 140456679309312 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.381819 140456679309312 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.382646 140456679309312 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.385152 140456679309312 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.385764 140456679309312 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.385847 140456679309312 transformer_layer.py:468] tlayer: End windows.
I0123 12:50:31.385882 140456679309312 transformer_layer.py:472] tlayer: final FFN.
I0123 12:50:31.385941 140456679309312 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.386071 140456679309312 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:50:31.386387 140456679309312 nn_components.py:325] mlp: activation = None
I0123 12:50:31.386430 140456679309312 nn_components.py:329] mlp: final activation = None
I0123 12:50:31.388340 140456679309312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.388433 140456679309312 nn_components.py:261] mlp: residual
I0123 12:50:31.390972 140456679309312 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.391054 140456679309312 transformer_base.py:443] tbase: final FFN
I0123 12:50:31.391498 140456679309312 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:50:31.393760 140456679309312 nn_components.py:329] mlp: final activation = None
I0123 12:50:31.395748 140456679309312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.395844 140456679309312 nn_components.py:261] mlp: residual
I0123 12:50:31.396137 140456679309312 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.396225 140456679309312 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 12:50:31.396337 140456679309312 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:50:31.396375 140456679309312 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:50:31.396405 140456679309312 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:50:31.398252 140456679309312 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.400710 140456679309312 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:50:31.406275 140456679309312 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.406538 140456679309312 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:50:31.409202 140456679309312 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:50:31.412989 140456679309312 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:50:31.413045 140456679309312 attention.py:418] Single window, no scan.
I0123 12:50:31.413085 140456679309312 transformer_layer.py:389] tlayer: self-attention.
I0123 12:50:31.413117 140456679309312 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.413222 140456679309312 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.413792 140456679309312 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.413869 140456679309312 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.414236 140456679309312 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.415018 140456679309312 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.417540 140456679309312 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.418169 140456679309312 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.418246 140456679309312 transformer_layer.py:468] tlayer: End windows.
I0123 12:50:31.418281 140456679309312 transformer_layer.py:472] tlayer: final FFN.
I0123 12:50:31.418340 140456679309312 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.418471 140456679309312 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:50:31.418797 140456679309312 nn_components.py:325] mlp: activation = None
I0123 12:50:31.418840 140456679309312 nn_components.py:329] mlp: final activation = None
I0123 12:50:31.420823 140456679309312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.420919 140456679309312 nn_components.py:261] mlp: residual
I0123 12:50:31.423765 140456679309312 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.423846 140456679309312 transformer_base.py:443] tbase: final FFN
I0123 12:50:31.424284 140456679309312 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:50:31.426626 140456679309312 nn_components.py:329] mlp: final activation = None
I0123 12:50:31.428537 140456679309312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.428633 140456679309312 nn_components.py:261] mlp: residual
I0123 12:50:31.428925 140456679309312 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.429005 140456679309312 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 12:50:31.429121 140456679309312 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:50:31.429161 140456679309312 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:50:31.429192 140456679309312 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:50:31.431049 140456679309312 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.433555 140456679309312 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:50:31.444031 140456679309312 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.444355 140456679309312 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:50:31.447149 140456679309312 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:50:31.451487 140456679309312 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:50:31.451545 140456679309312 attention.py:418] Single window, no scan.
I0123 12:50:31.451583 140456679309312 transformer_layer.py:389] tlayer: self-attention.
I0123 12:50:31.451615 140456679309312 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.451678 140456679309312 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.452288 140456679309312 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.452365 140456679309312 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.452741 140456679309312 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.453538 140456679309312 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.456159 140456679309312 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.456797 140456679309312 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.456875 140456679309312 transformer_layer.py:468] tlayer: End windows.
I0123 12:50:31.456911 140456679309312 transformer_layer.py:472] tlayer: final FFN.
I0123 12:50:31.456977 140456679309312 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.457108 140456679309312 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:50:31.457447 140456679309312 nn_components.py:325] mlp: activation = None
I0123 12:50:31.457491 140456679309312 nn_components.py:329] mlp: final activation = None
I0123 12:50:31.459495 140456679309312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.459590 140456679309312 nn_components.py:261] mlp: residual
I0123 12:50:31.462201 140456679309312 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.462282 140456679309312 transformer_base.py:443] tbase: final FFN
I0123 12:50:31.462721 140456679309312 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:50:31.465093 140456679309312 nn_components.py:329] mlp: final activation = None
I0123 12:50:31.467058 140456679309312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.467154 140456679309312 nn_components.py:261] mlp: residual
I0123 12:50:31.467454 140456679309312 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.467761 140456679309312 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:50:31.467830 140456679309312 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:50:31.467895 140456679309312 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:50:31.467953 140456679309312 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:50:31.468007 140456679309312 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:50:31.468060 140456679309312 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:50:31.468112 140456679309312 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:50:31.468164 140456679309312 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:50:31.468215 140456679309312 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:50:31.468267 140456679309312 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:50:31.468319 140456679309312 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:50:31.468371 140456679309312 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:50:31.468408 140456679309312 decoder_stack.py:344] dstack: Final layernorm.
I0123 12:50:31.472230 140456679309312 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:31.520960 140456679309312 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.521049 140456679309312 decoder_stack.py:333] dstack: autoregressive generator.
I0123 12:50:31.521102 140456679309312 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 12:50:31.521210 140456679309312 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:50:31.521250 140456679309312 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:50:31.521282 140456679309312 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:50:31.521348 140456679309312 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.523884 140456679309312 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:50:31.529499 140456679309312 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.529772 140456679309312 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:50:31.532473 140456679309312 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:50:31.549379 140456679309312 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:50:31.549436 140456679309312 attention.py:418] Single window, no scan.
I0123 12:50:31.549472 140456679309312 transformer_layer.py:389] tlayer: self-attention.
I0123 12:50:31.549504 140456679309312 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.549566 140456679309312 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.550729 140456679309312 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.550809 140456679309312 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.551534 140456679309312 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.553580 140456679309312 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.558469 140456679309312 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.559808 140456679309312 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.559894 140456679309312 transformer_layer.py:468] tlayer: End windows.
I0123 12:50:31.559930 140456679309312 transformer_layer.py:472] tlayer: final FFN.
I0123 12:50:31.559990 140456679309312 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.560118 140456679309312 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:50:31.560228 140456679309312 nn_components.py:325] mlp: activation = None
I0123 12:50:31.560267 140456679309312 nn_components.py:329] mlp: final activation = None
I0123 12:50:31.562221 140456679309312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.562318 140456679309312 nn_components.py:261] mlp: residual
I0123 12:50:31.564821 140456679309312 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.564900 140456679309312 transformer_base.py:443] tbase: final FFN
I0123 12:50:31.565009 140456679309312 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:50:31.567321 140456679309312 nn_components.py:329] mlp: final activation = None
I0123 12:50:31.569319 140456679309312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.569415 140456679309312 nn_components.py:261] mlp: residual
I0123 12:50:31.569723 140456679309312 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.569807 140456679309312 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 12:50:31.569917 140456679309312 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:50:31.569956 140456679309312 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:50:31.569988 140456679309312 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:50:31.570054 140456679309312 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.572368 140456679309312 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:50:31.578319 140456679309312 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.578582 140456679309312 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:50:31.581337 140456679309312 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:50:31.594939 140456679309312 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:50:31.594995 140456679309312 attention.py:418] Single window, no scan.
I0123 12:50:31.595030 140456679309312 transformer_layer.py:389] tlayer: self-attention.
I0123 12:50:31.595058 140456679309312 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.595123 140456679309312 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.595674 140456679309312 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.595750 140456679309312 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.596111 140456679309312 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.596814 140456679309312 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.599340 140456679309312 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.599952 140456679309312 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.600028 140456679309312 transformer_layer.py:468] tlayer: End windows.
I0123 12:50:31.600067 140456679309312 transformer_layer.py:472] tlayer: final FFN.
I0123 12:50:31.600125 140456679309312 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.600253 140456679309312 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:50:31.600362 140456679309312 nn_components.py:325] mlp: activation = None
I0123 12:50:31.600400 140456679309312 nn_components.py:329] mlp: final activation = None
I0123 12:50:31.602354 140456679309312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.602448 140456679309312 nn_components.py:261] mlp: residual
I0123 12:50:31.604895 140456679309312 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.604974 140456679309312 transformer_base.py:443] tbase: final FFN
I0123 12:50:31.605082 140456679309312 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:50:31.607314 140456679309312 nn_components.py:329] mlp: final activation = None
I0123 12:50:31.609250 140456679309312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.609344 140456679309312 nn_components.py:261] mlp: residual
I0123 12:50:31.609633 140456679309312 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.609723 140456679309312 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 12:50:31.609837 140456679309312 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:50:31.609877 140456679309312 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:50:31.609907 140456679309312 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:50:31.609969 140456679309312 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.612230 140456679309312 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:50:31.617725 140456679309312 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.617990 140456679309312 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:50:31.620693 140456679309312 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:50:31.633429 140456679309312 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:50:31.633486 140456679309312 attention.py:418] Single window, no scan.
I0123 12:50:31.633522 140456679309312 transformer_layer.py:389] tlayer: self-attention.
I0123 12:50:31.633552 140456679309312 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.633615 140456679309312 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.634183 140456679309312 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.634262 140456679309312 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.634627 140456679309312 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.635317 140456679309312 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.637821 140456679309312 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.638442 140456679309312 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.638517 140456679309312 transformer_layer.py:468] tlayer: End windows.
I0123 12:50:31.638551 140456679309312 transformer_layer.py:472] tlayer: final FFN.
I0123 12:50:31.638614 140456679309312 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.638741 140456679309312 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:50:31.638849 140456679309312 nn_components.py:325] mlp: activation = None
I0123 12:50:31.638887 140456679309312 nn_components.py:329] mlp: final activation = None
I0123 12:50:31.640813 140456679309312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.640906 140456679309312 nn_components.py:261] mlp: residual
I0123 12:50:31.643372 140456679309312 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.643450 140456679309312 transformer_base.py:443] tbase: final FFN
I0123 12:50:31.643556 140456679309312 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:50:31.645777 140456679309312 nn_components.py:329] mlp: final activation = None
I0123 12:50:31.647708 140456679309312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.647803 140456679309312 nn_components.py:261] mlp: residual
I0123 12:50:31.648092 140456679309312 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.648172 140456679309312 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 12:50:31.648282 140456679309312 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:50:31.648321 140456679309312 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:50:31.648351 140456679309312 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:50:31.648413 140456679309312 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.650678 140456679309312 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:50:31.656138 140456679309312 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.656398 140456679309312 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:50:31.659127 140456679309312 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:50:31.671837 140456679309312 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:50:31.671895 140456679309312 attention.py:418] Single window, no scan.
I0123 12:50:31.671929 140456679309312 transformer_layer.py:389] tlayer: self-attention.
I0123 12:50:31.671959 140456679309312 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.672023 140456679309312 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.672574 140456679309312 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.672649 140456679309312 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.673006 140456679309312 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.673707 140456679309312 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.676216 140456679309312 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.676845 140456679309312 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.676922 140456679309312 transformer_layer.py:468] tlayer: End windows.
I0123 12:50:31.676955 140456679309312 transformer_layer.py:472] tlayer: final FFN.
I0123 12:50:31.677013 140456679309312 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.677150 140456679309312 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:50:31.677260 140456679309312 nn_components.py:325] mlp: activation = None
I0123 12:50:31.677298 140456679309312 nn_components.py:329] mlp: final activation = None
I0123 12:50:31.679531 140456679309312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.679624 140456679309312 nn_components.py:261] mlp: residual
I0123 12:50:31.682069 140456679309312 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.682147 140456679309312 transformer_base.py:443] tbase: final FFN
I0123 12:50:31.682254 140456679309312 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:50:31.684474 140456679309312 nn_components.py:329] mlp: final activation = None
I0123 12:50:31.686356 140456679309312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.686450 140456679309312 nn_components.py:261] mlp: residual
I0123 12:50:31.686738 140456679309312 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.686817 140456679309312 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 12:50:31.686927 140456679309312 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:50:31.686965 140456679309312 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:50:31.686995 140456679309312 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:50:31.687057 140456679309312 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.689566 140456679309312 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:50:31.695162 140456679309312 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.695425 140456679309312 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:50:31.698112 140456679309312 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:50:31.710984 140456679309312 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:50:31.711040 140456679309312 attention.py:418] Single window, no scan.
I0123 12:50:31.711075 140456679309312 transformer_layer.py:389] tlayer: self-attention.
I0123 12:50:31.711105 140456679309312 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.711165 140456679309312 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.711722 140456679309312 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.711797 140456679309312 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.712158 140456679309312 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.712872 140456679309312 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.715449 140456679309312 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.716073 140456679309312 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.716151 140456679309312 transformer_layer.py:468] tlayer: End windows.
I0123 12:50:31.716186 140456679309312 transformer_layer.py:472] tlayer: final FFN.
I0123 12:50:31.716247 140456679309312 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.716387 140456679309312 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:50:31.716500 140456679309312 nn_components.py:325] mlp: activation = None
I0123 12:50:31.716539 140456679309312 nn_components.py:329] mlp: final activation = None
I0123 12:50:31.718425 140456679309312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.718520 140456679309312 nn_components.py:261] mlp: residual
I0123 12:50:31.720958 140456679309312 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.721037 140456679309312 transformer_base.py:443] tbase: final FFN
I0123 12:50:31.721143 140456679309312 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:50:31.723430 140456679309312 nn_components.py:329] mlp: final activation = None
I0123 12:50:31.725297 140456679309312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.725392 140456679309312 nn_components.py:261] mlp: residual
I0123 12:50:31.725687 140456679309312 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.725769 140456679309312 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 12:50:31.725878 140456679309312 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:50:31.725917 140456679309312 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:50:31.725947 140456679309312 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:50:31.726010 140456679309312 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.728281 140456679309312 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:50:31.733786 140456679309312 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.734047 140456679309312 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:50:31.736761 140456679309312 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:50:31.749462 140456679309312 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:50:31.749518 140456679309312 attention.py:418] Single window, no scan.
I0123 12:50:31.749552 140456679309312 transformer_layer.py:389] tlayer: self-attention.
I0123 12:50:31.749582 140456679309312 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.749651 140456679309312 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.750216 140456679309312 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.750292 140456679309312 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.750648 140456679309312 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.751352 140456679309312 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.753885 140456679309312 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.754513 140456679309312 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.754591 140456679309312 transformer_layer.py:468] tlayer: End windows.
I0123 12:50:31.754625 140456679309312 transformer_layer.py:472] tlayer: final FFN.
I0123 12:50:31.754684 140456679309312 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.754817 140456679309312 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:50:31.754933 140456679309312 nn_components.py:325] mlp: activation = None
I0123 12:50:31.754973 140456679309312 nn_components.py:329] mlp: final activation = None
I0123 12:50:31.756923 140456679309312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.757016 140456679309312 nn_components.py:261] mlp: residual
I0123 12:50:31.759449 140456679309312 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.759528 140456679309312 transformer_base.py:443] tbase: final FFN
I0123 12:50:31.759639 140456679309312 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:50:31.761871 140456679309312 nn_components.py:329] mlp: final activation = None
I0123 12:50:31.763743 140456679309312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.763837 140456679309312 nn_components.py:261] mlp: residual
I0123 12:50:31.764126 140456679309312 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.764206 140456679309312 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 12:50:31.764315 140456679309312 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:50:31.764353 140456679309312 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:50:31.764383 140456679309312 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:50:31.764444 140456679309312 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.766714 140456679309312 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:50:31.772250 140456679309312 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.772508 140456679309312 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:50:31.775143 140456679309312 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:50:31.788217 140456679309312 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:50:31.788273 140456679309312 attention.py:418] Single window, no scan.
I0123 12:50:31.788308 140456679309312 transformer_layer.py:389] tlayer: self-attention.
I0123 12:50:31.788339 140456679309312 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.788405 140456679309312 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.788959 140456679309312 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.789035 140456679309312 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.789395 140456679309312 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.790106 140456679309312 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.792612 140456679309312 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.793273 140456679309312 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.793351 140456679309312 transformer_layer.py:468] tlayer: End windows.
I0123 12:50:31.793385 140456679309312 transformer_layer.py:472] tlayer: final FFN.
I0123 12:50:31.793443 140456679309312 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.793573 140456679309312 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:50:31.793689 140456679309312 nn_components.py:325] mlp: activation = None
I0123 12:50:31.793735 140456679309312 nn_components.py:329] mlp: final activation = None
I0123 12:50:31.795645 140456679309312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.795739 140456679309312 nn_components.py:261] mlp: residual
I0123 12:50:31.798246 140456679309312 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.798325 140456679309312 transformer_base.py:443] tbase: final FFN
I0123 12:50:31.798430 140456679309312 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:50:31.800640 140456679309312 nn_components.py:329] mlp: final activation = None
I0123 12:50:31.802575 140456679309312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.802671 140456679309312 nn_components.py:261] mlp: residual
I0123 12:50:31.802962 140456679309312 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.803042 140456679309312 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 12:50:31.803151 140456679309312 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:50:31.803189 140456679309312 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:50:31.803220 140456679309312 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:50:31.803281 140456679309312 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.805532 140456679309312 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:50:31.811002 140456679309312 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.811272 140456679309312 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:50:31.813975 140456679309312 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:50:31.826561 140456679309312 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:50:31.826616 140456679309312 attention.py:418] Single window, no scan.
I0123 12:50:31.826651 140456679309312 transformer_layer.py:389] tlayer: self-attention.
I0123 12:50:31.826681 140456679309312 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.826743 140456679309312 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.827341 140456679309312 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.827416 140456679309312 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.827771 140456679309312 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.828483 140456679309312 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.831003 140456679309312 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.831631 140456679309312 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.831708 140456679309312 transformer_layer.py:468] tlayer: End windows.
I0123 12:50:31.831743 140456679309312 transformer_layer.py:472] tlayer: final FFN.
I0123 12:50:31.831802 140456679309312 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.831934 140456679309312 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:50:31.832045 140456679309312 nn_components.py:325] mlp: activation = None
I0123 12:50:31.832089 140456679309312 nn_components.py:329] mlp: final activation = None
I0123 12:50:31.834000 140456679309312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.834094 140456679309312 nn_components.py:261] mlp: residual
I0123 12:50:31.836575 140456679309312 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.836655 140456679309312 transformer_base.py:443] tbase: final FFN
I0123 12:50:31.836763 140456679309312 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:50:31.838983 140456679309312 nn_components.py:329] mlp: final activation = None
I0123 12:50:31.840856 140456679309312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.840950 140456679309312 nn_components.py:261] mlp: residual
I0123 12:50:31.841236 140456679309312 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.841315 140456679309312 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 12:50:31.841424 140456679309312 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:50:31.841463 140456679309312 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:50:31.841493 140456679309312 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:50:31.841556 140456679309312 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.843829 140456679309312 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:50:31.849336 140456679309312 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.849595 140456679309312 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:50:31.852229 140456679309312 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:50:31.864930 140456679309312 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:50:31.864987 140456679309312 attention.py:418] Single window, no scan.
I0123 12:50:31.865023 140456679309312 transformer_layer.py:389] tlayer: self-attention.
I0123 12:50:31.865053 140456679309312 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.865117 140456679309312 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.865685 140456679309312 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.865761 140456679309312 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.866136 140456679309312 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.866849 140456679309312 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.869361 140456679309312 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.870038 140456679309312 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.870117 140456679309312 transformer_layer.py:468] tlayer: End windows.
I0123 12:50:31.870152 140456679309312 transformer_layer.py:472] tlayer: final FFN.
I0123 12:50:31.870210 140456679309312 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.870344 140456679309312 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:50:31.870457 140456679309312 nn_components.py:325] mlp: activation = None
I0123 12:50:31.870495 140456679309312 nn_components.py:329] mlp: final activation = None
I0123 12:50:31.872389 140456679309312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.872482 140456679309312 nn_components.py:261] mlp: residual
I0123 12:50:31.874915 140456679309312 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.874995 140456679309312 transformer_base.py:443] tbase: final FFN
I0123 12:50:31.875101 140456679309312 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:50:31.877323 140456679309312 nn_components.py:329] mlp: final activation = None
I0123 12:50:31.879273 140456679309312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.879369 140456679309312 nn_components.py:261] mlp: residual
I0123 12:50:31.879659 140456679309312 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.879738 140456679309312 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 12:50:31.879846 140456679309312 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:50:31.879884 140456679309312 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:50:31.879914 140456679309312 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:50:31.879975 140456679309312 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.882257 140456679309312 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:50:31.887766 140456679309312 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.888025 140456679309312 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:50:31.891051 140456679309312 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:50:31.903725 140456679309312 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:50:31.903781 140456679309312 attention.py:418] Single window, no scan.
I0123 12:50:31.903816 140456679309312 transformer_layer.py:389] tlayer: self-attention.
I0123 12:50:31.903845 140456679309312 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.903906 140456679309312 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.904506 140456679309312 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.904583 140456679309312 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.904944 140456679309312 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.905632 140456679309312 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.908153 140456679309312 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.908777 140456679309312 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.908854 140456679309312 transformer_layer.py:468] tlayer: End windows.
I0123 12:50:31.908888 140456679309312 transformer_layer.py:472] tlayer: final FFN.
I0123 12:50:31.908946 140456679309312 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.909075 140456679309312 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:50:31.909184 140456679309312 nn_components.py:325] mlp: activation = None
I0123 12:50:31.909222 140456679309312 nn_components.py:329] mlp: final activation = None
I0123 12:50:31.911131 140456679309312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.911232 140456679309312 nn_components.py:261] mlp: residual
I0123 12:50:31.913724 140456679309312 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.913807 140456679309312 transformer_base.py:443] tbase: final FFN
I0123 12:50:31.913917 140456679309312 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:50:31.916149 140456679309312 nn_components.py:329] mlp: final activation = None
I0123 12:50:31.918122 140456679309312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.918217 140456679309312 nn_components.py:261] mlp: residual
I0123 12:50:31.918505 140456679309312 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.918586 140456679309312 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 12:50:31.918695 140456679309312 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:50:31.918733 140456679309312 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:50:31.918763 140456679309312 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:50:31.918826 140456679309312 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.921173 140456679309312 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:50:31.926706 140456679309312 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.926965 140456679309312 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:50:31.929680 140456679309312 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:50:31.942434 140456679309312 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:50:31.942489 140456679309312 attention.py:418] Single window, no scan.
I0123 12:50:31.942524 140456679309312 transformer_layer.py:389] tlayer: self-attention.
I0123 12:50:31.942554 140456679309312 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.942615 140456679309312 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.943172 140456679309312 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.943248 140456679309312 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.943604 140456679309312 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.944297 140456679309312 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.946828 140456679309312 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.947501 140456679309312 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.947579 140456679309312 transformer_layer.py:468] tlayer: End windows.
I0123 12:50:31.947613 140456679309312 transformer_layer.py:472] tlayer: final FFN.
I0123 12:50:31.947672 140456679309312 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.947804 140456679309312 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:50:31.947915 140456679309312 nn_components.py:325] mlp: activation = None
I0123 12:50:31.947953 140456679309312 nn_components.py:329] mlp: final activation = None
I0123 12:50:31.949851 140456679309312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.949951 140456679309312 nn_components.py:261] mlp: residual
I0123 12:50:31.952408 140456679309312 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.952487 140456679309312 transformer_base.py:443] tbase: final FFN
I0123 12:50:31.952599 140456679309312 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:50:31.954836 140456679309312 nn_components.py:329] mlp: final activation = None
I0123 12:50:31.956778 140456679309312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.956873 140456679309312 nn_components.py:261] mlp: residual
I0123 12:50:31.957163 140456679309312 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.957244 140456679309312 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 12:50:31.957352 140456679309312 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:50:31.957391 140456679309312 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:50:31.957421 140456679309312 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:50:31.957484 140456679309312 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.959765 140456679309312 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:50:31.965240 140456679309312 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.965502 140456679309312 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:50:31.968340 140456679309312 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:50:31.981009 140456679309312 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:50:31.981065 140456679309312 attention.py:418] Single window, no scan.
I0123 12:50:31.981099 140456679309312 transformer_layer.py:389] tlayer: self-attention.
I0123 12:50:31.981129 140456679309312 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.981191 140456679309312 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.981764 140456679309312 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.981842 140456679309312 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.982203 140456679309312 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.982900 140456679309312 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.985478 140456679309312 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.986104 140456679309312 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.986181 140456679309312 transformer_layer.py:468] tlayer: End windows.
I0123 12:50:31.986217 140456679309312 transformer_layer.py:472] tlayer: final FFN.
I0123 12:50:31.986274 140456679309312 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.986399 140456679309312 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:50:31.986506 140456679309312 nn_components.py:325] mlp: activation = None
I0123 12:50:31.986544 140456679309312 nn_components.py:329] mlp: final activation = None
I0123 12:50:31.988426 140456679309312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.988519 140456679309312 nn_components.py:261] mlp: residual
I0123 12:50:31.990951 140456679309312 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.991032 140456679309312 transformer_base.py:443] tbase: final FFN
I0123 12:50:31.991143 140456679309312 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:50:31.993759 140456679309312 nn_components.py:329] mlp: final activation = None
I0123 12:50:31.995647 140456679309312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.995743 140456679309312 nn_components.py:261] mlp: residual
I0123 12:50:31.996033 140456679309312 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:31.996118 140456679309312 decoder_stack.py:344] dstack: Final layernorm.
I0123 12:50:31.998983 140456679309312 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:32.054213 140456679309312 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.054298 140456679309312 decoder_stack.py:333] dstack: autoregressive generator.
I0123 12:50:32.054352 140456679309312 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 12:50:32.054459 140456679309312 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:50:32.054497 140456679309312 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:50:32.054528 140456679309312 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:50:32.054593 140456679309312 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.057000 140456679309312 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:50:32.062481 140456679309312 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.062747 140456679309312 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:50:32.065383 140456679309312 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:50:32.078060 140456679309312 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:50:32.078117 140456679309312 attention.py:418] Single window, no scan.
I0123 12:50:32.078154 140456679309312 transformer_layer.py:389] tlayer: self-attention.
I0123 12:50:32.078185 140456679309312 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.078247 140456679309312 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.078801 140456679309312 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.078877 140456679309312 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.079245 140456679309312 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.079931 140456679309312 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.082545 140456679309312 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.083172 140456679309312 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.083250 140456679309312 transformer_layer.py:468] tlayer: End windows.
I0123 12:50:32.083286 140456679309312 transformer_layer.py:472] tlayer: final FFN.
I0123 12:50:32.083347 140456679309312 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.083478 140456679309312 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:50:32.083597 140456679309312 nn_components.py:325] mlp: activation = None
I0123 12:50:32.083637 140456679309312 nn_components.py:329] mlp: final activation = None
I0123 12:50:32.085508 140456679309312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.085603 140456679309312 nn_components.py:261] mlp: residual
I0123 12:50:32.088230 140456679309312 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.088311 140456679309312 transformer_base.py:443] tbase: final FFN
I0123 12:50:32.088420 140456679309312 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:50:32.090780 140456679309312 nn_components.py:329] mlp: final activation = None
I0123 12:50:32.092651 140456679309312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.092747 140456679309312 nn_components.py:261] mlp: residual
I0123 12:50:32.093038 140456679309312 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.093117 140456679309312 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 12:50:32.093228 140456679309312 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:50:32.093267 140456679309312 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:50:32.093298 140456679309312 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:50:32.093363 140456679309312 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.095656 140456679309312 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:50:32.101078 140456679309312 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.101341 140456679309312 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:50:32.104033 140456679309312 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:50:32.116526 140456679309312 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:50:32.116582 140456679309312 attention.py:418] Single window, no scan.
I0123 12:50:32.116619 140456679309312 transformer_layer.py:389] tlayer: self-attention.
I0123 12:50:32.116649 140456679309312 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.116711 140456679309312 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.117269 140456679309312 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.117346 140456679309312 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.117719 140456679309312 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.118411 140456679309312 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.120976 140456679309312 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.121597 140456679309312 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.121680 140456679309312 transformer_layer.py:468] tlayer: End windows.
I0123 12:50:32.121716 140456679309312 transformer_layer.py:472] tlayer: final FFN.
I0123 12:50:32.121776 140456679309312 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.121903 140456679309312 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:50:32.122013 140456679309312 nn_components.py:325] mlp: activation = None
I0123 12:50:32.122057 140456679309312 nn_components.py:329] mlp: final activation = None
I0123 12:50:32.123915 140456679309312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.124008 140456679309312 nn_components.py:261] mlp: residual
I0123 12:50:32.126442 140456679309312 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.126522 140456679309312 transformer_base.py:443] tbase: final FFN
I0123 12:50:32.126631 140456679309312 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:50:32.128902 140456679309312 nn_components.py:329] mlp: final activation = None
I0123 12:50:32.130772 140456679309312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.130868 140456679309312 nn_components.py:261] mlp: residual
I0123 12:50:32.131155 140456679309312 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.131236 140456679309312 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 12:50:32.131345 140456679309312 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:50:32.131385 140456679309312 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:50:32.131417 140456679309312 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:50:32.131481 140456679309312 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.133750 140456679309312 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:50:32.139151 140456679309312 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.139414 140456679309312 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:50:32.142093 140456679309312 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:50:32.154556 140456679309312 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:50:32.154613 140456679309312 attention.py:418] Single window, no scan.
I0123 12:50:32.154650 140456679309312 transformer_layer.py:389] tlayer: self-attention.
I0123 12:50:32.154682 140456679309312 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.154744 140456679309312 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.155303 140456679309312 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.155380 140456679309312 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.155741 140456679309312 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.156425 140456679309312 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.159411 140456679309312 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.160034 140456679309312 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.160113 140456679309312 transformer_layer.py:468] tlayer: End windows.
I0123 12:50:32.160149 140456679309312 transformer_layer.py:472] tlayer: final FFN.
I0123 12:50:32.160209 140456679309312 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.160338 140456679309312 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:50:32.160446 140456679309312 nn_components.py:325] mlp: activation = None
I0123 12:50:32.160484 140456679309312 nn_components.py:329] mlp: final activation = None
I0123 12:50:32.162368 140456679309312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.162463 140456679309312 nn_components.py:261] mlp: residual
I0123 12:50:32.164892 140456679309312 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.164971 140456679309312 transformer_base.py:443] tbase: final FFN
I0123 12:50:32.165081 140456679309312 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:50:32.167353 140456679309312 nn_components.py:329] mlp: final activation = None
I0123 12:50:32.169213 140456679309312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.169308 140456679309312 nn_components.py:261] mlp: residual
I0123 12:50:32.169599 140456679309312 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.169686 140456679309312 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 12:50:32.169798 140456679309312 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:50:32.169837 140456679309312 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:50:32.169869 140456679309312 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:50:32.169933 140456679309312 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.172201 140456679309312 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:50:32.177633 140456679309312 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.177903 140456679309312 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:50:32.180594 140456679309312 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:50:32.193546 140456679309312 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:50:32.193601 140456679309312 attention.py:418] Single window, no scan.
I0123 12:50:32.193646 140456679309312 transformer_layer.py:389] tlayer: self-attention.
I0123 12:50:32.193686 140456679309312 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.193751 140456679309312 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.194314 140456679309312 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.194390 140456679309312 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.194755 140456679309312 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.195452 140456679309312 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.198039 140456679309312 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.198659 140456679309312 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.198735 140456679309312 transformer_layer.py:468] tlayer: End windows.
I0123 12:50:32.198769 140456679309312 transformer_layer.py:472] tlayer: final FFN.
I0123 12:50:32.198829 140456679309312 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.198956 140456679309312 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:50:32.199064 140456679309312 nn_components.py:325] mlp: activation = None
I0123 12:50:32.199104 140456679309312 nn_components.py:329] mlp: final activation = None
I0123 12:50:32.201005 140456679309312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.201096 140456679309312 nn_components.py:261] mlp: residual
I0123 12:50:32.203541 140456679309312 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.203619 140456679309312 transformer_base.py:443] tbase: final FFN
I0123 12:50:32.203726 140456679309312 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:50:32.206043 140456679309312 nn_components.py:329] mlp: final activation = None
I0123 12:50:32.207928 140456679309312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.208022 140456679309312 nn_components.py:261] mlp: residual
I0123 12:50:32.208312 140456679309312 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.208392 140456679309312 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 12:50:32.208501 140456679309312 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:50:32.208539 140456679309312 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:50:32.208569 140456679309312 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:50:32.208632 140456679309312 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.210923 140456679309312 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:50:32.216380 140456679309312 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.216637 140456679309312 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:50:32.219384 140456679309312 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:50:32.232076 140456679309312 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:50:32.232131 140456679309312 attention.py:418] Single window, no scan.
I0123 12:50:32.232166 140456679309312 transformer_layer.py:389] tlayer: self-attention.
I0123 12:50:32.232197 140456679309312 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.232262 140456679309312 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.232820 140456679309312 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.232895 140456679309312 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.233259 140456679309312 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.233962 140456679309312 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.236549 140456679309312 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.237169 140456679309312 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.237245 140456679309312 transformer_layer.py:468] tlayer: End windows.
I0123 12:50:32.237279 140456679309312 transformer_layer.py:472] tlayer: final FFN.
I0123 12:50:32.237337 140456679309312 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.237462 140456679309312 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:50:32.237572 140456679309312 nn_components.py:325] mlp: activation = None
I0123 12:50:32.237610 140456679309312 nn_components.py:329] mlp: final activation = None
I0123 12:50:32.239551 140456679309312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.239650 140456679309312 nn_components.py:261] mlp: residual
I0123 12:50:32.242123 140456679309312 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.242203 140456679309312 transformer_base.py:443] tbase: final FFN
I0123 12:50:32.242311 140456679309312 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:50:32.244636 140456679309312 nn_components.py:329] mlp: final activation = None
I0123 12:50:32.246536 140456679309312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.246630 140456679309312 nn_components.py:261] mlp: residual
I0123 12:50:32.246921 140456679309312 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.247001 140456679309312 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 12:50:32.247109 140456679309312 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:50:32.247148 140456679309312 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:50:32.247179 140456679309312 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:50:32.247242 140456679309312 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.249531 140456679309312 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:50:32.255054 140456679309312 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.255316 140456679309312 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:50:32.258097 140456679309312 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:50:32.270814 140456679309312 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:50:32.270868 140456679309312 attention.py:418] Single window, no scan.
I0123 12:50:32.270903 140456679309312 transformer_layer.py:389] tlayer: self-attention.
I0123 12:50:32.270934 140456679309312 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.270997 140456679309312 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.271553 140456679309312 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.271627 140456679309312 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.271991 140456679309312 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.272692 140456679309312 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.275696 140456679309312 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.276324 140456679309312 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.276401 140456679309312 transformer_layer.py:468] tlayer: End windows.
I0123 12:50:32.276436 140456679309312 transformer_layer.py:472] tlayer: final FFN.
I0123 12:50:32.276495 140456679309312 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.276621 140456679309312 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:50:32.276730 140456679309312 nn_components.py:325] mlp: activation = None
I0123 12:50:32.276767 140456679309312 nn_components.py:329] mlp: final activation = None
I0123 12:50:32.278692 140456679309312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.278791 140456679309312 nn_components.py:261] mlp: residual
I0123 12:50:32.281221 140456679309312 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.281299 140456679309312 transformer_base.py:443] tbase: final FFN
I0123 12:50:32.281407 140456679309312 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:50:32.283730 140456679309312 nn_components.py:329] mlp: final activation = None
I0123 12:50:32.285620 140456679309312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.285720 140456679309312 nn_components.py:261] mlp: residual
I0123 12:50:32.286009 140456679309312 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.286089 140456679309312 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 12:50:32.286197 140456679309312 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:50:32.286237 140456679309312 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:50:32.286268 140456679309312 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:50:32.286331 140456679309312 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.288600 140456679309312 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:50:32.294185 140456679309312 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.294446 140456679309312 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:50:32.297207 140456679309312 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:50:32.309911 140456679309312 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:50:32.309966 140456679309312 attention.py:418] Single window, no scan.
I0123 12:50:32.310001 140456679309312 transformer_layer.py:389] tlayer: self-attention.
I0123 12:50:32.310032 140456679309312 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.310094 140456679309312 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.310657 140456679309312 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.310732 140456679309312 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.311094 140456679309312 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.311790 140456679309312 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.314406 140456679309312 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.315037 140456679309312 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.315114 140456679309312 transformer_layer.py:468] tlayer: End windows.
I0123 12:50:32.315148 140456679309312 transformer_layer.py:472] tlayer: final FFN.
I0123 12:50:32.315207 140456679309312 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.315334 140456679309312 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:50:32.315445 140456679309312 nn_components.py:325] mlp: activation = None
I0123 12:50:32.315484 140456679309312 nn_components.py:329] mlp: final activation = None
I0123 12:50:32.317402 140456679309312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.317494 140456679309312 nn_components.py:261] mlp: residual
I0123 12:50:32.319959 140456679309312 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.320044 140456679309312 transformer_base.py:443] tbase: final FFN
I0123 12:50:32.320152 140456679309312 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:50:32.322471 140456679309312 nn_components.py:329] mlp: final activation = None
I0123 12:50:32.324348 140456679309312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.324441 140456679309312 nn_components.py:261] mlp: residual
I0123 12:50:32.324730 140456679309312 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.324811 140456679309312 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 12:50:32.324918 140456679309312 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:50:32.324957 140456679309312 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:50:32.324988 140456679309312 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:50:32.325050 140456679309312 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.327331 140456679309312 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:50:32.332821 140456679309312 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.333084 140456679309312 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:50:32.335824 140456679309312 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:50:32.348503 140456679309312 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:50:32.348557 140456679309312 attention.py:418] Single window, no scan.
I0123 12:50:32.348592 140456679309312 transformer_layer.py:389] tlayer: self-attention.
I0123 12:50:32.348623 140456679309312 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.348683 140456679309312 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.349247 140456679309312 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.349322 140456679309312 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.349690 140456679309312 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.350383 140456679309312 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.352958 140456679309312 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.353584 140456679309312 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.353669 140456679309312 transformer_layer.py:468] tlayer: End windows.
I0123 12:50:32.353704 140456679309312 transformer_layer.py:472] tlayer: final FFN.
I0123 12:50:32.353763 140456679309312 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.353892 140456679309312 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:50:32.354005 140456679309312 nn_components.py:325] mlp: activation = None
I0123 12:50:32.354043 140456679309312 nn_components.py:329] mlp: final activation = None
I0123 12:50:32.355908 140456679309312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.356000 140456679309312 nn_components.py:261] mlp: residual
I0123 12:50:32.358434 140456679309312 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.358696 140456679309312 transformer_base.py:443] tbase: final FFN
I0123 12:50:32.358810 140456679309312 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:50:32.361100 140456679309312 nn_components.py:329] mlp: final activation = None
I0123 12:50:32.362970 140456679309312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.363064 140456679309312 nn_components.py:261] mlp: residual
I0123 12:50:32.363352 140456679309312 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.363433 140456679309312 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 12:50:32.363540 140456679309312 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:50:32.363579 140456679309312 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:50:32.363610 140456679309312 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:50:32.363673 140456679309312 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.365927 140456679309312 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:50:32.371379 140456679309312 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.371639 140456679309312 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:50:32.374330 140456679309312 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:50:32.386880 140456679309312 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:50:32.386934 140456679309312 attention.py:418] Single window, no scan.
I0123 12:50:32.386970 140456679309312 transformer_layer.py:389] tlayer: self-attention.
I0123 12:50:32.387001 140456679309312 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.387061 140456679309312 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.387623 140456679309312 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.387699 140456679309312 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.388052 140456679309312 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.388748 140456679309312 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.391722 140456679309312 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.392347 140456679309312 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.392424 140456679309312 transformer_layer.py:468] tlayer: End windows.
I0123 12:50:32.392458 140456679309312 transformer_layer.py:472] tlayer: final FFN.
I0123 12:50:32.392517 140456679309312 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.392644 140456679309312 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:50:32.392752 140456679309312 nn_components.py:325] mlp: activation = None
I0123 12:50:32.392790 140456679309312 nn_components.py:329] mlp: final activation = None
I0123 12:50:32.394682 140456679309312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.394775 140456679309312 nn_components.py:261] mlp: residual
I0123 12:50:32.397217 140456679309312 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.397301 140456679309312 transformer_base.py:443] tbase: final FFN
I0123 12:50:32.397410 140456679309312 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:50:32.399726 140456679309312 nn_components.py:329] mlp: final activation = None
I0123 12:50:32.401612 140456679309312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.401712 140456679309312 nn_components.py:261] mlp: residual
I0123 12:50:32.402001 140456679309312 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.402081 140456679309312 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 12:50:32.402189 140456679309312 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:50:32.402226 140456679309312 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:50:32.402256 140456679309312 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:50:32.402318 140456679309312 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.404570 140456679309312 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:50:32.410038 140456679309312 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.410296 140456679309312 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:50:32.413000 140456679309312 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:50:32.425663 140456679309312 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:50:32.425717 140456679309312 attention.py:418] Single window, no scan.
I0123 12:50:32.425751 140456679309312 transformer_layer.py:389] tlayer: self-attention.
I0123 12:50:32.425782 140456679309312 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.425847 140456679309312 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.426409 140456679309312 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.426483 140456679309312 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.426841 140456679309312 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.427533 140456679309312 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.430104 140456679309312 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.430728 140456679309312 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.430804 140456679309312 transformer_layer.py:468] tlayer: End windows.
I0123 12:50:32.430839 140456679309312 transformer_layer.py:472] tlayer: final FFN.
I0123 12:50:32.430896 140456679309312 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.431022 140456679309312 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:50:32.431130 140456679309312 nn_components.py:325] mlp: activation = None
I0123 12:50:32.431168 140456679309312 nn_components.py:329] mlp: final activation = None
I0123 12:50:32.433474 140456679309312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.433568 140456679309312 nn_components.py:261] mlp: residual
I0123 12:50:32.435997 140456679309312 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.436075 140456679309312 transformer_base.py:443] tbase: final FFN
I0123 12:50:32.436186 140456679309312 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:50:32.438470 140456679309312 nn_components.py:329] mlp: final activation = None
I0123 12:50:32.440322 140456679309312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.440415 140456679309312 nn_components.py:261] mlp: residual
I0123 12:50:32.440704 140456679309312 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.440783 140456679309312 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 12:50:32.440889 140456679309312 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:50:32.440927 140456679309312 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:50:32.440958 140456679309312 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:50:32.441019 140456679309312 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.443283 140456679309312 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:50:32.448783 140456679309312 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.449047 140456679309312 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:50:32.451794 140456679309312 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:50:32.464504 140456679309312 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:50:32.464558 140456679309312 attention.py:418] Single window, no scan.
I0123 12:50:32.464593 140456679309312 transformer_layer.py:389] tlayer: self-attention.
I0123 12:50:32.464624 140456679309312 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.464686 140456679309312 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.465251 140456679309312 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.465327 140456679309312 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.465696 140456679309312 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.466405 140456679309312 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.469002 140456679309312 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.469626 140456679309312 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.469709 140456679309312 transformer_layer.py:468] tlayer: End windows.
I0123 12:50:32.469743 140456679309312 transformer_layer.py:472] tlayer: final FFN.
I0123 12:50:32.469800 140456679309312 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.469931 140456679309312 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:50:32.470039 140456679309312 nn_components.py:325] mlp: activation = None
I0123 12:50:32.470077 140456679309312 nn_components.py:329] mlp: final activation = None
I0123 12:50:32.471976 140456679309312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.472067 140456679309312 nn_components.py:261] mlp: residual
I0123 12:50:32.474563 140456679309312 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.474642 140456679309312 transformer_base.py:443] tbase: final FFN
I0123 12:50:32.474747 140456679309312 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:50:32.477044 140456679309312 nn_components.py:329] mlp: final activation = None
I0123 12:50:32.478919 140456679309312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.479012 140456679309312 nn_components.py:261] mlp: residual
I0123 12:50:32.479302 140456679309312 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.479382 140456679309312 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 12:50:32.479489 140456679309312 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:50:32.479526 140456679309312 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:50:32.479556 140456679309312 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:50:32.479618 140456679309312 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.481896 140456679309312 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:50:32.487360 140456679309312 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.487623 140456679309312 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:50:32.490353 140456679309312 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:50:32.502965 140456679309312 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:50:32.503020 140456679309312 attention.py:418] Single window, no scan.
I0123 12:50:32.503056 140456679309312 transformer_layer.py:389] tlayer: self-attention.
I0123 12:50:32.503087 140456679309312 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.503148 140456679309312 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.503704 140456679309312 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.503780 140456679309312 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.504150 140456679309312 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.504855 140456679309312 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.507807 140456679309312 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.508428 140456679309312 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.508506 140456679309312 transformer_layer.py:468] tlayer: End windows.
I0123 12:50:32.508541 140456679309312 transformer_layer.py:472] tlayer: final FFN.
I0123 12:50:32.508599 140456679309312 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.508724 140456679309312 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:50:32.508834 140456679309312 nn_components.py:325] mlp: activation = None
I0123 12:50:32.508871 140456679309312 nn_components.py:329] mlp: final activation = None
I0123 12:50:32.510764 140456679309312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.510856 140456679309312 nn_components.py:261] mlp: residual
I0123 12:50:32.513267 140456679309312 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.513345 140456679309312 transformer_base.py:443] tbase: final FFN
I0123 12:50:32.513450 140456679309312 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:50:32.515749 140456679309312 nn_components.py:329] mlp: final activation = None
I0123 12:50:32.517630 140456679309312 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.517732 140456679309312 nn_components.py:261] mlp: residual
I0123 12:50:32.518020 140456679309312 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:32.518104 140456679309312 decoder_stack.py:344] dstack: Final layernorm.
I0123 12:50:32.520965 140456679309312 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:36.940899 140456679309312 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
I0123 12:50:37.468712 140456679309312 training_loop.py:409] No working directory specified.
I0123 12:50:37.468844 140456679309312 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0123 12:50:37.469666 140456679309312 checkpoints.py:1062] Restoring legacy Flax checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0123 12:50:40.793456 140456679309312 training_loop.py:447] Only restoring trainable parameters.
I0123 12:50:40.794169 140456679309312 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0123 12:50:40.794250 140456679309312 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0123 12:50:40.794298 140456679309312 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:50:40.794341 140456679309312 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:50:40.794382 140456679309312 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:40.794421 140456679309312 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:50:40.794459 140456679309312 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:40.794497 140456679309312 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:40.794534 140456679309312 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:50:40.794570 140456679309312 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:50:40.794605 140456679309312 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:40.794641 140456679309312 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:50:40.794678 140456679309312 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:50:40.794714 140456679309312 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:50:40.794751 140456679309312 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:40.794788 140456679309312 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:50:40.794825 140456679309312 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:40.794861 140456679309312 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:40.794896 140456679309312 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:50:40.794931 140456679309312 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:50:40.794989 140456679309312 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:40.795028 140456679309312 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:50:40.795065 140456679309312 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:50:40.795101 140456679309312 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:50:40.795137 140456679309312 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:40.795172 140456679309312 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:50:40.795208 140456679309312 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:40.795243 140456679309312 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:40.795278 140456679309312 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:50:40.795312 140456679309312 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:50:40.795348 140456679309312 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:40.795383 140456679309312 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:50:40.795417 140456679309312 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:50:40.795452 140456679309312 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:50:40.795487 140456679309312 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:40.795522 140456679309312 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:50:40.795557 140456679309312 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:40.795591 140456679309312 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:40.795626 140456679309312 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:50:40.795661 140456679309312 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:50:40.795696 140456679309312 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:40.795731 140456679309312 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:50:40.795766 140456679309312 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:50:40.795801 140456679309312 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:50:40.795836 140456679309312 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:40.795872 140456679309312 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:50:40.795915 140456679309312 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:40.795953 140456679309312 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:40.795989 140456679309312 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:50:40.796024 140456679309312 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:50:40.796060 140456679309312 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:40.796096 140456679309312 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:50:40.796131 140456679309312 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:50:40.796166 140456679309312 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:50:40.796201 140456679309312 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:40.796237 140456679309312 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:50:40.796272 140456679309312 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:40.796308 140456679309312 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:40.796344 140456679309312 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:50:40.796380 140456679309312 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:50:40.796416 140456679309312 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:40.796452 140456679309312 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:50:40.796487 140456679309312 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:50:40.796523 140456679309312 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:50:40.796559 140456679309312 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:40.796595 140456679309312 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:50:40.796631 140456679309312 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:40.796667 140456679309312 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:40.796702 140456679309312 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:50:40.796737 140456679309312 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:50:40.796772 140456679309312 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:40.796807 140456679309312 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:50:40.796842 140456679309312 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:50:40.796883 140456679309312 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:50:40.796920 140456679309312 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:40.796960 140456679309312 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:50:40.796997 140456679309312 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:40.797033 140456679309312 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:40.797069 140456679309312 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:50:40.797105 140456679309312 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:50:40.797141 140456679309312 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:40.797177 140456679309312 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:50:40.797212 140456679309312 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:50:40.797248 140456679309312 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:50:40.797284 140456679309312 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:40.797320 140456679309312 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:50:40.797355 140456679309312 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:40.797389 140456679309312 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:40.797424 140456679309312 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:50:40.797458 140456679309312 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:50:40.797493 140456679309312 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:40.797528 140456679309312 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:50:40.797562 140456679309312 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:50:40.797597 140456679309312 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:50:40.797632 140456679309312 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:40.797683 140456679309312 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:50:40.797719 140456679309312 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:40.797756 140456679309312 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:40.797796 140456679309312 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:50:40.797831 140456679309312 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:50:40.797871 140456679309312 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:40.797908 140456679309312 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:50:40.797944 140456679309312 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:50:40.797978 140456679309312 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:50:40.798014 140456679309312 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:40.798049 140456679309312 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:50:40.798084 140456679309312 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:40.798118 140456679309312 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:40.798153 140456679309312 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:50:40.798187 140456679309312 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:50:40.798222 140456679309312 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:40.798257 140456679309312 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:50:40.798292 140456679309312 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:50:40.798329 140456679309312 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:50:40.798364 140456679309312 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:40.798399 140456679309312 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:50:40.798434 140456679309312 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:40.798469 140456679309312 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:40.798504 140456679309312 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:50:40.798538 140456679309312 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:50:40.798573 140456679309312 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:40.798608 140456679309312 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:50:40.798636 140456679309312 training_loop.py:725] Total parameters: 152072288
I0123 12:50:40.798844 140456679309312 training_loop.py:739] Total state size: 0
I0123 12:50:40.820753 140456679309312 training_loop.py:492] Training loop: creating task for mode beam_search
I0123 12:50:40.821040 140456679309312 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0123 12:50:40.821425 140456679309312 training_loop.py:652] Compiling mode beam_search with jit.
I0123 12:50:40.821782 140456679309312 training_loop.py:89] registering functions: dict_keys([])
I0123 12:50:40.842373 140456679309312 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = midpoint e b a; f = lc_tangent f b d, on_line f e d ? eqangle c b c e c f c a
I0123 12:50:40.902822 140456679309312 ddar.py:60] Depth 1/1000 time = 0.046846628189086914
I0123 12:50:41.026005 140456679309312 ddar.py:60] Depth 2/1000 time = 0.12309479713439941
I0123 12:50:41.284784 140456679309312 ddar.py:60] Depth 3/1000 time = 0.2586936950683594
I0123 12:50:41.573722 140456679309312 ddar.py:60] Depth 4/1000 time = 0.2887842655181885
I0123 12:50:42.014611 140456679309312 ddar.py:60] Depth 5/1000 time = 0.4407181739807129
I0123 12:50:42.305752 140456679309312 ddar.py:60] Depth 6/1000 time = 0.2906765937805176
I0123 12:50:42.597545 140456679309312 ddar.py:60] Depth 7/1000 time = 0.2877829074859619
I0123 12:50:42.599617 140456679309312 alphageometry.py:191] 
==========================
 * From theorem premises:
A B C D E F : Points
DA = DB [00]
DC = DB [01]
EA = EB [02]
A,B,E are collinear [03]
BF  BD [04]
E,F,D are collinear [05]

 * Auxiliary Constructions:
: Points


 * Proof steps:
001. EA = EB [02] & DA = DB [00]   EA:EB = DA:DB [06]
002. EA = EB [02] & DA = DB [00]   AB  DE [07]
003. EA:EB = DA:DB [06] & A,B,E are collinear [03]   ADE = EDB [08]
004. DC = DB [01]   DBC = BCD [09]
005. DC = DB [01] & DB = DA [00]   DA = DC [10]
006. DA = DC [10]   DAC = ACD [11]
007. A,B,E are collinear [03] & BF  BD [04] & AB  DE [07]   AED = FBD [12]
008. E,F,D are collinear [05] & ADE = EDB [08]   ADE = FDB [13]
009. AED = FBD [12] & ADE = FDB [13] (Similar Triangles)  DA:DE = DF:DB [14]
010. DA:DE = DF:DB [14] & DB = DA [00] & CD = BD [01]   DE:DC = DC:DF [15]
011. E,F,D are collinear [05]   EDC = FDC [16]
012. DE:DC = DC:DF [15] & EDC = FDC [16] (Similar Triangles)  DEC = FCD [17]
013. ADE = EDB [08] & DBC = BCD [09] & DAC = ACD [11] & DEC = FCD [17] (Angle chase)  BCE = FCA
==========================

