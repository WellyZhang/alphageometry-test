I0123 12:36:02.945953 140367016284160 inference_utils.py:69] Parsing gin configuration.
I0123 12:36:02.946056 140367016284160 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0123 12:36:02.946265 140367016284160 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0123 12:36:02.946301 140367016284160 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0123 12:36:02.946331 140367016284160 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0123 12:36:02.946359 140367016284160 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0123 12:36:02.946387 140367016284160 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0123 12:36:02.946415 140367016284160 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0123 12:36:02.946443 140367016284160 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0123 12:36:02.946470 140367016284160 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=32
I0123 12:36:02.946498 140367016284160 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0123 12:36:02.946525 140367016284160 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0123 12:36:02.946573 140367016284160 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0123 12:36:02.946713 140367016284160 resource_reader.py:55] Path not found: base_htrans.gin
I0123 12:36:02.946924 140367016284160 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0123 12:36:02.947025 140367016284160 resource_reader.py:55] Path not found: trainer_configuration.gin
I0123 12:36:02.953477 140367016284160 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0123 12:36:02.953596 140367016284160 resource_reader.py:55] Path not found: size/medium_150M.gin
I0123 12:36:02.953949 140367016284160 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0123 12:36:02.954071 140367016284160 resource_reader.py:55] Path not found: options/positions_t5.gin
I0123 12:36:02.954362 140367016284160 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0123 12:36:02.954465 140367016284160 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0123 12:36:02.954883 140367016284160 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0123 12:36:02.954984 140367016284160 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0123 12:36:02.958714 140367016284160 training_loop.py:334] ==== Training loop: initializing model ====
I0123 12:36:03.051393 140367016284160 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0123 12:36:03.052106 140367016284160 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0123 12:36:03.058884 140367016284160 training_loop.py:335] Process 0 of 1
I0123 12:36:03.058939 140367016284160 training_loop.py:336] Local device count = 1
I0123 12:36:03.058979 140367016284160 training_loop.py:337] Number of replicas = 1
I0123 12:36:03.059013 140367016284160 training_loop.py:339] Using random number seed 42
I0123 12:36:03.521092 140367016284160 training_loop.py:359] Initializing the model.
I0123 12:36:03.900731 140367016284160 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:03.901049 140367016284160 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 12:36:03.901165 140367016284160 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:36:03.901308 140367016284160 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:36:03.901395 140367016284160 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:36:03.901484 140367016284160 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:36:03.901559 140367016284160 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:36:03.901632 140367016284160 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:36:03.901716 140367016284160 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:36:03.901789 140367016284160 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:36:03.901860 140367016284160 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:36:03.901931 140367016284160 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:36:03.902001 140367016284160 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:36:03.902076 140367016284160 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:36:03.902118 140367016284160 attention.py:418] Single window, no scan.
I0123 12:36:03.902166 140367016284160 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 12:36:03.902291 140367016284160 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:36:03.902333 140367016284160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:36:03.902366 140367016284160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:36:03.904551 140367016284160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:03.910163 140367016284160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:36:03.921300 140367016284160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:03.921592 140367016284160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:36:03.926110 140367016284160 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:36:03.937744 140367016284160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:36:03.937811 140367016284160 attention.py:418] Single window, no scan.
I0123 12:36:03.937850 140367016284160 transformer_layer.py:389] tlayer: self-attention.
I0123 12:36:03.937886 140367016284160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:03.937952 140367016284160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:03.939207 140367016284160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:03.939289 140367016284160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:03.940024 140367016284160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:03.942555 140367016284160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:03.948987 140367016284160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:03.950275 140367016284160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:03.950365 140367016284160 transformer_layer.py:468] tlayer: End windows.
I0123 12:36:03.950402 140367016284160 transformer_layer.py:472] tlayer: final FFN.
I0123 12:36:03.950469 140367016284160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:03.950605 140367016284160 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:36:03.950961 140367016284160 nn_components.py:325] mlp: activation = None
I0123 12:36:03.951013 140367016284160 nn_components.py:329] mlp: final activation = None
I0123 12:36:03.952981 140367016284160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:03.953084 140367016284160 nn_components.py:261] mlp: residual
I0123 12:36:03.956127 140367016284160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:03.956212 140367016284160 transformer_base.py:443] tbase: final FFN
I0123 12:36:03.956657 140367016284160 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:36:03.967061 140367016284160 nn_components.py:329] mlp: final activation = None
I0123 12:36:03.976244 140367016284160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:03.976345 140367016284160 nn_components.py:261] mlp: residual
I0123 12:36:03.976646 140367016284160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:03.976730 140367016284160 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 12:36:03.976846 140367016284160 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:36:03.976887 140367016284160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:36:03.976920 140367016284160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:36:03.978949 140367016284160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:03.981388 140367016284160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:36:03.987154 140367016284160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:03.987419 140367016284160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:36:03.990077 140367016284160 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:36:03.993959 140367016284160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:36:03.994015 140367016284160 attention.py:418] Single window, no scan.
I0123 12:36:03.994052 140367016284160 transformer_layer.py:389] tlayer: self-attention.
I0123 12:36:03.994084 140367016284160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:03.994148 140367016284160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:03.994725 140367016284160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:03.994801 140367016284160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:03.995171 140367016284160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:03.995956 140367016284160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:03.998492 140367016284160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:03.999174 140367016284160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:03.999253 140367016284160 transformer_layer.py:468] tlayer: End windows.
I0123 12:36:03.999289 140367016284160 transformer_layer.py:472] tlayer: final FFN.
I0123 12:36:03.999349 140367016284160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:03.999479 140367016284160 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:36:03.999809 140367016284160 nn_components.py:325] mlp: activation = None
I0123 12:36:03.999853 140367016284160 nn_components.py:329] mlp: final activation = None
I0123 12:36:04.002357 140367016284160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.002529 140367016284160 nn_components.py:261] mlp: residual
I0123 12:36:04.005235 140367016284160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.005316 140367016284160 transformer_base.py:443] tbase: final FFN
I0123 12:36:04.005830 140367016284160 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:36:04.008156 140367016284160 nn_components.py:329] mlp: final activation = None
I0123 12:36:04.010071 140367016284160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.010167 140367016284160 nn_components.py:261] mlp: residual
I0123 12:36:04.010461 140367016284160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.010545 140367016284160 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 12:36:04.010658 140367016284160 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:36:04.010697 140367016284160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:36:04.010729 140367016284160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:36:04.012684 140367016284160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.015138 140367016284160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:36:04.021308 140367016284160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.021589 140367016284160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:36:04.024292 140367016284160 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:36:04.028245 140367016284160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:36:04.028306 140367016284160 attention.py:418] Single window, no scan.
I0123 12:36:04.028344 140367016284160 transformer_layer.py:389] tlayer: self-attention.
I0123 12:36:04.028376 140367016284160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.028440 140367016284160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.029019 140367016284160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.029095 140367016284160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.029465 140367016284160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.030244 140367016284160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.032855 140367016284160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.033677 140367016284160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.033760 140367016284160 transformer_layer.py:468] tlayer: End windows.
I0123 12:36:04.033797 140367016284160 transformer_layer.py:472] tlayer: final FFN.
I0123 12:36:04.033860 140367016284160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.033989 140367016284160 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:36:04.034477 140367016284160 nn_components.py:325] mlp: activation = None
I0123 12:36:04.034521 140367016284160 nn_components.py:329] mlp: final activation = None
I0123 12:36:04.036485 140367016284160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.036580 140367016284160 nn_components.py:261] mlp: residual
I0123 12:36:04.039207 140367016284160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.039294 140367016284160 transformer_base.py:443] tbase: final FFN
I0123 12:36:04.039735 140367016284160 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:36:04.042055 140367016284160 nn_components.py:329] mlp: final activation = None
I0123 12:36:04.044001 140367016284160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.044096 140367016284160 nn_components.py:261] mlp: residual
I0123 12:36:04.044395 140367016284160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.044477 140367016284160 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 12:36:04.044590 140367016284160 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:36:04.044630 140367016284160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:36:04.044662 140367016284160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:36:04.046610 140367016284160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.049047 140367016284160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:36:04.054771 140367016284160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.055045 140367016284160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:36:04.057791 140367016284160 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:36:04.061631 140367016284160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:36:04.061695 140367016284160 attention.py:418] Single window, no scan.
I0123 12:36:04.061732 140367016284160 transformer_layer.py:389] tlayer: self-attention.
I0123 12:36:04.061765 140367016284160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.061828 140367016284160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.062400 140367016284160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.062477 140367016284160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.062849 140367016284160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.063650 140367016284160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.066275 140367016284160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.066921 140367016284160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.067000 140367016284160 transformer_layer.py:468] tlayer: End windows.
I0123 12:36:04.067037 140367016284160 transformer_layer.py:472] tlayer: final FFN.
I0123 12:36:04.067099 140367016284160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.067230 140367016284160 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:36:04.067563 140367016284160 nn_components.py:325] mlp: activation = None
I0123 12:36:04.067608 140367016284160 nn_components.py:329] mlp: final activation = None
I0123 12:36:04.069560 140367016284160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.069660 140367016284160 nn_components.py:261] mlp: residual
I0123 12:36:04.072282 140367016284160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.072368 140367016284160 transformer_base.py:443] tbase: final FFN
I0123 12:36:04.072818 140367016284160 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:36:04.075135 140367016284160 nn_components.py:329] mlp: final activation = None
I0123 12:36:04.077169 140367016284160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.077267 140367016284160 nn_components.py:261] mlp: residual
I0123 12:36:04.077573 140367016284160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.077661 140367016284160 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 12:36:04.077776 140367016284160 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:36:04.077816 140367016284160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:36:04.077847 140367016284160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:36:04.079746 140367016284160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.082186 140367016284160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:36:04.087948 140367016284160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.088212 140367016284160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:36:04.090973 140367016284160 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:36:04.094791 140367016284160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:36:04.094847 140367016284160 attention.py:418] Single window, no scan.
I0123 12:36:04.094884 140367016284160 transformer_layer.py:389] tlayer: self-attention.
I0123 12:36:04.094917 140367016284160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.094981 140367016284160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.095920 140367016284160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.096000 140367016284160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.096371 140367016284160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.097158 140367016284160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.099718 140367016284160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.100348 140367016284160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.100433 140367016284160 transformer_layer.py:468] tlayer: End windows.
I0123 12:36:04.100470 140367016284160 transformer_layer.py:472] tlayer: final FFN.
I0123 12:36:04.100533 140367016284160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.100671 140367016284160 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:36:04.101008 140367016284160 nn_components.py:325] mlp: activation = None
I0123 12:36:04.101052 140367016284160 nn_components.py:329] mlp: final activation = None
I0123 12:36:04.103007 140367016284160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.103102 140367016284160 nn_components.py:261] mlp: residual
I0123 12:36:04.105705 140367016284160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.105786 140367016284160 transformer_base.py:443] tbase: final FFN
I0123 12:36:04.106230 140367016284160 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:36:04.108598 140367016284160 nn_components.py:329] mlp: final activation = None
I0123 12:36:04.110531 140367016284160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.110628 140367016284160 nn_components.py:261] mlp: residual
I0123 12:36:04.110930 140367016284160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.111012 140367016284160 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 12:36:04.111124 140367016284160 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:36:04.111163 140367016284160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:36:04.111195 140367016284160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:36:04.113075 140367016284160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.115599 140367016284160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:36:04.121344 140367016284160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.121604 140367016284160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:36:04.124292 140367016284160 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:36:04.128166 140367016284160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:36:04.128223 140367016284160 attention.py:418] Single window, no scan.
I0123 12:36:04.128264 140367016284160 transformer_layer.py:389] tlayer: self-attention.
I0123 12:36:04.128296 140367016284160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.128360 140367016284160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.128925 140367016284160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.129002 140367016284160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.129375 140367016284160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.130182 140367016284160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.132714 140367016284160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.133343 140367016284160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.133420 140367016284160 transformer_layer.py:468] tlayer: End windows.
I0123 12:36:04.133456 140367016284160 transformer_layer.py:472] tlayer: final FFN.
I0123 12:36:04.133516 140367016284160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.133655 140367016284160 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:36:04.133984 140367016284160 nn_components.py:325] mlp: activation = None
I0123 12:36:04.134027 140367016284160 nn_components.py:329] mlp: final activation = None
I0123 12:36:04.136395 140367016284160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.136491 140367016284160 nn_components.py:261] mlp: residual
I0123 12:36:04.139061 140367016284160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.139143 140367016284160 transformer_base.py:443] tbase: final FFN
I0123 12:36:04.139586 140367016284160 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:36:04.141966 140367016284160 nn_components.py:329] mlp: final activation = None
I0123 12:36:04.143939 140367016284160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.144035 140367016284160 nn_components.py:261] mlp: residual
I0123 12:36:04.144334 140367016284160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.144419 140367016284160 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 12:36:04.144531 140367016284160 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:36:04.144572 140367016284160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:36:04.144603 140367016284160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:36:04.146555 140367016284160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.148987 140367016284160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:36:04.154729 140367016284160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.154998 140367016284160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:36:04.157681 140367016284160 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:36:04.161570 140367016284160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:36:04.161625 140367016284160 attention.py:418] Single window, no scan.
I0123 12:36:04.161671 140367016284160 transformer_layer.py:389] tlayer: self-attention.
I0123 12:36:04.161705 140367016284160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.161769 140367016284160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.162342 140367016284160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.162418 140367016284160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.162786 140367016284160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.163591 140367016284160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.166119 140367016284160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.166801 140367016284160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.166881 140367016284160 transformer_layer.py:468] tlayer: End windows.
I0123 12:36:04.166917 140367016284160 transformer_layer.py:472] tlayer: final FFN.
I0123 12:36:04.166979 140367016284160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.167117 140367016284160 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:36:04.167448 140367016284160 nn_components.py:325] mlp: activation = None
I0123 12:36:04.167493 140367016284160 nn_components.py:329] mlp: final activation = None
I0123 12:36:04.169450 140367016284160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.169544 140367016284160 nn_components.py:261] mlp: residual
I0123 12:36:04.172102 140367016284160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.172185 140367016284160 transformer_base.py:443] tbase: final FFN
I0123 12:36:04.172975 140367016284160 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:36:04.175330 140367016284160 nn_components.py:329] mlp: final activation = None
I0123 12:36:04.177287 140367016284160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.177390 140367016284160 nn_components.py:261] mlp: residual
I0123 12:36:04.177695 140367016284160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.177780 140367016284160 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 12:36:04.177893 140367016284160 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:36:04.177933 140367016284160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:36:04.177965 140367016284160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:36:04.317220 140367016284160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.320108 140367016284160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:36:04.326085 140367016284160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.326384 140367016284160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:36:04.329200 140367016284160 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:36:04.333195 140367016284160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:36:04.333254 140367016284160 attention.py:418] Single window, no scan.
I0123 12:36:04.333293 140367016284160 transformer_layer.py:389] tlayer: self-attention.
I0123 12:36:04.333327 140367016284160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.333400 140367016284160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.334037 140367016284160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.334117 140367016284160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.334495 140367016284160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.335294 140367016284160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.338117 140367016284160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.338768 140367016284160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.338847 140367016284160 transformer_layer.py:468] tlayer: End windows.
I0123 12:36:04.338884 140367016284160 transformer_layer.py:472] tlayer: final FFN.
I0123 12:36:04.338947 140367016284160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.339077 140367016284160 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:36:04.339602 140367016284160 nn_components.py:325] mlp: activation = None
I0123 12:36:04.339647 140367016284160 nn_components.py:329] mlp: final activation = None
I0123 12:36:04.341600 140367016284160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.341703 140367016284160 nn_components.py:261] mlp: residual
I0123 12:36:04.344341 140367016284160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.344422 140367016284160 transformer_base.py:443] tbase: final FFN
I0123 12:36:04.344868 140367016284160 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:36:04.347260 140367016284160 nn_components.py:329] mlp: final activation = None
I0123 12:36:04.349305 140367016284160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.349413 140367016284160 nn_components.py:261] mlp: residual
I0123 12:36:04.349721 140367016284160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.349808 140367016284160 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 12:36:04.349921 140367016284160 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:36:04.349961 140367016284160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:36:04.349993 140367016284160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:36:04.351891 140367016284160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.354389 140367016284160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:36:04.360019 140367016284160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.360288 140367016284160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:36:04.363055 140367016284160 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:36:04.366899 140367016284160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:36:04.366956 140367016284160 attention.py:418] Single window, no scan.
I0123 12:36:04.366992 140367016284160 transformer_layer.py:389] tlayer: self-attention.
I0123 12:36:04.367023 140367016284160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.367086 140367016284160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.367705 140367016284160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.367784 140367016284160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.368153 140367016284160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.368938 140367016284160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.371479 140367016284160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.372109 140367016284160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.372188 140367016284160 transformer_layer.py:468] tlayer: End windows.
I0123 12:36:04.372224 140367016284160 transformer_layer.py:472] tlayer: final FFN.
I0123 12:36:04.372284 140367016284160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.372412 140367016284160 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:36:04.372742 140367016284160 nn_components.py:325] mlp: activation = None
I0123 12:36:04.372787 140367016284160 nn_components.py:329] mlp: final activation = None
I0123 12:36:04.374738 140367016284160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.374834 140367016284160 nn_components.py:261] mlp: residual
I0123 12:36:04.377491 140367016284160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.377573 140367016284160 transformer_base.py:443] tbase: final FFN
I0123 12:36:04.378018 140367016284160 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:36:04.380377 140367016284160 nn_components.py:329] mlp: final activation = None
I0123 12:36:04.382323 140367016284160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.382420 140367016284160 nn_components.py:261] mlp: residual
I0123 12:36:04.382717 140367016284160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.382806 140367016284160 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 12:36:04.382920 140367016284160 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:36:04.382961 140367016284160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:36:04.382994 140367016284160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:36:04.384869 140367016284160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.387347 140367016284160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:36:04.393432 140367016284160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.393711 140367016284160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:36:04.396384 140367016284160 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:36:04.400255 140367016284160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:36:04.400312 140367016284160 attention.py:418] Single window, no scan.
I0123 12:36:04.400348 140367016284160 transformer_layer.py:389] tlayer: self-attention.
I0123 12:36:04.400379 140367016284160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.400441 140367016284160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.401015 140367016284160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.401098 140367016284160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.401472 140367016284160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.402271 140367016284160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.404808 140367016284160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.405437 140367016284160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.405514 140367016284160 transformer_layer.py:468] tlayer: End windows.
I0123 12:36:04.405550 140367016284160 transformer_layer.py:472] tlayer: final FFN.
I0123 12:36:04.405610 140367016284160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.405750 140367016284160 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:36:04.406084 140367016284160 nn_components.py:325] mlp: activation = None
I0123 12:36:04.406134 140367016284160 nn_components.py:329] mlp: final activation = None
I0123 12:36:04.408117 140367016284160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.408212 140367016284160 nn_components.py:261] mlp: residual
I0123 12:36:04.410767 140367016284160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.410849 140367016284160 transformer_base.py:443] tbase: final FFN
I0123 12:36:04.411291 140367016284160 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:36:04.413636 140367016284160 nn_components.py:329] mlp: final activation = None
I0123 12:36:04.415574 140367016284160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.415670 140367016284160 nn_components.py:261] mlp: residual
I0123 12:36:04.415968 140367016284160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.416055 140367016284160 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 12:36:04.416171 140367016284160 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:36:04.416212 140367016284160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:36:04.416243 140367016284160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:36:04.418210 140367016284160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.420629 140367016284160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:36:04.426366 140367016284160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.426634 140367016284160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:36:04.429316 140367016284160 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:36:04.433186 140367016284160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:36:04.433242 140367016284160 attention.py:418] Single window, no scan.
I0123 12:36:04.433279 140367016284160 transformer_layer.py:389] tlayer: self-attention.
I0123 12:36:04.433311 140367016284160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.433374 140367016284160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.433959 140367016284160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.434036 140367016284160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.434400 140367016284160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.435188 140367016284160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.437706 140367016284160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.438388 140367016284160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.438466 140367016284160 transformer_layer.py:468] tlayer: End windows.
I0123 12:36:04.438503 140367016284160 transformer_layer.py:472] tlayer: final FFN.
I0123 12:36:04.438565 140367016284160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.438693 140367016284160 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:36:04.439170 140367016284160 nn_components.py:325] mlp: activation = None
I0123 12:36:04.439215 140367016284160 nn_components.py:329] mlp: final activation = None
I0123 12:36:04.441289 140367016284160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.441387 140367016284160 nn_components.py:261] mlp: residual
I0123 12:36:04.444178 140367016284160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.444261 140367016284160 transformer_base.py:443] tbase: final FFN
I0123 12:36:04.444754 140367016284160 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:36:04.447066 140367016284160 nn_components.py:329] mlp: final activation = None
I0123 12:36:04.449010 140367016284160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.449106 140367016284160 nn_components.py:261] mlp: residual
I0123 12:36:04.449407 140367016284160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.449489 140367016284160 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 12:36:04.449608 140367016284160 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:36:04.449655 140367016284160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:36:04.449690 140367016284160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:36:04.451621 140367016284160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.454046 140367016284160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:36:04.459778 140367016284160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.460045 140367016284160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:36:04.462750 140367016284160 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:36:04.466637 140367016284160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:36:04.466694 140367016284160 attention.py:418] Single window, no scan.
I0123 12:36:04.466731 140367016284160 transformer_layer.py:389] tlayer: self-attention.
I0123 12:36:04.466763 140367016284160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.466827 140367016284160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.467399 140367016284160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.467478 140367016284160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.467855 140367016284160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.468652 140367016284160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.471564 140367016284160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.472206 140367016284160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.472287 140367016284160 transformer_layer.py:468] tlayer: End windows.
I0123 12:36:04.472324 140367016284160 transformer_layer.py:472] tlayer: final FFN.
I0123 12:36:04.472386 140367016284160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.472516 140367016284160 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:36:04.472852 140367016284160 nn_components.py:325] mlp: activation = None
I0123 12:36:04.472897 140367016284160 nn_components.py:329] mlp: final activation = None
I0123 12:36:04.474835 140367016284160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.474931 140367016284160 nn_components.py:261] mlp: residual
I0123 12:36:04.477513 140367016284160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.477594 140367016284160 transformer_base.py:443] tbase: final FFN
I0123 12:36:04.478048 140367016284160 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:36:04.480322 140367016284160 nn_components.py:329] mlp: final activation = None
I0123 12:36:04.482262 140367016284160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.482360 140367016284160 nn_components.py:261] mlp: residual
I0123 12:36:04.482660 140367016284160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.482959 140367016284160 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:36:04.483032 140367016284160 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:36:04.483099 140367016284160 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:36:04.483158 140367016284160 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:36:04.483215 140367016284160 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:36:04.483271 140367016284160 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:36:04.483334 140367016284160 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:36:04.483388 140367016284160 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:36:04.483443 140367016284160 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:36:04.483498 140367016284160 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:36:04.483552 140367016284160 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:36:04.483606 140367016284160 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:36:04.483645 140367016284160 decoder_stack.py:344] dstack: Final layernorm.
I0123 12:36:04.487210 140367016284160 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:36:04.535703 140367016284160 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.535792 140367016284160 decoder_stack.py:333] dstack: autoregressive generator.
I0123 12:36:04.535847 140367016284160 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 12:36:04.535955 140367016284160 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:36:04.535995 140367016284160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:36:04.536027 140367016284160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:36:04.536093 140367016284160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.538579 140367016284160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:36:04.544466 140367016284160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.544737 140367016284160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:36:04.547478 140367016284160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:36:04.564480 140367016284160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:36:04.564537 140367016284160 attention.py:418] Single window, no scan.
I0123 12:36:04.564574 140367016284160 transformer_layer.py:389] tlayer: self-attention.
I0123 12:36:04.564608 140367016284160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.564672 140367016284160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.565840 140367016284160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.565921 140367016284160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.566670 140367016284160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.568738 140367016284160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.573518 140367016284160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.574843 140367016284160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.574932 140367016284160 transformer_layer.py:468] tlayer: End windows.
I0123 12:36:04.574969 140367016284160 transformer_layer.py:472] tlayer: final FFN.
I0123 12:36:04.575030 140367016284160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.575161 140367016284160 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:36:04.575273 140367016284160 nn_components.py:325] mlp: activation = None
I0123 12:36:04.575312 140367016284160 nn_components.py:329] mlp: final activation = None
I0123 12:36:04.577234 140367016284160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.577330 140367016284160 nn_components.py:261] mlp: residual
I0123 12:36:04.579791 140367016284160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.579873 140367016284160 transformer_base.py:443] tbase: final FFN
I0123 12:36:04.579983 140367016284160 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:36:04.582255 140367016284160 nn_components.py:329] mlp: final activation = None
I0123 12:36:04.584247 140367016284160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.584345 140367016284160 nn_components.py:261] mlp: residual
I0123 12:36:04.584639 140367016284160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.584721 140367016284160 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 12:36:04.584830 140367016284160 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:36:04.584870 140367016284160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:36:04.584903 140367016284160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:36:04.584968 140367016284160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.587274 140367016284160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:36:04.592822 140367016284160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.593086 140367016284160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:36:04.595872 140367016284160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:36:04.609338 140367016284160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:36:04.609395 140367016284160 attention.py:418] Single window, no scan.
I0123 12:36:04.609433 140367016284160 transformer_layer.py:389] tlayer: self-attention.
I0123 12:36:04.609464 140367016284160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.609529 140367016284160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.610106 140367016284160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.610184 140367016284160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.610546 140367016284160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.611249 140367016284160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.613777 140367016284160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.614398 140367016284160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.614475 140367016284160 transformer_layer.py:468] tlayer: End windows.
I0123 12:36:04.614516 140367016284160 transformer_layer.py:472] tlayer: final FFN.
I0123 12:36:04.614582 140367016284160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.614718 140367016284160 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:36:04.614830 140367016284160 nn_components.py:325] mlp: activation = None
I0123 12:36:04.614869 140367016284160 nn_components.py:329] mlp: final activation = None
I0123 12:36:04.616841 140367016284160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.616934 140367016284160 nn_components.py:261] mlp: residual
I0123 12:36:04.619363 140367016284160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.619443 140367016284160 transformer_base.py:443] tbase: final FFN
I0123 12:36:04.619553 140367016284160 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:36:04.621817 140367016284160 nn_components.py:329] mlp: final activation = None
I0123 12:36:04.623760 140367016284160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.623858 140367016284160 nn_components.py:261] mlp: residual
I0123 12:36:04.624150 140367016284160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.624233 140367016284160 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 12:36:04.624345 140367016284160 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:36:04.624385 140367016284160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:36:04.624417 140367016284160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:36:04.624483 140367016284160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.626800 140367016284160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:36:04.632409 140367016284160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.632676 140367016284160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:36:04.635404 140367016284160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:36:04.648719 140367016284160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:36:04.648777 140367016284160 attention.py:418] Single window, no scan.
I0123 12:36:04.648813 140367016284160 transformer_layer.py:389] tlayer: self-attention.
I0123 12:36:04.648845 140367016284160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.648908 140367016284160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.649482 140367016284160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.649561 140367016284160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.649935 140367016284160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.650644 140367016284160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.653148 140367016284160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.653797 140367016284160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.653877 140367016284160 transformer_layer.py:468] tlayer: End windows.
I0123 12:36:04.653911 140367016284160 transformer_layer.py:472] tlayer: final FFN.
I0123 12:36:04.653977 140367016284160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.654108 140367016284160 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:36:04.654220 140367016284160 nn_components.py:325] mlp: activation = None
I0123 12:36:04.654258 140367016284160 nn_components.py:329] mlp: final activation = None
I0123 12:36:04.656216 140367016284160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.656311 140367016284160 nn_components.py:261] mlp: residual
I0123 12:36:04.658775 140367016284160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.658860 140367016284160 transformer_base.py:443] tbase: final FFN
I0123 12:36:04.658970 140367016284160 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:36:04.661212 140367016284160 nn_components.py:329] mlp: final activation = None
I0123 12:36:04.667297 140367016284160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.667433 140367016284160 nn_components.py:261] mlp: residual
I0123 12:36:04.667750 140367016284160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.667838 140367016284160 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 12:36:04.667955 140367016284160 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:36:04.667997 140367016284160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:36:04.668029 140367016284160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:36:04.668100 140367016284160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.670498 140367016284160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:36:04.676064 140367016284160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.676334 140367016284160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:36:04.679112 140367016284160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:36:04.692342 140367016284160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:36:04.692398 140367016284160 attention.py:418] Single window, no scan.
I0123 12:36:04.692436 140367016284160 transformer_layer.py:389] tlayer: self-attention.
I0123 12:36:04.692468 140367016284160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.692531 140367016284160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.693131 140367016284160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.693208 140367016284160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.693583 140367016284160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.694308 140367016284160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.696874 140367016284160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.697514 140367016284160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.697592 140367016284160 transformer_layer.py:468] tlayer: End windows.
I0123 12:36:04.697628 140367016284160 transformer_layer.py:472] tlayer: final FFN.
I0123 12:36:04.697698 140367016284160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.697838 140367016284160 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:36:04.697951 140367016284160 nn_components.py:325] mlp: activation = None
I0123 12:36:04.697991 140367016284160 nn_components.py:329] mlp: final activation = None
I0123 12:36:04.699971 140367016284160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.700066 140367016284160 nn_components.py:261] mlp: residual
I0123 12:36:04.702553 140367016284160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.702633 140367016284160 transformer_base.py:443] tbase: final FFN
I0123 12:36:04.702747 140367016284160 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:36:04.705036 140367016284160 nn_components.py:329] mlp: final activation = None
I0123 12:36:04.706952 140367016284160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.707049 140367016284160 nn_components.py:261] mlp: residual
I0123 12:36:04.707341 140367016284160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.707423 140367016284160 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 12:36:04.707534 140367016284160 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:36:04.707574 140367016284160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:36:04.707606 140367016284160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:36:04.707671 140367016284160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.710308 140367016284160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:36:04.715829 140367016284160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.716099 140367016284160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:36:04.718769 140367016284160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:36:04.731861 140367016284160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:36:04.731919 140367016284160 attention.py:418] Single window, no scan.
I0123 12:36:04.731955 140367016284160 transformer_layer.py:389] tlayer: self-attention.
I0123 12:36:04.731986 140367016284160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.732049 140367016284160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.732609 140367016284160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.732690 140367016284160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.733060 140367016284160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.733792 140367016284160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.736393 140367016284160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.737043 140367016284160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.737136 140367016284160 transformer_layer.py:468] tlayer: End windows.
I0123 12:36:04.737174 140367016284160 transformer_layer.py:472] tlayer: final FFN.
I0123 12:36:04.737236 140367016284160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.737377 140367016284160 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:36:04.737490 140367016284160 nn_components.py:325] mlp: activation = None
I0123 12:36:04.737531 140367016284160 nn_components.py:329] mlp: final activation = None
I0123 12:36:04.739456 140367016284160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.739552 140367016284160 nn_components.py:261] mlp: residual
I0123 12:36:04.742016 140367016284160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.742096 140367016284160 transformer_base.py:443] tbase: final FFN
I0123 12:36:04.742206 140367016284160 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:36:04.744696 140367016284160 nn_components.py:329] mlp: final activation = None
I0123 12:36:04.746615 140367016284160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.746712 140367016284160 nn_components.py:261] mlp: residual
I0123 12:36:04.747001 140367016284160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.747084 140367016284160 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 12:36:04.747193 140367016284160 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:36:04.747233 140367016284160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:36:04.747265 140367016284160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:36:04.747329 140367016284160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.749614 140367016284160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:36:04.755143 140367016284160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.755408 140367016284160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:36:04.758156 140367016284160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:36:04.771108 140367016284160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:36:04.771164 140367016284160 attention.py:418] Single window, no scan.
I0123 12:36:04.771200 140367016284160 transformer_layer.py:389] tlayer: self-attention.
I0123 12:36:04.771231 140367016284160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.771297 140367016284160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.771868 140367016284160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.771945 140367016284160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.772315 140367016284160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.773039 140367016284160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.775643 140367016284160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.776286 140367016284160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.776367 140367016284160 transformer_layer.py:468] tlayer: End windows.
I0123 12:36:04.776404 140367016284160 transformer_layer.py:472] tlayer: final FFN.
I0123 12:36:04.776468 140367016284160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.776598 140367016284160 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:36:04.776716 140367016284160 nn_components.py:325] mlp: activation = None
I0123 12:36:04.776755 140367016284160 nn_components.py:329] mlp: final activation = None
I0123 12:36:04.778728 140367016284160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.778824 140367016284160 nn_components.py:261] mlp: residual
I0123 12:36:04.781263 140367016284160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.781343 140367016284160 transformer_base.py:443] tbase: final FFN
I0123 12:36:04.781454 140367016284160 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:36:04.783732 140367016284160 nn_components.py:329] mlp: final activation = None
I0123 12:36:04.785607 140367016284160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.785712 140367016284160 nn_components.py:261] mlp: residual
I0123 12:36:04.786001 140367016284160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.786084 140367016284160 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 12:36:04.786194 140367016284160 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:36:04.786234 140367016284160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:36:04.786265 140367016284160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:36:04.786329 140367016284160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.788602 140367016284160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:36:04.794226 140367016284160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.794493 140367016284160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:36:04.797149 140367016284160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:36:04.810164 140367016284160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:36:04.810220 140367016284160 attention.py:418] Single window, no scan.
I0123 12:36:04.810256 140367016284160 transformer_layer.py:389] tlayer: self-attention.
I0123 12:36:04.810287 140367016284160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.810351 140367016284160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.810922 140367016284160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.810999 140367016284160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.811364 140367016284160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.812069 140367016284160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.814586 140367016284160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.815596 140367016284160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.815675 140367016284160 transformer_layer.py:468] tlayer: End windows.
I0123 12:36:04.815711 140367016284160 transformer_layer.py:472] tlayer: final FFN.
I0123 12:36:04.815771 140367016284160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.815904 140367016284160 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:36:04.816016 140367016284160 nn_components.py:325] mlp: activation = None
I0123 12:36:04.816062 140367016284160 nn_components.py:329] mlp: final activation = None
I0123 12:36:04.817989 140367016284160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.818084 140367016284160 nn_components.py:261] mlp: residual
I0123 12:36:04.820513 140367016284160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.820595 140367016284160 transformer_base.py:443] tbase: final FFN
I0123 12:36:04.820706 140367016284160 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:36:04.822965 140367016284160 nn_components.py:329] mlp: final activation = None
I0123 12:36:04.824931 140367016284160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.825028 140367016284160 nn_components.py:261] mlp: residual
I0123 12:36:04.825322 140367016284160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.825405 140367016284160 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 12:36:04.825516 140367016284160 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:36:04.825556 140367016284160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:36:04.825588 140367016284160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:36:04.825662 140367016284160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.827942 140367016284160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:36:04.833458 140367016284160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.833741 140367016284160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:36:04.836462 140367016284160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:36:04.849404 140367016284160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:36:04.849463 140367016284160 attention.py:418] Single window, no scan.
I0123 12:36:04.849500 140367016284160 transformer_layer.py:389] tlayer: self-attention.
I0123 12:36:04.849531 140367016284160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.849595 140367016284160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.850218 140367016284160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.850296 140367016284160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.850663 140367016284160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.851372 140367016284160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.853916 140367016284160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.854552 140367016284160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.854630 140367016284160 transformer_layer.py:468] tlayer: End windows.
I0123 12:36:04.854666 140367016284160 transformer_layer.py:472] tlayer: final FFN.
I0123 12:36:04.854729 140367016284160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.854861 140367016284160 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:36:04.854975 140367016284160 nn_components.py:325] mlp: activation = None
I0123 12:36:04.855021 140367016284160 nn_components.py:329] mlp: final activation = None
I0123 12:36:04.856939 140367016284160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.857033 140367016284160 nn_components.py:261] mlp: residual
I0123 12:36:04.859534 140367016284160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.859615 140367016284160 transformer_base.py:443] tbase: final FFN
I0123 12:36:04.859723 140367016284160 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:36:04.861984 140367016284160 nn_components.py:329] mlp: final activation = None
I0123 12:36:04.863871 140367016284160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.863970 140367016284160 nn_components.py:261] mlp: residual
I0123 12:36:04.864264 140367016284160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.864346 140367016284160 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 12:36:04.864458 140367016284160 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:36:04.864498 140367016284160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:36:04.864530 140367016284160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:36:04.864593 140367016284160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.866868 140367016284160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:36:04.872419 140367016284160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.872684 140367016284160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:36:04.875450 140367016284160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:36:04.888473 140367016284160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:36:04.888530 140367016284160 attention.py:418] Single window, no scan.
I0123 12:36:04.888567 140367016284160 transformer_layer.py:389] tlayer: self-attention.
I0123 12:36:04.888599 140367016284160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.888661 140367016284160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.889237 140367016284160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.889315 140367016284160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.889687 140367016284160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.890398 140367016284160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.892913 140367016284160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.893600 140367016284160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.893688 140367016284160 transformer_layer.py:468] tlayer: End windows.
I0123 12:36:04.893724 140367016284160 transformer_layer.py:472] tlayer: final FFN.
I0123 12:36:04.893785 140367016284160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.893918 140367016284160 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:36:04.894032 140367016284160 nn_components.py:325] mlp: activation = None
I0123 12:36:04.894071 140367016284160 nn_components.py:329] mlp: final activation = None
I0123 12:36:04.895977 140367016284160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.896071 140367016284160 nn_components.py:261] mlp: residual
I0123 12:36:04.898499 140367016284160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.898580 140367016284160 transformer_base.py:443] tbase: final FFN
I0123 12:36:04.898689 140367016284160 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:36:04.900914 140367016284160 nn_components.py:329] mlp: final activation = None
I0123 12:36:04.902874 140367016284160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.902972 140367016284160 nn_components.py:261] mlp: residual
I0123 12:36:04.903263 140367016284160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.903345 140367016284160 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 12:36:04.903455 140367016284160 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:36:04.903494 140367016284160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:36:04.903526 140367016284160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:36:04.903590 140367016284160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.905867 140367016284160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:36:04.911346 140367016284160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.911611 140367016284160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:36:04.914343 140367016284160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:36:04.927584 140367016284160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:36:04.927641 140367016284160 attention.py:418] Single window, no scan.
I0123 12:36:04.927678 140367016284160 transformer_layer.py:389] tlayer: self-attention.
I0123 12:36:04.927711 140367016284160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.927773 140367016284160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.928387 140367016284160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.928466 140367016284160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.928833 140367016284160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.929544 140367016284160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.932089 140367016284160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.932723 140367016284160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.932800 140367016284160 transformer_layer.py:468] tlayer: End windows.
I0123 12:36:04.932835 140367016284160 transformer_layer.py:472] tlayer: final FFN.
I0123 12:36:04.932894 140367016284160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.933024 140367016284160 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:36:04.933138 140367016284160 nn_components.py:325] mlp: activation = None
I0123 12:36:04.933176 140367016284160 nn_components.py:329] mlp: final activation = None
I0123 12:36:04.935088 140367016284160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.935190 140367016284160 nn_components.py:261] mlp: residual
I0123 12:36:04.937703 140367016284160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.937783 140367016284160 transformer_base.py:443] tbase: final FFN
I0123 12:36:04.937893 140367016284160 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:36:04.940144 140367016284160 nn_components.py:329] mlp: final activation = None
I0123 12:36:04.942037 140367016284160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.942132 140367016284160 nn_components.py:261] mlp: residual
I0123 12:36:04.942420 140367016284160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.942503 140367016284160 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 12:36:04.942614 140367016284160 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:36:04.942653 140367016284160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:36:04.942684 140367016284160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:36:04.942749 140367016284160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.945023 140367016284160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:36:04.950582 140367016284160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.950848 140367016284160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:36:04.953521 140367016284160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:36:04.966502 140367016284160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:36:04.966558 140367016284160 attention.py:418] Single window, no scan.
I0123 12:36:04.966595 140367016284160 transformer_layer.py:389] tlayer: self-attention.
I0123 12:36:04.966627 140367016284160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.966690 140367016284160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.967249 140367016284160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.967367 140367016284160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.967731 140367016284160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.968431 140367016284160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.970970 140367016284160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.971660 140367016284160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.971740 140367016284160 transformer_layer.py:468] tlayer: End windows.
I0123 12:36:04.971776 140367016284160 transformer_layer.py:472] tlayer: final FFN.
I0123 12:36:04.971836 140367016284160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.971972 140367016284160 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:36:04.972084 140367016284160 nn_components.py:325] mlp: activation = None
I0123 12:36:04.972124 140367016284160 nn_components.py:329] mlp: final activation = None
I0123 12:36:04.974049 140367016284160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.974151 140367016284160 nn_components.py:261] mlp: residual
I0123 12:36:04.976619 140367016284160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.976699 140367016284160 transformer_base.py:443] tbase: final FFN
I0123 12:36:04.976809 140367016284160 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:36:04.979057 140367016284160 nn_components.py:329] mlp: final activation = None
I0123 12:36:04.980990 140367016284160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.981086 140367016284160 nn_components.py:261] mlp: residual
I0123 12:36:04.981377 140367016284160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.981458 140367016284160 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 12:36:04.981568 140367016284160 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:36:04.981608 140367016284160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:36:04.981646 140367016284160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:36:04.981714 140367016284160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.983994 140367016284160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:36:04.989492 140367016284160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:04.989762 140367016284160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:36:04.992496 140367016284160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:36:05.005392 140367016284160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:36:05.005450 140367016284160 attention.py:418] Single window, no scan.
I0123 12:36:05.005486 140367016284160 transformer_layer.py:389] tlayer: self-attention.
I0123 12:36:05.005519 140367016284160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:05.005582 140367016284160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:05.006152 140367016284160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:05.006231 140367016284160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:05.006596 140367016284160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:05.007348 140367016284160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:05.009884 140367016284160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:05.010520 140367016284160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:05.010599 140367016284160 transformer_layer.py:468] tlayer: End windows.
I0123 12:36:05.010636 140367016284160 transformer_layer.py:472] tlayer: final FFN.
I0123 12:36:05.010696 140367016284160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:05.010829 140367016284160 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:36:05.010941 140367016284160 nn_components.py:325] mlp: activation = None
I0123 12:36:05.010981 140367016284160 nn_components.py:329] mlp: final activation = None
I0123 12:36:05.012896 140367016284160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:05.012990 140367016284160 nn_components.py:261] mlp: residual
I0123 12:36:05.015432 140367016284160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:05.015512 140367016284160 transformer_base.py:443] tbase: final FFN
I0123 12:36:05.015621 140367016284160 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:36:05.017936 140367016284160 nn_components.py:329] mlp: final activation = None
I0123 12:36:05.019824 140367016284160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:05.019920 140367016284160 nn_components.py:261] mlp: residual
I0123 12:36:05.020210 140367016284160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:05.020300 140367016284160 decoder_stack.py:344] dstack: Final layernorm.
I0123 12:36:05.023229 140367016284160 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:36:05.079555 140367016284160 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.079645 140367016284160 decoder_stack.py:333] dstack: autoregressive generator.
I0123 12:36:05.079700 140367016284160 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 12:36:05.079806 140367016284160 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:36:05.079844 140367016284160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:36:05.079875 140367016284160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:36:05.079936 140367016284160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.082624 140367016284160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:36:05.088055 140367016284160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.088319 140367016284160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:36:05.090947 140367016284160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:36:05.103526 140367016284160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:36:05.103582 140367016284160 attention.py:418] Single window, no scan.
I0123 12:36:05.103618 140367016284160 transformer_layer.py:389] tlayer: self-attention.
I0123 12:36:05.103649 140367016284160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.103713 140367016284160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.104273 140367016284160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.104349 140367016284160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.104704 140367016284160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.105386 140367016284160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.107899 140367016284160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.108521 140367016284160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.108598 140367016284160 transformer_layer.py:468] tlayer: End windows.
I0123 12:36:05.108634 140367016284160 transformer_layer.py:472] tlayer: final FFN.
I0123 12:36:05.108695 140367016284160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.108827 140367016284160 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:36:05.108948 140367016284160 nn_components.py:325] mlp: activation = None
I0123 12:36:05.108988 140367016284160 nn_components.py:329] mlp: final activation = None
I0123 12:36:05.110864 140367016284160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.110960 140367016284160 nn_components.py:261] mlp: residual
I0123 12:36:05.113377 140367016284160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.113458 140367016284160 transformer_base.py:443] tbase: final FFN
I0123 12:36:05.113568 140367016284160 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:36:05.115819 140367016284160 nn_components.py:329] mlp: final activation = None
I0123 12:36:05.117677 140367016284160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.117777 140367016284160 nn_components.py:261] mlp: residual
I0123 12:36:05.118071 140367016284160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.118154 140367016284160 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 12:36:05.118264 140367016284160 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:36:05.118304 140367016284160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:36:05.118335 140367016284160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:36:05.118399 140367016284160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.120630 140367016284160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:36:05.125982 140367016284160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.126241 140367016284160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:36:05.128912 140367016284160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:36:05.141342 140367016284160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:36:05.141399 140367016284160 attention.py:418] Single window, no scan.
I0123 12:36:05.141435 140367016284160 transformer_layer.py:389] tlayer: self-attention.
I0123 12:36:05.141466 140367016284160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.141529 140367016284160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.142099 140367016284160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.142180 140367016284160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.142546 140367016284160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.143235 140367016284160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.145772 140367016284160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.146389 140367016284160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.146468 140367016284160 transformer_layer.py:468] tlayer: End windows.
I0123 12:36:05.146503 140367016284160 transformer_layer.py:472] tlayer: final FFN.
I0123 12:36:05.146564 140367016284160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.146693 140367016284160 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:36:05.146802 140367016284160 nn_components.py:325] mlp: activation = None
I0123 12:36:05.146846 140367016284160 nn_components.py:329] mlp: final activation = None
I0123 12:36:05.148687 140367016284160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.148781 140367016284160 nn_components.py:261] mlp: residual
I0123 12:36:05.151181 140367016284160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.151263 140367016284160 transformer_base.py:443] tbase: final FFN
I0123 12:36:05.151373 140367016284160 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:36:05.153632 140367016284160 nn_components.py:329] mlp: final activation = None
I0123 12:36:05.155498 140367016284160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.155594 140367016284160 nn_components.py:261] mlp: residual
I0123 12:36:05.155889 140367016284160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.155973 140367016284160 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 12:36:05.156084 140367016284160 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:36:05.156124 140367016284160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:36:05.156155 140367016284160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:36:05.156220 140367016284160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.158459 140367016284160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:36:05.163851 140367016284160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.164115 140367016284160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:36:05.166795 140367016284160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:36:05.179400 140367016284160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:36:05.179456 140367016284160 attention.py:418] Single window, no scan.
I0123 12:36:05.179494 140367016284160 transformer_layer.py:389] tlayer: self-attention.
I0123 12:36:05.179526 140367016284160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.179589 140367016284160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.180152 140367016284160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.180229 140367016284160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.180591 140367016284160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.181278 140367016284160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.183804 140367016284160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.184442 140367016284160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.184520 140367016284160 transformer_layer.py:468] tlayer: End windows.
I0123 12:36:05.184555 140367016284160 transformer_layer.py:472] tlayer: final FFN.
I0123 12:36:05.184615 140367016284160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.184745 140367016284160 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:36:05.184855 140367016284160 nn_components.py:325] mlp: activation = None
I0123 12:36:05.184895 140367016284160 nn_components.py:329] mlp: final activation = None
I0123 12:36:05.186780 140367016284160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.186876 140367016284160 nn_components.py:261] mlp: residual
I0123 12:36:05.189290 140367016284160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.189371 140367016284160 transformer_base.py:443] tbase: final FFN
I0123 12:36:05.189480 140367016284160 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:36:05.192210 140367016284160 nn_components.py:329] mlp: final activation = None
I0123 12:36:05.194076 140367016284160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.194175 140367016284160 nn_components.py:261] mlp: residual
I0123 12:36:05.194464 140367016284160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.194546 140367016284160 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 12:36:05.194655 140367016284160 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:36:05.194694 140367016284160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:36:05.194727 140367016284160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:36:05.194791 140367016284160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.197023 140367016284160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:36:05.202398 140367016284160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.202662 140367016284160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:36:05.205347 140367016284160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:36:05.217983 140367016284160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:36:05.218042 140367016284160 attention.py:418] Single window, no scan.
I0123 12:36:05.218083 140367016284160 transformer_layer.py:389] tlayer: self-attention.
I0123 12:36:05.218135 140367016284160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.218203 140367016284160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.218786 140367016284160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.218865 140367016284160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.219242 140367016284160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.219955 140367016284160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.222535 140367016284160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.223160 140367016284160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.223236 140367016284160 transformer_layer.py:468] tlayer: End windows.
I0123 12:36:05.223270 140367016284160 transformer_layer.py:472] tlayer: final FFN.
I0123 12:36:05.223330 140367016284160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.223460 140367016284160 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:36:05.223569 140367016284160 nn_components.py:325] mlp: activation = None
I0123 12:36:05.223609 140367016284160 nn_components.py:329] mlp: final activation = None
I0123 12:36:05.225497 140367016284160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.225590 140367016284160 nn_components.py:261] mlp: residual
I0123 12:36:05.228026 140367016284160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.228106 140367016284160 transformer_base.py:443] tbase: final FFN
I0123 12:36:05.228214 140367016284160 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:36:05.230515 140367016284160 nn_components.py:329] mlp: final activation = None
I0123 12:36:05.232372 140367016284160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.232467 140367016284160 nn_components.py:261] mlp: residual
I0123 12:36:05.232758 140367016284160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.232839 140367016284160 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 12:36:05.232946 140367016284160 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:36:05.232984 140367016284160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:36:05.233014 140367016284160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:36:05.233076 140367016284160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.235321 140367016284160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:36:05.240729 140367016284160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.240989 140367016284160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:36:05.243698 140367016284160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:36:05.256416 140367016284160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:36:05.256470 140367016284160 attention.py:418] Single window, no scan.
I0123 12:36:05.256504 140367016284160 transformer_layer.py:389] tlayer: self-attention.
I0123 12:36:05.256534 140367016284160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.256594 140367016284160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.257156 140367016284160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.257232 140367016284160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.257592 140367016284160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.258300 140367016284160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.260854 140367016284160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.261476 140367016284160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.261552 140367016284160 transformer_layer.py:468] tlayer: End windows.
I0123 12:36:05.261585 140367016284160 transformer_layer.py:472] tlayer: final FFN.
I0123 12:36:05.261649 140367016284160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.261779 140367016284160 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:36:05.261888 140367016284160 nn_components.py:325] mlp: activation = None
I0123 12:36:05.261925 140367016284160 nn_components.py:329] mlp: final activation = None
I0123 12:36:05.263804 140367016284160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.263902 140367016284160 nn_components.py:261] mlp: residual
I0123 12:36:05.266314 140367016284160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.266394 140367016284160 transformer_base.py:443] tbase: final FFN
I0123 12:36:05.266502 140367016284160 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:36:05.268776 140367016284160 nn_components.py:329] mlp: final activation = None
I0123 12:36:05.270736 140367016284160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.270831 140367016284160 nn_components.py:261] mlp: residual
I0123 12:36:05.271120 140367016284160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.271199 140367016284160 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 12:36:05.271307 140367016284160 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:36:05.271344 140367016284160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:36:05.271375 140367016284160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:36:05.271437 140367016284160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.273681 140367016284160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:36:05.279094 140367016284160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.279356 140367016284160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:36:05.282094 140367016284160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:36:05.294922 140367016284160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:36:05.294976 140367016284160 attention.py:418] Single window, no scan.
I0123 12:36:05.295011 140367016284160 transformer_layer.py:389] tlayer: self-attention.
I0123 12:36:05.295041 140367016284160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.295109 140367016284160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.295687 140367016284160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.295766 140367016284160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.296143 140367016284160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.296862 140367016284160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.299449 140367016284160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.300072 140367016284160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.300149 140367016284160 transformer_layer.py:468] tlayer: End windows.
I0123 12:36:05.300183 140367016284160 transformer_layer.py:472] tlayer: final FFN.
I0123 12:36:05.300241 140367016284160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.300367 140367016284160 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:36:05.300474 140367016284160 nn_components.py:325] mlp: activation = None
I0123 12:36:05.300511 140367016284160 nn_components.py:329] mlp: final activation = None
I0123 12:36:05.302411 140367016284160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.302508 140367016284160 nn_components.py:261] mlp: residual
I0123 12:36:05.304907 140367016284160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.304986 140367016284160 transformer_base.py:443] tbase: final FFN
I0123 12:36:05.305095 140367016284160 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:36:05.307790 140367016284160 nn_components.py:329] mlp: final activation = None
I0123 12:36:05.309674 140367016284160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.309775 140367016284160 nn_components.py:261] mlp: residual
I0123 12:36:05.310066 140367016284160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.310146 140367016284160 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 12:36:05.310255 140367016284160 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:36:05.310293 140367016284160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:36:05.310323 140367016284160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:36:05.310386 140367016284160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.312642 140367016284160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:36:05.318121 140367016284160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.318381 140367016284160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:36:05.321078 140367016284160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:36:05.333770 140367016284160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:36:05.333824 140367016284160 attention.py:418] Single window, no scan.
I0123 12:36:05.333859 140367016284160 transformer_layer.py:389] tlayer: self-attention.
I0123 12:36:05.333889 140367016284160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.333951 140367016284160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.334516 140367016284160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.334590 140367016284160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.334953 140367016284160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.335643 140367016284160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.338207 140367016284160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.338839 140367016284160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.338914 140367016284160 transformer_layer.py:468] tlayer: End windows.
I0123 12:36:05.338948 140367016284160 transformer_layer.py:472] tlayer: final FFN.
I0123 12:36:05.339007 140367016284160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.339134 140367016284160 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:36:05.339243 140367016284160 nn_components.py:325] mlp: activation = None
I0123 12:36:05.339281 140367016284160 nn_components.py:329] mlp: final activation = None
I0123 12:36:05.341176 140367016284160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.341268 140367016284160 nn_components.py:261] mlp: residual
I0123 12:36:05.343695 140367016284160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.343780 140367016284160 transformer_base.py:443] tbase: final FFN
I0123 12:36:05.343890 140367016284160 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:36:05.346199 140367016284160 nn_components.py:329] mlp: final activation = None
I0123 12:36:05.348064 140367016284160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.348159 140367016284160 nn_components.py:261] mlp: residual
I0123 12:36:05.348444 140367016284160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.348524 140367016284160 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 12:36:05.348633 140367016284160 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:36:05.348671 140367016284160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:36:05.348701 140367016284160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:36:05.348763 140367016284160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.351003 140367016284160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:36:05.356465 140367016284160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.356730 140367016284160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:36:05.359450 140367016284160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:36:05.372219 140367016284160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:36:05.372274 140367016284160 attention.py:418] Single window, no scan.
I0123 12:36:05.372313 140367016284160 transformer_layer.py:389] tlayer: self-attention.
I0123 12:36:05.372343 140367016284160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.372406 140367016284160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.372974 140367016284160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.373050 140367016284160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.373411 140367016284160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.374116 140367016284160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.376689 140367016284160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.377320 140367016284160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.377396 140367016284160 transformer_layer.py:468] tlayer: End windows.
I0123 12:36:05.377431 140367016284160 transformer_layer.py:472] tlayer: final FFN.
I0123 12:36:05.377489 140367016284160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.377616 140367016284160 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:36:05.377732 140367016284160 nn_components.py:325] mlp: activation = None
I0123 12:36:05.377770 140367016284160 nn_components.py:329] mlp: final activation = None
I0123 12:36:05.379659 140367016284160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.379752 140367016284160 nn_components.py:261] mlp: residual
I0123 12:36:05.382191 140367016284160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.382277 140367016284160 transformer_base.py:443] tbase: final FFN
I0123 12:36:05.382387 140367016284160 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:36:05.384761 140367016284160 nn_components.py:329] mlp: final activation = None
I0123 12:36:05.386803 140367016284160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.386902 140367016284160 nn_components.py:261] mlp: residual
I0123 12:36:05.387203 140367016284160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.387289 140367016284160 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 12:36:05.387402 140367016284160 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:36:05.387441 140367016284160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:36:05.387473 140367016284160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:36:05.387537 140367016284160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.389873 140367016284160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:36:05.395547 140367016284160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.395818 140367016284160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:36:05.398548 140367016284160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:36:05.411757 140367016284160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:36:05.411813 140367016284160 attention.py:418] Single window, no scan.
I0123 12:36:05.411849 140367016284160 transformer_layer.py:389] tlayer: self-attention.
I0123 12:36:05.411881 140367016284160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.411944 140367016284160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.412523 140367016284160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.412600 140367016284160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.412976 140367016284160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.413711 140367016284160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.416379 140367016284160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.417004 140367016284160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.417080 140367016284160 transformer_layer.py:468] tlayer: End windows.
I0123 12:36:05.417114 140367016284160 transformer_layer.py:472] tlayer: final FFN.
I0123 12:36:05.417172 140367016284160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.417297 140367016284160 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:36:05.417405 140367016284160 nn_components.py:325] mlp: activation = None
I0123 12:36:05.417443 140367016284160 nn_components.py:329] mlp: final activation = None
I0123 12:36:05.419387 140367016284160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.419484 140367016284160 nn_components.py:261] mlp: residual
I0123 12:36:05.421982 140367016284160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.422067 140367016284160 transformer_base.py:443] tbase: final FFN
I0123 12:36:05.422180 140367016284160 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:36:05.424908 140367016284160 nn_components.py:329] mlp: final activation = None
I0123 12:36:05.426788 140367016284160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.426887 140367016284160 nn_components.py:261] mlp: residual
I0123 12:36:05.427190 140367016284160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.427274 140367016284160 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 12:36:05.427387 140367016284160 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:36:05.427427 140367016284160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:36:05.427458 140367016284160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:36:05.427522 140367016284160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.429789 140367016284160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:36:05.435338 140367016284160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.435600 140367016284160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:36:05.438310 140367016284160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:36:05.451105 140367016284160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:36:05.451159 140367016284160 attention.py:418] Single window, no scan.
I0123 12:36:05.451194 140367016284160 transformer_layer.py:389] tlayer: self-attention.
I0123 12:36:05.451225 140367016284160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.451286 140367016284160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.451855 140367016284160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.451931 140367016284160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.452292 140367016284160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.452993 140367016284160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.455562 140367016284160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.456192 140367016284160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.456269 140367016284160 transformer_layer.py:468] tlayer: End windows.
I0123 12:36:05.456304 140367016284160 transformer_layer.py:472] tlayer: final FFN.
I0123 12:36:05.456361 140367016284160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.456488 140367016284160 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:36:05.456597 140367016284160 nn_components.py:325] mlp: activation = None
I0123 12:36:05.456634 140367016284160 nn_components.py:329] mlp: final activation = None
I0123 12:36:05.458967 140367016284160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.459067 140367016284160 nn_components.py:261] mlp: residual
I0123 12:36:05.461480 140367016284160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.461559 140367016284160 transformer_base.py:443] tbase: final FFN
I0123 12:36:05.461679 140367016284160 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:36:05.463960 140367016284160 nn_components.py:329] mlp: final activation = None
I0123 12:36:05.465806 140367016284160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.465901 140367016284160 nn_components.py:261] mlp: residual
I0123 12:36:05.466186 140367016284160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.466265 140367016284160 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 12:36:05.466372 140367016284160 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:36:05.466409 140367016284160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:36:05.466439 140367016284160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:36:05.466500 140367016284160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.468734 140367016284160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:36:05.474147 140367016284160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.474407 140367016284160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:36:05.477105 140367016284160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:36:05.490118 140367016284160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:36:05.490173 140367016284160 attention.py:418] Single window, no scan.
I0123 12:36:05.490207 140367016284160 transformer_layer.py:389] tlayer: self-attention.
I0123 12:36:05.490238 140367016284160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.490299 140367016284160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.490863 140367016284160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.490939 140367016284160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.491304 140367016284160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.492004 140367016284160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.494592 140367016284160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.495222 140367016284160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.495299 140367016284160 transformer_layer.py:468] tlayer: End windows.
I0123 12:36:05.495333 140367016284160 transformer_layer.py:472] tlayer: final FFN.
I0123 12:36:05.495390 140367016284160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.495521 140367016284160 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:36:05.495633 140367016284160 nn_components.py:325] mlp: activation = None
I0123 12:36:05.495670 140367016284160 nn_components.py:329] mlp: final activation = None
I0123 12:36:05.497539 140367016284160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.497631 140367016284160 nn_components.py:261] mlp: residual
I0123 12:36:05.500047 140367016284160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.500128 140367016284160 transformer_base.py:443] tbase: final FFN
I0123 12:36:05.500236 140367016284160 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:36:05.502541 140367016284160 nn_components.py:329] mlp: final activation = None
I0123 12:36:05.504395 140367016284160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.504490 140367016284160 nn_components.py:261] mlp: residual
I0123 12:36:05.504775 140367016284160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.504855 140367016284160 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 12:36:05.504964 140367016284160 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:36:05.505002 140367016284160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:36:05.505031 140367016284160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:36:05.505092 140367016284160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.507351 140367016284160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:36:05.512785 140367016284160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.513046 140367016284160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:36:05.515722 140367016284160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:36:05.528429 140367016284160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:36:05.528483 140367016284160 attention.py:418] Single window, no scan.
I0123 12:36:05.528519 140367016284160 transformer_layer.py:389] tlayer: self-attention.
I0123 12:36:05.528549 140367016284160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.528609 140367016284160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.529179 140367016284160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.529255 140367016284160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.529621 140367016284160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.530323 140367016284160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.532871 140367016284160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.533490 140367016284160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.533570 140367016284160 transformer_layer.py:468] tlayer: End windows.
I0123 12:36:05.533603 140367016284160 transformer_layer.py:472] tlayer: final FFN.
I0123 12:36:05.533667 140367016284160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.533800 140367016284160 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:36:05.533910 140367016284160 nn_components.py:325] mlp: activation = None
I0123 12:36:05.533949 140367016284160 nn_components.py:329] mlp: final activation = None
I0123 12:36:05.535826 140367016284160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.535918 140367016284160 nn_components.py:261] mlp: residual
I0123 12:36:05.538328 140367016284160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.538407 140367016284160 transformer_base.py:443] tbase: final FFN
I0123 12:36:05.538516 140367016284160 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:36:05.541187 140367016284160 nn_components.py:329] mlp: final activation = None
I0123 12:36:05.543073 140367016284160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.543168 140367016284160 nn_components.py:261] mlp: residual
I0123 12:36:05.543454 140367016284160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:05.543538 140367016284160 decoder_stack.py:344] dstack: Final layernorm.
I0123 12:36:05.546405 140367016284160 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:36:09.921514 140367016284160 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
I0123 12:36:10.480636 140367016284160 training_loop.py:409] No working directory specified.
I0123 12:36:10.480769 140367016284160 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0123 12:36:10.481581 140367016284160 checkpoints.py:1062] Restoring legacy Flax checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0123 12:36:13.301478 140367016284160 training_loop.py:447] Only restoring trainable parameters.
I0123 12:36:13.302216 140367016284160 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0123 12:36:13.302277 140367016284160 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0123 12:36:13.302324 140367016284160 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:36:13.302366 140367016284160 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:36:13.302408 140367016284160 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:36:13.302449 140367016284160 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:36:13.302489 140367016284160 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:36:13.302528 140367016284160 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:36:13.302570 140367016284160 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:36:13.302610 140367016284160 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:36:13.302650 140367016284160 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:36:13.302689 140367016284160 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:36:13.302727 140367016284160 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:36:13.302765 140367016284160 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:36:13.302800 140367016284160 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:36:13.302835 140367016284160 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:36:13.302870 140367016284160 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:36:13.302906 140367016284160 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:36:13.302942 140367016284160 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:36:13.302978 140367016284160 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:36:13.303025 140367016284160 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:36:13.303063 140367016284160 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:36:13.303098 140367016284160 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:36:13.303135 140367016284160 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:36:13.303171 140367016284160 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:36:13.303206 140367016284160 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:36:13.303241 140367016284160 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:36:13.303275 140367016284160 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:36:13.303310 140367016284160 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:36:13.303345 140367016284160 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:36:13.303381 140367016284160 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:36:13.303416 140367016284160 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:36:13.303451 140367016284160 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:36:13.303485 140367016284160 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:36:13.303520 140367016284160 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:36:13.303556 140367016284160 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:36:13.303591 140367016284160 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:36:13.303627 140367016284160 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:36:13.303662 140367016284160 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:36:13.303697 140367016284160 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:36:13.303733 140367016284160 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:36:13.303768 140367016284160 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:36:13.303803 140367016284160 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:36:13.303839 140367016284160 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:36:13.303874 140367016284160 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:36:13.303911 140367016284160 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:36:13.303952 140367016284160 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:36:13.303989 140367016284160 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:36:13.304024 140367016284160 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:36:13.304059 140367016284160 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:36:13.304094 140367016284160 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:36:13.304130 140367016284160 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:36:13.304164 140367016284160 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:36:13.304199 140367016284160 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:36:13.304234 140367016284160 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:36:13.304269 140367016284160 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:36:13.304303 140367016284160 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:36:13.304338 140367016284160 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:36:13.304373 140367016284160 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:36:13.304408 140367016284160 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:36:13.304442 140367016284160 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:36:13.304477 140367016284160 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:36:13.304512 140367016284160 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:36:13.304547 140367016284160 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:36:13.304581 140367016284160 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:36:13.304615 140367016284160 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:36:13.304650 140367016284160 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:36:13.304685 140367016284160 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:36:13.304719 140367016284160 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:36:13.304755 140367016284160 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:36:13.304790 140367016284160 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:36:13.304825 140367016284160 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:36:13.304860 140367016284160 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:36:13.304899 140367016284160 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:36:13.304935 140367016284160 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:36:13.304971 140367016284160 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:36:13.305007 140367016284160 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:36:13.305041 140367016284160 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:36:13.305075 140367016284160 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:36:13.305109 140367016284160 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:36:13.305145 140367016284160 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:36:13.305180 140367016284160 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:36:13.305215 140367016284160 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:36:13.305250 140367016284160 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:36:13.305284 140367016284160 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:36:13.305319 140367016284160 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:36:13.305355 140367016284160 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:36:13.305389 140367016284160 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:36:13.305425 140367016284160 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:36:13.305460 140367016284160 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:36:13.305496 140367016284160 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:36:13.305532 140367016284160 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:36:13.305567 140367016284160 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:36:13.305602 140367016284160 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:36:13.305637 140367016284160 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:36:13.305686 140367016284160 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:36:13.305723 140367016284160 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:36:13.305759 140367016284160 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:36:13.305795 140367016284160 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:36:13.305830 140367016284160 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:36:13.305870 140367016284160 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:36:13.306081 140367016284160 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:36:13.306116 140367016284160 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:36:13.306151 140367016284160 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:36:13.306186 140367016284160 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:36:13.306221 140367016284160 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:36:13.306257 140367016284160 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:36:13.306291 140367016284160 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:36:13.306325 140367016284160 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:36:13.306360 140367016284160 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:36:13.306395 140367016284160 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:36:13.306431 140367016284160 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:36:13.306466 140367016284160 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:36:13.306501 140367016284160 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:36:13.306535 140367016284160 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:36:13.306571 140367016284160 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:36:13.306606 140367016284160 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:36:13.306640 140367016284160 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:36:13.306675 140367016284160 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:36:13.306710 140367016284160 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:36:13.306745 140367016284160 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:36:13.306780 140367016284160 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:36:13.306807 140367016284160 training_loop.py:725] Total parameters: 152072288
I0123 12:36:13.307229 140367016284160 training_loop.py:739] Total state size: 0
I0123 12:36:13.329587 140367016284160 training_loop.py:492] Training loop: creating task for mode beam_search
I0123 12:36:13.329872 140367016284160 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0123 12:36:13.330167 140367016284160 training_loop.py:652] Compiling mode beam_search with jit.
I0123 12:36:13.330491 140367016284160 training_loop.py:89] registering functions: dict_keys([])
I0123 12:36:13.347383 140367016284160 graph.py:499] a b c = triangle a b c; d = midpoint d b c; e = midpoint e a c; f = midpoint f a b; g = incenter g a b c; h = foot h g a b; i = foot i g a c; j = foot j g b c; k = foot k j a g; l = mirror l j k; m = foot m i b g; n = mirror n i m; o = foot o h c g; p = mirror p h o; q = on_line q d l, on_line q p f ? coll n e q
I0123 12:36:14.821835 140367016284160 ddar.py:60] Depth 1/1000 time = 1.4072861671447754
I0123 12:36:25.814635 140367016284160 ddar.py:60] Depth 2/1000 time = 10.992566347122192
I0123 12:36:54.902555 140367016284160 ddar.py:60] Depth 3/1000 time = 29.08755588531494
I0123 12:37:30.798776 140367016284160 ddar.py:60] Depth 4/1000 time = 35.89579391479492
I0123 12:38:08.935729 140367016284160 ddar.py:60] Depth 5/1000 time = 38.13647413253784
I0123 12:38:47.790584 140367016284160 ddar.py:60] Depth 6/1000 time = 38.85440158843994
I0123 12:39:26.635077 140367016284160 ddar.py:60] Depth 7/1000 time = 38.8440625667572
I0123 12:39:26.902386 140367016284160 alphageometry.py:191] 
==========================
 * From theorem premises:
A B C D E F G H I J K L M N O P Q : Points
B,C,D are collinear [00]
DB = DC [01]
E,C,A are collinear [02]
EA = EC [03]
B,F,A are collinear [04]
FA = FB [05]
GCA = BCG [06]
BAG = GAC [07]
B,H,A are collinear [08]
GH  AB [09]
I,C,A are collinear [10]
GI  AC [11]
J,B,C are collinear [12]
GJ  BC [13]
K,A,G are collinear [14]
JK  AG [15]
J,K,L are collinear [16]
KJ = KL [17]
B,M,G are collinear [18]
IM  BG [19]
I,M,N are collinear [20]
MI = MN [21]
C,O,G are collinear [22]
HO  CG [23]
H,O,P are collinear [24]
OH = OP [25]
F,P,Q are collinear [26]
L,Q,D are collinear [27]

 * Auxiliary Constructions:
: Points


 * Proof steps:
001. I,C,A are collinear [10] & J,B,C are collinear [12] & GJ  BC [13] & GI  AC [11]   GIC = CJG [28]
002. I,C,A are collinear [10] & J,B,C are collinear [12] & GCA = BCG [06]   GCI = JCG [29]
003. GIC = CJG [28] & GCI = JCG [29] (Similar Triangles)  CI = CJ [30]
004. GIC = CJG [28] & GCI = JCG [29] (Similar Triangles)  GI = GJ [31]
005. I,C,A are collinear [10] & J,B,C are collinear [12] & GJ  BC [13] & GI  AC [11]   GIC = GJC [32]
006. GIC = GJC [32]   I,C,J,G are concyclic [33]
007. CI = CJ [30] & GI = GJ [31] & I,C,J,G are concyclic [33]   IG  CI [34]
008. CI = CJ [30] & GI = GJ [31] & I,C,J,G are concyclic [33]   JG  CJ [35]
009. I,C,J,G are concyclic [33]   IJC = IGC [36]
010. I,C,J,G are concyclic [33]   ICG = IJG [37]
011. I,C,A are collinear [10] & B,H,A are collinear [08] & GH  AB [09] & GI  AC [11]   GIA = AHG [38]
012. I,C,A are collinear [10] & B,H,A are collinear [08] & GAC = BAG [07]   GAI = HAG [39]
013. GIA = AHG [38] & GAI = HAG [39] (Similar Triangles)  GI = GH [40]
014. GIA = AHG [38] & GAI = HAG [39] (Similar Triangles)  AI = AH [41]
015. GI = GJ [31] & GI = GH [40]   G is the circumcenter of \Delta JIH [42]
016. J,B,C are collinear [12] & BC  GJ [13]   JB  JG [43]
017. G is the circumcenter of \Delta JIH [42] & JB  JG [43]   BJI = JHI [44]
018. C,O,G are collinear [22] & IJC = IGC [36] & J,B,C are collinear [12] & BJI = JHI [44]   IHJ = (IG-CO) [45]
019. J,K,L are collinear [16] & KJ = KL [17]   K is midpoint of JL [46]
020. K,A,G are collinear [14] & J,L,K are collinear [16] & AG  JK [15]   GK  JL [47]
021. K is midpoint of JL [46] & GK  JL [47]   GJ = GL [48]
022. H,O,P are collinear [24] & OH = OP [25]   O is midpoint of HP [49]
023. C,O,G are collinear [22] & H,O,P are collinear [24] & CG  HO [23]   GO  HP [50]
024. O is midpoint of HP [49] & GO  HP [50]   GH = GP [51]
025. GJ = GL [48] & GI = GJ [31] & GI = GH [40] & GH = GP [51]   G is the circumcenter of \Delta HLP [52]
026. GJ = GL [48] & GI = GJ [31] & GI = GH [40] & GH = GP [51]   G is the circumcenter of \Delta JLP [53]
027. G is the circumcenter of \Delta HLP [52] & O is midpoint of HP [49]   HLP = HGO [54]
028. GI = GH [40] & GI = GJ [31] & GJ = GL [48]   GL = GH [55]
029. GI = GH [40] & GI = GJ [31] & GJ = GL [48]   G is the circumcenter of \Delta JLH [56]
030. GL = GH [55]   HLG = GHL [57]
031. G is the circumcenter of \Delta JLH [56] & K is midpoint of JL [46]   LHJ = LGK [58]
032. K,A,G are collinear [14] & C,O,G are collinear [22] & HLP = HGO [54] & HLG = GHL [57] & LHJ = LGK [58]   (JH-KA) = (CO-LP) [59]
033. IHJ = (IG-CO) [45] & (JH-KA) = (CO-LP) [59]   (IG-LP) = (IH-KA) [60]
034. GI = GH [40] & AI = AH [41]   GA  IH [61]
035. K,A,G are collinear [14] & AG  JK [15]   KA  JK [62]
036. C,O,G are collinear [22] & CG  HO [23]   CO  HO [63]
037. KA  JK [62] & CO  HO [63]   (KA-HO) = (JK-CO) [64]
038. I,C,A are collinear [10] & B,H,A are collinear [08] & GH  AB [09] & GI  AC [11]   GIA = GHA [65]
039. GIA = GHA [65]   I,H,A,G are concyclic [66]
040. I,H,A,G are concyclic [66]   IHA = IGA [67]
041. I,H,A,G are concyclic [66]   IHG = IAG [68]
042. B,H,A are collinear [08] & GH  AB [09]   GH  HB [69]
043. G is the circumcenter of \Delta JIH [42] & GH  HB [69]   BHJ = HIJ [70]
044. GI = GJ [31] & CI = CJ [30]   GC  JI [71]
045. K,A,G are collinear [14] & IHA = IGA [67] & B,H,A are collinear [08] & BHJ = HIJ [70] & GC  JI [71] & HO  CG [23]   OHJ = (IG-KA) [72]
046. G is the circumcenter of \Delta JLP [53] & K is midpoint of JL [46]   JPL = JGK [73]
047. GH = GP [51] & GI = GH [40] & GI = GJ [31]   GJ = GP [74]
048. GH = GP [51] & GI = GH [40] & GI = GJ [31]   G is the circumcenter of \Delta JPH [75]
049. GJ = GP [74]   PJG = GPJ [76]
050. G is the circumcenter of \Delta JPH [75] & O is midpoint of HP [49]   PJH = PGO [77]
051. C,O,G are collinear [22] & K,A,G are collinear [14] & JPL = JGK [73] & PJG = GPJ [76] & PJH = PGO [77]   (CO-JH) = (LP-KA) [78]
052. OHJ = (IG-KA) [72] & (CO-JH) = (LP-KA) [78]   HOC = (IG-LP) [79]
053. B,C,D are collinear [00] & DB = DC [01]   D is midpoint of BC [80]
054. B,F,A are collinear [04] & FA = FB [05]   F is midpoint of BA [81]
055. D is midpoint of BC [80] & F is midpoint of BA [81]   DF  CA [82]
056. IG  CI [34] & (IG-LP) = (IH-KA) [60] & K,A,G are collinear [14] & GA  IH [61] & JK  AG [15] & (KA-HO) = (JK-CO) [64] & C,O,G are collinear [22] & HOC = (IG-LP) [79] & I,C,A are collinear [10] & DF  AC [82]   FD  PL [83]
057. FD  PL [83] & F,P,Q are collinear [26] & L,Q,D are collinear [27]   FQ:FD = PQ:LP [84]
058. E,C,A are collinear [02] & EA = EC [03]   E is midpoint of CA [85]
059. D is midpoint of BC [80] & E is midpoint of CA [85]   DE  BA [86]
060. E,C,A are collinear [02] & AB  DE [86] & AC  DF [82]   EDF = DEC [87]
061. E is midpoint of CA [85] & F is midpoint of BA [81]   EF  CB [88]
062. B,C,D are collinear [00] & E,C,A are collinear [02] & BC  EF [88] & AC  DF [82]   EFD = DCE [89]
063. EDF = DEC [87] & EFD = DCE [89] (Similar Triangles)  DF = EC [90]
064. FQ:FD = PQ:LP [84] & DF = EC [90] & EA = EC [03]   FQ:EA = PQ:LP [91]
065. EF  BC [88] & E,C,A are collinear [02] & B,F,A are collinear [04]   AE:EF = AC:CB [92]
066. AI = AH [41] & GI = GH [40] & I,H,A,G are concyclic [66]   HG  AH [93]
067. I,C,A are collinear [10] & E,C,A are collinear [02] & GI  AC [11]   IG  EI [94]
068. G is the circumcenter of \Delta JIH [42] & IG  EI [94]   EIJ = IHJ [95]
069. K,A,G are collinear [14] & EIJ = IHJ [95] & I,C,A are collinear [10] & E,C,A are collinear [02] & IHG = IAG [68] & GC  JI [71] & HO  CG [23]   (KA-HG) = OHJ [96]
070. I,M,N are collinear [20] & MI = MN [21]   M is midpoint of IN [97]
071. M,B,G are collinear [18] & I,M,N are collinear [20] & BG  IM [19]   GM  IN [98]
072. M is midpoint of IN [97] & GM  IN [98]   GI = GN [99]
073. GI = GJ [31] & GI = GN [99]   G is the circumcenter of \Delta JIN [100]
074. GI = GJ [31] & GI = GN [99]   GJ = GN [101]
075. G is the circumcenter of \Delta JIN [100] & M is midpoint of IN [97]   NJI = NGM [102]
076. GJ = GN [101]   GJN = JNG [103]
077. GJ = GL [48] & GI = GN [99] & GI = GJ [31]   G is the circumcenter of \Delta JLN [104]
078. GJ = GL [48] & GI = GN [99] & GI = GJ [31]   G is the circumcenter of \Delta ILN [105]
079. G is the circumcenter of \Delta JLN [104] & K is midpoint of JL [46]   JNL = JGK [106]
080. K,A,G are collinear [14] & B,M,G are collinear [18] & NJI = NGM [102] & GJN = JNG [103] & JNL = JGK [106] & GC  JI [71] & HO  CG [23]   (KA-LN) = (HO-BM) [107]
081. (KA-HG) = OHJ [96] & (KA-LN) = (HO-BM) [107]   (JH-BM) = (HG-LN) [108]
082. B,F,A are collinear [04] & GH  AB [09]   HG  BF [109]
083. B,M,G are collinear [18] & BG  IM [19]   BM  IM [110]
084. HG  BF [109] & BM  IM [110]   (HG-BM) = (BF-IM) [111]
085. J,B,C are collinear [12] & B,H,A are collinear [08] & GH  AB [09] & GJ  BC [13]   GJB = GHB [112]
086. GJB = GHB [112]   J,B,H,G are concyclic [113]
087. J,B,H,G are concyclic [113]   JBG = JHG [114]
088. J,B,H,G are concyclic [113]   JHB = JGB [115]
089. G is the circumcenter of \Delta JLH [56] & JB  JG [43]   BJL = JHL [116]
090. G is the circumcenter of \Delta JLH [56] & GH  HB [69]   BHJ = HLJ [117]
091. (HG-BM) = (BF-IM) [111] & B,M,G are collinear [18] & B,F,A are collinear [04] & JBG = JHG [114] & J,B,C are collinear [12] & BJL = JHL [116] & J,K,L are collinear [16] & BHJ = HLJ [117] & B,H,A are collinear [08] & AB  DE [86]   (JH-ED) = (IM-ED) [118]
092. (JH-ED) = (IM-ED) [118]   JH  IM [119]
093. KA  JK [62] & BM  IM [110]   (KA-IM) = (JK-BM) [120]
094. B,M,G are collinear [18] & JBG = JHG [114] & J,B,C are collinear [12] & BJI = JHI [44] & GC  JI [71] & HO  CG [23]   IHO = (HG-BM) [121]
095. G is the circumcenter of \Delta ILN [105] & M is midpoint of IN [97]   ILN = IGM [122]
096. GI = GJ [31] & GJ = GL [48]   GL = GI [123]
097. GI = GJ [31] & GJ = GL [48]   G is the circumcenter of \Delta JLI [124]
098. GL = GI [123]   ILG = GIL [125]
099. G is the circumcenter of \Delta JLI [124] & K is midpoint of JL [46]   LIJ = LGK [126]
100. K,A,G are collinear [14] & B,M,G are collinear [18] & ILN = IGM [122] & ILG = GIL [125] & LIJ = LGK [126] & GC  JI [71] & HO  CG [23]   (HO-KA) = (BM-LN) [127]
101. IHO = (HG-BM) [121] & (HO-KA) = (BM-LN) [127]   (IH-KA) = (HG-LN) [128]
102. HG  AH [93] & (JH-BM) = (HG-LN) [108] & B,M,G are collinear [18] & HJ  IM [119] & (KA-IM) = (JK-BM) [120] & K,A,G are collinear [14] & (IH-KA) = (HG-LN) [128] & GA  IH [61] & JK  AG [15] & B,H,A are collinear [08] & IG  CI [34] & (IG-LP) = (IH-KA) [60] & (KA-HO) = (JK-CO) [64] & C,O,G are collinear [22] & HOC = (IG-LP) [79] & I,C,A are collinear [10]   NLP = BAC [129]
103. C,O,G are collinear [22] & ICG = IJG [37] & I,C,A are collinear [10] & EIJ = IHJ [95] & E,C,A are collinear [02]   IHJ = (CO-JG) [130]
104. GI = GN [99] & GI = GH [40] & GH = GP [51]   G is the circumcenter of \Delta HNP [131]
105. GI = GN [99] & GI = GH [40] & GH = GP [51]   G is the circumcenter of \Delta INP [132]
106. G is the circumcenter of \Delta HNP [131] & O is midpoint of HP [49]   HNP = HGO [133]
107. GI = GH [40] & GI = GN [99]   GN = GH [134]
108. GI = GH [40] & GI = GN [99]   G is the circumcenter of \Delta INH [135]
109. GN = GH [134]   HNG = GHN [136]
110. G is the circumcenter of \Delta INH [135] & M is midpoint of IN [97]   NHI = NGM [137]
111. B,M,G are collinear [18] & C,O,G are collinear [22] & HNP = HGO [133] & HNG = GHN [136] & NHI = NGM [137]   (IH-BM) = (CO-PN) [138]
112. IHJ = (CO-JG) [130] & (IH-BM) = (CO-PN) [138]   (JH-BM) = (JG-PN) [139]
113. CO  HO [63] & BM  IM [110]   (CO-IM) = (HO-BM) [140]
114. B,M,G are collinear [18] & JHB = JGB [115] & B,H,A are collinear [08] & BHJ = HIJ [70] & GC  JI [71] & HO  CG [23]   IHO = (BM-JG) [141]
115. G is the circumcenter of \Delta INP [132] & M is midpoint of IN [97]   IPN = IGM [142]
116. GH = GP [51] & GI = GH [40]   GI = GP [143]
117. GH = GP [51] & GI = GH [40]   G is the circumcenter of \Delta IPH [144]
118. GI = GP [143]   GIP = IPG [145]
119. G is the circumcenter of \Delta IPH [144] & O is midpoint of HP [49]   PIH = PGO [146]
120. C,O,G are collinear [22] & B,M,G are collinear [18] & IPN = IGM [142] & GIP = IPG [145] & PIH = PGO [146]   (IH-CO) = (BM-PN) [147]
121. IHO = (BM-JG) [141] & (IH-CO) = (BM-PN) [147]   HOC = (JG-PN) [148]
122. JG  CJ [35] & (JH-BM) = (JG-PN) [139] & B,M,G are collinear [18] & HJ  IM [119] & (CO-IM) = (HO-BM) [140] & C,O,G are collinear [22] & HOC = (JG-PN) [148] & J,B,C are collinear [12] & IG  CI [34] & (IG-LP) = (IH-KA) [60] & K,A,G are collinear [14] & GA  IH [61] & JK  AG [15] & (KA-HO) = (JK-CO) [64] & HOC = (IG-LP) [79] & I,C,A are collinear [10]   NPL = BCA [149]
123. NLP = BAC [129] & NPL = BCA [149] (Similar Triangles)  LP:PN = CA:BC [150]
124. AE:EF = AC:CB [92] & EA = EC [03] & LP:PN = CA:BC [150]   LP:PN = EA:EF [151]
125. FQ:EA = PQ:LP [91] & LP:PN = EA:EF [151]   PQ:PN = FQ:EF [152]
126. B,F,A are collinear [04] & E,C,A are collinear [02] & NLP = BAC [129]   NLP = FAE [153]
127. E,C,A are collinear [02] & JG  CJ [35] & (JH-BM) = (JG-PN) [139] & B,M,G are collinear [18] & HJ  IM [119] & (CO-IM) = (HO-BM) [140] & C,O,G are collinear [22] & HOC = (JG-PN) [148] & J,B,C are collinear [12] & IG  CI [34] & (IG-LP) = (IH-KA) [60] & K,A,G are collinear [14] & GA  IH [61] & JK  AG [15] & (KA-HO) = (JK-CO) [64] & HOC = (IG-LP) [79] & I,C,A are collinear [10] & BC  EF [88]   NPL = FEA [154]
128. NLP = FAE [153] & NPL = FEA [154] (Similar Triangles)  LNP = AFE [155]
129. F,P,Q are collinear [26] & LNP = AFE [155] & B,F,A are collinear [04] & HG  AH [93] & (JH-BM) = (HG-LN) [108] & B,M,G are collinear [18] & HJ  IM [119] & (KA-IM) = (JK-BM) [120] & K,A,G are collinear [14] & (IH-KA) = (HG-LN) [128] & GA  IH [61] & JK  AG [15] & B,H,A are collinear [08]   QFE = QPN [156]
130. PQ:PN = FQ:EF [152] & QFE = QPN [156] (Similar Triangles)  FQE = PQN [157]
131. FQE = PQN [157] & F,P,Q are collinear [26]   EQ  NQ [158]
132. EQ  NQ [158]   E,Q,N are collinear
==========================

