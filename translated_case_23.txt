I0123 11:00:18.143404 140516435906560 inference_utils.py:69] Parsing gin configuration.
I0123 11:00:18.143508 140516435906560 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0123 11:00:18.143719 140516435906560 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0123 11:00:18.143755 140516435906560 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0123 11:00:18.143785 140516435906560 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0123 11:00:18.143813 140516435906560 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0123 11:00:18.143841 140516435906560 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0123 11:00:18.143868 140516435906560 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0123 11:00:18.143895 140516435906560 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0123 11:00:18.143923 140516435906560 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=32
I0123 11:00:18.143949 140516435906560 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0123 11:00:18.143975 140516435906560 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0123 11:00:18.144022 140516435906560 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0123 11:00:18.144159 140516435906560 resource_reader.py:55] Path not found: base_htrans.gin
I0123 11:00:18.144369 140516435906560 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0123 11:00:18.144477 140516435906560 resource_reader.py:55] Path not found: trainer_configuration.gin
I0123 11:00:18.150702 140516435906560 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0123 11:00:18.150833 140516435906560 resource_reader.py:55] Path not found: size/medium_150M.gin
I0123 11:00:18.151155 140516435906560 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0123 11:00:18.151263 140516435906560 resource_reader.py:55] Path not found: options/positions_t5.gin
I0123 11:00:18.151542 140516435906560 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0123 11:00:18.151645 140516435906560 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0123 11:00:18.152053 140516435906560 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0123 11:00:18.152155 140516435906560 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0123 11:00:18.155861 140516435906560 training_loop.py:334] ==== Training loop: initializing model ====
I0123 11:00:18.258372 140516435906560 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0123 11:00:18.259121 140516435906560 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0123 11:00:18.266233 140516435906560 training_loop.py:335] Process 0 of 1
I0123 11:00:18.266294 140516435906560 training_loop.py:336] Local device count = 1
I0123 11:00:18.266335 140516435906560 training_loop.py:337] Number of replicas = 1
I0123 11:00:18.266369 140516435906560 training_loop.py:339] Using random number seed 42
I0123 11:00:18.748078 140516435906560 training_loop.py:359] Initializing the model.
I0123 11:00:19.154267 140516435906560 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.154551 140516435906560 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 11:00:19.154657 140516435906560 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:00:19.154738 140516435906560 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:00:19.154816 140516435906560 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:00:19.154900 140516435906560 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:00:19.154973 140516435906560 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:00:19.155041 140516435906560 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:00:19.155110 140516435906560 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:00:19.155179 140516435906560 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:00:19.155246 140516435906560 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:00:19.155313 140516435906560 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:00:19.155380 140516435906560 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:00:19.155447 140516435906560 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:00:19.155487 140516435906560 attention.py:418] Single window, no scan.
I0123 11:00:19.155533 140516435906560 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 11:00:19.155645 140516435906560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:00:19.155688 140516435906560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:00:19.155720 140516435906560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:00:19.157747 140516435906560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.163124 140516435906560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:00:19.173830 140516435906560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.174125 140516435906560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:00:19.178531 140516435906560 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:00:19.189220 140516435906560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:00:19.189285 140516435906560 attention.py:418] Single window, no scan.
I0123 11:00:19.189327 140516435906560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:00:19.189362 140516435906560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.189429 140516435906560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.190626 140516435906560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.190709 140516435906560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.191437 140516435906560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.193921 140516435906560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.199677 140516435906560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.201391 140516435906560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.201483 140516435906560 transformer_layer.py:468] tlayer: End windows.
I0123 11:00:19.201521 140516435906560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:00:19.201586 140516435906560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.201727 140516435906560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:00:19.202069 140516435906560 nn_components.py:325] mlp: activation = None
I0123 11:00:19.202121 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:00:19.204048 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.204155 140516435906560 nn_components.py:261] mlp: residual
I0123 11:00:19.207084 140516435906560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.207169 140516435906560 transformer_base.py:443] tbase: final FFN
I0123 11:00:19.207675 140516435906560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:00:19.217865 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:00:19.226785 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.226891 140516435906560 nn_components.py:261] mlp: residual
I0123 11:00:19.227196 140516435906560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.227281 140516435906560 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 11:00:19.227393 140516435906560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:00:19.227435 140516435906560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:00:19.227468 140516435906560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:00:19.229330 140516435906560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.231852 140516435906560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:00:19.237548 140516435906560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.237825 140516435906560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:00:19.240512 140516435906560 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:00:19.244377 140516435906560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:00:19.244438 140516435906560 attention.py:418] Single window, no scan.
I0123 11:00:19.244478 140516435906560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:00:19.244511 140516435906560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.244575 140516435906560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.245153 140516435906560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.245237 140516435906560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.245610 140516435906560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.246422 140516435906560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.248940 140516435906560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.249577 140516435906560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.249667 140516435906560 transformer_layer.py:468] tlayer: End windows.
I0123 11:00:19.249706 140516435906560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:00:19.249775 140516435906560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.249908 140516435906560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:00:19.250243 140516435906560 nn_components.py:325] mlp: activation = None
I0123 11:00:19.250292 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:00:19.252264 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.252362 140516435906560 nn_components.py:261] mlp: residual
I0123 11:00:19.254931 140516435906560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.255016 140516435906560 transformer_base.py:443] tbase: final FFN
I0123 11:00:19.255461 140516435906560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:00:19.257815 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:00:19.259746 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.259844 140516435906560 nn_components.py:261] mlp: residual
I0123 11:00:19.260148 140516435906560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.260232 140516435906560 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 11:00:19.260344 140516435906560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:00:19.260385 140516435906560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:00:19.260417 140516435906560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:00:19.262341 140516435906560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.264744 140516435906560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:00:19.270778 140516435906560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.271047 140516435906560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:00:19.273744 140516435906560 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:00:19.277609 140516435906560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:00:19.277675 140516435906560 attention.py:418] Single window, no scan.
I0123 11:00:19.277714 140516435906560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:00:19.277747 140516435906560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.277812 140516435906560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.278379 140516435906560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.278460 140516435906560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.278830 140516435906560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.279612 140516435906560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.282162 140516435906560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.282842 140516435906560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.282924 140516435906560 transformer_layer.py:468] tlayer: End windows.
I0123 11:00:19.282961 140516435906560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:00:19.283025 140516435906560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.283159 140516435906560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:00:19.283485 140516435906560 nn_components.py:325] mlp: activation = None
I0123 11:00:19.283532 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:00:19.285477 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.285575 140516435906560 nn_components.py:261] mlp: residual
I0123 11:00:19.288106 140516435906560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.288197 140516435906560 transformer_base.py:443] tbase: final FFN
I0123 11:00:19.288692 140516435906560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:00:19.291025 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:00:19.292976 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.293076 140516435906560 nn_components.py:261] mlp: residual
I0123 11:00:19.293379 140516435906560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.293463 140516435906560 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 11:00:19.293575 140516435906560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:00:19.293618 140516435906560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:00:19.293658 140516435906560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:00:19.295569 140516435906560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.298008 140516435906560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:00:19.303727 140516435906560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.303998 140516435906560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:00:19.306686 140516435906560 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:00:19.310504 140516435906560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:00:19.310564 140516435906560 attention.py:418] Single window, no scan.
I0123 11:00:19.310602 140516435906560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:00:19.310638 140516435906560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.310702 140516435906560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.311261 140516435906560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.311341 140516435906560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.311712 140516435906560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.312494 140516435906560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.315739 140516435906560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.316490 140516435906560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.316577 140516435906560 transformer_layer.py:468] tlayer: End windows.
I0123 11:00:19.316615 140516435906560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:00:19.316680 140516435906560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.316820 140516435906560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:00:19.317175 140516435906560 nn_components.py:325] mlp: activation = None
I0123 11:00:19.317221 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:00:19.319194 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.319291 140516435906560 nn_components.py:261] mlp: residual
I0123 11:00:19.321899 140516435906560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.321995 140516435906560 transformer_base.py:443] tbase: final FFN
I0123 11:00:19.322428 140516435906560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:00:19.324723 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:00:19.326692 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.326793 140516435906560 nn_components.py:261] mlp: residual
I0123 11:00:19.327092 140516435906560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.327177 140516435906560 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 11:00:19.327289 140516435906560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:00:19.327331 140516435906560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:00:19.327365 140516435906560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:00:19.329291 140516435906560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.331716 140516435906560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:00:19.337411 140516435906560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.337696 140516435906560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:00:19.340419 140516435906560 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:00:19.344224 140516435906560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:00:19.344285 140516435906560 attention.py:418] Single window, no scan.
I0123 11:00:19.344323 140516435906560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:00:19.344357 140516435906560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.344421 140516435906560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.345002 140516435906560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.345085 140516435906560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.345459 140516435906560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.346252 140516435906560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.349154 140516435906560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.349796 140516435906560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.349879 140516435906560 transformer_layer.py:468] tlayer: End windows.
I0123 11:00:19.349916 140516435906560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:00:19.349978 140516435906560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.350119 140516435906560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:00:19.350448 140516435906560 nn_components.py:325] mlp: activation = None
I0123 11:00:19.350495 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:00:19.352413 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.352511 140516435906560 nn_components.py:261] mlp: residual
I0123 11:00:19.355096 140516435906560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.355180 140516435906560 transformer_base.py:443] tbase: final FFN
I0123 11:00:19.355605 140516435906560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:00:19.357941 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:00:19.359931 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.360030 140516435906560 nn_components.py:261] mlp: residual
I0123 11:00:19.360335 140516435906560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.360421 140516435906560 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 11:00:19.360534 140516435906560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:00:19.360577 140516435906560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:00:19.360610 140516435906560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:00:19.362479 140516435906560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.364915 140516435906560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:00:19.370635 140516435906560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.370901 140516435906560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:00:19.373625 140516435906560 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:00:19.377404 140516435906560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:00:19.377466 140516435906560 attention.py:418] Single window, no scan.
I0123 11:00:19.377505 140516435906560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:00:19.377538 140516435906560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.377603 140516435906560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.378234 140516435906560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.378317 140516435906560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.378701 140516435906560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.379500 140516435906560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.382007 140516435906560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.382640 140516435906560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.382720 140516435906560 transformer_layer.py:468] tlayer: End windows.
I0123 11:00:19.382757 140516435906560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:00:19.382819 140516435906560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.382957 140516435906560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:00:19.383287 140516435906560 nn_components.py:325] mlp: activation = None
I0123 11:00:19.383335 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:00:19.385255 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.385356 140516435906560 nn_components.py:261] mlp: residual
I0123 11:00:19.388006 140516435906560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.388092 140516435906560 transformer_base.py:443] tbase: final FFN
I0123 11:00:19.388528 140516435906560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:00:19.390903 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:00:19.392856 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.392956 140516435906560 nn_components.py:261] mlp: residual
I0123 11:00:19.393259 140516435906560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.393343 140516435906560 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 11:00:19.393454 140516435906560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:00:19.393496 140516435906560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:00:19.393529 140516435906560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:00:19.395397 140516435906560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.397903 140516435906560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:00:19.403582 140516435906560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.403849 140516435906560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:00:19.406506 140516435906560 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:00:19.410301 140516435906560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:00:19.410361 140516435906560 attention.py:418] Single window, no scan.
I0123 11:00:19.410400 140516435906560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:00:19.410433 140516435906560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.410498 140516435906560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.411064 140516435906560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.411143 140516435906560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.411508 140516435906560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.412301 140516435906560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.414794 140516435906560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.415419 140516435906560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.415500 140516435906560 transformer_layer.py:468] tlayer: End windows.
I0123 11:00:19.415537 140516435906560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:00:19.415598 140516435906560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.415735 140516435906560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:00:19.416059 140516435906560 nn_components.py:325] mlp: activation = None
I0123 11:00:19.416106 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:00:19.418110 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.418209 140516435906560 nn_components.py:261] mlp: residual
I0123 11:00:19.420736 140516435906560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.420819 140516435906560 transformer_base.py:443] tbase: final FFN
I0123 11:00:19.421246 140516435906560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:00:19.423905 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:00:19.425845 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.425951 140516435906560 nn_components.py:261] mlp: residual
I0123 11:00:19.426257 140516435906560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.426343 140516435906560 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 11:00:19.426454 140516435906560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:00:19.426496 140516435906560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:00:19.426530 140516435906560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:00:19.565047 140516435906560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.568171 140516435906560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:00:19.574148 140516435906560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.574455 140516435906560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:00:19.577207 140516435906560 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:00:19.581181 140516435906560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:00:19.581245 140516435906560 attention.py:418] Single window, no scan.
I0123 11:00:19.581285 140516435906560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:00:19.581321 140516435906560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.581391 140516435906560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.582014 140516435906560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.582096 140516435906560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.582481 140516435906560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.583281 140516435906560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.585905 140516435906560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.586556 140516435906560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.586639 140516435906560 transformer_layer.py:468] tlayer: End windows.
I0123 11:00:19.586677 140516435906560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:00:19.586740 140516435906560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.586872 140516435906560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:00:19.587218 140516435906560 nn_components.py:325] mlp: activation = None
I0123 11:00:19.587267 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:00:19.589196 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.589294 140516435906560 nn_components.py:261] mlp: residual
I0123 11:00:19.591910 140516435906560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.591994 140516435906560 transformer_base.py:443] tbase: final FFN
I0123 11:00:19.592430 140516435906560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:00:19.594805 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:00:19.596750 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.596863 140516435906560 nn_components.py:261] mlp: residual
I0123 11:00:19.597160 140516435906560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.597246 140516435906560 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 11:00:19.597360 140516435906560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:00:19.597402 140516435906560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:00:19.597436 140516435906560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:00:19.599456 140516435906560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.601916 140516435906560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:00:19.607684 140516435906560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.607960 140516435906560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:00:19.610699 140516435906560 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:00:19.614515 140516435906560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:00:19.614575 140516435906560 attention.py:418] Single window, no scan.
I0123 11:00:19.614614 140516435906560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:00:19.614648 140516435906560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.614713 140516435906560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.615283 140516435906560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.615364 140516435906560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.615735 140516435906560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.616520 140516435906560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.619097 140516435906560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.619735 140516435906560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.619817 140516435906560 transformer_layer.py:468] tlayer: End windows.
I0123 11:00:19.619854 140516435906560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:00:19.619917 140516435906560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.620049 140516435906560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:00:19.620380 140516435906560 nn_components.py:325] mlp: activation = None
I0123 11:00:19.620428 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:00:19.622388 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.622488 140516435906560 nn_components.py:261] mlp: residual
I0123 11:00:19.625097 140516435906560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.625181 140516435906560 transformer_base.py:443] tbase: final FFN
I0123 11:00:19.625613 140516435906560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:00:19.627934 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:00:19.629930 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.630029 140516435906560 nn_components.py:261] mlp: residual
I0123 11:00:19.630329 140516435906560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.630420 140516435906560 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 11:00:19.630532 140516435906560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:00:19.630574 140516435906560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:00:19.630608 140516435906560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:00:19.632451 140516435906560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.635134 140516435906560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:00:19.640748 140516435906560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.641022 140516435906560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:00:19.644093 140516435906560 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:00:19.647865 140516435906560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:00:19.647925 140516435906560 attention.py:418] Single window, no scan.
I0123 11:00:19.647964 140516435906560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:00:19.647997 140516435906560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.648063 140516435906560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.648670 140516435906560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.648750 140516435906560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.649121 140516435906560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.649919 140516435906560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.652407 140516435906560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.653044 140516435906560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.653127 140516435906560 transformer_layer.py:468] tlayer: End windows.
I0123 11:00:19.653164 140516435906560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:00:19.653226 140516435906560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.653361 140516435906560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:00:19.653697 140516435906560 nn_components.py:325] mlp: activation = None
I0123 11:00:19.653745 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:00:19.655681 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.655779 140516435906560 nn_components.py:261] mlp: residual
I0123 11:00:19.658365 140516435906560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.658449 140516435906560 transformer_base.py:443] tbase: final FFN
I0123 11:00:19.658883 140516435906560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:00:19.661221 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:00:19.663153 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.663253 140516435906560 nn_components.py:261] mlp: residual
I0123 11:00:19.663549 140516435906560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.663640 140516435906560 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 11:00:19.663754 140516435906560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:00:19.663797 140516435906560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:00:19.663831 140516435906560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:00:19.665682 140516435906560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.668170 140516435906560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:00:19.673812 140516435906560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.674081 140516435906560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:00:19.676736 140516435906560 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:00:19.680548 140516435906560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:00:19.680608 140516435906560 attention.py:418] Single window, no scan.
I0123 11:00:19.680647 140516435906560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:00:19.680680 140516435906560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.680744 140516435906560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.681305 140516435906560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.681385 140516435906560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.681763 140516435906560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.682541 140516435906560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.685031 140516435906560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.685672 140516435906560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.685753 140516435906560 transformer_layer.py:468] tlayer: End windows.
I0123 11:00:19.685790 140516435906560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:00:19.685850 140516435906560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.685981 140516435906560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:00:19.686298 140516435906560 nn_components.py:325] mlp: activation = None
I0123 11:00:19.686344 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:00:19.688296 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.688392 140516435906560 nn_components.py:261] mlp: residual
I0123 11:00:19.691209 140516435906560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.691311 140516435906560 transformer_base.py:443] tbase: final FFN
I0123 11:00:19.691737 140516435906560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:00:19.694081 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:00:19.696003 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.696103 140516435906560 nn_components.py:261] mlp: residual
I0123 11:00:19.696403 140516435906560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.696488 140516435906560 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 11:00:19.696611 140516435906560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:00:19.696654 140516435906560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:00:19.696687 140516435906560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:00:19.698594 140516435906560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.701006 140516435906560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:00:19.706661 140516435906560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.706924 140516435906560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:00:19.709581 140516435906560 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:00:19.713418 140516435906560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:00:19.713480 140516435906560 attention.py:418] Single window, no scan.
I0123 11:00:19.713519 140516435906560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:00:19.713553 140516435906560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.713617 140516435906560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.714196 140516435906560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.714277 140516435906560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.714647 140516435906560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.715429 140516435906560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.717954 140516435906560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.718949 140516435906560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.719033 140516435906560 transformer_layer.py:468] tlayer: End windows.
I0123 11:00:19.719070 140516435906560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:00:19.719134 140516435906560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.719271 140516435906560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:00:19.719597 140516435906560 nn_components.py:325] mlp: activation = None
I0123 11:00:19.719645 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:00:19.721590 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.721695 140516435906560 nn_components.py:261] mlp: residual
I0123 11:00:19.724245 140516435906560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.724329 140516435906560 transformer_base.py:443] tbase: final FFN
I0123 11:00:19.724819 140516435906560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:00:19.727106 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:00:19.729034 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.729134 140516435906560 nn_components.py:261] mlp: residual
I0123 11:00:19.729443 140516435906560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.729739 140516435906560 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:00:19.729814 140516435906560 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:00:19.729882 140516435906560 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:00:19.729943 140516435906560 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:00:19.730001 140516435906560 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:00:19.730057 140516435906560 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:00:19.730112 140516435906560 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:00:19.730167 140516435906560 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:00:19.730221 140516435906560 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:00:19.730274 140516435906560 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:00:19.730330 140516435906560 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:00:19.730386 140516435906560 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:00:19.730426 140516435906560 decoder_stack.py:344] dstack: Final layernorm.
I0123 11:00:19.733989 140516435906560 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:00:19.782178 140516435906560 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.782269 140516435906560 decoder_stack.py:333] dstack: autoregressive generator.
I0123 11:00:19.782326 140516435906560 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 11:00:19.782432 140516435906560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:00:19.782474 140516435906560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:00:19.782508 140516435906560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:00:19.782573 140516435906560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.785055 140516435906560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:00:19.790625 140516435906560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.790894 140516435906560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:00:19.793565 140516435906560 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:00:19.810361 140516435906560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:00:19.810424 140516435906560 attention.py:418] Single window, no scan.
I0123 11:00:19.810463 140516435906560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:00:19.810497 140516435906560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.810564 140516435906560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.811715 140516435906560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.811798 140516435906560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.812517 140516435906560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.814568 140516435906560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.819383 140516435906560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.820717 140516435906560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.820810 140516435906560 transformer_layer.py:468] tlayer: End windows.
I0123 11:00:19.820849 140516435906560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:00:19.820913 140516435906560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.821053 140516435906560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:00:19.821168 140516435906560 nn_components.py:325] mlp: activation = None
I0123 11:00:19.821213 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:00:19.823164 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.823264 140516435906560 nn_components.py:261] mlp: residual
I0123 11:00:19.825759 140516435906560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.825844 140516435906560 transformer_base.py:443] tbase: final FFN
I0123 11:00:19.825957 140516435906560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:00:19.828241 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:00:19.830229 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.830331 140516435906560 nn_components.py:261] mlp: residual
I0123 11:00:19.830630 140516435906560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.830716 140516435906560 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 11:00:19.830829 140516435906560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:00:19.830871 140516435906560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:00:19.830904 140516435906560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:00:19.830970 140516435906560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.833292 140516435906560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:00:19.838896 140516435906560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.839164 140516435906560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:00:19.841897 140516435906560 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:00:19.855162 140516435906560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:00:19.855222 140516435906560 attention.py:418] Single window, no scan.
I0123 11:00:19.855262 140516435906560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:00:19.855295 140516435906560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.855361 140516435906560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.855922 140516435906560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.856001 140516435906560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.856367 140516435906560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.857071 140516435906560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.859606 140516435906560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.860241 140516435906560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.860321 140516435906560 transformer_layer.py:468] tlayer: End windows.
I0123 11:00:19.860365 140516435906560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:00:19.860429 140516435906560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.860566 140516435906560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:00:19.860681 140516435906560 nn_components.py:325] mlp: activation = None
I0123 11:00:19.860724 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:00:19.862723 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.862822 140516435906560 nn_components.py:261] mlp: residual
I0123 11:00:19.865298 140516435906560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.865382 140516435906560 transformer_base.py:443] tbase: final FFN
I0123 11:00:19.865492 140516435906560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:00:19.867823 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:00:19.869788 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.869898 140516435906560 nn_components.py:261] mlp: residual
I0123 11:00:19.870212 140516435906560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.870298 140516435906560 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 11:00:19.870410 140516435906560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:00:19.870452 140516435906560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:00:19.870485 140516435906560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:00:19.870552 140516435906560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.872896 140516435906560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:00:19.878458 140516435906560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.878725 140516435906560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:00:19.881449 140516435906560 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:00:19.894388 140516435906560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:00:19.894450 140516435906560 attention.py:418] Single window, no scan.
I0123 11:00:19.894488 140516435906560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:00:19.894521 140516435906560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.894585 140516435906560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.895152 140516435906560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.895232 140516435906560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.895598 140516435906560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.896304 140516435906560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.898806 140516435906560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.899476 140516435906560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.899558 140516435906560 transformer_layer.py:468] tlayer: End windows.
I0123 11:00:19.899595 140516435906560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:00:19.899664 140516435906560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.899798 140516435906560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:00:19.899910 140516435906560 nn_components.py:325] mlp: activation = None
I0123 11:00:19.899952 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:00:19.901908 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.902008 140516435906560 nn_components.py:261] mlp: residual
I0123 11:00:19.904495 140516435906560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.904579 140516435906560 transformer_base.py:443] tbase: final FFN
I0123 11:00:19.904689 140516435906560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:00:19.906951 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:00:19.908904 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.909003 140516435906560 nn_components.py:261] mlp: residual
I0123 11:00:19.909296 140516435906560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.909382 140516435906560 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 11:00:19.909493 140516435906560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:00:19.909534 140516435906560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:00:19.909569 140516435906560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:00:19.909636 140516435906560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.911965 140516435906560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:00:19.917537 140516435906560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.917814 140516435906560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:00:19.920556 140516435906560 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:00:19.933368 140516435906560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:00:19.933429 140516435906560 attention.py:418] Single window, no scan.
I0123 11:00:19.933468 140516435906560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:00:19.933501 140516435906560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.933565 140516435906560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.934134 140516435906560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.934217 140516435906560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.934589 140516435906560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.935313 140516435906560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.937813 140516435906560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.938451 140516435906560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.938533 140516435906560 transformer_layer.py:468] tlayer: End windows.
I0123 11:00:19.938570 140516435906560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:00:19.938634 140516435906560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.938789 140516435906560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:00:19.938909 140516435906560 nn_components.py:325] mlp: activation = None
I0123 11:00:19.938951 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:00:19.940920 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.941016 140516435906560 nn_components.py:261] mlp: residual
I0123 11:00:19.943503 140516435906560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.943587 140516435906560 transformer_base.py:443] tbase: final FFN
I0123 11:00:19.943698 140516435906560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:00:19.945953 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:00:19.947842 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.947943 140516435906560 nn_components.py:261] mlp: residual
I0123 11:00:19.948237 140516435906560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.948323 140516435906560 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 11:00:19.948436 140516435906560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:00:19.948478 140516435906560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:00:19.948512 140516435906560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:00:19.948578 140516435906560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.951209 140516435906560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:00:19.956755 140516435906560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.957027 140516435906560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:00:19.959668 140516435906560 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:00:19.972498 140516435906560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:00:19.972561 140516435906560 attention.py:418] Single window, no scan.
I0123 11:00:19.972601 140516435906560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:00:19.972635 140516435906560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.972702 140516435906560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.973266 140516435906560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.973348 140516435906560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.977933 140516435906560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.978840 140516435906560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.981590 140516435906560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.982292 140516435906560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.982379 140516435906560 transformer_layer.py:468] tlayer: End windows.
I0123 11:00:19.982417 140516435906560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:00:19.982495 140516435906560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.982639 140516435906560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:00:19.982763 140516435906560 nn_components.py:325] mlp: activation = None
I0123 11:00:19.982806 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:00:19.984867 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.984965 140516435906560 nn_components.py:261] mlp: residual
I0123 11:00:19.987509 140516435906560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.987594 140516435906560 transformer_base.py:443] tbase: final FFN
I0123 11:00:19.987708 140516435906560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:00:19.990072 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:00:19.991991 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.992089 140516435906560 nn_components.py:261] mlp: residual
I0123 11:00:19.992379 140516435906560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.992467 140516435906560 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 11:00:19.992585 140516435906560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:00:19.992629 140516435906560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:00:19.992663 140516435906560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:00:19.992732 140516435906560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:19.995052 140516435906560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:00:20.000604 140516435906560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.000870 140516435906560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:00:20.003647 140516435906560 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:00:20.016783 140516435906560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:00:20.016844 140516435906560 attention.py:418] Single window, no scan.
I0123 11:00:20.016883 140516435906560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:00:20.016916 140516435906560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.016981 140516435906560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.017560 140516435906560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.017646 140516435906560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.018026 140516435906560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.018744 140516435906560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.021265 140516435906560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.021902 140516435906560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.021984 140516435906560 transformer_layer.py:468] tlayer: End windows.
I0123 11:00:20.022021 140516435906560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:00:20.022083 140516435906560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.022222 140516435906560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:00:20.022346 140516435906560 nn_components.py:325] mlp: activation = None
I0123 11:00:20.022388 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:00:20.024384 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.024481 140516435906560 nn_components.py:261] mlp: residual
I0123 11:00:20.026962 140516435906560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.027047 140516435906560 transformer_base.py:443] tbase: final FFN
I0123 11:00:20.027159 140516435906560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:00:20.029426 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:00:20.031332 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.031433 140516435906560 nn_components.py:261] mlp: residual
I0123 11:00:20.031725 140516435906560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.031810 140516435906560 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 11:00:20.031920 140516435906560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:00:20.031961 140516435906560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:00:20.031994 140516435906560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:00:20.032059 140516435906560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.034368 140516435906560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:00:20.040012 140516435906560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.040280 140516435906560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:00:20.042940 140516435906560 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:00:20.055907 140516435906560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:00:20.055968 140516435906560 attention.py:418] Single window, no scan.
I0123 11:00:20.056008 140516435906560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:00:20.056041 140516435906560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.056105 140516435906560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.056679 140516435906560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.056760 140516435906560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.057130 140516435906560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.057838 140516435906560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.060356 140516435906560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.061346 140516435906560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.061429 140516435906560 transformer_layer.py:468] tlayer: End windows.
I0123 11:00:20.061466 140516435906560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:00:20.061532 140516435906560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.061677 140516435906560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:00:20.061795 140516435906560 nn_components.py:325] mlp: activation = None
I0123 11:00:20.061844 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:00:20.063792 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.063890 140516435906560 nn_components.py:261] mlp: residual
I0123 11:00:20.066365 140516435906560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.066449 140516435906560 transformer_base.py:443] tbase: final FFN
I0123 11:00:20.066561 140516435906560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:00:20.068808 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:00:20.070765 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.070865 140516435906560 nn_components.py:261] mlp: residual
I0123 11:00:20.071162 140516435906560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.071249 140516435906560 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 11:00:20.071362 140516435906560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:00:20.071404 140516435906560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:00:20.071439 140516435906560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:00:20.071504 140516435906560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.073795 140516435906560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:00:20.079299 140516435906560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.079583 140516435906560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:00:20.082287 140516435906560 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:00:20.095162 140516435906560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:00:20.095224 140516435906560 attention.py:418] Single window, no scan.
I0123 11:00:20.095262 140516435906560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:00:20.095295 140516435906560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.095360 140516435906560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.095971 140516435906560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.096055 140516435906560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.096422 140516435906560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.097136 140516435906560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.099715 140516435906560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.100344 140516435906560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.100428 140516435906560 transformer_layer.py:468] tlayer: End windows.
I0123 11:00:20.100466 140516435906560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:00:20.100529 140516435906560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.100666 140516435906560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:00:20.100783 140516435906560 nn_components.py:325] mlp: activation = None
I0123 11:00:20.100832 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:00:20.102765 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.102863 140516435906560 nn_components.py:261] mlp: residual
I0123 11:00:20.105368 140516435906560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.105452 140516435906560 transformer_base.py:443] tbase: final FFN
I0123 11:00:20.105564 140516435906560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:00:20.107833 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:00:20.109759 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.109859 140516435906560 nn_components.py:261] mlp: residual
I0123 11:00:20.110155 140516435906560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.110241 140516435906560 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 11:00:20.110354 140516435906560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:00:20.110395 140516435906560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:00:20.110429 140516435906560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:00:20.110496 140516435906560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.112807 140516435906560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:00:20.118429 140516435906560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.118696 140516435906560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:00:20.121355 140516435906560 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:00:20.134286 140516435906560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:00:20.134348 140516435906560 attention.py:418] Single window, no scan.
I0123 11:00:20.134387 140516435906560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:00:20.134420 140516435906560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.134485 140516435906560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.135054 140516435906560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.135134 140516435906560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.135504 140516435906560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.136202 140516435906560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.138753 140516435906560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.139439 140516435906560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.139522 140516435906560 transformer_layer.py:468] tlayer: End windows.
I0123 11:00:20.139560 140516435906560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:00:20.139624 140516435906560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.139763 140516435906560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:00:20.139878 140516435906560 nn_components.py:325] mlp: activation = None
I0123 11:00:20.139920 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:00:20.141870 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.141970 140516435906560 nn_components.py:261] mlp: residual
I0123 11:00:20.144440 140516435906560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.144524 140516435906560 transformer_base.py:443] tbase: final FFN
I0123 11:00:20.144635 140516435906560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:00:20.146910 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:00:20.148888 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.148988 140516435906560 nn_components.py:261] mlp: residual
I0123 11:00:20.149282 140516435906560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.149368 140516435906560 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 11:00:20.149481 140516435906560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:00:20.149521 140516435906560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:00:20.149554 140516435906560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:00:20.149621 140516435906560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.151914 140516435906560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:00:20.157421 140516435906560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.157694 140516435906560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:00:20.160421 140516435906560 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:00:20.173661 140516435906560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:00:20.173722 140516435906560 attention.py:418] Single window, no scan.
I0123 11:00:20.173760 140516435906560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:00:20.173793 140516435906560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.173857 140516435906560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.174473 140516435906560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.174555 140516435906560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.174917 140516435906560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.175619 140516435906560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.178125 140516435906560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.178759 140516435906560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.178841 140516435906560 transformer_layer.py:468] tlayer: End windows.
I0123 11:00:20.178878 140516435906560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:00:20.178938 140516435906560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.179071 140516435906560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:00:20.179185 140516435906560 nn_components.py:325] mlp: activation = None
I0123 11:00:20.179227 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:00:20.181156 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.181261 140516435906560 nn_components.py:261] mlp: residual
I0123 11:00:20.183772 140516435906560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.183860 140516435906560 transformer_base.py:443] tbase: final FFN
I0123 11:00:20.183973 140516435906560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:00:20.186246 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:00:20.188136 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.188236 140516435906560 nn_components.py:261] mlp: residual
I0123 11:00:20.188528 140516435906560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.188614 140516435906560 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 11:00:20.188727 140516435906560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:00:20.188770 140516435906560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:00:20.188804 140516435906560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:00:20.188872 140516435906560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.191185 140516435906560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:00:20.196824 140516435906560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.197091 140516435906560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:00:20.199761 140516435906560 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:00:20.212604 140516435906560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:00:20.212665 140516435906560 attention.py:418] Single window, no scan.
I0123 11:00:20.212704 140516435906560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:00:20.212739 140516435906560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.212804 140516435906560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.213368 140516435906560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.213449 140516435906560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.213822 140516435906560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.214529 140516435906560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.217068 140516435906560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.217748 140516435906560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.217832 140516435906560 transformer_layer.py:468] tlayer: End windows.
I0123 11:00:20.217869 140516435906560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:00:20.217930 140516435906560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.218067 140516435906560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:00:20.218180 140516435906560 nn_components.py:325] mlp: activation = None
I0123 11:00:20.218223 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:00:20.220143 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.220249 140516435906560 nn_components.py:261] mlp: residual
I0123 11:00:20.222718 140516435906560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.222806 140516435906560 transformer_base.py:443] tbase: final FFN
I0123 11:00:20.222916 140516435906560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:00:20.225155 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:00:20.227124 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.227225 140516435906560 nn_components.py:261] mlp: residual
I0123 11:00:20.227518 140516435906560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.227603 140516435906560 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 11:00:20.227716 140516435906560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:00:20.227758 140516435906560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:00:20.227792 140516435906560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:00:20.227856 140516435906560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.230153 140516435906560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:00:20.235692 140516435906560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.235961 140516435906560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:00:20.238706 140516435906560 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:00:20.251519 140516435906560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:00:20.251581 140516435906560 attention.py:418] Single window, no scan.
I0123 11:00:20.251621 140516435906560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:00:20.251655 140516435906560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.251725 140516435906560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.252293 140516435906560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.252375 140516435906560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.252739 140516435906560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.253494 140516435906560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.256019 140516435906560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.256652 140516435906560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.256733 140516435906560 transformer_layer.py:468] tlayer: End windows.
I0123 11:00:20.256770 140516435906560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:00:20.256832 140516435906560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.256965 140516435906560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:00:20.257078 140516435906560 nn_components.py:325] mlp: activation = None
I0123 11:00:20.257120 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:00:20.259028 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.259127 140516435906560 nn_components.py:261] mlp: residual
I0123 11:00:20.261603 140516435906560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.261693 140516435906560 transformer_base.py:443] tbase: final FFN
I0123 11:00:20.261804 140516435906560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:00:20.264123 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:00:20.266029 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.266127 140516435906560 nn_components.py:261] mlp: residual
I0123 11:00:20.266422 140516435906560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.266518 140516435906560 decoder_stack.py:344] dstack: Final layernorm.
I0123 11:00:20.269438 140516435906560 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:00:20.325035 140516435906560 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.325129 140516435906560 decoder_stack.py:333] dstack: autoregressive generator.
I0123 11:00:20.325187 140516435906560 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 11:00:20.325293 140516435906560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:00:20.325337 140516435906560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:00:20.325370 140516435906560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:00:20.325436 140516435906560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.328130 140516435906560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:00:20.333583 140516435906560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.333859 140516435906560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:00:20.336468 140516435906560 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:00:20.348954 140516435906560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:00:20.349016 140516435906560 attention.py:418] Single window, no scan.
I0123 11:00:20.349055 140516435906560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:00:20.349088 140516435906560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.349153 140516435906560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.349720 140516435906560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.349800 140516435906560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.350167 140516435906560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.350850 140516435906560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.353366 140516435906560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.353991 140516435906560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.354073 140516435906560 transformer_layer.py:468] tlayer: End windows.
I0123 11:00:20.354110 140516435906560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:00:20.354172 140516435906560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.354306 140516435906560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:00:20.354428 140516435906560 nn_components.py:325] mlp: activation = None
I0123 11:00:20.354472 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:00:20.356337 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.356434 140516435906560 nn_components.py:261] mlp: residual
I0123 11:00:20.358867 140516435906560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.358952 140516435906560 transformer_base.py:443] tbase: final FFN
I0123 11:00:20.359066 140516435906560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:00:20.361344 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:00:20.363211 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.363311 140516435906560 nn_components.py:261] mlp: residual
I0123 11:00:20.363606 140516435906560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.363694 140516435906560 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 11:00:20.363805 140516435906560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:00:20.363846 140516435906560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:00:20.363879 140516435906560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:00:20.363945 140516435906560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.366239 140516435906560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:00:20.371705 140516435906560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.371976 140516435906560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:00:20.374657 140516435906560 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:00:20.387084 140516435906560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:00:20.387145 140516435906560 attention.py:418] Single window, no scan.
I0123 11:00:20.387184 140516435906560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:00:20.387218 140516435906560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.387284 140516435906560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.387845 140516435906560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.387926 140516435906560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.388290 140516435906560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.388975 140516435906560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.391521 140516435906560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.392146 140516435906560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.392227 140516435906560 transformer_layer.py:468] tlayer: End windows.
I0123 11:00:20.392264 140516435906560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:00:20.392328 140516435906560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.392459 140516435906560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:00:20.392572 140516435906560 nn_components.py:325] mlp: activation = None
I0123 11:00:20.392622 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:00:20.394500 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.394599 140516435906560 nn_components.py:261] mlp: residual
I0123 11:00:20.397047 140516435906560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.397131 140516435906560 transformer_base.py:443] tbase: final FFN
I0123 11:00:20.397242 140516435906560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:00:20.399534 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:00:20.401393 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.401493 140516435906560 nn_components.py:261] mlp: residual
I0123 11:00:20.401794 140516435906560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.401881 140516435906560 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 11:00:20.401993 140516435906560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:00:20.402034 140516435906560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:00:20.402069 140516435906560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:00:20.402134 140516435906560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.404399 140516435906560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:00:20.409819 140516435906560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.410085 140516435906560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:00:20.412773 140516435906560 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:00:20.425112 140516435906560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:00:20.425173 140516435906560 attention.py:418] Single window, no scan.
I0123 11:00:20.425211 140516435906560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:00:20.425245 140516435906560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.425310 140516435906560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.425881 140516435906560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.425961 140516435906560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.426325 140516435906560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.427014 140516435906560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.429548 140516435906560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.430169 140516435906560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.430252 140516435906560 transformer_layer.py:468] tlayer: End windows.
I0123 11:00:20.430291 140516435906560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:00:20.430354 140516435906560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.430488 140516435906560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:00:20.430602 140516435906560 nn_components.py:325] mlp: activation = None
I0123 11:00:20.430646 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:00:20.432510 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.432608 140516435906560 nn_components.py:261] mlp: residual
I0123 11:00:20.435013 140516435906560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.435098 140516435906560 transformer_base.py:443] tbase: final FFN
I0123 11:00:20.435209 140516435906560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:00:20.437918 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:00:20.439761 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.439861 140516435906560 nn_components.py:261] mlp: residual
I0123 11:00:20.440151 140516435906560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.440236 140516435906560 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 11:00:20.440346 140516435906560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:00:20.440386 140516435906560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:00:20.440420 140516435906560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:00:20.440484 140516435906560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.442738 140516435906560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:00:20.448152 140516435906560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.448419 140516435906560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:00:20.451142 140516435906560 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:00:20.463660 140516435906560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:00:20.463721 140516435906560 attention.py:418] Single window, no scan.
I0123 11:00:20.463768 140516435906560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:00:20.463812 140516435906560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.463880 140516435906560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.464450 140516435906560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.464528 140516435906560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.464891 140516435906560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.465590 140516435906560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.468218 140516435906560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.468841 140516435906560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.468921 140516435906560 transformer_layer.py:468] tlayer: End windows.
I0123 11:00:20.468957 140516435906560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:00:20.469019 140516435906560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.469150 140516435906560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:00:20.469261 140516435906560 nn_components.py:325] mlp: activation = None
I0123 11:00:20.469303 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:00:20.471208 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.471304 140516435906560 nn_components.py:261] mlp: residual
I0123 11:00:20.473749 140516435906560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.473831 140516435906560 transformer_base.py:443] tbase: final FFN
I0123 11:00:20.473943 140516435906560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:00:20.476269 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:00:20.478157 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.478255 140516435906560 nn_components.py:261] mlp: residual
I0123 11:00:20.478545 140516435906560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.478629 140516435906560 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 11:00:20.478738 140516435906560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:00:20.478777 140516435906560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:00:20.478810 140516435906560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:00:20.478874 140516435906560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.481135 140516435906560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:00:20.486597 140516435906560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.486864 140516435906560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:00:20.489581 140516435906560 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:00:20.502177 140516435906560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:00:20.502236 140516435906560 attention.py:418] Single window, no scan.
I0123 11:00:20.502277 140516435906560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:00:20.502311 140516435906560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.502375 140516435906560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.502928 140516435906560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.503007 140516435906560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.503369 140516435906560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.504061 140516435906560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.506613 140516435906560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.507241 140516435906560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.507322 140516435906560 transformer_layer.py:468] tlayer: End windows.
I0123 11:00:20.507359 140516435906560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:00:20.507421 140516435906560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.507551 140516435906560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:00:20.507661 140516435906560 nn_components.py:325] mlp: activation = None
I0123 11:00:20.507703 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:00:20.509591 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.509699 140516435906560 nn_components.py:261] mlp: residual
I0123 11:00:20.512161 140516435906560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.512243 140516435906560 transformer_base.py:443] tbase: final FFN
I0123 11:00:20.512354 140516435906560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:00:20.514649 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:00:20.516538 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.516638 140516435906560 nn_components.py:261] mlp: residual
I0123 11:00:20.516930 140516435906560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.517014 140516435906560 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 11:00:20.517123 140516435906560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:00:20.517163 140516435906560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:00:20.517195 140516435906560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:00:20.517261 140516435906560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.519544 140516435906560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:00:20.524992 140516435906560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.525255 140516435906560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:00:20.527977 140516435906560 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:00:20.540534 140516435906560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:00:20.540593 140516435906560 attention.py:418] Single window, no scan.
I0123 11:00:20.540630 140516435906560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:00:20.540663 140516435906560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.540727 140516435906560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.541287 140516435906560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.541364 140516435906560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.541733 140516435906560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.542436 140516435906560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.544992 140516435906560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.545617 140516435906560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.545703 140516435906560 transformer_layer.py:468] tlayer: End windows.
I0123 11:00:20.545739 140516435906560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:00:20.545799 140516435906560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.545928 140516435906560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:00:20.546039 140516435906560 nn_components.py:325] mlp: activation = None
I0123 11:00:20.546081 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:00:20.547954 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.548062 140516435906560 nn_components.py:261] mlp: residual
I0123 11:00:20.550495 140516435906560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.550576 140516435906560 transformer_base.py:443] tbase: final FFN
I0123 11:00:20.550686 140516435906560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:00:20.553373 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:00:20.555249 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.555347 140516435906560 nn_components.py:261] mlp: residual
I0123 11:00:20.555636 140516435906560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.555719 140516435906560 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 11:00:20.555826 140516435906560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:00:20.555866 140516435906560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:00:20.555897 140516435906560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:00:20.555961 140516435906560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.558216 140516435906560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:00:20.563755 140516435906560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.564022 140516435906560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:00:20.566758 140516435906560 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:00:20.579394 140516435906560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:00:20.579455 140516435906560 attention.py:418] Single window, no scan.
I0123 11:00:20.579492 140516435906560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:00:20.579525 140516435906560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.579587 140516435906560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.580157 140516435906560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.580235 140516435906560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.580599 140516435906560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.581293 140516435906560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.583846 140516435906560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.584476 140516435906560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.584556 140516435906560 transformer_layer.py:468] tlayer: End windows.
I0123 11:00:20.584591 140516435906560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:00:20.584651 140516435906560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.584781 140516435906560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:00:20.584889 140516435906560 nn_components.py:325] mlp: activation = None
I0123 11:00:20.584930 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:00:20.586822 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.586920 140516435906560 nn_components.py:261] mlp: residual
I0123 11:00:20.589361 140516435906560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.589443 140516435906560 transformer_base.py:443] tbase: final FFN
I0123 11:00:20.589553 140516435906560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:00:20.591864 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:00:20.593738 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.593836 140516435906560 nn_components.py:261] mlp: residual
I0123 11:00:20.594124 140516435906560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.594208 140516435906560 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 11:00:20.594318 140516435906560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:00:20.594358 140516435906560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:00:20.594390 140516435906560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:00:20.594454 140516435906560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.596703 140516435906560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:00:20.602181 140516435906560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.602446 140516435906560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:00:20.605144 140516435906560 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:00:20.617713 140516435906560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:00:20.617772 140516435906560 attention.py:418] Single window, no scan.
I0123 11:00:20.617809 140516435906560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:00:20.617841 140516435906560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.617903 140516435906560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.618460 140516435906560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.618542 140516435906560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.618906 140516435906560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.619594 140516435906560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.622126 140516435906560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.622753 140516435906560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.622832 140516435906560 transformer_layer.py:468] tlayer: End windows.
I0123 11:00:20.622868 140516435906560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:00:20.622927 140516435906560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.623056 140516435906560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:00:20.623164 140516435906560 nn_components.py:325] mlp: activation = None
I0123 11:00:20.623205 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:00:20.625076 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.625171 140516435906560 nn_components.py:261] mlp: residual
I0123 11:00:20.627633 140516435906560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.627722 140516435906560 transformer_base.py:443] tbase: final FFN
I0123 11:00:20.627833 140516435906560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:00:20.630123 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:00:20.631995 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.632092 140516435906560 nn_components.py:261] mlp: residual
I0123 11:00:20.632381 140516435906560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.632464 140516435906560 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 11:00:20.632571 140516435906560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:00:20.632611 140516435906560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:00:20.632643 140516435906560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:00:20.632705 140516435906560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.634945 140516435906560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:00:20.640389 140516435906560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.640652 140516435906560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:00:20.643352 140516435906560 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:00:20.655945 140516435906560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:00:20.656003 140516435906560 attention.py:418] Single window, no scan.
I0123 11:00:20.656040 140516435906560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:00:20.656074 140516435906560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.656136 140516435906560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.656697 140516435906560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.656775 140516435906560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.657131 140516435906560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.657828 140516435906560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.660401 140516435906560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.661020 140516435906560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.661100 140516435906560 transformer_layer.py:468] tlayer: End windows.
I0123 11:00:20.661135 140516435906560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:00:20.661195 140516435906560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.661325 140516435906560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:00:20.661441 140516435906560 nn_components.py:325] mlp: activation = None
I0123 11:00:20.661483 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:00:20.663361 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.663458 140516435906560 nn_components.py:261] mlp: residual
I0123 11:00:20.665905 140516435906560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.665995 140516435906560 transformer_base.py:443] tbase: final FFN
I0123 11:00:20.666106 140516435906560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:00:20.668789 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:00:20.670672 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.670772 140516435906560 nn_components.py:261] mlp: residual
I0123 11:00:20.671061 140516435906560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.671145 140516435906560 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 11:00:20.671253 140516435906560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:00:20.671293 140516435906560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:00:20.671325 140516435906560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:00:20.671388 140516435906560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.673649 140516435906560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:00:20.679111 140516435906560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.679377 140516435906560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:00:20.682097 140516435906560 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:00:20.694721 140516435906560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:00:20.694781 140516435906560 attention.py:418] Single window, no scan.
I0123 11:00:20.694818 140516435906560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:00:20.694850 140516435906560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.694916 140516435906560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.695478 140516435906560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.695556 140516435906560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.695914 140516435906560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.696607 140516435906560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.699172 140516435906560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.699800 140516435906560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.699882 140516435906560 transformer_layer.py:468] tlayer: End windows.
I0123 11:00:20.699918 140516435906560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:00:20.699977 140516435906560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.700106 140516435906560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:00:20.700215 140516435906560 nn_components.py:325] mlp: activation = None
I0123 11:00:20.700256 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:00:20.702645 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.702743 140516435906560 nn_components.py:261] mlp: residual
I0123 11:00:20.705138 140516435906560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.705219 140516435906560 transformer_base.py:443] tbase: final FFN
I0123 11:00:20.705335 140516435906560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:00:20.707590 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:00:20.709442 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.709539 140516435906560 nn_components.py:261] mlp: residual
I0123 11:00:20.709833 140516435906560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.709918 140516435906560 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 11:00:20.710026 140516435906560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:00:20.710065 140516435906560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:00:20.710097 140516435906560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:00:20.710159 140516435906560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.712388 140516435906560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:00:20.717792 140516435906560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.718057 140516435906560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:00:20.720734 140516435906560 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:00:20.733241 140516435906560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:00:20.733301 140516435906560 attention.py:418] Single window, no scan.
I0123 11:00:20.733338 140516435906560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:00:20.733371 140516435906560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.733434 140516435906560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.734002 140516435906560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.734081 140516435906560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.734441 140516435906560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.735140 140516435906560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.737694 140516435906560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.738323 140516435906560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.738402 140516435906560 transformer_layer.py:468] tlayer: End windows.
I0123 11:00:20.738438 140516435906560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:00:20.738497 140516435906560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.738625 140516435906560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:00:20.738733 140516435906560 nn_components.py:325] mlp: activation = None
I0123 11:00:20.738774 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:00:20.740673 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.740768 140516435906560 nn_components.py:261] mlp: residual
I0123 11:00:20.743191 140516435906560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.743273 140516435906560 transformer_base.py:443] tbase: final FFN
I0123 11:00:20.743382 140516435906560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:00:20.745694 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:00:20.747570 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.747666 140516435906560 nn_components.py:261] mlp: residual
I0123 11:00:20.747953 140516435906560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.748036 140516435906560 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 11:00:20.748144 140516435906560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:00:20.748185 140516435906560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:00:20.748217 140516435906560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:00:20.748280 140516435906560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.750551 140516435906560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:00:20.756062 140516435906560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.756328 140516435906560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:00:20.759025 140516435906560 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:00:20.771593 140516435906560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:00:20.771652 140516435906560 attention.py:418] Single window, no scan.
I0123 11:00:20.771688 140516435906560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:00:20.771720 140516435906560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.771783 140516435906560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.772333 140516435906560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.772411 140516435906560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.772773 140516435906560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.773473 140516435906560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.776006 140516435906560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.776629 140516435906560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.776709 140516435906560 transformer_layer.py:468] tlayer: End windows.
I0123 11:00:20.776745 140516435906560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:00:20.776804 140516435906560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.776933 140516435906560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:00:20.777044 140516435906560 nn_components.py:325] mlp: activation = None
I0123 11:00:20.777086 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:00:20.778982 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.779079 140516435906560 nn_components.py:261] mlp: residual
I0123 11:00:20.781497 140516435906560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.781578 140516435906560 transformer_base.py:443] tbase: final FFN
I0123 11:00:20.781696 140516435906560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:00:20.784369 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:00:20.786273 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.786373 140516435906560 nn_components.py:261] mlp: residual
I0123 11:00:20.786665 140516435906560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:20.786754 140516435906560 decoder_stack.py:344] dstack: Final layernorm.
I0123 11:00:20.789609 140516435906560 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:00:25.201209 140516435906560 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
I0123 11:00:25.758711 140516435906560 training_loop.py:409] No working directory specified.
I0123 11:00:25.758846 140516435906560 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0123 11:00:25.759620 140516435906560 checkpoints.py:1062] Restoring legacy Flax checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0123 11:00:28.943744 140516435906560 training_loop.py:447] Only restoring trainable parameters.
I0123 11:00:28.944440 140516435906560 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0123 11:00:28.944500 140516435906560 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0123 11:00:28.944547 140516435906560 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:00:28.944591 140516435906560 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:00:28.944633 140516435906560 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:00:28.944675 140516435906560 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:00:28.944715 140516435906560 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:00:28.944755 140516435906560 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:00:28.944793 140516435906560 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:00:28.944831 140516435906560 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:00:28.944870 140516435906560 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:00:28.944909 140516435906560 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:00:28.944947 140516435906560 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:00:28.944985 140516435906560 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:00:28.945023 140516435906560 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:00:28.945062 140516435906560 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:00:28.945101 140516435906560 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:00:28.945139 140516435906560 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:00:28.945177 140516435906560 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:00:28.945214 140516435906560 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:00:28.945269 140516435906560 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:00:28.945310 140516435906560 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:00:28.945349 140516435906560 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:00:28.945388 140516435906560 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:00:28.945425 140516435906560 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:00:28.945463 140516435906560 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:00:28.945500 140516435906560 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:00:28.945538 140516435906560 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:00:28.945574 140516435906560 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:00:28.945611 140516435906560 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:00:28.945659 140516435906560 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:00:28.945700 140516435906560 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:00:28.945738 140516435906560 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:00:28.945775 140516435906560 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:00:28.945811 140516435906560 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:00:28.945849 140516435906560 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:00:28.945887 140516435906560 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:00:28.945926 140516435906560 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:00:28.945966 140516435906560 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:00:28.946005 140516435906560 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:00:28.946045 140516435906560 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:00:28.946085 140516435906560 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:00:28.946126 140516435906560 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:00:28.946166 140516435906560 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:00:28.946206 140516435906560 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:00:28.946245 140516435906560 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:00:28.946291 140516435906560 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:00:28.946332 140516435906560 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:00:28.946371 140516435906560 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:00:28.946410 140516435906560 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:00:28.946449 140516435906560 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:00:28.946488 140516435906560 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:00:28.946529 140516435906560 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:00:28.946568 140516435906560 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:00:28.946608 140516435906560 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:00:28.946646 140516435906560 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:00:28.946686 140516435906560 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:00:28.946726 140516435906560 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:00:28.946766 140516435906560 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:00:28.946805 140516435906560 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:00:28.946845 140516435906560 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:00:28.946884 140516435906560 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:00:28.946923 140516435906560 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:00:28.946963 140516435906560 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:00:28.947002 140516435906560 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:00:28.947041 140516435906560 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:00:28.947082 140516435906560 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:00:28.947120 140516435906560 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:00:28.947159 140516435906560 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:00:28.947198 140516435906560 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:00:28.947237 140516435906560 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:00:28.947275 140516435906560 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:00:28.947315 140516435906560 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:00:28.947360 140516435906560 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:00:28.947401 140516435906560 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:00:28.947440 140516435906560 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:00:28.947480 140516435906560 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:00:28.947520 140516435906560 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:00:28.947558 140516435906560 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:00:28.947595 140516435906560 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:00:28.947632 140516435906560 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:00:28.947670 140516435906560 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:00:28.947707 140516435906560 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:00:28.947745 140516435906560 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:00:28.947782 140516435906560 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:00:28.947819 140516435906560 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:00:28.947857 140516435906560 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:00:28.947895 140516435906560 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:00:28.947933 140516435906560 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:00:28.947971 140516435906560 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:00:28.948009 140516435906560 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:00:28.948047 140516435906560 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:00:28.948084 140516435906560 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:00:28.948121 140516435906560 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:00:28.948158 140516435906560 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:00:28.948196 140516435906560 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:00:28.948233 140516435906560 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:00:28.948271 140516435906560 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:00:28.948308 140516435906560 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:00:28.948346 140516435906560 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:00:28.948389 140516435906560 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:00:28.948427 140516435906560 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:00:28.948465 140516435906560 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:00:28.948503 140516435906560 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:00:28.948540 140516435906560 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:00:28.948578 140516435906560 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:00:28.948616 140516435906560 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:00:28.948655 140516435906560 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:00:28.948693 140516435906560 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:00:28.948731 140516435906560 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:00:28.948769 140516435906560 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:00:28.948807 140516435906560 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:00:28.948844 140516435906560 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:00:28.948882 140516435906560 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:00:28.948920 140516435906560 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:00:28.948957 140516435906560 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:00:28.948994 140516435906560 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:00:28.949031 140516435906560 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:00:28.949069 140516435906560 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:00:28.949106 140516435906560 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:00:28.949142 140516435906560 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:00:28.949179 140516435906560 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:00:28.949209 140516435906560 training_loop.py:725] Total parameters: 152072288
I0123 11:00:28.949421 140516435906560 training_loop.py:739] Total state size: 0
I0123 11:00:28.972216 140516435906560 training_loop.py:492] Training loop: creating task for mode beam_search
I0123 11:00:28.972458 140516435906560 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0123 11:00:28.972789 140516435906560 training_loop.py:652] Compiling mode beam_search with jit.
I0123 11:00:28.973109 140516435906560 training_loop.py:89] registering functions: dict_keys([])
I0123 11:00:28.989520 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k ? perp i l l f
I0123 11:00:30.026882 140516435906560 ddar.py:60] Depth 1/1000 time = 0.995398998260498
I0123 11:00:33.262934 140516435906560 ddar.py:60] Depth 2/1000 time = 3.2358598709106445
I0123 11:00:42.572182 140516435906560 ddar.py:60] Depth 3/1000 time = 9.309005737304688
I0123 11:00:50.160404 140516435906560 ddar.py:60] Depth 4/1000 time = 7.587871789932251
I0123 11:00:57.800651 140516435906560 ddar.py:60] Depth 5/1000 time = 7.639911413192749
I0123 11:01:05.395725 140516435906560 ddar.py:60] Depth 6/1000 time = 7.594430446624756
I0123 11:01:13.325925 140516435906560 ddar.py:60] Depth 7/1000 time = 7.82318639755249
I0123 11:01:13.326220 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:01:13.326312 140516435906560 alphageometry.py:540] Depth 0. There are 1 nodes to expand:
I0123 11:01:13.326351 140516435906560 alphageometry.py:544] {S} a : ; b : ; c : ; d : C a c d 00 T a c b d 01 ; e : C b c e 02 T a e b c 03 ; f : C a e f 04 C b d f 05 ; g : D a g b g 06 D b g c g 07 ; h : C a b h 08 D a h b h 09 ; i : C c h i 10 D c g g i 11 ; j : D d j e j 12 D e j i j 13 ; k : C g j k 14 T g j i k 15 ; l : C i k l 16 D i k k l 17 ? T i l l f {F1} x00
I0123 11:01:13.326392 140516435906560 alphageometry.py:549] Decoding from {S} a : ; b : ; c : ; d : C a c d 00 T a c b d 01 ; e : C b c e 02 T a e b c 03 ; f : C a e f 04 C b d f 05 ; g : D a g b g 06 D b g c g 07 ; h : C a b h 08 D a h b h 09 ; i : C c h i 10 D c g g i 11 ; j : D d j e j 12 D e j i j 13 ; k : C g j k 14 T g j i k 15 ; l : C i k l 16 D i k k l 17 ? T i l l f {F1} x00
I0123 11:01:13.471671 140516435906560 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.471857 140516435906560 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 11:01:13.471967 140516435906560 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 11:01:13.472048 140516435906560 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 11:01:13.472124 140516435906560 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 11:01:13.472197 140516435906560 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 11:01:13.472272 140516435906560 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 11:01:13.472344 140516435906560 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 11:01:13.472414 140516435906560 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 11:01:13.472483 140516435906560 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 11:01:13.472551 140516435906560 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 11:01:13.472620 140516435906560 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 11:01:13.472690 140516435906560 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 11:01:13.472757 140516435906560 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 11:01:13.472795 140516435906560 attention.py:418] Single window, no scan.
I0123 11:01:13.472839 140516435906560 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 11:01:13.472948 140516435906560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:13.472987 140516435906560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:13.473018 140516435906560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:13.474910 140516435906560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.477454 140516435906560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:13.483412 140516435906560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.483715 140516435906560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:13.486445 140516435906560 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:01:13.490628 140516435906560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:13.490690 140516435906560 attention.py:418] Single window, no scan.
I0123 11:01:13.490731 140516435906560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:13.490767 140516435906560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.490835 140516435906560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.491461 140516435906560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.491542 140516435906560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.491912 140516435906560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.492726 140516435906560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.495310 140516435906560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.495949 140516435906560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.496032 140516435906560 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:13.496068 140516435906560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:13.496131 140516435906560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.496263 140516435906560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:13.496595 140516435906560 nn_components.py:325] mlp: activation = None
I0123 11:01:13.496643 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:01:13.498720 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.498828 140516435906560 nn_components.py:261] mlp: residual
I0123 11:01:13.501345 140516435906560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.501430 140516435906560 transformer_base.py:443] tbase: final FFN
I0123 11:01:13.501871 140516435906560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:13.504263 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:01:13.506227 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.506328 140516435906560 nn_components.py:261] mlp: residual
I0123 11:01:13.506632 140516435906560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.506721 140516435906560 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 11:01:13.506835 140516435906560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:13.506876 140516435906560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:13.506908 140516435906560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:13.509332 140516435906560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.511750 140516435906560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:13.517462 140516435906560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.517729 140516435906560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:13.520420 140516435906560 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:01:13.524127 140516435906560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:13.524186 140516435906560 attention.py:418] Single window, no scan.
I0123 11:01:13.524223 140516435906560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:13.524254 140516435906560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.524318 140516435906560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.524880 140516435906560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.524960 140516435906560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.525318 140516435906560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.526107 140516435906560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.528559 140516435906560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.529243 140516435906560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.529324 140516435906560 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:13.529361 140516435906560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:13.529421 140516435906560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.529556 140516435906560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:13.529883 140516435906560 nn_components.py:325] mlp: activation = None
I0123 11:01:13.529929 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:01:13.531834 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.531930 140516435906560 nn_components.py:261] mlp: residual
I0123 11:01:13.534398 140516435906560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.534481 140516435906560 transformer_base.py:443] tbase: final FFN
I0123 11:01:13.534900 140516435906560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:13.537259 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:01:13.539208 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.539304 140516435906560 nn_components.py:261] mlp: residual
I0123 11:01:13.539598 140516435906560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.539684 140516435906560 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 11:01:13.539792 140516435906560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:13.539831 140516435906560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:13.539863 140516435906560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:13.541658 140516435906560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.544001 140516435906560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:13.549981 140516435906560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.550251 140516435906560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:13.552817 140516435906560 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:01:13.556570 140516435906560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:13.556628 140516435906560 attention.py:418] Single window, no scan.
I0123 11:01:13.556666 140516435906560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:13.556698 140516435906560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.556762 140516435906560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.557370 140516435906560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.557449 140516435906560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.557818 140516435906560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.558616 140516435906560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.561132 140516435906560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.561766 140516435906560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.561848 140516435906560 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:13.561884 140516435906560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:13.561944 140516435906560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.562078 140516435906560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:13.562407 140516435906560 nn_components.py:325] mlp: activation = None
I0123 11:01:13.562455 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:01:13.564462 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.564558 140516435906560 nn_components.py:261] mlp: residual
I0123 11:01:13.567073 140516435906560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.567158 140516435906560 transformer_base.py:443] tbase: final FFN
I0123 11:01:13.567605 140516435906560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:13.569924 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:01:13.571937 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.572034 140516435906560 nn_components.py:261] mlp: residual
I0123 11:01:13.572325 140516435906560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.572410 140516435906560 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 11:01:13.572518 140516435906560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:13.572556 140516435906560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:13.572589 140516435906560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:13.574485 140516435906560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.576850 140516435906560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:13.582781 140516435906560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.583054 140516435906560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:13.585601 140516435906560 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:01:13.589305 140516435906560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:13.589363 140516435906560 attention.py:418] Single window, no scan.
I0123 11:01:13.589400 140516435906560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:13.589432 140516435906560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.589494 140516435906560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.590050 140516435906560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.590130 140516435906560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.590488 140516435906560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.591255 140516435906560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.593704 140516435906560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.594335 140516435906560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.594419 140516435906560 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:13.594456 140516435906560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:13.594518 140516435906560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.594656 140516435906560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:13.595040 140516435906560 nn_components.py:325] mlp: activation = None
I0123 11:01:13.595088 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:01:13.597053 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.597148 140516435906560 nn_components.py:261] mlp: residual
I0123 11:01:13.599630 140516435906560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.599714 140516435906560 transformer_base.py:443] tbase: final FFN
I0123 11:01:13.600153 140516435906560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:13.602486 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:01:13.604494 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.604592 140516435906560 nn_components.py:261] mlp: residual
I0123 11:01:13.604891 140516435906560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.604975 140516435906560 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 11:01:13.605090 140516435906560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:13.605131 140516435906560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:13.605164 140516435906560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:13.607018 140516435906560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.609359 140516435906560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:13.615127 140516435906560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.615386 140516435906560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:13.618068 140516435906560 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:01:13.621719 140516435906560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:13.621778 140516435906560 attention.py:418] Single window, no scan.
I0123 11:01:13.621814 140516435906560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:13.621847 140516435906560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.621910 140516435906560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.622853 140516435906560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.622932 140516435906560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.623294 140516435906560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.624071 140516435906560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.626541 140516435906560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.627161 140516435906560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.627240 140516435906560 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:13.627275 140516435906560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:13.627336 140516435906560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.627465 140516435906560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:13.627781 140516435906560 nn_components.py:325] mlp: activation = None
I0123 11:01:13.627825 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:01:13.629791 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.629885 140516435906560 nn_components.py:261] mlp: residual
I0123 11:01:13.632362 140516435906560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.632445 140516435906560 transformer_base.py:443] tbase: final FFN
I0123 11:01:13.632874 140516435906560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:13.635176 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:01:13.637105 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.637202 140516435906560 nn_components.py:261] mlp: residual
I0123 11:01:13.637493 140516435906560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.637578 140516435906560 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 11:01:13.637694 140516435906560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:13.637736 140516435906560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:13.637768 140516435906560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:13.639637 140516435906560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.641951 140516435906560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:13.647530 140516435906560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.647792 140516435906560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:13.650410 140516435906560 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:01:13.654079 140516435906560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:13.654139 140516435906560 attention.py:418] Single window, no scan.
I0123 11:01:13.654176 140516435906560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:13.654209 140516435906560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.654272 140516435906560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.654825 140516435906560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.654904 140516435906560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.655264 140516435906560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.656039 140516435906560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.658599 140516435906560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.659273 140516435906560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.659353 140516435906560 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:13.659389 140516435906560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:13.659448 140516435906560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.659609 140516435906560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:13.659935 140516435906560 nn_components.py:325] mlp: activation = None
I0123 11:01:13.659979 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:01:13.661921 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.662021 140516435906560 nn_components.py:261] mlp: residual
I0123 11:01:13.664511 140516435906560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.664593 140516435906560 transformer_base.py:443] tbase: final FFN
I0123 11:01:13.665014 140516435906560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:13.667389 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:01:13.669346 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.669443 140516435906560 nn_components.py:261] mlp: residual
I0123 11:01:13.669741 140516435906560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.669825 140516435906560 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 11:01:13.669933 140516435906560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:13.669972 140516435906560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:13.670006 140516435906560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:13.671847 140516435906560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.674172 140516435906560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:13.679913 140516435906560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.680174 140516435906560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:13.682737 140516435906560 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:01:13.686373 140516435906560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:13.686439 140516435906560 attention.py:418] Single window, no scan.
I0123 11:01:13.686510 140516435906560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:13.686544 140516435906560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.686657 140516435906560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.687217 140516435906560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.687295 140516435906560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.687649 140516435906560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.688408 140516435906560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.690866 140516435906560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.691482 140516435906560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.691562 140516435906560 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:13.691597 140516435906560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:13.691656 140516435906560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.691786 140516435906560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:13.692101 140516435906560 nn_components.py:325] mlp: activation = None
I0123 11:01:13.692146 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:01:13.694116 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.694212 140516435906560 nn_components.py:261] mlp: residual
I0123 11:01:13.696653 140516435906560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.696735 140516435906560 transformer_base.py:443] tbase: final FFN
I0123 11:01:13.697160 140516435906560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:13.699421 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:01:13.701342 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.701437 140516435906560 nn_components.py:261] mlp: residual
I0123 11:01:13.701738 140516435906560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.701823 140516435906560 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 11:01:13.701930 140516435906560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:13.701969 140516435906560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:13.702001 140516435906560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:13.703858 140516435906560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.706165 140516435906560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:13.711770 140516435906560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.712031 140516435906560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:13.714673 140516435906560 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:01:13.718503 140516435906560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:13.718560 140516435906560 attention.py:418] Single window, no scan.
I0123 11:01:13.718596 140516435906560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:13.718636 140516435906560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.718702 140516435906560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.719264 140516435906560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.719344 140516435906560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.719709 140516435906560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.720478 140516435906560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.722932 140516435906560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.723602 140516435906560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.723682 140516435906560 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:13.723718 140516435906560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:13.723778 140516435906560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.723909 140516435906560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:13.724228 140516435906560 nn_components.py:325] mlp: activation = None
I0123 11:01:13.724273 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:01:13.726197 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.726292 140516435906560 nn_components.py:261] mlp: residual
I0123 11:01:13.728764 140516435906560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.728845 140516435906560 transformer_base.py:443] tbase: final FFN
I0123 11:01:13.729270 140516435906560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:13.731953 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:01:13.733918 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.734020 140516435906560 nn_components.py:261] mlp: residual
I0123 11:01:13.734323 140516435906560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.734411 140516435906560 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 11:01:13.734523 140516435906560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:13.734563 140516435906560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:13.734597 140516435906560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:13.736360 140516435906560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.738680 140516435906560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:13.744368 140516435906560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.744638 140516435906560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:13.747210 140516435906560 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:01:13.750870 140516435906560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:13.750929 140516435906560 attention.py:418] Single window, no scan.
I0123 11:01:13.750965 140516435906560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:13.750997 140516435906560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.751068 140516435906560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.751684 140516435906560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.751763 140516435906560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.752125 140516435906560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.752894 140516435906560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.755416 140516435906560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.756032 140516435906560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.756113 140516435906560 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:13.756150 140516435906560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:13.756213 140516435906560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.756355 140516435906560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:13.756686 140516435906560 nn_components.py:325] mlp: activation = None
I0123 11:01:13.756733 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:01:13.758715 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.758815 140516435906560 nn_components.py:261] mlp: residual
I0123 11:01:13.761364 140516435906560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.761447 140516435906560 transformer_base.py:443] tbase: final FFN
I0123 11:01:13.761884 140516435906560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:13.764170 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:01:13.766097 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.766195 140516435906560 nn_components.py:261] mlp: residual
I0123 11:01:13.766498 140516435906560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.766584 140516435906560 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 11:01:13.766696 140516435906560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:13.766737 140516435906560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:13.766770 140516435906560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:13.768571 140516435906560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.771063 140516435906560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:13.776745 140516435906560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.777003 140516435906560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:13.779596 140516435906560 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:01:13.783200 140516435906560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:13.783259 140516435906560 attention.py:418] Single window, no scan.
I0123 11:01:13.783295 140516435906560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:13.783328 140516435906560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.783454 140516435906560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.784014 140516435906560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.784092 140516435906560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.784450 140516435906560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.785207 140516435906560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.787659 140516435906560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.788283 140516435906560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.788363 140516435906560 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:13.788400 140516435906560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:13.788460 140516435906560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.788591 140516435906560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:13.788905 140516435906560 nn_components.py:325] mlp: activation = None
I0123 11:01:13.788949 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:01:13.790866 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.790961 140516435906560 nn_components.py:261] mlp: residual
I0123 11:01:13.793479 140516435906560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.793562 140516435906560 transformer_base.py:443] tbase: final FFN
I0123 11:01:13.793987 140516435906560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:13.796273 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:01:13.798218 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.798315 140516435906560 nn_components.py:261] mlp: residual
I0123 11:01:13.798607 140516435906560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.798690 140516435906560 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 11:01:13.798798 140516435906560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:13.798837 140516435906560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:13.798870 140516435906560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:13.800677 140516435906560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.803120 140516435906560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:13.808777 140516435906560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.809046 140516435906560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:13.811733 140516435906560 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:01:13.815508 140516435906560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:13.815570 140516435906560 attention.py:418] Single window, no scan.
I0123 11:01:13.815609 140516435906560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:13.815644 140516435906560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.815763 140516435906560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.816344 140516435906560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.816423 140516435906560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.816791 140516435906560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.817567 140516435906560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.820277 140516435906560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.820903 140516435906560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.820983 140516435906560 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:13.821019 140516435906560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:13.821238 140516435906560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.821369 140516435906560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:13.821695 140516435906560 nn_components.py:325] mlp: activation = None
I0123 11:01:13.821742 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:01:13.823714 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.823811 140516435906560 nn_components.py:261] mlp: residual
I0123 11:01:13.826389 140516435906560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.826476 140516435906560 transformer_base.py:443] tbase: final FFN
I0123 11:01:13.826916 140516435906560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:13.829219 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:01:13.831178 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.831281 140516435906560 nn_components.py:261] mlp: residual
I0123 11:01:13.831588 140516435906560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.831675 140516435906560 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 11:01:13.831785 140516435906560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:13.831825 140516435906560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:13.831857 140516435906560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:13.833647 140516435906560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.836162 140516435906560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:13.841845 140516435906560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.842119 140516435906560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:13.844835 140516435906560 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:01:13.848578 140516435906560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:13.848640 140516435906560 attention.py:418] Single window, no scan.
I0123 11:01:13.848678 140516435906560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:13.848711 140516435906560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.848832 140516435906560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.849408 140516435906560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.849497 140516435906560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.849884 140516435906560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.850680 140516435906560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.853221 140516435906560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.853870 140516435906560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.853955 140516435906560 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:13.853992 140516435906560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:13.854055 140516435906560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.854191 140516435906560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:13.854522 140516435906560 nn_components.py:325] mlp: activation = None
I0123 11:01:13.854568 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:01:13.856564 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.856665 140516435906560 nn_components.py:261] mlp: residual
I0123 11:01:13.859710 140516435906560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.859796 140516435906560 transformer_base.py:443] tbase: final FFN
I0123 11:01:13.860233 140516435906560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:13.862601 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:01:13.864587 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.864687 140516435906560 nn_components.py:261] mlp: residual
I0123 11:01:13.864992 140516435906560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.865257 140516435906560 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 11:01:13.865331 140516435906560 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 11:01:13.865393 140516435906560 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 11:01:13.865451 140516435906560 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 11:01:13.865508 140516435906560 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 11:01:13.865564 140516435906560 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 11:01:13.865620 140516435906560 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 11:01:13.865685 140516435906560 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 11:01:13.865743 140516435906560 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 11:01:13.865798 140516435906560 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 11:01:13.865853 140516435906560 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 11:01:13.865906 140516435906560 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 11:01:13.865945 140516435906560 decoder_stack.py:344] dstack: Final layernorm.
I0123 11:01:13.868956 140516435906560 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:01:13.915221 140516435906560 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:13.915312 140516435906560 decoder_stack.py:333] dstack: autoregressive generator.
I0123 11:01:13.915368 140516435906560 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 11:01:13.915477 140516435906560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:13.915519 140516435906560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:13.915552 140516435906560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:13.915618 140516435906560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:13.918074 140516435906560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:13.923611 140516435906560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:13.923878 140516435906560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:13.926507 140516435906560 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:13.939834 140516435906560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:13.939894 140516435906560 attention.py:418] Single window, no scan.
I0123 11:01:13.939932 140516435906560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:13.939965 140516435906560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:13.940029 140516435906560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:13.940598 140516435906560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:13.940679 140516435906560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:13.941050 140516435906560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:13.941771 140516435906560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:13.944391 140516435906560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:13.945013 140516435906560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:13.945091 140516435906560 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:13.945127 140516435906560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:13.945187 140516435906560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:13.945318 140516435906560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:13.945428 140516435906560 nn_components.py:325] mlp: activation = None
I0123 11:01:13.945467 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:01:13.947372 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:13.947472 140516435906560 nn_components.py:261] mlp: residual
I0123 11:01:13.949885 140516435906560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:13.949968 140516435906560 transformer_base.py:443] tbase: final FFN
I0123 11:01:13.950082 140516435906560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:13.952379 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:01:13.954259 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:13.954360 140516435906560 nn_components.py:261] mlp: residual
I0123 11:01:13.954672 140516435906560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:13.954761 140516435906560 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 11:01:13.954874 140516435906560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:13.954914 140516435906560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:13.954947 140516435906560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:13.955013 140516435906560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:13.957307 140516435906560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:13.962805 140516435906560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:13.963078 140516435906560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:13.965804 140516435906560 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:13.978381 140516435906560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:13.978443 140516435906560 attention.py:418] Single window, no scan.
I0123 11:01:13.978482 140516435906560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:13.978515 140516435906560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:13.978581 140516435906560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:13.979161 140516435906560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:13.979245 140516435906560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:13.979621 140516435906560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:13.980561 140516435906560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:13.983068 140516435906560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:13.983711 140516435906560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:13.983790 140516435906560 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:13.983826 140516435906560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:13.983886 140516435906560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:13.984016 140516435906560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:13.984125 140516435906560 nn_components.py:325] mlp: activation = None
I0123 11:01:13.984164 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:01:13.986034 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:13.986130 140516435906560 nn_components.py:261] mlp: residual
I0123 11:01:13.988608 140516435906560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:13.988692 140516435906560 transformer_base.py:443] tbase: final FFN
I0123 11:01:13.988801 140516435906560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:13.991065 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:01:13.993118 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:13.993217 140516435906560 nn_components.py:261] mlp: residual
I0123 11:01:13.993522 140516435906560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:13.993618 140516435906560 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 11:01:13.993742 140516435906560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:13.993785 140516435906560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:13.993819 140516435906560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:13.993885 140516435906560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:13.996194 140516435906560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:14.001787 140516435906560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.002063 140516435906560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:14.004816 140516435906560 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:14.017547 140516435906560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:14.017607 140516435906560 attention.py:418] Single window, no scan.
I0123 11:01:14.017654 140516435906560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:14.017690 140516435906560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.017753 140516435906560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.018330 140516435906560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.018413 140516435906560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.018792 140516435906560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.019567 140516435906560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.022128 140516435906560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.022779 140516435906560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.022862 140516435906560 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:14.022899 140516435906560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:14.022964 140516435906560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.023101 140516435906560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:14.023215 140516435906560 nn_components.py:325] mlp: activation = None
I0123 11:01:14.023256 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:01:14.025174 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.025273 140516435906560 nn_components.py:261] mlp: residual
I0123 11:01:14.027782 140516435906560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.027868 140516435906560 transformer_base.py:443] tbase: final FFN
I0123 11:01:14.027981 140516435906560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:14.030328 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:01:14.032229 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.032330 140516435906560 nn_components.py:261] mlp: residual
I0123 11:01:14.032635 140516435906560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.032732 140516435906560 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 11:01:14.032848 140516435906560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:14.032890 140516435906560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:14.032923 140516435906560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:14.032991 140516435906560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.035350 140516435906560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:14.040939 140516435906560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.041198 140516435906560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:14.044372 140516435906560 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:14.056995 140516435906560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:14.057053 140516435906560 attention.py:418] Single window, no scan.
I0123 11:01:14.057090 140516435906560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:14.057122 140516435906560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.057183 140516435906560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.057742 140516435906560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.057822 140516435906560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.058195 140516435906560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.058967 140516435906560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.061498 140516435906560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.062133 140516435906560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.062217 140516435906560 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:14.062254 140516435906560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:14.062317 140516435906560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.062453 140516435906560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:14.062567 140516435906560 nn_components.py:325] mlp: activation = None
I0123 11:01:14.062609 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:01:14.064484 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.064578 140516435906560 nn_components.py:261] mlp: residual
I0123 11:01:14.066977 140516435906560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.067059 140516435906560 transformer_base.py:443] tbase: final FFN
I0123 11:01:14.067170 140516435906560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:14.069426 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:01:14.071350 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.071453 140516435906560 nn_components.py:261] mlp: residual
I0123 11:01:14.071753 140516435906560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.071837 140516435906560 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 11:01:14.071954 140516435906560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:14.071994 140516435906560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:14.072026 140516435906560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:14.072090 140516435906560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.074334 140516435906560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:14.079773 140516435906560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.080036 140516435906560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:14.082720 140516435906560 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:14.095320 140516435906560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:14.095380 140516435906560 attention.py:418] Single window, no scan.
I0123 11:01:14.095416 140516435906560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:14.095448 140516435906560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.095510 140516435906560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.096058 140516435906560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.096138 140516435906560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.096498 140516435906560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.097233 140516435906560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.099691 140516435906560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.100310 140516435906560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.100390 140516435906560 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:14.100428 140516435906560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:14.100491 140516435906560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.100626 140516435906560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:14.100739 140516435906560 nn_components.py:325] mlp: activation = None
I0123 11:01:14.100780 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:01:14.102657 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.102753 140516435906560 nn_components.py:261] mlp: residual
I0123 11:01:14.105153 140516435906560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.105234 140516435906560 transformer_base.py:443] tbase: final FFN
I0123 11:01:14.105342 140516435906560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:14.107610 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:01:14.109466 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.109563 140516435906560 nn_components.py:261] mlp: residual
I0123 11:01:14.109860 140516435906560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.109946 140516435906560 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 11:01:14.110064 140516435906560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:14.110105 140516435906560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:14.110137 140516435906560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:14.110202 140516435906560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.112455 140516435906560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:14.117840 140516435906560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.118101 140516435906560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:14.120789 140516435906560 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:14.133436 140516435906560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:14.133496 140516435906560 attention.py:418] Single window, no scan.
I0123 11:01:14.133532 140516435906560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:14.133564 140516435906560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.133627 140516435906560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.134190 140516435906560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.134269 140516435906560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.134641 140516435906560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.135404 140516435906560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.137851 140516435906560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.138492 140516435906560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.138575 140516435906560 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:14.138612 140516435906560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:14.138676 140516435906560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.138811 140516435906560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:14.138924 140516435906560 nn_components.py:325] mlp: activation = None
I0123 11:01:14.138966 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:01:14.140828 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.140924 140516435906560 nn_components.py:261] mlp: residual
I0123 11:01:14.143371 140516435906560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.143455 140516435906560 transformer_base.py:443] tbase: final FFN
I0123 11:01:14.143569 140516435906560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:14.145814 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:01:14.147733 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.147830 140516435906560 nn_components.py:261] mlp: residual
I0123 11:01:14.148124 140516435906560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.148207 140516435906560 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 11:01:14.148317 140516435906560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:14.148365 140516435906560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:14.148399 140516435906560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:14.148464 140516435906560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.150735 140516435906560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:14.156274 140516435906560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.156538 140516435906560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:14.159696 140516435906560 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:14.172209 140516435906560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:14.172269 140516435906560 attention.py:418] Single window, no scan.
I0123 11:01:14.172305 140516435906560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:14.172338 140516435906560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.172401 140516435906560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.172954 140516435906560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.173034 140516435906560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.173398 140516435906560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.174164 140516435906560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.176699 140516435906560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.177321 140516435906560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.177404 140516435906560 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:14.177441 140516435906560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:14.177502 140516435906560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.177638 140516435906560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:14.177758 140516435906560 nn_components.py:325] mlp: activation = None
I0123 11:01:14.177799 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:01:14.179889 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.179985 140516435906560 nn_components.py:261] mlp: residual
I0123 11:01:14.182404 140516435906560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.182491 140516435906560 transformer_base.py:443] tbase: final FFN
I0123 11:01:14.182604 140516435906560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:14.184906 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:01:14.186765 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.186864 140516435906560 nn_components.py:261] mlp: residual
I0123 11:01:14.187155 140516435906560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.187238 140516435906560 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 11:01:14.187350 140516435906560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:14.187391 140516435906560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:14.187432 140516435906560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:14.187502 140516435906560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.189797 140516435906560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:14.195252 140516435906560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.195516 140516435906560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:14.198189 140516435906560 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:14.210570 140516435906560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:14.210629 140516435906560 attention.py:418] Single window, no scan.
I0123 11:01:14.210665 140516435906560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:14.210698 140516435906560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.210760 140516435906560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.211316 140516435906560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.211395 140516435906560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.211757 140516435906560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.212496 140516435906560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.214999 140516435906560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.215641 140516435906560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.215721 140516435906560 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:14.215757 140516435906560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:14.215816 140516435906560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.215947 140516435906560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:14.216057 140516435906560 nn_components.py:325] mlp: activation = None
I0123 11:01:14.216097 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:01:14.217958 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.218057 140516435906560 nn_components.py:261] mlp: residual
I0123 11:01:14.220521 140516435906560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.220603 140516435906560 transformer_base.py:443] tbase: final FFN
I0123 11:01:14.220712 140516435906560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:14.223015 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:01:14.224939 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.225036 140516435906560 nn_components.py:261] mlp: residual
I0123 11:01:14.225329 140516435906560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.225415 140516435906560 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 11:01:14.225523 140516435906560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:14.225562 140516435906560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:14.225596 140516435906560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:14.225687 140516435906560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.227996 140516435906560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:14.233368 140516435906560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.233629 140516435906560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:14.236487 140516435906560 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:14.248921 140516435906560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:14.248980 140516435906560 attention.py:418] Single window, no scan.
I0123 11:01:14.249017 140516435906560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:14.249050 140516435906560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.249114 140516435906560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.249676 140516435906560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.249757 140516435906560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.250123 140516435906560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.250835 140516435906560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.253392 140516435906560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.254013 140516435906560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.254092 140516435906560 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:14.254128 140516435906560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:14.254188 140516435906560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.254315 140516435906560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:14.254423 140516435906560 nn_components.py:325] mlp: activation = None
I0123 11:01:14.254461 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:01:14.256320 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.256416 140516435906560 nn_components.py:261] mlp: residual
I0123 11:01:14.258876 140516435906560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.258962 140516435906560 transformer_base.py:443] tbase: final FFN
I0123 11:01:14.259076 140516435906560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:14.261358 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:01:14.263212 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.263310 140516435906560 nn_components.py:261] mlp: residual
I0123 11:01:14.263602 140516435906560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.263687 140516435906560 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 11:01:14.263796 140516435906560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:14.263835 140516435906560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:14.263867 140516435906560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:14.263939 140516435906560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.266212 140516435906560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:14.271640 140516435906560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.271905 140516435906560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:14.275005 140516435906560 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:14.287648 140516435906560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:14.287708 140516435906560 attention.py:418] Single window, no scan.
I0123 11:01:14.287746 140516435906560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:14.287779 140516435906560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.287842 140516435906560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.288400 140516435906560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.288481 140516435906560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.288847 140516435906560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.289539 140516435906560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.292082 140516435906560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.292701 140516435906560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.292782 140516435906560 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:14.292819 140516435906560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:14.292881 140516435906560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.293012 140516435906560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:14.293121 140516435906560 nn_components.py:325] mlp: activation = None
I0123 11:01:14.293161 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:01:14.295159 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.295254 140516435906560 nn_components.py:261] mlp: residual
I0123 11:01:14.297886 140516435906560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.297969 140516435906560 transformer_base.py:443] tbase: final FFN
I0123 11:01:14.298082 140516435906560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:14.300370 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:01:14.302237 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.302336 140516435906560 nn_components.py:261] mlp: residual
I0123 11:01:14.302629 140516435906560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.302715 140516435906560 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 11:01:14.302824 140516435906560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:14.302864 140516435906560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:14.302896 140516435906560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:14.302959 140516435906560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.305214 140516435906560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:14.310644 140516435906560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.310910 140516435906560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:14.313878 140516435906560 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:14.326490 140516435906560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:14.326553 140516435906560 attention.py:418] Single window, no scan.
I0123 11:01:14.326591 140516435906560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:14.326625 140516435906560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.326690 140516435906560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.327269 140516435906560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.327351 140516435906560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.327722 140516435906560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.328429 140516435906560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.331063 140516435906560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.331704 140516435906560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.331784 140516435906560 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:14.331820 140516435906560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:14.331880 140516435906560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.332011 140516435906560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:14.332121 140516435906560 nn_components.py:325] mlp: activation = None
I0123 11:01:14.332162 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:01:14.334026 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.334126 140516435906560 nn_components.py:261] mlp: residual
I0123 11:01:14.336610 140516435906560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.336693 140516435906560 transformer_base.py:443] tbase: final FFN
I0123 11:01:14.336805 140516435906560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:14.339162 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:01:14.341076 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.341172 140516435906560 nn_components.py:261] mlp: residual
I0123 11:01:14.341466 140516435906560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.341550 140516435906560 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 11:01:14.341664 140516435906560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:14.341707 140516435906560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:14.341740 140516435906560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:14.341805 140516435906560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.344144 140516435906560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:14.349698 140516435906560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.349975 140516435906560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:14.352669 140516435906560 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:14.365202 140516435906560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:14.365264 140516435906560 attention.py:418] Single window, no scan.
I0123 11:01:14.365303 140516435906560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:14.365337 140516435906560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.365403 140516435906560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.365993 140516435906560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.366076 140516435906560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.366454 140516435906560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.367155 140516435906560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.369675 140516435906560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.370323 140516435906560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.370405 140516435906560 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:14.370442 140516435906560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:14.370503 140516435906560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.370638 140516435906560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:14.370752 140516435906560 nn_components.py:325] mlp: activation = None
I0123 11:01:14.370795 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:01:14.372684 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.372778 140516435906560 nn_components.py:261] mlp: residual
I0123 11:01:14.375230 140516435906560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.375312 140516435906560 transformer_base.py:443] tbase: final FFN
I0123 11:01:14.375422 140516435906560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:14.377721 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:01:14.379629 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.379745 140516435906560 nn_components.py:261] mlp: residual
I0123 11:01:14.380040 140516435906560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.380126 140516435906560 decoder_stack.py:344] dstack: Final layernorm.
I0123 11:01:14.382990 140516435906560 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:01:14.433746 140516435906560 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.433838 140516435906560 decoder_stack.py:333] dstack: autoregressive generator.
I0123 11:01:14.433894 140516435906560 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 11:01:14.434004 140516435906560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:14.434054 140516435906560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:14.434089 140516435906560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:14.434157 140516435906560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.436527 140516435906560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:14.442143 140516435906560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.442419 140516435906560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:14.445083 140516435906560 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:14.458370 140516435906560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:14.458432 140516435906560 attention.py:418] Single window, no scan.
I0123 11:01:14.458470 140516435906560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:14.458504 140516435906560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.458571 140516435906560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.459148 140516435906560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.459230 140516435906560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.459606 140516435906560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.460316 140516435906560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.462877 140516435906560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.463515 140516435906560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.463598 140516435906560 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:14.463635 140516435906560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:14.463697 140516435906560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.463833 140516435906560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:14.463948 140516435906560 nn_components.py:325] mlp: activation = None
I0123 11:01:14.463989 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:01:14.465986 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.466087 140516435906560 nn_components.py:261] mlp: residual
I0123 11:01:14.468566 140516435906560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.468651 140516435906560 transformer_base.py:443] tbase: final FFN
I0123 11:01:14.468760 140516435906560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:14.471040 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:01:14.472932 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.473029 140516435906560 nn_components.py:261] mlp: residual
I0123 11:01:14.473322 140516435906560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.473405 140516435906560 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 11:01:14.473511 140516435906560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:14.473559 140516435906560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:14.473591 140516435906560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:14.473663 140516435906560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.475970 140516435906560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:14.481630 140516435906560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.481915 140516435906560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:14.484643 140516435906560 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:14.497596 140516435906560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:14.497666 140516435906560 attention.py:418] Single window, no scan.
I0123 11:01:14.497706 140516435906560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:14.497741 140516435906560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.497807 140516435906560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.498388 140516435906560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.498469 140516435906560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.498843 140516435906560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.499551 140516435906560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.502105 140516435906560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.502757 140516435906560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.502840 140516435906560 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:14.502877 140516435906560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:14.502941 140516435906560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.503076 140516435906560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:14.503190 140516435906560 nn_components.py:325] mlp: activation = None
I0123 11:01:14.503233 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:01:14.505245 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.505344 140516435906560 nn_components.py:261] mlp: residual
I0123 11:01:14.507866 140516435906560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.507954 140516435906560 transformer_base.py:443] tbase: final FFN
I0123 11:01:14.508070 140516435906560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:14.510375 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:01:14.512319 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.512420 140516435906560 nn_components.py:261] mlp: residual
I0123 11:01:14.512726 140516435906560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.512813 140516435906560 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 11:01:14.512925 140516435906560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:14.512967 140516435906560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:14.513008 140516435906560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:14.513077 140516435906560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.515479 140516435906560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:14.521216 140516435906560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.521489 140516435906560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:14.524191 140516435906560 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:14.537075 140516435906560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:14.537137 140516435906560 attention.py:418] Single window, no scan.
I0123 11:01:14.537175 140516435906560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:14.537208 140516435906560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.537274 140516435906560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.537858 140516435906560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.537943 140516435906560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.538320 140516435906560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.539035 140516435906560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.541563 140516435906560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.542211 140516435906560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.542294 140516435906560 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:14.542331 140516435906560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:14.542394 140516435906560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.542527 140516435906560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:14.542639 140516435906560 nn_components.py:325] mlp: activation = None
I0123 11:01:14.542680 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:01:14.544660 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.544759 140516435906560 nn_components.py:261] mlp: residual
I0123 11:01:14.547272 140516435906560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.547358 140516435906560 transformer_base.py:443] tbase: final FFN
I0123 11:01:14.547472 140516435906560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:14.549764 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:01:14.551703 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.551805 140516435906560 nn_components.py:261] mlp: residual
I0123 11:01:14.552109 140516435906560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.552196 140516435906560 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 11:01:14.552310 140516435906560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:14.552352 140516435906560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:14.552384 140516435906560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:14.552457 140516435906560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.554799 140516435906560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:14.560492 140516435906560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.560767 140516435906560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:14.563557 140516435906560 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:14.577181 140516435906560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:14.577243 140516435906560 attention.py:418] Single window, no scan.
I0123 11:01:14.577281 140516435906560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:14.577315 140516435906560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.577382 140516435906560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.577970 140516435906560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.578053 140516435906560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.578431 140516435906560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.579147 140516435906560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.581705 140516435906560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.582348 140516435906560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.582429 140516435906560 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:14.582467 140516435906560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:14.582529 140516435906560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.582664 140516435906560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:14.582776 140516435906560 nn_components.py:325] mlp: activation = None
I0123 11:01:14.582817 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:01:14.584810 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.584910 140516435906560 nn_components.py:261] mlp: residual
I0123 11:01:14.587409 140516435906560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.587496 140516435906560 transformer_base.py:443] tbase: final FFN
I0123 11:01:14.587610 140516435906560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:14.589885 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:01:14.591789 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.591889 140516435906560 nn_components.py:261] mlp: residual
I0123 11:01:14.592193 140516435906560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.592280 140516435906560 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 11:01:14.592393 140516435906560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:14.592433 140516435906560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:14.592466 140516435906560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:14.592531 140516435906560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.594864 140516435906560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:14.600547 140516435906560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.600821 140516435906560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:14.603524 140516435906560 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:14.616352 140516435906560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:14.616413 140516435906560 attention.py:418] Single window, no scan.
I0123 11:01:14.616451 140516435906560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:14.616484 140516435906560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.616549 140516435906560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.617123 140516435906560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.617204 140516435906560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.617579 140516435906560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.618306 140516435906560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.620849 140516435906560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.621487 140516435906560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.621570 140516435906560 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:14.621607 140516435906560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:14.621676 140516435906560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.621812 140516435906560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:14.621924 140516435906560 nn_components.py:325] mlp: activation = None
I0123 11:01:14.621966 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:01:14.623957 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.624056 140516435906560 nn_components.py:261] mlp: residual
I0123 11:01:14.626556 140516435906560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.626642 140516435906560 transformer_base.py:443] tbase: final FFN
I0123 11:01:14.626757 140516435906560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:14.629053 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:01:14.630991 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.631093 140516435906560 nn_components.py:261] mlp: residual
I0123 11:01:14.631400 140516435906560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.631487 140516435906560 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 11:01:14.631600 140516435906560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:14.631644 140516435906560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:14.631684 140516435906560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:14.631764 140516435906560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.634176 140516435906560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:14.639877 140516435906560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.640154 140516435906560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:14.642856 140516435906560 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:14.655850 140516435906560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:14.655913 140516435906560 attention.py:418] Single window, no scan.
I0123 11:01:14.655951 140516435906560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:14.655984 140516435906560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.656049 140516435906560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.656625 140516435906560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.656705 140516435906560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.657081 140516435906560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.657804 140516435906560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.660382 140516435906560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.661026 140516435906560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.661107 140516435906560 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:14.661144 140516435906560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:14.661207 140516435906560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.661344 140516435906560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:14.661455 140516435906560 nn_components.py:325] mlp: activation = None
I0123 11:01:14.661497 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:01:14.663509 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.663611 140516435906560 nn_components.py:261] mlp: residual
I0123 11:01:14.666108 140516435906560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.666195 140516435906560 transformer_base.py:443] tbase: final FFN
I0123 11:01:14.666308 140516435906560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:14.668602 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:01:14.670581 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.670682 140516435906560 nn_components.py:261] mlp: residual
I0123 11:01:14.670985 140516435906560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.671071 140516435906560 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 11:01:14.671183 140516435906560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:14.671224 140516435906560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:14.671257 140516435906560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:14.671322 140516435906560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.673858 140516435906560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:14.679702 140516435906560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.679978 140516435906560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:14.682693 140516435906560 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:14.696251 140516435906560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:14.696312 140516435906560 attention.py:418] Single window, no scan.
I0123 11:01:14.696351 140516435906560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:14.696384 140516435906560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.696449 140516435906560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.697030 140516435906560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.697112 140516435906560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.697487 140516435906560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.698213 140516435906560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.700791 140516435906560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.701428 140516435906560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.701509 140516435906560 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:14.701546 140516435906560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:14.701609 140516435906560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.701754 140516435906560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:14.701870 140516435906560 nn_components.py:325] mlp: activation = None
I0123 11:01:14.701912 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:01:14.703916 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.704014 140516435906560 nn_components.py:261] mlp: residual
I0123 11:01:14.706496 140516435906560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.706582 140516435906560 transformer_base.py:443] tbase: final FFN
I0123 11:01:14.706695 140516435906560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:14.708973 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:01:14.710904 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.711005 140516435906560 nn_components.py:261] mlp: residual
I0123 11:01:14.711320 140516435906560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.711407 140516435906560 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 11:01:14.711520 140516435906560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:14.711562 140516435906560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:14.711595 140516435906560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:14.711661 140516435906560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.713984 140516435906560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:14.719665 140516435906560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.719939 140516435906560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:14.722636 140516435906560 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:14.735641 140516435906560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:14.735702 140516435906560 attention.py:418] Single window, no scan.
I0123 11:01:14.735740 140516435906560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:14.735774 140516435906560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.735841 140516435906560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.736422 140516435906560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.736504 140516435906560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.736882 140516435906560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.737603 140516435906560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.740200 140516435906560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.740845 140516435906560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.740926 140516435906560 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:14.740963 140516435906560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:14.741026 140516435906560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.741161 140516435906560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:14.741277 140516435906560 nn_components.py:325] mlp: activation = None
I0123 11:01:14.741317 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:01:14.743379 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.743478 140516435906560 nn_components.py:261] mlp: residual
I0123 11:01:14.746001 140516435906560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.746086 140516435906560 transformer_base.py:443] tbase: final FFN
I0123 11:01:14.746201 140516435906560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:14.748499 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:01:14.750435 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.750536 140516435906560 nn_components.py:261] mlp: residual
I0123 11:01:14.750842 140516435906560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.750928 140516435906560 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 11:01:14.751042 140516435906560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:14.751082 140516435906560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:14.751116 140516435906560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:14.751180 140516435906560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.753514 140516435906560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:14.759240 140516435906560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.759522 140516435906560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:14.762252 140516435906560 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:14.775218 140516435906560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:14.775280 140516435906560 attention.py:418] Single window, no scan.
I0123 11:01:14.775319 140516435906560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:14.775352 140516435906560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.775417 140516435906560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.775995 140516435906560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.776076 140516435906560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.776449 140516435906560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.777165 140516435906560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.779746 140516435906560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.780386 140516435906560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.780468 140516435906560 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:14.780505 140516435906560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:14.780566 140516435906560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.780699 140516435906560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:14.780811 140516435906560 nn_components.py:325] mlp: activation = None
I0123 11:01:14.780852 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:01:14.782865 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.782963 140516435906560 nn_components.py:261] mlp: residual
I0123 11:01:14.785449 140516435906560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.785533 140516435906560 transformer_base.py:443] tbase: final FFN
I0123 11:01:14.785653 140516435906560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:14.788148 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:01:14.790077 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.790178 140516435906560 nn_components.py:261] mlp: residual
I0123 11:01:14.790481 140516435906560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.790565 140516435906560 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 11:01:14.790677 140516435906560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:14.790719 140516435906560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:14.790752 140516435906560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:14.790817 140516435906560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.793125 140516435906560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:14.798799 140516435906560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.799083 140516435906560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:14.801796 140516435906560 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:14.815134 140516435906560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:14.815196 140516435906560 attention.py:418] Single window, no scan.
I0123 11:01:14.815235 140516435906560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:14.815268 140516435906560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.815332 140516435906560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.815910 140516435906560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.815992 140516435906560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.816364 140516435906560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.817078 140516435906560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.819651 140516435906560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.820298 140516435906560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.820380 140516435906560 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:14.820418 140516435906560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:14.820480 140516435906560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.820615 140516435906560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:14.820729 140516435906560 nn_components.py:325] mlp: activation = None
I0123 11:01:14.820770 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:01:14.822793 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.822895 140516435906560 nn_components.py:261] mlp: residual
I0123 11:01:14.825376 140516435906560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.825460 140516435906560 transformer_base.py:443] tbase: final FFN
I0123 11:01:14.825575 140516435906560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:14.827864 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:01:14.829797 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.829898 140516435906560 nn_components.py:261] mlp: residual
I0123 11:01:14.830202 140516435906560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.830284 140516435906560 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 11:01:14.830397 140516435906560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:14.830437 140516435906560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:14.830470 140516435906560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:14.830538 140516435906560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.832858 140516435906560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:14.838559 140516435906560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.838834 140516435906560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:14.841629 140516435906560 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:14.854526 140516435906560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:14.854588 140516435906560 attention.py:418] Single window, no scan.
I0123 11:01:14.854627 140516435906560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:14.854661 140516435906560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.854726 140516435906560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.855298 140516435906560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.855378 140516435906560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.855749 140516435906560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.856462 140516435906560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.859054 140516435906560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.859697 140516435906560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.859779 140516435906560 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:14.859820 140516435906560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:14.859883 140516435906560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.860017 140516435906560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:14.860130 140516435906560 nn_components.py:325] mlp: activation = None
I0123 11:01:14.860172 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:01:14.862187 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.862285 140516435906560 nn_components.py:261] mlp: residual
I0123 11:01:14.864774 140516435906560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.864857 140516435906560 transformer_base.py:443] tbase: final FFN
I0123 11:01:14.864971 140516435906560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:14.867275 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:01:14.869207 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.869307 140516435906560 nn_components.py:261] mlp: residual
I0123 11:01:14.869611 140516435906560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.869705 140516435906560 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 11:01:14.869820 140516435906560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:01:14.869861 140516435906560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:01:14.869894 140516435906560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:01:14.869959 140516435906560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.872297 140516435906560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:01:14.878030 140516435906560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.878304 140516435906560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:01:14.881001 140516435906560 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:01:14.894128 140516435906560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:01:14.894189 140516435906560 attention.py:418] Single window, no scan.
I0123 11:01:14.894228 140516435906560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:01:14.894263 140516435906560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.894328 140516435906560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.894912 140516435906560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.894993 140516435906560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.895370 140516435906560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.896087 140516435906560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.898667 140516435906560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.899312 140516435906560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.899396 140516435906560 transformer_layer.py:468] tlayer: End windows.
I0123 11:01:14.899435 140516435906560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:01:14.899499 140516435906560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.899635 140516435906560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:01:14.899750 140516435906560 nn_components.py:325] mlp: activation = None
I0123 11:01:14.899791 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:01:14.901816 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.901915 140516435906560 nn_components.py:261] mlp: residual
I0123 11:01:14.904415 140516435906560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.904499 140516435906560 transformer_base.py:443] tbase: final FFN
I0123 11:01:14.904611 140516435906560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:01:14.906927 140516435906560 nn_components.py:329] mlp: final activation = None
I0123 11:01:14.908869 140516435906560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.908969 140516435906560 nn_components.py:261] mlp: residual
I0123 11:01:14.909274 140516435906560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:14.909364 140516435906560 decoder_stack.py:344] dstack: Final layernorm.
I0123 11:01:14.912353 140516435906560 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:01:29.023556 140516435906560 alphageometry.py:566] LM output (score=-1.618073): "m : C a c m 18 D a m c m 19 ;"
I0123 11:01:29.023860 140516435906560 alphageometry.py:567] Translation: "m = on_line m a c, on_bline m c a"

I0123 11:01:29.023923 140516435906560 alphageometry.py:576] Solving: "a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a c, on_bline m c a ? perp i l l f"
I0123 11:01:29.024089 140516435906560 graph.py:498] 
I0123 11:01:29.024150 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a c, on_bline m c a ? perp i l l f
I0123 11:01:30.716165 140516435906560 ddar.py:60] Depth 1/1000 time = 1.6410317420959473
I0123 11:01:36.220932 140516435906560 ddar.py:60] Depth 2/1000 time = 5.504594802856445
I0123 11:01:48.404031 140516435906560 ddar.py:60] Depth 3/1000 time = 12.182914972305298
I0123 11:01:59.888531 140516435906560 ddar.py:60] Depth 4/1000 time = 11.484301567077637
I0123 11:02:11.002941 140516435906560 ddar.py:60] Depth 5/1000 time = 11.114204168319702
I0123 11:02:22.013728 140516435906560 ddar.py:60] Depth 6/1000 time = 11.010145664215088
I0123 11:02:33.507794 140516435906560 ddar.py:60] Depth 7/1000 time = 11.31712818145752
I0123 11:02:33.508273 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:02:33.508415 140516435906560 alphageometry.py:566] LM output (score=-1.671970): "m : C a e m 18 D a m e m 19 ;"
I0123 11:02:33.508459 140516435906560 alphageometry.py:567] Translation: "m = on_line m a e, on_bline m e a"

I0123 11:02:33.508510 140516435906560 alphageometry.py:576] Solving: "a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a e, on_bline m e a ? perp i l l f"
I0123 11:02:33.508679 140516435906560 graph.py:498] 
I0123 11:02:33.508745 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a e, on_bline m e a ? perp i l l f
I0123 11:02:35.007727 140516435906560 ddar.py:60] Depth 1/1000 time = 1.4464755058288574
I0123 11:02:39.830796 140516435906560 ddar.py:60] Depth 2/1000 time = 4.822875022888184
I0123 11:02:49.606135 140516435906560 ddar.py:60] Depth 3/1000 time = 9.775134563446045
I0123 11:02:58.754645 140516435906560 ddar.py:60] Depth 4/1000 time = 9.148255586624146
I0123 11:03:07.789253 140516435906560 ddar.py:60] Depth 5/1000 time = 9.034254312515259
I0123 11:03:17.045112 140516435906560 ddar.py:60] Depth 6/1000 time = 9.25528335571289
I0123 11:03:26.041627 140516435906560 ddar.py:60] Depth 7/1000 time = 8.876270532608032
I0123 11:03:26.041992 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:03:26.042095 140516435906560 alphageometry.py:566] LM output (score=-1.781676): "m : C d e m 18 D d m e m 19 ;"
I0123 11:03:26.042136 140516435906560 alphageometry.py:567] Translation: "m = on_line m d e, on_bline m e d"

I0123 11:03:26.042178 140516435906560 alphageometry.py:576] Solving: "a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m d e, on_bline m e d ? perp i l l f"
I0123 11:03:26.042343 140516435906560 graph.py:498] 
I0123 11:03:26.042407 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m d e, on_bline m e d ? perp i l l f
I0123 11:03:27.362291 140516435906560 ddar.py:60] Depth 1/1000 time = 1.2674903869628906
I0123 11:03:31.828185 140516435906560 ddar.py:60] Depth 2/1000 time = 4.465713024139404
I0123 11:03:45.678581 140516435906560 ddar.py:60] Depth 3/1000 time = 13.850123167037964
I0123 11:03:54.995257 140516435906560 ddar.py:60] Depth 4/1000 time = 9.31632137298584
I0123 11:04:04.995739 140516435906560 ddar.py:60] Depth 5/1000 time = 10.000216960906982
I0123 11:04:15.544620 140516435906560 ddar.py:60] Depth 6/1000 time = 10.548547983169556
I0123 11:04:26.254395 140516435906560 ddar.py:60] Depth 7/1000 time = 10.709543466567993
I0123 11:04:36.464058 140516435906560 ddar.py:60] Depth 8/1000 time = 10.209428548812866
I0123 11:04:46.803452 140516435906560 ddar.py:60] Depth 9/1000 time = 10.339122295379639
I0123 11:04:57.374932 140516435906560 ddar.py:60] Depth 10/1000 time = 10.570751667022705
I0123 11:05:07.928987 140516435906560 ddar.py:60] Depth 11/1000 time = 10.42478632926941
I0123 11:05:18.468605 140516435906560 ddar.py:60] Depth 12/1000 time = 10.481431007385254
I0123 11:05:18.468986 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:05:18.469086 140516435906560 alphageometry.py:566] LM output (score=-1.866398): "m : C e i m 18 D e m i m 19 ;"
I0123 11:05:18.469128 140516435906560 alphageometry.py:567] Translation: "m = on_line m e i, on_bline m i e"

I0123 11:05:18.469169 140516435906560 alphageometry.py:576] Solving: "a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m e i, on_bline m i e ? perp i l l f"
I0123 11:05:18.469327 140516435906560 graph.py:498] 
I0123 11:05:18.469393 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m e i, on_bline m i e ? perp i l l f
I0123 11:05:19.804968 140516435906560 ddar.py:60] Depth 1/1000 time = 1.2837049961090088
I0123 11:05:24.360240 140516435906560 ddar.py:60] Depth 2/1000 time = 4.555093288421631
I0123 11:05:35.539603 140516435906560 ddar.py:60] Depth 3/1000 time = 11.179139137268066
I0123 11:05:44.586071 140516435906560 ddar.py:60] Depth 4/1000 time = 9.046247482299805
I0123 11:05:53.422264 140516435906560 ddar.py:60] Depth 5/1000 time = 8.835963726043701
I0123 11:06:02.419327 140516435906560 ddar.py:60] Depth 6/1000 time = 8.996433734893799
I0123 11:06:11.987721 140516435906560 ddar.py:60] Depth 7/1000 time = 9.438137531280518
I0123 11:06:11.988099 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:06:11.988194 140516435906560 alphageometry.py:566] LM output (score=-2.006857): "m : C b c m 18 D b c c m 19 ;"
I0123 11:06:11.988233 140516435906560 alphageometry.py:567] Translation: "m = on_line m b c, on_circle m c b"

I0123 11:06:11.988272 140516435906560 alphageometry.py:576] Solving: "a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m b c, on_circle m c b ? perp i l l f"
I0123 11:06:11.988444 140516435906560 graph.py:498] 
I0123 11:06:11.988516 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m b c, on_circle m c b ? perp i l l f
I0123 11:06:13.464799 140516435906560 ddar.py:60] Depth 1/1000 time = 1.422776699066162
I0123 11:06:18.348561 140516435906560 ddar.py:60] Depth 2/1000 time = 4.883583068847656
I0123 11:06:28.829793 140516435906560 ddar.py:60] Depth 3/1000 time = 10.481042861938477
I0123 11:06:37.808026 140516435906560 ddar.py:60] Depth 4/1000 time = 8.977998733520508
I0123 11:06:46.734869 140516435906560 ddar.py:60] Depth 5/1000 time = 8.926623821258545
I0123 11:06:55.796979 140516435906560 ddar.py:60] Depth 6/1000 time = 9.061573266983032
I0123 11:07:04.761939 140516435906560 ddar.py:60] Depth 7/1000 time = 8.84289026260376
I0123 11:07:04.762221 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:07:04.762322 140516435906560 alphageometry.py:566] LM output (score=-2.120116): "m : C b e m 18 D b m e m 19 ;"
I0123 11:07:04.762375 140516435906560 alphageometry.py:567] Translation: "m = on_line m b e, on_bline m e b"

I0123 11:07:04.762413 140516435906560 alphageometry.py:576] Solving: "a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m b e, on_bline m e b ? perp i l l f"
I0123 11:07:04.762563 140516435906560 graph.py:498] 
I0123 11:07:04.762625 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m b e, on_bline m e b ? perp i l l f
I0123 11:07:06.250917 140516435906560 ddar.py:60] Depth 1/1000 time = 1.4365792274475098
I0123 11:07:10.393103 140516435906560 ddar.py:60] Depth 2/1000 time = 4.142014980316162
I0123 11:07:22.475070 140516435906560 ddar.py:60] Depth 3/1000 time = 12.081748247146606
I0123 11:07:31.908799 140516435906560 ddar.py:60] Depth 4/1000 time = 9.43348503112793
I0123 11:07:41.091319 140516435906560 ddar.py:60] Depth 5/1000 time = 9.182295322418213
I0123 11:07:50.214420 140516435906560 ddar.py:60] Depth 6/1000 time = 9.122486591339111
I0123 11:07:59.809140 140516435906560 ddar.py:60] Depth 7/1000 time = 9.470805644989014
I0123 11:07:59.809597 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:07:59.809713 140516435906560 alphageometry.py:566] LM output (score=-2.135467): "m : C b d m 18 D b m d m 19 ;"
I0123 11:07:59.809751 140516435906560 alphageometry.py:567] Translation: "m = on_line m b d, on_bline m d b"

I0123 11:07:59.809802 140516435906560 alphageometry.py:576] Solving: "a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m b d, on_bline m d b ? perp i l l f"
I0123 11:07:59.809965 140516435906560 graph.py:498] 
I0123 11:07:59.810023 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m b d, on_bline m d b ? perp i l l f
I0123 11:08:01.597038 140516435906560 ddar.py:60] Depth 1/1000 time = 1.7342662811279297
I0123 11:08:06.468970 140516435906560 ddar.py:60] Depth 2/1000 time = 4.871773719787598
I0123 11:08:15.699099 140516435906560 ddar.py:60] Depth 3/1000 time = 9.229954242706299
I0123 11:08:24.623902 140516435906560 ddar.py:60] Depth 4/1000 time = 8.924601316452026
I0123 11:08:33.869848 140516435906560 ddar.py:60] Depth 5/1000 time = 9.245747566223145
I0123 11:08:42.799784 140516435906560 ddar.py:60] Depth 6/1000 time = 8.929366111755371
I0123 11:08:51.998239 140516435906560 ddar.py:60] Depth 7/1000 time = 9.073897123336792
I0123 11:08:51.998518 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:08:51.998610 140516435906560 alphageometry.py:566] LM output (score=-2.161459): "m : C b c m 18 D a b a m 19 ;"
I0123 11:08:51.998646 140516435906560 alphageometry.py:567] Translation: "m = on_line m b c, on_circle m a b"

I0123 11:08:51.998685 140516435906560 alphageometry.py:576] Solving: "a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m b c, on_circle m a b ? perp i l l f"
I0123 11:08:51.998836 140516435906560 graph.py:498] 
I0123 11:08:51.998895 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m b c, on_circle m a b ? perp i l l f
I0123 11:08:53.561101 140516435906560 ddar.py:60] Depth 1/1000 time = 1.5104777812957764
I0123 11:08:57.775956 140516435906560 ddar.py:60] Depth 2/1000 time = 4.214692115783691
I0123 11:09:07.523749 140516435906560 ddar.py:60] Depth 3/1000 time = 9.7475745677948
I0123 11:09:18.703680 140516435906560 ddar.py:60] Depth 4/1000 time = 11.179702758789062
I0123 11:09:29.421309 140516435906560 ddar.py:60] Depth 5/1000 time = 10.717383861541748
I0123 11:09:39.774634 140516435906560 ddar.py:60] Depth 6/1000 time = 10.352770805358887
I0123 11:09:50.740034 140516435906560 ddar.py:60] Depth 7/1000 time = 10.840372800827026
I0123 11:09:50.758003 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:09:50.758107 140516435906560 alphageometry.py:566] LM output (score=-2.188449): "m : D a m c m 18 ;"
I0123 11:09:50.758144 140516435906560 alphageometry.py:567] Translation: "m = on_bline m c a"

I0123 11:09:50.758188 140516435906560 alphageometry.py:576] Solving: "a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_bline m c a ? perp i l l f"
I0123 11:09:50.758338 140516435906560 graph.py:498] 
I0123 11:09:50.758395 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_bline m c a ? perp i l l f
I0123 11:09:52.136142 140516435906560 ddar.py:60] Depth 1/1000 time = 1.3249146938323975
I0123 11:09:55.781377 140516435906560 ddar.py:60] Depth 2/1000 time = 3.6450695991516113
I0123 11:10:05.024861 140516435906560 ddar.py:60] Depth 3/1000 time = 9.243267059326172
I0123 11:10:13.331743 140516435906560 ddar.py:60] Depth 4/1000 time = 8.30657148361206
I0123 11:10:21.547353 140516435906560 ddar.py:60] Depth 5/1000 time = 8.215394973754883
I0123 11:10:30.131843 140516435906560 ddar.py:60] Depth 6/1000 time = 8.583927154541016
I0123 11:10:38.689790 140516435906560 ddar.py:60] Depth 7/1000 time = 8.433475494384766
I0123 11:10:38.689993 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:10:38.690083 140516435906560 alphageometry.py:566] LM output (score=-2.196834): "m : C a d m 18 D a m d m 19 ;"
I0123 11:10:38.690119 140516435906560 alphageometry.py:567] Translation: "m = on_line m a d, on_bline m d a"

I0123 11:10:38.690155 140516435906560 alphageometry.py:576] Solving: "a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a d, on_bline m d a ? perp i l l f"
I0123 11:10:38.690310 140516435906560 graph.py:498] 
I0123 11:10:38.690368 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a d, on_bline m d a ? perp i l l f
I0123 11:10:40.207745 140516435906560 ddar.py:60] Depth 1/1000 time = 1.4646632671356201
I0123 11:10:45.349534 140516435906560 ddar.py:60] Depth 2/1000 time = 5.141611099243164
I0123 11:10:55.489097 140516435906560 ddar.py:60] Depth 3/1000 time = 10.139354228973389
I0123 11:11:04.745588 140516435906560 ddar.py:60] Depth 4/1000 time = 9.256275653839111
I0123 11:11:14.057292 140516435906560 ddar.py:60] Depth 5/1000 time = 9.311476945877075
I0123 11:11:23.254976 140516435906560 ddar.py:60] Depth 6/1000 time = 9.197080850601196
I0123 11:11:33.131610 140516435906560 ddar.py:60] Depth 7/1000 time = 9.751904010772705
I0123 11:11:33.132042 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:11:33.132161 140516435906560 alphageometry.py:566] LM output (score=-2.241080): "m : C b c m 18 D b c b m 19 ;"
I0123 11:11:33.132210 140516435906560 alphageometry.py:567] Translation: "m = on_line m b c, on_circle m b c"

I0123 11:11:33.132260 140516435906560 alphageometry.py:576] Solving: "a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m b c, on_circle m b c ? perp i l l f"
I0123 11:11:33.132422 140516435906560 graph.py:498] 
I0123 11:11:33.132481 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m b c, on_circle m b c ? perp i l l f
I0123 11:11:34.712684 140516435906560 ddar.py:60] Depth 1/1000 time = 1.527883768081665
I0123 11:11:39.346015 140516435906560 ddar.py:60] Depth 2/1000 time = 4.633158206939697
I0123 11:11:47.207057 140516435906560 ddar.py:60] Depth 3/1000 time = 7.860872030258179
I0123 11:11:55.677428 140516435906560 ddar.py:60] Depth 4/1000 time = 8.470115423202515
I0123 11:12:04.161045 140516435906560 ddar.py:60] Depth 5/1000 time = 8.483288049697876
I0123 11:12:12.690776 140516435906560 ddar.py:60] Depth 6/1000 time = 8.529139995574951
I0123 11:12:21.413517 140516435906560 ddar.py:60] Depth 7/1000 time = 8.606293678283691
I0123 11:12:21.413731 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:12:21.413818 140516435906560 alphageometry.py:566] LM output (score=-2.349589): "m : C e f m 18 D e m f m 19 ;"
I0123 11:12:21.413853 140516435906560 alphageometry.py:567] Translation: "m = on_line m e f, on_bline m f e"

I0123 11:12:21.413890 140516435906560 alphageometry.py:576] Solving: "a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m e f, on_bline m f e ? perp i l l f"
I0123 11:12:21.414036 140516435906560 graph.py:498] 
I0123 11:12:21.414095 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m e f, on_bline m f e ? perp i l l f
I0123 11:12:22.962837 140516435906560 ddar.py:60] Depth 1/1000 time = 1.4967594146728516
I0123 11:12:27.183935 140516435906560 ddar.py:60] Depth 2/1000 time = 4.22094464302063
I0123 11:12:35.851815 140516435906560 ddar.py:60] Depth 3/1000 time = 8.66764235496521
I0123 11:12:44.324595 140516435906560 ddar.py:60] Depth 4/1000 time = 8.4724440574646
I0123 11:12:53.124964 140516435906560 ddar.py:60] Depth 5/1000 time = 8.800171375274658
I0123 11:13:01.628477 140516435906560 ddar.py:60] Depth 6/1000 time = 8.503289937973022
I0123 11:13:09.727194 140516435906560 ddar.py:60] Depth 7/1000 time = 8.098216533660889
I0123 11:13:18.793021 140516435906560 ddar.py:60] Depth 8/1000 time = 8.953244686126709
I0123 11:13:18.793247 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:13:18.793333 140516435906560 alphageometry.py:566] LM output (score=-2.403554): "m : C d i m 18 D d m i m 19 ;"
I0123 11:13:18.793369 140516435906560 alphageometry.py:567] Translation: "m = on_line m d i, on_bline m i d"

I0123 11:13:18.793406 140516435906560 alphageometry.py:576] Solving: "a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m d i, on_bline m i d ? perp i l l f"
I0123 11:13:18.793554 140516435906560 graph.py:498] 
I0123 11:13:18.793610 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m d i, on_bline m i d ? perp i l l f
I0123 11:13:19.900472 140516435906560 ddar.py:60] Depth 1/1000 time = 1.0537984371185303
I0123 11:13:24.347258 140516435906560 ddar.py:60] Depth 2/1000 time = 4.446625232696533
I0123 11:13:34.514695 140516435906560 ddar.py:60] Depth 3/1000 time = 10.167265892028809
I0123 11:13:43.541047 140516435906560 ddar.py:60] Depth 4/1000 time = 9.026113748550415
I0123 11:13:52.496406 140516435906560 ddar.py:60] Depth 5/1000 time = 8.955110549926758
I0123 11:14:01.876385 140516435906560 ddar.py:60] Depth 6/1000 time = 9.379353523254395
I0123 11:14:11.065997 140516435906560 ddar.py:60] Depth 7/1000 time = 9.059725999832153
I0123 11:14:11.066206 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:14:11.066297 140516435906560 alphageometry.py:566] LM output (score=-2.597354): "m : T a e k m 18 ;"
I0123 11:14:11.066334 140516435906560 alphageometry.py:567] Translation: "m = on_tline m k a e"

I0123 11:14:11.066370 140516435906560 alphageometry.py:576] Solving: "a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_tline m k a e ? perp i l l f"
I0123 11:14:11.066521 140516435906560 graph.py:498] 
I0123 11:14:11.066582 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_tline m k a e ? perp i l l f
I0123 11:14:12.059902 140516435906560 ddar.py:60] Depth 1/1000 time = 0.9482958316802979
I0123 11:14:15.799107 140516435906560 ddar.py:60] Depth 2/1000 time = 3.7390334606170654
I0123 11:14:24.675065 140516435906560 ddar.py:60] Depth 3/1000 time = 8.875701189041138
I0123 11:14:32.703318 140516435906560 ddar.py:60] Depth 4/1000 time = 8.027916431427002
I0123 11:14:40.365984 140516435906560 ddar.py:60] Depth 5/1000 time = 7.662478446960449
I0123 11:14:48.108572 140516435906560 ddar.py:60] Depth 6/1000 time = 7.7419726848602295
I0123 11:14:55.988209 140516435906560 ddar.py:60] Depth 7/1000 time = 7.771533250808716
I0123 11:14:55.988405 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:14:55.988492 140516435906560 alphageometry.py:566] LM output (score=-2.649368): "m : P a e d m 18 ;"
I0123 11:14:55.988528 140516435906560 alphageometry.py:567] Translation: "m = on_pline m d a e"

I0123 11:14:55.988565 140516435906560 alphageometry.py:576] Solving: "a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_pline m d a e ? perp i l l f"
I0123 11:14:55.988716 140516435906560 graph.py:498] 
I0123 11:14:55.988775 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_pline m d a e ? perp i l l f
I0123 11:14:56.985507 140516435906560 ddar.py:60] Depth 1/1000 time = 0.9536452293395996
I0123 11:15:00.960788 140516435906560 ddar.py:60] Depth 2/1000 time = 3.9751193523406982
I0123 11:15:09.628449 140516435906560 ddar.py:60] Depth 3/1000 time = 8.667454957962036
I0123 11:15:17.330169 140516435906560 ddar.py:60] Depth 4/1000 time = 7.701473951339722
I0123 11:15:25.446925 140516435906560 ddar.py:60] Depth 5/1000 time = 8.116556644439697
I0123 11:15:33.187274 140516435906560 ddar.py:60] Depth 6/1000 time = 7.739755153656006
I0123 11:15:41.099784 140516435906560 ddar.py:60] Depth 7/1000 time = 7.800800323486328
I0123 11:15:41.100013 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:15:41.100110 140516435906560 alphageometry.py:566] LM output (score=-2.661753): "m : C b c m 18 D b m c m 19 ;"
I0123 11:15:41.100147 140516435906560 alphageometry.py:567] Translation: "m = on_line m b c, on_bline m c b"

I0123 11:15:41.100185 140516435906560 alphageometry.py:576] Solving: "a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m b c, on_bline m c b ? perp i l l f"
I0123 11:15:41.100336 140516435906560 graph.py:498] 
I0123 11:15:41.100393 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m b c, on_bline m c b ? perp i l l f
I0123 11:15:42.761607 140516435906560 ddar.py:60] Depth 1/1000 time = 1.6094958782196045
I0123 11:15:48.383015 140516435906560 ddar.py:60] Depth 2/1000 time = 5.621168613433838
I0123 11:15:59.209223 140516435906560 ddar.py:60] Depth 3/1000 time = 10.825923919677734
I0123 11:16:10.333663 140516435906560 ddar.py:60] Depth 4/1000 time = 11.124182939529419
I0123 11:16:21.345767 140516435906560 ddar.py:60] Depth 5/1000 time = 11.011751651763916
I0123 11:16:32.370215 140516435906560 ddar.py:60] Depth 6/1000 time = 11.023644208908081
I0123 11:16:43.633897 140516435906560 ddar.py:60] Depth 7/1000 time = 11.116999864578247
I0123 11:16:43.634107 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:16:43.634197 140516435906560 alphageometry.py:566] LM output (score=-2.721665): "m : D a d d m 18 T a m b d 19 ;"
I0123 11:16:43.634233 140516435906560 alphageometry.py:567] Translation: "m = on_circle m d a, on_tline m a b d"

I0123 11:16:43.634270 140516435906560 alphageometry.py:576] Solving: "a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_circle m d a, on_tline m a b d ? perp i l l f"
I0123 11:16:43.634425 140516435906560 graph.py:498] 
I0123 11:16:43.634486 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_circle m d a, on_tline m a b d ? perp i l l f
I0123 11:16:45.293825 140516435906560 ddar.py:60] Depth 1/1000 time = 1.6074328422546387
I0123 11:16:51.179592 140516435906560 ddar.py:60] Depth 2/1000 time = 5.885585784912109
I0123 11:17:02.424867 140516435906560 ddar.py:60] Depth 3/1000 time = 11.24507451057434
I0123 11:17:13.361284 140516435906560 ddar.py:60] Depth 4/1000 time = 10.936200141906738
I0123 11:17:23.769880 140516435906560 ddar.py:60] Depth 5/1000 time = 10.408365249633789
I0123 11:17:34.200059 140516435906560 ddar.py:60] Depth 6/1000 time = 10.429622173309326
I0123 11:17:44.828150 140516435906560 ddar.py:60] Depth 7/1000 time = 10.495596170425415
I0123 11:17:44.828484 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:17:44.828574 140516435906560 alphageometry.py:566] LM output (score=-2.750929): "m : P a e b m 18 ;"
I0123 11:17:44.828611 140516435906560 alphageometry.py:567] Translation: "m = on_pline m b a e"

I0123 11:17:44.828647 140516435906560 alphageometry.py:576] Solving: "a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_pline m b a e ? perp i l l f"
I0123 11:17:44.828793 140516435906560 graph.py:498] 
I0123 11:17:44.828850 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_pline m b a e ? perp i l l f
I0123 11:17:45.832230 140516435906560 ddar.py:60] Depth 1/1000 time = 0.9598197937011719
I0123 11:17:49.819744 140516435906560 ddar.py:60] Depth 2/1000 time = 3.987354040145874
I0123 11:17:59.943475 140516435906560 ddar.py:60] Depth 3/1000 time = 10.12348484992981
I0123 11:18:07.816512 140516435906560 ddar.py:60] Depth 4/1000 time = 7.8727030754089355
I0123 11:18:15.708573 140516435906560 ddar.py:60] Depth 5/1000 time = 7.8918304443359375
I0123 11:18:23.162220 140516435906560 ddar.py:60] Depth 6/1000 time = 7.45294713973999
I0123 11:18:31.054664 140516435906560 ddar.py:60] Depth 7/1000 time = 7.780194520950317
I0123 11:18:31.054865 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:18:31.054952 140516435906560 alphageometry.py:566] LM output (score=-2.792546): "m : T a b f m 18 ;"
I0123 11:18:31.054989 140516435906560 alphageometry.py:567] Translation: "m = on_tline m f a b"

I0123 11:18:31.055027 140516435906560 alphageometry.py:576] Solving: "a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_tline m f a b ? perp i l l f"
I0123 11:18:31.055176 140516435906560 graph.py:498] 
I0123 11:18:31.055232 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_tline m f a b ? perp i l l f
I0123 11:18:32.467588 140516435906560 ddar.py:60] Depth 1/1000 time = 1.3681960105895996
I0123 11:18:36.194305 140516435906560 ddar.py:60] Depth 2/1000 time = 3.7265617847442627
I0123 11:18:47.623528 140516435906560 ddar.py:60] Depth 3/1000 time = 11.429014682769775
I0123 11:18:55.702387 140516435906560 ddar.py:60] Depth 4/1000 time = 8.078661441802979
I0123 11:19:04.216228 140516435906560 ddar.py:60] Depth 5/1000 time = 8.513626337051392
I0123 11:19:12.547842 140516435906560 ddar.py:60] Depth 6/1000 time = 8.331002950668335
I0123 11:19:20.931862 140516435906560 ddar.py:60] Depth 7/1000 time = 8.271430492401123
I0123 11:19:20.932215 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:19:20.932332 140516435906560 alphageometry.py:566] LM output (score=-2.795057): "m : P a b c m 18 T a b a m 19 ;"
I0123 11:19:20.932369 140516435906560 alphageometry.py:567] Translation: "m = on_pline m c a b, on_tline m a a b"

I0123 11:19:20.932418 140516435906560 alphageometry.py:576] Solving: "a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_pline m c a b, on_tline m a a b ? perp i l l f"
I0123 11:19:20.932590 140516435906560 graph.py:498] 
I0123 11:19:20.932652 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_pline m c a b, on_tline m a a b ? perp i l l f
I0123 11:19:22.057406 140516435906560 ddar.py:60] Depth 1/1000 time = 1.0760011672973633
I0123 11:19:26.218759 140516435906560 ddar.py:60] Depth 2/1000 time = 4.161157608032227
I0123 11:19:37.380125 140516435906560 ddar.py:60] Depth 3/1000 time = 11.161191701889038
I0123 11:19:46.530514 140516435906560 ddar.py:60] Depth 4/1000 time = 9.150116443634033
I0123 11:19:55.582828 140516435906560 ddar.py:60] Depth 5/1000 time = 9.05197787284851
I0123 11:20:05.197429 140516435906560 ddar.py:60] Depth 6/1000 time = 9.613914012908936
I0123 11:20:14.492665 140516435906560 ddar.py:60] Depth 7/1000 time = 9.167322397232056
I0123 11:20:14.495555 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:20:14.495661 140516435906560 alphageometry.py:566] LM output (score=-2.803493): "m : T a b j m 18 ;"
I0123 11:20:14.495698 140516435906560 alphageometry.py:567] Translation: "m = on_tline m j a b"

I0123 11:20:14.495737 140516435906560 alphageometry.py:576] Solving: "a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_tline m j a b ? perp i l l f"
I0123 11:20:14.495885 140516435906560 graph.py:498] 
I0123 11:20:14.495945 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_tline m j a b ? perp i l l f
I0123 11:20:15.498969 140516435906560 ddar.py:60] Depth 1/1000 time = 0.9594035148620605
I0123 11:20:19.620892 140516435906560 ddar.py:60] Depth 2/1000 time = 4.1217663288116455
I0123 11:20:29.298824 140516435906560 ddar.py:60] Depth 3/1000 time = 9.67770528793335
I0123 11:20:37.519069 140516435906560 ddar.py:60] Depth 4/1000 time = 8.21993088722229
I0123 11:20:45.786220 140516435906560 ddar.py:60] Depth 5/1000 time = 8.266915798187256
I0123 11:20:53.558043 140516435906560 ddar.py:60] Depth 6/1000 time = 7.771103143692017
I0123 11:21:02.194212 140516435906560 ddar.py:60] Depth 7/1000 time = 8.522786378860474
I0123 11:21:02.194437 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:21:02.194546 140516435906560 alphageometry.py:566] LM output (score=-2.818505): "m : D a c a m 18 T a b c m 19 ;"
I0123 11:21:02.194587 140516435906560 alphageometry.py:567] Translation: "m = on_circle m a c, on_tline m c a b"

I0123 11:21:02.194624 140516435906560 alphageometry.py:576] Solving: "a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_circle m a c, on_tline m c a b ? perp i l l f"
I0123 11:21:02.194784 140516435906560 graph.py:498] 
I0123 11:21:02.194848 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_circle m a c, on_tline m c a b ? perp i l l f
I0123 11:21:03.349892 140516435906560 ddar.py:60] Depth 1/1000 time = 1.0981462001800537
I0123 11:21:07.373413 140516435906560 ddar.py:60] Depth 2/1000 time = 4.023305654525757
I0123 11:21:21.189312 140516435906560 ddar.py:60] Depth 3/1000 time = 13.815610408782959
I0123 11:21:32.231726 140516435906560 ddar.py:60] Depth 4/1000 time = 11.042136430740356
I0123 11:21:42.609374 140516435906560 ddar.py:60] Depth 5/1000 time = 10.377289533615112
I0123 11:21:53.491956 140516435906560 ddar.py:60] Depth 6/1000 time = 10.8823561668396
I0123 11:22:03.784736 140516435906560 ddar.py:60] Depth 7/1000 time = 10.292297124862671
I0123 11:22:14.308210 140516435906560 ddar.py:60] Depth 8/1000 time = 10.392910480499268
I0123 11:22:24.766440 140516435906560 ddar.py:60] Depth 9/1000 time = 10.425178289413452
I0123 11:22:24.766646 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:22:24.766737 140516435906560 alphageometry.py:566] LM output (score=-2.832835): "m : D b d b m 18 D c d c m 19 ;"
I0123 11:22:24.766774 140516435906560 alphageometry.py:567] Translation: "m = on_circle m b d, on_circle m c d"

I0123 11:22:24.766810 140516435906560 alphageometry.py:576] Solving: "a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_circle m b d, on_circle m c d ? perp i l l f"
I0123 11:22:24.766978 140516435906560 graph.py:498] 
I0123 11:22:24.767042 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_circle m b d, on_circle m c d ? perp i l l f
I0123 11:22:26.405138 140516435906560 ddar.py:60] Depth 1/1000 time = 1.574105978012085
I0123 11:22:31.048703 140516435906560 ddar.py:60] Depth 2/1000 time = 4.64340877532959
I0123 11:22:46.852421 140516435906560 ddar.py:60] Depth 3/1000 time = 15.803515672683716
I0123 11:22:59.460273 140516435906560 ddar.py:60] Depth 4/1000 time = 12.607539176940918
I0123 11:23:12.312894 140516435906560 ddar.py:60] Depth 5/1000 time = 12.852242231369019
I0123 11:23:24.756936 140516435906560 ddar.py:60] Depth 6/1000 time = 12.443774700164795
I0123 11:23:37.110807 140516435906560 ddar.py:60] Depth 7/1000 time = 12.353086948394775
I0123 11:23:49.794036 140516435906560 ddar.py:60] Depth 8/1000 time = 12.49468469619751
I0123 11:23:49.798484 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:23:49.798581 140516435906560 alphageometry.py:566] LM output (score=-2.838696): "m : P a e c m 18 ;"
I0123 11:23:49.798618 140516435906560 alphageometry.py:567] Translation: "m = on_pline m c a e"

I0123 11:23:49.798658 140516435906560 alphageometry.py:576] Solving: "a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_pline m c a e ? perp i l l f"
I0123 11:23:49.798810 140516435906560 graph.py:498] 
I0123 11:23:49.798868 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_pline m c a e ? perp i l l f
I0123 11:23:51.297474 140516435906560 ddar.py:60] Depth 1/1000 time = 1.4548418521881104
I0123 11:23:55.472173 140516435906560 ddar.py:60] Depth 2/1000 time = 4.174519777297974
I0123 11:24:04.756708 140516435906560 ddar.py:60] Depth 3/1000 time = 9.284287929534912
I0123 11:24:12.761595 140516435906560 ddar.py:60] Depth 4/1000 time = 8.004635334014893
I0123 11:24:20.282284 140516435906560 ddar.py:60] Depth 5/1000 time = 7.520448684692383
I0123 11:24:28.318728 140516435906560 ddar.py:60] Depth 6/1000 time = 8.035784006118774
I0123 11:24:36.550327 140516435906560 ddar.py:60] Depth 7/1000 time = 8.118879318237305
I0123 11:24:36.550524 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:24:36.550610 140516435906560 alphageometry.py:566] LM output (score=-2.852722): "m : D a c a m 18 T a c a m 19 ;"
I0123 11:24:36.550646 140516435906560 alphageometry.py:567] Translation: "m = on_circle m a c, on_tline m a a c"

I0123 11:24:36.550683 140516435906560 alphageometry.py:576] Solving: "a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_circle m a c, on_tline m a a c ? perp i l l f"
I0123 11:24:36.550836 140516435906560 graph.py:498] 
I0123 11:24:36.550893 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_circle m a c, on_tline m a a c ? perp i l l f
I0123 11:24:37.649065 140516435906560 ddar.py:60] Depth 1/1000 time = 1.0431976318359375
I0123 11:24:41.429856 140516435906560 ddar.py:60] Depth 2/1000 time = 3.7805840969085693
I0123 11:24:51.702418 140516435906560 ddar.py:60] Depth 3/1000 time = 10.272262573242188
I0123 11:24:59.610271 140516435906560 ddar.py:60] Depth 4/1000 time = 7.907652378082275
I0123 11:25:08.104593 140516435906560 ddar.py:60] Depth 5/1000 time = 8.494088888168335
I0123 11:25:16.586266 140516435906560 ddar.py:60] Depth 6/1000 time = 8.481057167053223
I0123 11:25:24.514530 140516435906560 ddar.py:60] Depth 7/1000 time = 7.927514553070068
I0123 11:25:33.166649 140516435906560 ddar.py:60] Depth 8/1000 time = 8.53189468383789
I0123 11:25:33.166865 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:25:33.166957 140516435906560 alphageometry.py:566] LM output (score=-2.858718): "m : T a b a m 18 ;"
I0123 11:25:33.166992 140516435906560 alphageometry.py:567] Translation: "m = on_tline m a a b"

I0123 11:25:33.167028 140516435906560 alphageometry.py:576] Solving: "a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_tline m a a b ? perp i l l f"
I0123 11:25:33.167177 140516435906560 graph.py:498] 
I0123 11:25:33.167237 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_tline m a a b ? perp i l l f
I0123 11:25:34.201264 140516435906560 ddar.py:60] Depth 1/1000 time = 0.9911892414093018
I0123 11:25:38.967049 140516435906560 ddar.py:60] Depth 2/1000 time = 4.765626430511475
I0123 11:25:48.650739 140516435906560 ddar.py:60] Depth 3/1000 time = 9.68345022201538
I0123 11:25:56.879510 140516435906560 ddar.py:60] Depth 4/1000 time = 8.228436708450317
I0123 11:26:04.501635 140516435906560 ddar.py:60] Depth 5/1000 time = 7.621939182281494
I0123 11:26:12.703399 140516435906560 ddar.py:60] Depth 6/1000 time = 8.201229810714722
I0123 11:26:20.543316 140516435906560 ddar.py:60] Depth 7/1000 time = 7.727451324462891
I0123 11:26:20.543520 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:26:20.543613 140516435906560 alphageometry.py:566] LM output (score=-2.891913): "m : T a c c m 18 ;"
I0123 11:26:20.543649 140516435906560 alphageometry.py:567] Translation: "m = on_tline m c a c"

I0123 11:26:20.543685 140516435906560 alphageometry.py:576] Solving: "a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_tline m c a c ? perp i l l f"
I0123 11:26:20.543842 140516435906560 graph.py:498] 
I0123 11:26:20.543903 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_tline m c a c ? perp i l l f
I0123 11:26:21.557796 140516435906560 ddar.py:60] Depth 1/1000 time = 0.9705157279968262
I0123 11:26:25.286513 140516435906560 ddar.py:60] Depth 2/1000 time = 3.7285585403442383
I0123 11:26:35.055292 140516435906560 ddar.py:60] Depth 3/1000 time = 9.768585920333862
I0123 11:26:42.978524 140516435906560 ddar.py:60] Depth 4/1000 time = 7.923014163970947
I0123 11:26:50.350655 140516435906560 ddar.py:60] Depth 5/1000 time = 7.371883392333984
I0123 11:26:58.351535 140516435906560 ddar.py:60] Depth 6/1000 time = 8.000127077102661
I0123 11:27:05.949970 140516435906560 ddar.py:60] Depth 7/1000 time = 7.482471227645874
I0123 11:27:05.950286 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:27:05.950382 140516435906560 alphageometry.py:566] LM output (score=-2.910350): "m : P a b c m 18 ;"
I0123 11:27:05.950419 140516435906560 alphageometry.py:567] Translation: "m = on_pline m c a b"

I0123 11:27:05.950467 140516435906560 alphageometry.py:576] Solving: "a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_pline m c a b ? perp i l l f"
I0123 11:27:05.950619 140516435906560 graph.py:498] 
I0123 11:27:05.950676 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_pline m c a b ? perp i l l f
I0123 11:27:07.491669 140516435906560 ddar.py:60] Depth 1/1000 time = 1.4976308345794678
I0123 11:27:11.620954 140516435906560 ddar.py:60] Depth 2/1000 time = 4.129060745239258
I0123 11:27:20.170917 140516435906560 ddar.py:60] Depth 3/1000 time = 8.549647331237793
I0123 11:27:28.331041 140516435906560 ddar.py:60] Depth 4/1000 time = 8.159940958023071
I0123 11:27:35.940757 140516435906560 ddar.py:60] Depth 5/1000 time = 7.6094629764556885
I0123 11:27:44.090047 140516435906560 ddar.py:60] Depth 6/1000 time = 8.148680686950684
I0123 11:27:52.418709 140516435906560 ddar.py:60] Depth 7/1000 time = 8.220450162887573
I0123 11:27:52.418907 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:27:52.418997 140516435906560 alphageometry.py:566] LM output (score=-2.938192): "m : D a g g m 18 ;"
I0123 11:27:52.419033 140516435906560 alphageometry.py:567] Translation: "m = on_circle m g a"

I0123 11:27:52.419070 140516435906560 alphageometry.py:576] Solving: "a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_circle m g a ? perp i l l f"
I0123 11:27:52.419218 140516435906560 graph.py:498] 
I0123 11:27:52.419277 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_circle m g a ? perp i l l f
I0123 11:27:53.918805 140516435906560 ddar.py:60] Depth 1/1000 time = 1.4493606090545654
I0123 11:27:59.886829 140516435906560 ddar.py:60] Depth 2/1000 time = 5.9677557945251465
I0123 11:28:15.592840 140516435906560 ddar.py:60] Depth 3/1000 time = 15.705771446228027
I0123 11:28:27.530978 140516435906560 ddar.py:60] Depth 4/1000 time = 11.93781065940857
I0123 11:28:39.980994 140516435906560 ddar.py:60] Depth 5/1000 time = 12.449734210968018
I0123 11:28:52.484910 140516435906560 ddar.py:60] Depth 6/1000 time = 12.503063917160034
I0123 11:29:05.378754 140516435906560 ddar.py:60] Depth 7/1000 time = 12.745799541473389
I0123 11:29:05.379146 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:29:05.379258 140516435906560 alphageometry.py:566] LM output (score=-2.952922): "m : P a c e m 18 ;"
I0123 11:29:05.379295 140516435906560 alphageometry.py:567] Translation: "m = on_pline m e a c"

I0123 11:29:05.379345 140516435906560 alphageometry.py:576] Solving: "a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_pline m e a c ? perp i l l f"
I0123 11:29:05.379513 140516435906560 graph.py:498] 
I0123 11:29:05.379574 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_pline m e a c ? perp i l l f
I0123 11:29:06.380682 140516435906560 ddar.py:60] Depth 1/1000 time = 0.9574673175811768
I0123 11:29:10.197427 140516435906560 ddar.py:60] Depth 2/1000 time = 3.816577196121216
I0123 11:29:18.281092 140516435906560 ddar.py:60] Depth 3/1000 time = 8.083482027053833
I0123 11:29:26.569559 140516435906560 ddar.py:60] Depth 4/1000 time = 8.288257360458374
I0123 11:29:34.237977 140516435906560 ddar.py:60] Depth 5/1000 time = 7.668197870254517
I0123 11:29:42.420755 140516435906560 ddar.py:60] Depth 6/1000 time = 8.182106971740723
I0123 11:29:50.317155 140516435906560 ddar.py:60] Depth 7/1000 time = 7.78280234336853
I0123 11:29:50.317371 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:29:50.317461 140516435906560 alphageometry.py:566] LM output (score=-2.969660): "m : D a e a m 18 D b e b m 19 ;"
I0123 11:29:50.317497 140516435906560 alphageometry.py:567] Translation: "m = on_circle m a e, on_circle m b e"

I0123 11:29:50.317534 140516435906560 alphageometry.py:576] Solving: "a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_circle m a e, on_circle m b e ? perp i l l f"
I0123 11:29:50.317691 140516435906560 graph.py:498] 
I0123 11:29:50.317752 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_circle m a e, on_circle m b e ? perp i l l f
I0123 11:29:52.061798 140516435906560 ddar.py:60] Depth 1/1000 time = 1.6813533306121826
I0123 11:29:55.947845 140516435906560 ddar.py:60] Depth 2/1000 time = 3.8858840465545654
I0123 11:30:10.707135 140516435906560 ddar.py:60] Depth 3/1000 time = 14.75905704498291
I0123 11:30:28.362414 140516435906560 ddar.py:60] Depth 4/1000 time = 17.654892683029175
I0123 11:30:43.326381 140516435906560 ddar.py:60] Depth 5/1000 time = 14.96361517906189
I0123 11:30:57.787057 140516435906560 ddar.py:60] Depth 6/1000 time = 14.459897994995117
I0123 11:31:12.553948 140516435906560 ddar.py:60] Depth 7/1000 time = 14.583176612854004
I0123 11:31:12.573018 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:31:12.573176 140516435906560 alphageometry.py:566] LM output (score=-3.012653): "m : P a d f m 18 ;"
I0123 11:31:12.573217 140516435906560 alphageometry.py:567] Translation: "m = on_pline m f a d"

I0123 11:31:12.573269 140516435906560 alphageometry.py:576] Solving: "a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_pline m f a d ? perp i l l f"
I0123 11:31:12.573437 140516435906560 graph.py:498] 
I0123 11:31:12.573495 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_pline m f a d ? perp i l l f
I0123 11:31:13.576932 140516435906560 ddar.py:60] Depth 1/1000 time = 0.9597620964050293
I0123 11:31:17.641433 140516435906560 ddar.py:60] Depth 2/1000 time = 4.064321517944336
I0123 11:31:26.195841 140516435906560 ddar.py:60] Depth 3/1000 time = 8.554180383682251
I0123 11:31:33.795398 140516435906560 ddar.py:60] Depth 4/1000 time = 7.5990424156188965
I0123 11:31:41.286484 140516435906560 ddar.py:60] Depth 5/1000 time = 7.4908928871154785
I0123 11:31:49.446211 140516435906560 ddar.py:60] Depth 6/1000 time = 8.159122467041016
I0123 11:31:57.297729 140516435906560 ddar.py:60] Depth 7/1000 time = 7.740180969238281
I0123 11:31:57.297941 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:31:57.298238 140516435906560 alphageometry.py:540] Depth 1. There are 32 nodes to expand:
I0123 11:31:57.298275 140516435906560 alphageometry.py:544] {S} a : ; b : ; c : ; d : C a c d 00 T a c b d 01 ; e : C b c e 02 T a e b c 03 ; f : C a e f 04 C b d f 05 ; g : D a g b g 06 D b g c g 07 ; h : C a b h 08 D a h b h 09 ; i : C c h i 10 D c g g i 11 ; j : D d j e j 12 D e j i j 13 ; k : C g j k 14 T g j i k 15 ; l : C i k l 16 D i k k l 17 ? T i l l f {F1} x00 m : C a c m 18 D a m c m 19 ; x00
I0123 11:31:57.298318 140516435906560 alphageometry.py:544] {S} a : ; b : ; c : ; d : C a c d 00 T a c b d 01 ; e : C b c e 02 T a e b c 03 ; f : C a e f 04 C b d f 05 ; g : D a g b g 06 D b g c g 07 ; h : C a b h 08 D a h b h 09 ; i : C c h i 10 D c g g i 11 ; j : D d j e j 12 D e j i j 13 ; k : C g j k 14 T g j i k 15 ; l : C i k l 16 D i k k l 17 ? T i l l f {F1} x00 m : C a e m 18 D a m e m 19 ; x00
I0123 11:31:57.298345 140516435906560 alphageometry.py:544] {S} a : ; b : ; c : ; d : C a c d 00 T a c b d 01 ; e : C b c e 02 T a e b c 03 ; f : C a e f 04 C b d f 05 ; g : D a g b g 06 D b g c g 07 ; h : C a b h 08 D a h b h 09 ; i : C c h i 10 D c g g i 11 ; j : D d j e j 12 D e j i j 13 ; k : C g j k 14 T g j i k 15 ; l : C i k l 16 D i k k l 17 ? T i l l f {F1} x00 m : C d e m 18 D d m e m 19 ; x00
I0123 11:31:57.298370 140516435906560 alphageometry.py:544] {S} a : ; b : ; c : ; d : C a c d 00 T a c b d 01 ; e : C b c e 02 T a e b c 03 ; f : C a e f 04 C b d f 05 ; g : D a g b g 06 D b g c g 07 ; h : C a b h 08 D a h b h 09 ; i : C c h i 10 D c g g i 11 ; j : D d j e j 12 D e j i j 13 ; k : C g j k 14 T g j i k 15 ; l : C i k l 16 D i k k l 17 ? T i l l f {F1} x00 m : C e i m 18 D e m i m 19 ; x00
I0123 11:31:57.298394 140516435906560 alphageometry.py:544] {S} a : ; b : ; c : ; d : C a c d 00 T a c b d 01 ; e : C b c e 02 T a e b c 03 ; f : C a e f 04 C b d f 05 ; g : D a g b g 06 D b g c g 07 ; h : C a b h 08 D a h b h 09 ; i : C c h i 10 D c g g i 11 ; j : D d j e j 12 D e j i j 13 ; k : C g j k 14 T g j i k 15 ; l : C i k l 16 D i k k l 17 ? T i l l f {F1} x00 m : C b c m 18 D b c c m 19 ; x00
I0123 11:31:57.298418 140516435906560 alphageometry.py:544] {S} a : ; b : ; c : ; d : C a c d 00 T a c b d 01 ; e : C b c e 02 T a e b c 03 ; f : C a e f 04 C b d f 05 ; g : D a g b g 06 D b g c g 07 ; h : C a b h 08 D a h b h 09 ; i : C c h i 10 D c g g i 11 ; j : D d j e j 12 D e j i j 13 ; k : C g j k 14 T g j i k 15 ; l : C i k l 16 D i k k l 17 ? T i l l f {F1} x00 m : C b e m 18 D b m e m 19 ; x00
I0123 11:31:57.298442 140516435906560 alphageometry.py:544] {S} a : ; b : ; c : ; d : C a c d 00 T a c b d 01 ; e : C b c e 02 T a e b c 03 ; f : C a e f 04 C b d f 05 ; g : D a g b g 06 D b g c g 07 ; h : C a b h 08 D a h b h 09 ; i : C c h i 10 D c g g i 11 ; j : D d j e j 12 D e j i j 13 ; k : C g j k 14 T g j i k 15 ; l : C i k l 16 D i k k l 17 ? T i l l f {F1} x00 m : C b d m 18 D b m d m 19 ; x00
I0123 11:31:57.298465 140516435906560 alphageometry.py:544] {S} a : ; b : ; c : ; d : C a c d 00 T a c b d 01 ; e : C b c e 02 T a e b c 03 ; f : C a e f 04 C b d f 05 ; g : D a g b g 06 D b g c g 07 ; h : C a b h 08 D a h b h 09 ; i : C c h i 10 D c g g i 11 ; j : D d j e j 12 D e j i j 13 ; k : C g j k 14 T g j i k 15 ; l : C i k l 16 D i k k l 17 ? T i l l f {F1} x00 m : C b c m 18 D a b a m 19 ; x00
I0123 11:31:57.298489 140516435906560 alphageometry.py:544] {S} a : ; b : ; c : ; d : C a c d 00 T a c b d 01 ; e : C b c e 02 T a e b c 03 ; f : C a e f 04 C b d f 05 ; g : D a g b g 06 D b g c g 07 ; h : C a b h 08 D a h b h 09 ; i : C c h i 10 D c g g i 11 ; j : D d j e j 12 D e j i j 13 ; k : C g j k 14 T g j i k 15 ; l : C i k l 16 D i k k l 17 ? T i l l f {F1} x00 m : D a m c m 18 ; x00
I0123 11:31:57.298513 140516435906560 alphageometry.py:544] {S} a : ; b : ; c : ; d : C a c d 00 T a c b d 01 ; e : C b c e 02 T a e b c 03 ; f : C a e f 04 C b d f 05 ; g : D a g b g 06 D b g c g 07 ; h : C a b h 08 D a h b h 09 ; i : C c h i 10 D c g g i 11 ; j : D d j e j 12 D e j i j 13 ; k : C g j k 14 T g j i k 15 ; l : C i k l 16 D i k k l 17 ? T i l l f {F1} x00 m : C a d m 18 D a m d m 19 ; x00
I0123 11:31:57.298537 140516435906560 alphageometry.py:544] {S} a : ; b : ; c : ; d : C a c d 00 T a c b d 01 ; e : C b c e 02 T a e b c 03 ; f : C a e f 04 C b d f 05 ; g : D a g b g 06 D b g c g 07 ; h : C a b h 08 D a h b h 09 ; i : C c h i 10 D c g g i 11 ; j : D d j e j 12 D e j i j 13 ; k : C g j k 14 T g j i k 15 ; l : C i k l 16 D i k k l 17 ? T i l l f {F1} x00 m : C b c m 18 D b c b m 19 ; x00
I0123 11:31:57.298564 140516435906560 alphageometry.py:544] {S} a : ; b : ; c : ; d : C a c d 00 T a c b d 01 ; e : C b c e 02 T a e b c 03 ; f : C a e f 04 C b d f 05 ; g : D a g b g 06 D b g c g 07 ; h : C a b h 08 D a h b h 09 ; i : C c h i 10 D c g g i 11 ; j : D d j e j 12 D e j i j 13 ; k : C g j k 14 T g j i k 15 ; l : C i k l 16 D i k k l 17 ? T i l l f {F1} x00 m : C e f m 18 D e m f m 19 ; x00
I0123 11:31:57.298588 140516435906560 alphageometry.py:544] {S} a : ; b : ; c : ; d : C a c d 00 T a c b d 01 ; e : C b c e 02 T a e b c 03 ; f : C a e f 04 C b d f 05 ; g : D a g b g 06 D b g c g 07 ; h : C a b h 08 D a h b h 09 ; i : C c h i 10 D c g g i 11 ; j : D d j e j 12 D e j i j 13 ; k : C g j k 14 T g j i k 15 ; l : C i k l 16 D i k k l 17 ? T i l l f {F1} x00 m : C d i m 18 D d m i m 19 ; x00
I0123 11:31:57.298611 140516435906560 alphageometry.py:544] {S} a : ; b : ; c : ; d : C a c d 00 T a c b d 01 ; e : C b c e 02 T a e b c 03 ; f : C a e f 04 C b d f 05 ; g : D a g b g 06 D b g c g 07 ; h : C a b h 08 D a h b h 09 ; i : C c h i 10 D c g g i 11 ; j : D d j e j 12 D e j i j 13 ; k : C g j k 14 T g j i k 15 ; l : C i k l 16 D i k k l 17 ? T i l l f {F1} x00 m : T a e k m 18 ; x00
I0123 11:31:57.298634 140516435906560 alphageometry.py:544] {S} a : ; b : ; c : ; d : C a c d 00 T a c b d 01 ; e : C b c e 02 T a e b c 03 ; f : C a e f 04 C b d f 05 ; g : D a g b g 06 D b g c g 07 ; h : C a b h 08 D a h b h 09 ; i : C c h i 10 D c g g i 11 ; j : D d j e j 12 D e j i j 13 ; k : C g j k 14 T g j i k 15 ; l : C i k l 16 D i k k l 17 ? T i l l f {F1} x00 m : P a e d m 18 ; x00
I0123 11:31:57.298657 140516435906560 alphageometry.py:544] {S} a : ; b : ; c : ; d : C a c d 00 T a c b d 01 ; e : C b c e 02 T a e b c 03 ; f : C a e f 04 C b d f 05 ; g : D a g b g 06 D b g c g 07 ; h : C a b h 08 D a h b h 09 ; i : C c h i 10 D c g g i 11 ; j : D d j e j 12 D e j i j 13 ; k : C g j k 14 T g j i k 15 ; l : C i k l 16 D i k k l 17 ? T i l l f {F1} x00 m : C b c m 18 D b m c m 19 ; x00
I0123 11:31:57.298681 140516435906560 alphageometry.py:544] {S} a : ; b : ; c : ; d : C a c d 00 T a c b d 01 ; e : C b c e 02 T a e b c 03 ; f : C a e f 04 C b d f 05 ; g : D a g b g 06 D b g c g 07 ; h : C a b h 08 D a h b h 09 ; i : C c h i 10 D c g g i 11 ; j : D d j e j 12 D e j i j 13 ; k : C g j k 14 T g j i k 15 ; l : C i k l 16 D i k k l 17 ? T i l l f {F1} x00 m : D a d d m 18 T a m b d 19 ; x00
I0123 11:31:57.298704 140516435906560 alphageometry.py:544] {S} a : ; b : ; c : ; d : C a c d 00 T a c b d 01 ; e : C b c e 02 T a e b c 03 ; f : C a e f 04 C b d f 05 ; g : D a g b g 06 D b g c g 07 ; h : C a b h 08 D a h b h 09 ; i : C c h i 10 D c g g i 11 ; j : D d j e j 12 D e j i j 13 ; k : C g j k 14 T g j i k 15 ; l : C i k l 16 D i k k l 17 ? T i l l f {F1} x00 m : P a e b m 18 ; x00
I0123 11:31:57.298728 140516435906560 alphageometry.py:544] {S} a : ; b : ; c : ; d : C a c d 00 T a c b d 01 ; e : C b c e 02 T a e b c 03 ; f : C a e f 04 C b d f 05 ; g : D a g b g 06 D b g c g 07 ; h : C a b h 08 D a h b h 09 ; i : C c h i 10 D c g g i 11 ; j : D d j e j 12 D e j i j 13 ; k : C g j k 14 T g j i k 15 ; l : C i k l 16 D i k k l 17 ? T i l l f {F1} x00 m : T a b f m 18 ; x00
I0123 11:31:57.298751 140516435906560 alphageometry.py:544] {S} a : ; b : ; c : ; d : C a c d 00 T a c b d 01 ; e : C b c e 02 T a e b c 03 ; f : C a e f 04 C b d f 05 ; g : D a g b g 06 D b g c g 07 ; h : C a b h 08 D a h b h 09 ; i : C c h i 10 D c g g i 11 ; j : D d j e j 12 D e j i j 13 ; k : C g j k 14 T g j i k 15 ; l : C i k l 16 D i k k l 17 ? T i l l f {F1} x00 m : P a b c m 18 T a b a m 19 ; x00
I0123 11:31:57.298775 140516435906560 alphageometry.py:544] {S} a : ; b : ; c : ; d : C a c d 00 T a c b d 01 ; e : C b c e 02 T a e b c 03 ; f : C a e f 04 C b d f 05 ; g : D a g b g 06 D b g c g 07 ; h : C a b h 08 D a h b h 09 ; i : C c h i 10 D c g g i 11 ; j : D d j e j 12 D e j i j 13 ; k : C g j k 14 T g j i k 15 ; l : C i k l 16 D i k k l 17 ? T i l l f {F1} x00 m : T a b j m 18 ; x00
I0123 11:31:57.298802 140516435906560 alphageometry.py:544] {S} a : ; b : ; c : ; d : C a c d 00 T a c b d 01 ; e : C b c e 02 T a e b c 03 ; f : C a e f 04 C b d f 05 ; g : D a g b g 06 D b g c g 07 ; h : C a b h 08 D a h b h 09 ; i : C c h i 10 D c g g i 11 ; j : D d j e j 12 D e j i j 13 ; k : C g j k 14 T g j i k 15 ; l : C i k l 16 D i k k l 17 ? T i l l f {F1} x00 m : D a c a m 18 T a b c m 19 ; x00
I0123 11:31:57.298827 140516435906560 alphageometry.py:544] {S} a : ; b : ; c : ; d : C a c d 00 T a c b d 01 ; e : C b c e 02 T a e b c 03 ; f : C a e f 04 C b d f 05 ; g : D a g b g 06 D b g c g 07 ; h : C a b h 08 D a h b h 09 ; i : C c h i 10 D c g g i 11 ; j : D d j e j 12 D e j i j 13 ; k : C g j k 14 T g j i k 15 ; l : C i k l 16 D i k k l 17 ? T i l l f {F1} x00 m : D b d b m 18 D c d c m 19 ; x00
I0123 11:31:57.298851 140516435906560 alphageometry.py:544] {S} a : ; b : ; c : ; d : C a c d 00 T a c b d 01 ; e : C b c e 02 T a e b c 03 ; f : C a e f 04 C b d f 05 ; g : D a g b g 06 D b g c g 07 ; h : C a b h 08 D a h b h 09 ; i : C c h i 10 D c g g i 11 ; j : D d j e j 12 D e j i j 13 ; k : C g j k 14 T g j i k 15 ; l : C i k l 16 D i k k l 17 ? T i l l f {F1} x00 m : P a e c m 18 ; x00
I0123 11:31:57.298874 140516435906560 alphageometry.py:544] {S} a : ; b : ; c : ; d : C a c d 00 T a c b d 01 ; e : C b c e 02 T a e b c 03 ; f : C a e f 04 C b d f 05 ; g : D a g b g 06 D b g c g 07 ; h : C a b h 08 D a h b h 09 ; i : C c h i 10 D c g g i 11 ; j : D d j e j 12 D e j i j 13 ; k : C g j k 14 T g j i k 15 ; l : C i k l 16 D i k k l 17 ? T i l l f {F1} x00 m : D a c a m 18 T a c a m 19 ; x00
I0123 11:31:57.298898 140516435906560 alphageometry.py:544] {S} a : ; b : ; c : ; d : C a c d 00 T a c b d 01 ; e : C b c e 02 T a e b c 03 ; f : C a e f 04 C b d f 05 ; g : D a g b g 06 D b g c g 07 ; h : C a b h 08 D a h b h 09 ; i : C c h i 10 D c g g i 11 ; j : D d j e j 12 D e j i j 13 ; k : C g j k 14 T g j i k 15 ; l : C i k l 16 D i k k l 17 ? T i l l f {F1} x00 m : T a b a m 18 ; x00
I0123 11:31:57.298922 140516435906560 alphageometry.py:544] {S} a : ; b : ; c : ; d : C a c d 00 T a c b d 01 ; e : C b c e 02 T a e b c 03 ; f : C a e f 04 C b d f 05 ; g : D a g b g 06 D b g c g 07 ; h : C a b h 08 D a h b h 09 ; i : C c h i 10 D c g g i 11 ; j : D d j e j 12 D e j i j 13 ; k : C g j k 14 T g j i k 15 ; l : C i k l 16 D i k k l 17 ? T i l l f {F1} x00 m : T a c c m 18 ; x00
I0123 11:31:57.298944 140516435906560 alphageometry.py:544] {S} a : ; b : ; c : ; d : C a c d 00 T a c b d 01 ; e : C b c e 02 T a e b c 03 ; f : C a e f 04 C b d f 05 ; g : D a g b g 06 D b g c g 07 ; h : C a b h 08 D a h b h 09 ; i : C c h i 10 D c g g i 11 ; j : D d j e j 12 D e j i j 13 ; k : C g j k 14 T g j i k 15 ; l : C i k l 16 D i k k l 17 ? T i l l f {F1} x00 m : P a b c m 18 ; x00
I0123 11:31:57.298967 140516435906560 alphageometry.py:544] {S} a : ; b : ; c : ; d : C a c d 00 T a c b d 01 ; e : C b c e 02 T a e b c 03 ; f : C a e f 04 C b d f 05 ; g : D a g b g 06 D b g c g 07 ; h : C a b h 08 D a h b h 09 ; i : C c h i 10 D c g g i 11 ; j : D d j e j 12 D e j i j 13 ; k : C g j k 14 T g j i k 15 ; l : C i k l 16 D i k k l 17 ? T i l l f {F1} x00 m : D a g g m 18 ; x00
I0123 11:31:57.298990 140516435906560 alphageometry.py:544] {S} a : ; b : ; c : ; d : C a c d 00 T a c b d 01 ; e : C b c e 02 T a e b c 03 ; f : C a e f 04 C b d f 05 ; g : D a g b g 06 D b g c g 07 ; h : C a b h 08 D a h b h 09 ; i : C c h i 10 D c g g i 11 ; j : D d j e j 12 D e j i j 13 ; k : C g j k 14 T g j i k 15 ; l : C i k l 16 D i k k l 17 ? T i l l f {F1} x00 m : P a c e m 18 ; x00
I0123 11:31:57.299013 140516435906560 alphageometry.py:544] {S} a : ; b : ; c : ; d : C a c d 00 T a c b d 01 ; e : C b c e 02 T a e b c 03 ; f : C a e f 04 C b d f 05 ; g : D a g b g 06 D b g c g 07 ; h : C a b h 08 D a h b h 09 ; i : C c h i 10 D c g g i 11 ; j : D d j e j 12 D e j i j 13 ; k : C g j k 14 T g j i k 15 ; l : C i k l 16 D i k k l 17 ? T i l l f {F1} x00 m : D a e a m 18 D b e b m 19 ; x00
I0123 11:31:57.299038 140516435906560 alphageometry.py:544] {S} a : ; b : ; c : ; d : C a c d 00 T a c b d 01 ; e : C b c e 02 T a e b c 03 ; f : C a e f 04 C b d f 05 ; g : D a g b g 06 D b g c g 07 ; h : C a b h 08 D a h b h 09 ; i : C c h i 10 D c g g i 11 ; j : D d j e j 12 D e j i j 13 ; k : C g j k 14 T g j i k 15 ; l : C i k l 16 D i k k l 17 ? T i l l f {F1} x00 m : P a d f m 18 ; x00
I0123 11:31:57.299066 140516435906560 alphageometry.py:549] Decoding from {S} a : ; b : ; c : ; d : C a c d 00 T a c b d 01 ; e : C b c e 02 T a e b c 03 ; f : C a e f 04 C b d f 05 ; g : D a g b g 06 D b g c g 07 ; h : C a b h 08 D a h b h 09 ; i : C c h i 10 D c g g i 11 ; j : D d j e j 12 D e j i j 13 ; k : C g j k 14 T g j i k 15 ; l : C i k l 16 D i k k l 17 ? T i l l f {F1} x00 m : C a c m 18 D a m c m 19 ; x00
I0123 11:32:05.324516 140516435906560 alphageometry.py:566] LM output (score=-0.904983): "n : C b c n 20 D b n c n 21 ;"
I0123 11:32:05.324669 140516435906560 alphageometry.py:567] Translation: "n = on_line n b c, on_bline n c b"

I0123 11:32:05.324710 140516435906560 alphageometry.py:576] Solving: "a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a c, on_bline m c a; n = on_line n b c, on_bline n c b ? perp i l l f"
I0123 11:32:05.324864 140516435906560 graph.py:498] 
I0123 11:32:05.324925 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a c, on_bline m c a; n = on_line n b c, on_bline n c b ? perp i l l f
I0123 11:32:08.326248 140516435906560 ddar.py:60] Depth 1/1000 time = 2.9371843338012695
I0123 11:32:17.983662 140516435906560 ddar.py:60] Depth 2/1000 time = 9.657204627990723
I0123 11:32:43.619999 140516435906560 ddar.py:60] Depth 3/1000 time = 25.636056900024414
I0123 11:33:04.819598 140516435906560 ddar.py:60] Depth 4/1000 time = 21.199344396591187
I0123 11:33:25.981534 140516435906560 ddar.py:60] Depth 5/1000 time = 21.161555290222168
I0123 11:33:47.753357 140516435906560 ddar.py:60] Depth 6/1000 time = 21.770822286605835
I0123 11:34:09.197520 140516435906560 ddar.py:60] Depth 7/1000 time = 21.21246886253357
I0123 11:34:09.197876 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:34:09.197945 140516435906560 alphageometry.py:566] LM output (score=-1.418371): "n : C c e n 20 D c n e n 21 ;"
I0123 11:34:09.197983 140516435906560 alphageometry.py:567] Translation: "n = on_line n c e, on_bline n e c"

I0123 11:34:09.198022 140516435906560 alphageometry.py:576] Solving: "a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a c, on_bline m c a; n = on_line n c e, on_bline n e c ? perp i l l f"
I0123 11:34:09.198193 140516435906560 graph.py:498] 
I0123 11:34:09.198253 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a c, on_bline m c a; n = on_line n c e, on_bline n e c ? perp i l l f
I0123 11:34:11.557085 140516435906560 ddar.py:60] Depth 1/1000 time = 2.2915420532226562
I0123 11:34:19.410003 140516435906560 ddar.py:60] Depth 2/1000 time = 7.852733373641968
I0123 11:34:34.685785 140516435906560 ddar.py:60] Depth 3/1000 time = 15.275518417358398
I0123 11:34:50.739232 140516435906560 ddar.py:60] Depth 4/1000 time = 16.0530526638031
I0123 11:35:06.128041 140516435906560 ddar.py:60] Depth 5/1000 time = 15.388434886932373
I0123 11:35:22.187203 140516435906560 ddar.py:60] Depth 6/1000 time = 16.05824375152588
I0123 11:35:37.877114 140516435906560 ddar.py:60] Depth 7/1000 time = 15.502094507217407
I0123 11:35:54.104922 140516435906560 ddar.py:60] Depth 8/1000 time = 16.208439111709595
I0123 11:35:54.105210 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:35:54.105280 140516435906560 alphageometry.py:566] LM output (score=-1.430547): "n : T a e k n 20 ;"
I0123 11:35:54.105316 140516435906560 alphageometry.py:567] Translation: "n = on_tline n k a e"

I0123 11:35:54.105360 140516435906560 alphageometry.py:576] Solving: "a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a c, on_bline m c a; n = on_tline n k a e ? perp i l l f"
I0123 11:35:54.105532 140516435906560 graph.py:498] 
I0123 11:35:54.105594 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a c, on_bline m c a; n = on_tline n k a e ? perp i l l f
I0123 11:35:55.514435 140516435906560 ddar.py:60] Depth 1/1000 time = 1.351348876953125
I0123 11:36:02.170630 140516435906560 ddar.py:60] Depth 2/1000 time = 6.656026840209961
I0123 11:36:15.144533 140516435906560 ddar.py:60] Depth 3/1000 time = 12.973692893981934
I0123 11:36:27.912885 140516435906560 ddar.py:60] Depth 4/1000 time = 12.768077611923218
I0123 11:36:40.821177 140516435906560 ddar.py:60] Depth 5/1000 time = 12.907903671264648
I0123 11:36:53.003511 140516435906560 ddar.py:60] Depth 6/1000 time = 12.181294441223145
I0123 11:37:05.299832 140516435906560 ddar.py:60] Depth 7/1000 time = 12.142473220825195
I0123 11:37:05.300226 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:37:05.300304 140516435906560 alphageometry.py:566] LM output (score=-1.836906): "n : C b e n 20 D b n e n 21 ;"
I0123 11:37:05.300338 140516435906560 alphageometry.py:567] Translation: "n = on_line n b e, on_bline n e b"

I0123 11:37:05.300391 140516435906560 alphageometry.py:576] Solving: "a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a c, on_bline m c a; n = on_line n b e, on_bline n e b ? perp i l l f"
I0123 11:37:05.300576 140516435906560 graph.py:498] 
I0123 11:37:05.300638 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a c, on_bline m c a; n = on_line n b e, on_bline n e b ? perp i l l f
I0123 11:37:08.364155 140516435906560 ddar.py:60] Depth 1/1000 time = 2.9967334270477295
I0123 11:37:15.233129 140516435906560 ddar.py:60] Depth 2/1000 time = 6.868804693222046
I0123 11:37:30.375716 140516435906560 ddar.py:60] Depth 3/1000 time = 15.142393589019775
I0123 11:37:45.262092 140516435906560 ddar.py:60] Depth 4/1000 time = 14.886141538619995
I0123 11:37:59.491976 140516435906560 ddar.py:60] Depth 5/1000 time = 14.229590654373169
I0123 11:38:14.490800 140516435906560 ddar.py:60] Depth 6/1000 time = 14.998000860214233
I0123 11:38:28.969604 140516435906560 ddar.py:60] Depth 7/1000 time = 14.297802925109863
I0123 11:38:28.969834 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:38:28.969895 140516435906560 alphageometry.py:566] LM output (score=-1.980115): "n : C e i n 20 D e n i n 21 ;"
I0123 11:38:28.969928 140516435906560 alphageometry.py:567] Translation: "n = on_line n e i, on_bline n i e"

I0123 11:38:28.969963 140516435906560 alphageometry.py:576] Solving: "a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a c, on_bline m c a; n = on_line n e i, on_bline n i e ? perp i l l f"
I0123 11:38:28.970140 140516435906560 graph.py:498] 
I0123 11:38:28.970200 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a c, on_bline m c a; n = on_line n e i, on_bline n i e ? perp i l l f
I0123 11:38:31.255134 140516435906560 ddar.py:60] Depth 1/1000 time = 2.2194931507110596
I0123 11:38:37.939792 140516435906560 ddar.py:60] Depth 2/1000 time = 6.684382438659668
I0123 11:38:52.339349 140516435906560 ddar.py:60] Depth 3/1000 time = 14.399290800094604
I0123 11:39:05.591977 140516435906560 ddar.py:60] Depth 4/1000 time = 13.252269268035889
I0123 11:39:18.939711 140516435906560 ddar.py:60] Depth 5/1000 time = 13.347499132156372
I0123 11:39:32.262247 140516435906560 ddar.py:60] Depth 6/1000 time = 13.321709632873535
I0123 11:39:46.512445 140516435906560 ddar.py:60] Depth 7/1000 time = 14.066929817199707
I0123 11:39:46.512852 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:39:46.512941 140516435906560 alphageometry.py:566] LM output (score=-2.013023): "n : T a g g n 20 ;"
I0123 11:39:46.512975 140516435906560 alphageometry.py:567] Translation: "n = on_tline n g a g"

I0123 11:39:46.513025 140516435906560 alphageometry.py:576] Solving: "a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a c, on_bline m c a; n = on_tline n g a g ? perp i l l f"
I0123 11:39:46.513209 140516435906560 graph.py:498] 
I0123 11:39:46.513267 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a c, on_bline m c a; n = on_tline n g a g ? perp i l l f
I0123 11:39:47.898295 140516435906560 ddar.py:60] Depth 1/1000 time = 1.3288018703460693
I0123 11:39:54.917251 140516435906560 ddar.py:60] Depth 2/1000 time = 7.0187883377075195
I0123 11:40:11.322287 140516435906560 ddar.py:60] Depth 3/1000 time = 16.40479874610901
I0123 11:40:24.556321 140516435906560 ddar.py:60] Depth 4/1000 time = 13.233688354492188
I0123 11:40:37.044832 140516435906560 ddar.py:60] Depth 5/1000 time = 12.4882972240448
I0123 11:40:49.526043 140516435906560 ddar.py:60] Depth 6/1000 time = 12.480540990829468
I0123 11:41:02.882685 140516435906560 ddar.py:60] Depth 7/1000 time = 13.180530071258545
I0123 11:41:02.883030 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:41:02.883092 140516435906560 alphageometry.py:566] LM output (score=-2.017633): "n : T a b f n 20 ;"
I0123 11:41:02.883127 140516435906560 alphageometry.py:567] Translation: "n = on_tline n f a b"

I0123 11:41:02.883163 140516435906560 alphageometry.py:576] Solving: "a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a c, on_bline m c a; n = on_tline n f a b ? perp i l l f"
I0123 11:41:02.883320 140516435906560 graph.py:498] 
I0123 11:41:02.883379 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a c, on_bline m c a; n = on_tline n f a b ? perp i l l f
I0123 11:41:04.291803 140516435906560 ddar.py:60] Depth 1/1000 time = 1.3531157970428467
I0123 11:41:11.199348 140516435906560 ddar.py:60] Depth 2/1000 time = 6.907382965087891
I0123 11:41:26.832944 140516435906560 ddar.py:60] Depth 3/1000 time = 15.633325815200806
I0123 11:41:38.741885 140516435906560 ddar.py:60] Depth 4/1000 time = 11.908535480499268
I0123 11:41:51.656462 140516435906560 ddar.py:60] Depth 5/1000 time = 12.914243221282959
I0123 11:42:03.789113 140516435906560 ddar.py:60] Depth 6/1000 time = 12.131890535354614
I0123 11:42:15.943179 140516435906560 ddar.py:60] Depth 7/1000 time = 12.001633167266846
I0123 11:42:15.943551 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:42:15.943629 140516435906560 alphageometry.py:566] LM output (score=-2.122510): "n : C a e n 20 D a n e n 21 ;"
I0123 11:42:15.943663 140516435906560 alphageometry.py:567] Translation: "n = on_line n a e, on_bline n e a"

I0123 11:42:15.943712 140516435906560 alphageometry.py:576] Solving: "a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a c, on_bline m c a; n = on_line n a e, on_bline n e a ? perp i l l f"
I0123 11:42:15.943891 140516435906560 graph.py:498] 
I0123 11:42:15.943950 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a c, on_bline m c a; n = on_line n a e, on_bline n e a ? perp i l l f
I0123 11:42:18.471262 140516435906560 ddar.py:60] Depth 1/1000 time = 2.458557605743408
I0123 11:42:27.602160 140516435906560 ddar.py:60] Depth 2/1000 time = 9.130715370178223
I0123 11:42:43.742491 140516435906560 ddar.py:60] Depth 3/1000 time = 16.140118837356567
I0123 11:43:01.004576 140516435906560 ddar.py:60] Depth 4/1000 time = 17.261847734451294
I0123 11:43:18.331132 140516435906560 ddar.py:60] Depth 5/1000 time = 17.32628583908081
I0123 11:43:35.777422 140516435906560 ddar.py:60] Depth 6/1000 time = 17.445487022399902
I0123 11:43:52.647387 140516435906560 ddar.py:60] Depth 7/1000 time = 16.672109365463257
I0123 11:44:10.136328 140516435906560 ddar.py:60] Depth 8/1000 time = 17.460689783096313
I0123 11:44:10.136732 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:44:10.136813 140516435906560 alphageometry.py:566] LM output (score=-2.127080): "n : C e k n 20 D e n k n 21 ;"
I0123 11:44:10.136847 140516435906560 alphageometry.py:567] Translation: "n = on_line n e k, on_bline n k e"

I0123 11:44:10.136898 140516435906560 alphageometry.py:576] Solving: "a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a c, on_bline m c a; n = on_line n e k, on_bline n k e ? perp i l l f"
I0123 11:44:10.137084 140516435906560 graph.py:498] 
I0123 11:44:10.137145 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a c, on_bline m c a; n = on_line n e k, on_bline n k e ? perp i l l f
I0123 11:44:12.428715 140516435906560 ddar.py:60] Depth 1/1000 time = 2.225476026535034
I0123 11:44:18.078402 140516435906560 ddar.py:60] Depth 2/1000 time = 5.6495201587677
I0123 11:44:31.528881 140516435906560 ddar.py:60] Depth 3/1000 time = 13.450268745422363
I0123 11:44:44.107317 140516435906560 ddar.py:60] Depth 4/1000 time = 12.578183650970459
I0123 11:44:55.953021 140516435906560 ddar.py:60] Depth 5/1000 time = 11.845482110977173
I0123 11:45:07.845692 140516435906560 ddar.py:60] Depth 6/1000 time = 11.891965627670288
I0123 11:45:19.975634 140516435906560 ddar.py:60] Depth 7/1000 time = 11.971828699111938
I0123 11:45:19.976010 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:45:19.976092 140516435906560 alphageometry.py:566] LM output (score=-2.192892): "n : C d e n 20 D d n e n 21 ;"
I0123 11:45:19.976125 140516435906560 alphageometry.py:567] Translation: "n = on_line n d e, on_bline n e d"

I0123 11:45:19.976175 140516435906560 alphageometry.py:576] Solving: "a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a c, on_bline m c a; n = on_line n d e, on_bline n e d ? perp i l l f"
I0123 11:45:19.976349 140516435906560 graph.py:498] 
I0123 11:45:19.976408 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a c, on_bline m c a; n = on_line n d e, on_bline n e d ? perp i l l f
I0123 11:45:22.344007 140516435906560 ddar.py:60] Depth 1/1000 time = 2.299691677093506
I0123 11:45:28.458411 140516435906560 ddar.py:60] Depth 2/1000 time = 6.114053249359131
I0123 11:45:48.755345 140516435906560 ddar.py:60] Depth 3/1000 time = 20.29672861099243
I0123 11:46:02.300376 140516435906560 ddar.py:60] Depth 4/1000 time = 13.544747114181519
I0123 11:46:17.762981 140516435906560 ddar.py:60] Depth 5/1000 time = 15.462271690368652
I0123 11:46:33.429975 140516435906560 ddar.py:60] Depth 6/1000 time = 15.666568279266357
I0123 11:46:49.201401 140516435906560 ddar.py:60] Depth 7/1000 time = 15.77102780342102
I0123 11:47:04.131739 140516435906560 ddar.py:60] Depth 8/1000 time = 14.93000078201294
I0123 11:47:19.928276 140516435906560 ddar.py:60] Depth 9/1000 time = 15.796280860900879
I0123 11:47:34.923881 140516435906560 ddar.py:60] Depth 10/1000 time = 14.994832992553711
I0123 11:47:51.008910 140516435906560 ddar.py:60] Depth 11/1000 time = 15.915189027786255
I0123 11:48:06.339539 140516435906560 ddar.py:60] Depth 12/1000 time = 15.258883714675903
I0123 11:48:06.339948 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:48:06.340038 140516435906560 alphageometry.py:566] LM output (score=-2.366118): "n : T m h h n 20 ;"
I0123 11:48:06.340072 140516435906560 alphageometry.py:567] Translation: "n = on_tline n h m h"

I0123 11:48:06.340123 140516435906560 alphageometry.py:576] Solving: "a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a c, on_bline m c a; n = on_tline n h m h ? perp i l l f"
I0123 11:48:06.340307 140516435906560 graph.py:498] 
I0123 11:48:06.340367 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a c, on_bline m c a; n = on_tline n h m h ? perp i l l f
I0123 11:48:08.604882 140516435906560 ddar.py:60] Depth 1/1000 time = 2.208475112915039
I0123 11:48:14.102148 140516435906560 ddar.py:60] Depth 2/1000 time = 5.49710750579834
I0123 11:48:28.097774 140516435906560 ddar.py:60] Depth 3/1000 time = 13.995392322540283
I0123 11:48:41.444424 140516435906560 ddar.py:60] Depth 4/1000 time = 13.3462233543396
I0123 11:48:54.750369 140516435906560 ddar.py:60] Depth 5/1000 time = 13.305564641952515
I0123 11:49:08.903801 140516435906560 ddar.py:60] Depth 6/1000 time = 14.152604818344116
I0123 11:49:22.532530 140516435906560 ddar.py:60] Depth 7/1000 time = 13.469244241714478
I0123 11:49:36.199814 140516435906560 ddar.py:60] Depth 8/1000 time = 13.649860382080078
I0123 11:49:36.200238 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:49:36.200327 140516435906560 alphageometry.py:566] LM output (score=-2.374977): "n : C e f n 20 D e n f n 21 ;"
I0123 11:49:36.200362 140516435906560 alphageometry.py:567] Translation: "n = on_line n e f, on_bline n f e"

I0123 11:49:36.200417 140516435906560 alphageometry.py:576] Solving: "a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a c, on_bline m c a; n = on_line n e f, on_bline n f e ? perp i l l f"
I0123 11:49:36.200604 140516435906560 graph.py:498] 
I0123 11:49:36.200663 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a c, on_bline m c a; n = on_line n e f, on_bline n f e ? perp i l l f
I0123 11:49:38.758967 140516435906560 ddar.py:60] Depth 1/1000 time = 2.4919612407684326
I0123 11:49:46.082896 140516435906560 ddar.py:60] Depth 2/1000 time = 7.323744773864746
I0123 11:50:01.786214 140516435906560 ddar.py:60] Depth 3/1000 time = 15.703056335449219
I0123 11:50:14.603734 140516435906560 ddar.py:60] Depth 4/1000 time = 12.817104578018188
I0123 11:50:28.238581 140516435906560 ddar.py:60] Depth 5/1000 time = 13.63448166847229
I0123 11:50:41.927727 140516435906560 ddar.py:60] Depth 6/1000 time = 13.688430070877075
I0123 11:50:55.061173 140516435906560 ddar.py:60] Depth 7/1000 time = 12.970375537872314
I0123 11:50:55.061684 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:50:55.061766 140516435906560 alphageometry.py:566] LM output (score=-2.384512): "n : T e j e n 20 ;"
I0123 11:50:55.061803 140516435906560 alphageometry.py:567] Translation: "n = on_tline n e e j"

I0123 11:50:55.061856 140516435906560 alphageometry.py:576] Solving: "a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a c, on_bline m c a; n = on_tline n e e j ? perp i l l f"
I0123 11:50:55.062040 140516435906560 graph.py:498] 
I0123 11:50:55.062101 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a c, on_bline m c a; n = on_tline n e e j ? perp i l l f
I0123 11:50:57.324566 140516435906560 ddar.py:60] Depth 1/1000 time = 2.2048285007476807
I0123 11:51:03.451508 140516435906560 ddar.py:60] Depth 2/1000 time = 6.126779317855835
I0123 11:51:18.540702 140516435906560 ddar.py:60] Depth 3/1000 time = 15.088968276977539
I0123 11:51:31.012694 140516435906560 ddar.py:60] Depth 4/1000 time = 12.471683263778687
I0123 11:51:43.442300 140516435906560 ddar.py:60] Depth 5/1000 time = 12.429250478744507
I0123 11:51:55.840775 140516435906560 ddar.py:60] Depth 6/1000 time = 12.397753238677979
I0123 11:52:07.478631 140516435906560 ddar.py:60] Depth 7/1000 time = 11.470731496810913
I0123 11:52:07.479002 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:52:07.479084 140516435906560 alphageometry.py:566] LM output (score=-2.414781): "n : C k l n 20 D k n l n 21 ;"
I0123 11:52:07.479119 140516435906560 alphageometry.py:567] Translation: "n = on_line n k l, on_bline n l k"

I0123 11:52:07.479168 140516435906560 alphageometry.py:576] Solving: "a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a c, on_bline m c a; n = on_line n k l, on_bline n l k ? perp i l l f"
I0123 11:52:07.479363 140516435906560 graph.py:498] 
I0123 11:52:07.479424 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a c, on_bline m c a; n = on_line n k l, on_bline n l k ? perp i l l f
I0123 11:52:10.851196 140516435906560 ddar.py:60] Depth 1/1000 time = 2.4485015869140625
I0123 11:52:17.810132 140516435906560 ddar.py:60] Depth 2/1000 time = 6.958756446838379
I0123 11:52:31.494956 140516435906560 ddar.py:60] Depth 3/1000 time = 13.684608936309814
I0123 11:52:44.187045 140516435906560 ddar.py:60] Depth 4/1000 time = 12.691797733306885
I0123 11:52:57.779130 140516435906560 ddar.py:60] Depth 5/1000 time = 13.591693878173828
I0123 11:53:10.491055 140516435906560 ddar.py:60] Depth 6/1000 time = 12.709893941879272
I0123 11:53:24.387728 140516435906560 ddar.py:60] Depth 7/1000 time = 13.737022161483765
I0123 11:53:24.387988 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:53:24.388041 140516435906560 alphageometry.py:566] LM output (score=-2.458851): "n : C c f n 20 D c n f n 21 ;"
I0123 11:53:24.388075 140516435906560 alphageometry.py:567] Translation: "n = on_line n c f, on_bline n f c"

I0123 11:53:24.388117 140516435906560 alphageometry.py:576] Solving: "a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a c, on_bline m c a; n = on_line n c f, on_bline n f c ? perp i l l f"
I0123 11:53:24.388280 140516435906560 graph.py:498] 
I0123 11:53:24.388340 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a c, on_bline m c a; n = on_line n c f, on_bline n f c ? perp i l l f
I0123 11:53:25.989157 140516435906560 ddar.py:60] Depth 1/1000 time = 1.5340142250061035
I0123 11:53:33.994486 140516435906560 ddar.py:60] Depth 2/1000 time = 8.00516676902771
I0123 11:53:58.554952 140516435906560 ddar.py:60] Depth 3/1000 time = 24.560192108154297
I0123 11:54:18.578326 140516435906560 ddar.py:60] Depth 4/1000 time = 20.02291178703308
I0123 11:54:41.374452 140516435906560 ddar.py:60] Depth 5/1000 time = 22.79565405845642
I0123 11:55:04.162333 140516435906560 ddar.py:60] Depth 6/1000 time = 22.787476539611816
I0123 11:55:27.265295 140516435906560 ddar.py:60] Depth 7/1000 time = 23.101882219314575
I0123 11:55:50.811793 140516435906560 ddar.py:60] Depth 8/1000 time = 23.317626237869263
I0123 11:55:50.821604 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:55:50.821675 140516435906560 alphageometry.py:566] LM output (score=-2.459446): "n : C b m n 20 D b n m n 21 ;"
I0123 11:55:50.821711 140516435906560 alphageometry.py:567] Translation: "n = on_line n b m, on_bline n m b"

I0123 11:55:50.821751 140516435906560 alphageometry.py:576] Solving: "a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a c, on_bline m c a; n = on_line n b m, on_bline n m b ? perp i l l f"
I0123 11:55:50.821918 140516435906560 graph.py:498] 
I0123 11:55:50.821976 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a c, on_bline m c a; n = on_line n b m, on_bline n m b ? perp i l l f
I0123 11:55:52.368429 140516435906560 ddar.py:60] Depth 1/1000 time = 1.4795186519622803
I0123 11:55:59.480795 140516435906560 ddar.py:60] Depth 2/1000 time = 7.112194776535034
I0123 11:56:14.277002 140516435906560 ddar.py:60] Depth 3/1000 time = 14.79599905014038
I0123 11:56:28.238044 140516435906560 ddar.py:60] Depth 4/1000 time = 13.960789203643799
I0123 11:56:42.580229 140516435906560 ddar.py:60] Depth 5/1000 time = 14.341851234436035
I0123 11:56:57.016446 140516435906560 ddar.py:60] Depth 6/1000 time = 14.435806274414062
I0123 11:57:11.523504 140516435906560 ddar.py:60] Depth 7/1000 time = 14.506105661392212
I0123 11:57:26.111380 140516435906560 ddar.py:60] Depth 8/1000 time = 14.40428376197815
I0123 11:57:26.120295 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:57:26.120358 140516435906560 alphageometry.py:566] LM output (score=-2.543510): "n : T a b j n 20 ;"
I0123 11:57:26.120394 140516435906560 alphageometry.py:567] Translation: "n = on_tline n j a b"

I0123 11:57:26.120433 140516435906560 alphageometry.py:576] Solving: "a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a c, on_bline m c a; n = on_tline n j a b ? perp i l l f"
I0123 11:57:26.120596 140516435906560 graph.py:498] 
I0123 11:57:26.120655 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a c, on_bline m c a; n = on_tline n j a b ? perp i l l f
I0123 11:57:27.538647 140516435906560 ddar.py:60] Depth 1/1000 time = 1.3614282608032227
I0123 11:57:34.864400 140516435906560 ddar.py:60] Depth 2/1000 time = 7.325507879257202
I0123 11:57:49.001483 140516435906560 ddar.py:60] Depth 3/1000 time = 14.136759519577026
I0123 11:58:02.019693 140516435906560 ddar.py:60] Depth 4/1000 time = 13.017942667007446
I0123 11:58:15.004183 140516435906560 ddar.py:60] Depth 5/1000 time = 12.984151840209961
I0123 11:58:28.888606 140516435906560 ddar.py:60] Depth 6/1000 time = 13.88341760635376
I0123 11:58:42.317410 140516435906560 ddar.py:60] Depth 7/1000 time = 13.27092719078064
I0123 11:58:42.317704 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 11:58:42.317785 140516435906560 alphageometry.py:566] LM output (score=-2.592459): "n : C e h n 20 D e n h n 21 ;"
I0123 11:58:42.317822 140516435906560 alphageometry.py:567] Translation: "n = on_line n e h, on_bline n h e"

I0123 11:58:42.317863 140516435906560 alphageometry.py:576] Solving: "a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a c, on_bline m c a; n = on_line n e h, on_bline n h e ? perp i l l f"
I0123 11:58:42.318050 140516435906560 graph.py:498] 
I0123 11:58:42.318115 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a c, on_bline m c a; n = on_line n e h, on_bline n h e ? perp i l l f
I0123 11:58:43.821061 140516435906560 ddar.py:60] Depth 1/1000 time = 1.435514211654663
I0123 11:58:50.962300 140516435906560 ddar.py:60] Depth 2/1000 time = 7.140955448150635
I0123 11:59:02.978621 140516435906560 ddar.py:60] Depth 3/1000 time = 12.016090393066406
I0123 11:59:16.067503 140516435906560 ddar.py:60] Depth 4/1000 time = 13.08857011795044
I0123 11:59:29.016952 140516435906560 ddar.py:60] Depth 5/1000 time = 12.949203729629517
I0123 11:59:42.966518 140516435906560 ddar.py:60] Depth 6/1000 time = 13.94931173324585
I0123 11:59:56.081356 140516435906560 ddar.py:60] Depth 7/1000 time = 13.114563226699829
I0123 12:00:09.071422 140516435906560 ddar.py:60] Depth 8/1000 time = 12.989199876785278
I0123 12:00:22.192324 140516435906560 ddar.py:60] Depth 9/1000 time = 13.120119571685791
I0123 12:00:35.483965 140516435906560 ddar.py:60] Depth 10/1000 time = 13.126907348632812
I0123 12:00:35.502041 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:00:35.502104 140516435906560 alphageometry.py:566] LM output (score=-2.636791): "n : C e k n 20 D e k k n 21 ;"
I0123 12:00:35.502139 140516435906560 alphageometry.py:567] Translation: "n = on_line n e k, on_circle n k e"

I0123 12:00:35.502176 140516435906560 alphageometry.py:576] Solving: "a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a c, on_bline m c a; n = on_line n e k, on_circle n k e ? perp i l l f"
I0123 12:00:35.502335 140516435906560 graph.py:498] 
I0123 12:00:35.502393 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a c, on_bline m c a; n = on_line n e k, on_circle n k e ? perp i l l f
I0123 12:00:37.121390 140516435906560 ddar.py:60] Depth 1/1000 time = 1.5541627407073975
I0123 12:00:43.969498 140516435906560 ddar.py:60] Depth 2/1000 time = 6.8479461669921875
I0123 12:00:57.233143 140516435906560 ddar.py:60] Depth 3/1000 time = 13.263379573822021
I0123 12:01:09.993132 140516435906560 ddar.py:60] Depth 4/1000 time = 12.759557008743286
I0123 12:01:22.775589 140516435906560 ddar.py:60] Depth 5/1000 time = 12.782118082046509
I0123 12:01:36.422302 140516435906560 ddar.py:60] Depth 6/1000 time = 13.645958185195923
I0123 12:01:49.656259 140516435906560 ddar.py:60] Depth 7/1000 time = 13.068334341049194
I0123 12:01:49.656512 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:01:49.656567 140516435906560 alphageometry.py:566] LM output (score=-2.651197): "n : T a c f n 20 ;"
I0123 12:01:49.656600 140516435906560 alphageometry.py:567] Translation: "n = on_tline n f a c"

I0123 12:01:49.656638 140516435906560 alphageometry.py:576] Solving: "a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a c, on_bline m c a; n = on_tline n f a c ? perp i l l f"
I0123 12:01:49.656804 140516435906560 graph.py:498] 
I0123 12:01:49.656862 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a c, on_bline m c a; n = on_tline n f a c ? perp i l l f
I0123 12:01:51.324015 140516435906560 ddar.py:60] Depth 1/1000 time = 1.614008903503418
I0123 12:01:58.535245 140516435906560 ddar.py:60] Depth 2/1000 time = 7.21105694770813
I0123 12:02:12.898598 140516435906560 ddar.py:60] Depth 3/1000 time = 14.363146781921387
I0123 12:02:26.770805 140516435906560 ddar.py:60] Depth 4/1000 time = 13.871968746185303
I0123 12:02:39.620598 140516435906560 ddar.py:60] Depth 5/1000 time = 12.849469423294067
I0123 12:02:53.426232 140516435906560 ddar.py:60] Depth 6/1000 time = 13.804781198501587
I0123 12:03:06.534379 140516435906560 ddar.py:60] Depth 7/1000 time = 12.955508470535278
I0123 12:03:06.534707 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:03:06.534767 140516435906560 alphageometry.py:566] LM output (score=-2.684638): "n : C e j n 20 D e j j n 21 ;"
I0123 12:03:06.534801 140516435906560 alphageometry.py:567] Translation: "n = on_line n e j, on_circle n j e"

I0123 12:03:06.534838 140516435906560 alphageometry.py:576] Solving: "a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a c, on_bline m c a; n = on_line n e j, on_circle n j e ? perp i l l f"
I0123 12:03:06.535014 140516435906560 graph.py:498] 
I0123 12:03:06.535075 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a c, on_bline m c a; n = on_line n e j, on_circle n j e ? perp i l l f
I0123 12:03:09.036586 140516435906560 ddar.py:60] Depth 1/1000 time = 2.4403364658355713
I0123 12:03:16.142604 140516435906560 ddar.py:60] Depth 2/1000 time = 7.105781078338623
I0123 12:03:34.983906 140516435906560 ddar.py:60] Depth 3/1000 time = 18.840917587280273
I0123 12:03:50.441158 140516435906560 ddar.py:60] Depth 4/1000 time = 15.456803321838379
I0123 12:04:05.936803 140516435906560 ddar.py:60] Depth 5/1000 time = 15.495295286178589
I0123 12:04:21.502514 140516435906560 ddar.py:60] Depth 6/1000 time = 15.564993858337402
I0123 12:04:37.138751 140516435906560 ddar.py:60] Depth 7/1000 time = 15.434628963470459
I0123 12:04:37.139003 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:04:37.139057 140516435906560 alphageometry.py:566] LM output (score=-2.764745): "n : C e m n 20 D e n m n 21 ;"
I0123 12:04:37.139090 140516435906560 alphageometry.py:567] Translation: "n = on_line n e m, on_bline n m e"

I0123 12:04:37.139129 140516435906560 alphageometry.py:576] Solving: "a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a c, on_bline m c a; n = on_line n e m, on_bline n m e ? perp i l l f"
I0123 12:04:37.139292 140516435906560 graph.py:498] 
I0123 12:04:37.139351 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a c, on_bline m c a; n = on_line n e m, on_bline n m e ? perp i l l f
I0123 12:04:39.611180 140516435906560 ddar.py:60] Depth 1/1000 time = 2.4056453704833984
I0123 12:04:46.172086 140516435906560 ddar.py:60] Depth 2/1000 time = 6.5606701374053955
I0123 12:04:57.945731 140516435906560 ddar.py:60] Depth 3/1000 time = 11.773324489593506
I0123 12:05:11.109194 140516435906560 ddar.py:60] Depth 4/1000 time = 13.163235664367676
I0123 12:05:24.292753 140516435906560 ddar.py:60] Depth 5/1000 time = 13.183265447616577
I0123 12:05:37.511968 140516435906560 ddar.py:60] Depth 6/1000 time = 13.218287706375122
I0123 12:05:50.684461 140516435906560 ddar.py:60] Depth 7/1000 time = 13.171512603759766
I0123 12:06:04.158448 140516435906560 ddar.py:60] Depth 8/1000 time = 13.313902616500854
I0123 12:06:17.621727 140516435906560 ddar.py:60] Depth 9/1000 time = 13.446836233139038
I0123 12:06:17.622111 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:06:17.622197 140516435906560 alphageometry.py:566] LM output (score=-2.770815): "n : T a e e n 20 ;"
I0123 12:06:17.622231 140516435906560 alphageometry.py:567] Translation: "n = on_tline n e a e"

I0123 12:06:17.622284 140516435906560 alphageometry.py:576] Solving: "a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a c, on_bline m c a; n = on_tline n e a e ? perp i l l f"
I0123 12:06:17.622470 140516435906560 graph.py:498] 
I0123 12:06:17.622530 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a c, on_bline m c a; n = on_tline n e a e ? perp i l l f
I0123 12:06:20.260756 140516435906560 ddar.py:60] Depth 1/1000 time = 2.5850484371185303
I0123 12:06:27.502823 140516435906560 ddar.py:60] Depth 2/1000 time = 7.241903781890869
I0123 12:06:41.860846 140516435906560 ddar.py:60] Depth 3/1000 time = 14.357810497283936
I0123 12:06:55.795480 140516435906560 ddar.py:60] Depth 4/1000 time = 13.934364080429077
I0123 12:07:08.727215 140516435906560 ddar.py:60] Depth 5/1000 time = 12.931407690048218
I0123 12:07:21.723389 140516435906560 ddar.py:60] Depth 6/1000 time = 12.994686365127563
I0123 12:07:36.006748 140516435906560 ddar.py:60] Depth 7/1000 time = 14.132001638412476
I0123 12:07:36.006980 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:07:36.007033 140516435906560 alphageometry.py:566] LM output (score=-2.780950): "n : T e f k n 20 ;"
I0123 12:07:36.007068 140516435906560 alphageometry.py:567] Translation: "n = on_tline n k e f"

I0123 12:07:36.007105 140516435906560 alphageometry.py:576] Solving: "a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a c, on_bline m c a; n = on_tline n k e f ? perp i l l f"
I0123 12:07:36.007264 140516435906560 graph.py:498] 
I0123 12:07:36.007324 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a c, on_bline m c a; n = on_tline n k e f ? perp i l l f
I0123 12:07:37.412316 140516435906560 ddar.py:60] Depth 1/1000 time = 1.3488564491271973
I0123 12:07:43.375293 140516435906560 ddar.py:60] Depth 2/1000 time = 5.962765693664551
I0123 12:07:56.744927 140516435906560 ddar.py:60] Depth 3/1000 time = 13.369347333908081
I0123 12:08:08.989483 140516435906560 ddar.py:60] Depth 4/1000 time = 12.244336605072021
I0123 12:08:21.233339 140516435906560 ddar.py:60] Depth 5/1000 time = 12.2435941696167
I0123 12:08:32.434688 140516435906560 ddar.py:60] Depth 6/1000 time = 11.200519800186157
I0123 12:08:44.842247 140516435906560 ddar.py:60] Depth 7/1000 time = 12.261338472366333
I0123 12:08:44.842550 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:08:44.842620 140516435906560 alphageometry.py:566] LM output (score=-2.821999): "n : T e f f n 20 ;"
I0123 12:08:44.842653 140516435906560 alphageometry.py:567] Translation: "n = on_tline n f e f"

I0123 12:08:44.842696 140516435906560 alphageometry.py:576] Solving: "a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a c, on_bline m c a; n = on_tline n f e f ? perp i l l f"
I0123 12:08:44.842864 140516435906560 graph.py:498] 
I0123 12:08:44.842922 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a c, on_bline m c a; n = on_tline n f e f ? perp i l l f
I0123 12:08:47.258988 140516435906560 ddar.py:60] Depth 1/1000 time = 2.3587355613708496
I0123 12:08:52.886722 140516435906560 ddar.py:60] Depth 2/1000 time = 5.6275646686553955
I0123 12:09:05.938716 140516435906560 ddar.py:60] Depth 3/1000 time = 13.051789045333862
I0123 12:09:17.293836 140516435906560 ddar.py:60] Depth 4/1000 time = 11.354849100112915
I0123 12:09:29.728570 140516435906560 ddar.py:60] Depth 5/1000 time = 12.434387445449829
I0123 12:09:42.123653 140516435906560 ddar.py:60] Depth 6/1000 time = 12.394198894500732
I0123 12:09:54.746758 140516435906560 ddar.py:60] Depth 7/1000 time = 12.472698211669922
I0123 12:09:54.747115 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:09:54.747195 140516435906560 alphageometry.py:566] LM output (score=-2.833119): "n : T a c j n 20 ;"
I0123 12:09:54.747230 140516435906560 alphageometry.py:567] Translation: "n = on_tline n j a c"

I0123 12:09:54.747282 140516435906560 alphageometry.py:576] Solving: "a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a c, on_bline m c a; n = on_tline n j a c ? perp i l l f"
I0123 12:09:54.747453 140516435906560 graph.py:498] 
I0123 12:09:54.747511 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a c, on_bline m c a; n = on_tline n j a c ? perp i l l f
I0123 12:09:56.192894 140516435906560 ddar.py:60] Depth 1/1000 time = 1.387359619140625
I0123 12:10:02.909130 140516435906560 ddar.py:60] Depth 2/1000 time = 6.716062307357788
I0123 12:10:16.586645 140516435906560 ddar.py:60] Depth 3/1000 time = 13.677266359329224
I0123 12:10:29.184445 140516435906560 ddar.py:60] Depth 4/1000 time = 12.597396850585938
I0123 12:10:41.836390 140516435906560 ddar.py:60] Depth 5/1000 time = 12.651591777801514
I0123 12:10:54.500762 140516435906560 ddar.py:60] Depth 6/1000 time = 12.663486242294312
I0123 12:11:06.345552 140516435906560 ddar.py:60] Depth 7/1000 time = 11.691189050674438
I0123 12:11:06.345921 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:11:06.346004 140516435906560 alphageometry.py:566] LM output (score=-2.834566): "n : C f k n 20 D f k k n 21 ;"
I0123 12:11:06.346039 140516435906560 alphageometry.py:567] Translation: "n = on_line n f k, on_circle n k f"

I0123 12:11:06.346091 140516435906560 alphageometry.py:576] Solving: "a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a c, on_bline m c a; n = on_line n f k, on_circle n k f ? perp i l l f"
I0123 12:11:06.346275 140516435906560 graph.py:498] 
I0123 12:11:06.346337 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a c, on_bline m c a; n = on_line n f k, on_circle n k f ? perp i l l f
I0123 12:11:09.031535 140516435906560 ddar.py:60] Depth 1/1000 time = 2.618354082107544
I0123 12:11:15.428589 140516435906560 ddar.py:60] Depth 2/1000 time = 6.396821975708008
I0123 12:11:30.072384 140516435906560 ddar.py:60] Depth 3/1000 time = 14.643465757369995
I0123 12:11:42.249153 140516435906560 ddar.py:60] Depth 4/1000 time = 12.176242589950562
I0123 12:11:55.469975 140516435906560 ddar.py:60] Depth 5/1000 time = 13.2201988697052
I0123 12:12:08.747162 140516435906560 ddar.py:60] Depth 6/1000 time = 13.27600359916687
I0123 12:12:22.478924 140516435906560 ddar.py:60] Depth 7/1000 time = 13.56728458404541
I0123 12:12:22.479691 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:12:22.479748 140516435906560 alphageometry.py:566] LM output (score=-2.889072): "n : T e i k n 20 ;"
I0123 12:12:22.479781 140516435906560 alphageometry.py:567] Translation: "n = on_tline n k e i"

I0123 12:12:22.479820 140516435906560 alphageometry.py:576] Solving: "a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a c, on_bline m c a; n = on_tline n k e i ? perp i l l f"
I0123 12:12:22.479999 140516435906560 graph.py:498] 
I0123 12:12:22.480057 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a c, on_bline m c a; n = on_tline n k e i ? perp i l l f
I0123 12:12:23.896605 140516435906560 ddar.py:60] Depth 1/1000 time = 1.3609569072723389
I0123 12:12:29.940234 140516435906560 ddar.py:60] Depth 2/1000 time = 6.043471097946167
I0123 12:12:44.481327 140516435906560 ddar.py:60] Depth 3/1000 time = 14.540902137756348
I0123 12:12:56.217288 140516435906560 ddar.py:60] Depth 4/1000 time = 11.735687732696533
I0123 12:13:08.977483 140516435906560 ddar.py:60] Depth 5/1000 time = 12.759786367416382
I0123 12:13:21.959305 140516435906560 ddar.py:60] Depth 6/1000 time = 12.980878114700317
I0123 12:13:34.003714 140516435906560 ddar.py:60] Depth 7/1000 time = 11.87828016281128
I0123 12:13:34.003958 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:13:34.004017 140516435906560 alphageometry.py:566] LM output (score=-2.930186): "n : T e h h n 20 ;"
I0123 12:13:34.004052 140516435906560 alphageometry.py:567] Translation: "n = on_tline n h e h"

I0123 12:13:34.004093 140516435906560 alphageometry.py:576] Solving: "a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a c, on_bline m c a; n = on_tline n h e h ? perp i l l f"
I0123 12:13:34.004251 140516435906560 graph.py:498] 
I0123 12:13:34.004309 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a c, on_bline m c a; n = on_tline n h e h ? perp i l l f
I0123 12:13:35.404864 140516435906560 ddar.py:60] Depth 1/1000 time = 1.3439199924468994
I0123 12:13:41.650950 140516435906560 ddar.py:60] Depth 2/1000 time = 6.245905876159668
I0123 12:13:56.156839 140516435906560 ddar.py:60] Depth 3/1000 time = 14.505681276321411
I0123 12:14:09.062127 140516435906560 ddar.py:60] Depth 4/1000 time = 12.905057191848755
I0123 12:14:20.784391 140516435906560 ddar.py:60] Depth 5/1000 time = 11.721966028213501
I0123 12:14:33.707844 140516435906560 ddar.py:60] Depth 6/1000 time = 12.922616481781006
I0123 12:14:45.811830 140516435906560 ddar.py:60] Depth 7/1000 time = 11.929456233978271
I0123 12:14:45.812073 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:14:45.812129 140516435906560 alphageometry.py:566] LM output (score=-2.987675): "n : C e h n 20 D e h h n 21 ;"
I0123 12:14:45.812164 140516435906560 alphageometry.py:567] Translation: "n = on_line n e h, on_circle n h e"

I0123 12:14:45.812205 140516435906560 alphageometry.py:576] Solving: "a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a c, on_bline m c a; n = on_line n e h, on_circle n h e ? perp i l l f"
I0123 12:14:45.812369 140516435906560 graph.py:498] 
I0123 12:14:45.812428 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a c, on_bline m c a; n = on_line n e h, on_circle n h e ? perp i l l f
I0123 12:14:48.476226 140516435906560 ddar.py:60] Depth 1/1000 time = 2.5970404148101807
I0123 12:14:56.417470 140516435906560 ddar.py:60] Depth 2/1000 time = 7.9410011768341064
I0123 12:15:14.675195 140516435906560 ddar.py:60] Depth 3/1000 time = 18.257325887680054
I0123 12:15:33.607850 140516435906560 ddar.py:60] Depth 4/1000 time = 18.932283401489258
I0123 12:15:52.540434 140516435906560 ddar.py:60] Depth 5/1000 time = 18.932310342788696
I0123 12:16:12.669257 140516435906560 ddar.py:60] Depth 6/1000 time = 20.128554105758667
I0123 12:16:31.781994 140516435906560 ddar.py:60] Depth 7/1000 time = 19.11247205734253
I0123 12:16:50.969969 140516435906560 ddar.py:60] Depth 8/1000 time = 19.187291383743286
I0123 12:17:10.249161 140516435906560 ddar.py:60] Depth 9/1000 time = 19.074902772903442
I0123 12:17:29.386874 140516435906560 ddar.py:60] Depth 10/1000 time = 19.13744616508484
I0123 12:17:48.673007 140516435906560 ddar.py:60] Depth 11/1000 time = 19.285917043685913
I0123 12:18:08.153515 140516435906560 ddar.py:60] Depth 12/1000 time = 19.425045251846313
I0123 12:18:08.153757 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:18:08.153824 140516435906560 alphageometry.py:566] LM output (score=-3.002541): "n : C f k n 20 D f n k n 21 ;"
I0123 12:18:08.153859 140516435906560 alphageometry.py:567] Translation: "n = on_line n f k, on_bline n k f"

I0123 12:18:08.153895 140516435906560 alphageometry.py:576] Solving: "a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a c, on_bline m c a; n = on_line n f k, on_bline n k f ? perp i l l f"
I0123 12:18:08.154062 140516435906560 graph.py:498] 
I0123 12:18:08.154120 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a c, on_bline m c a; n = on_line n f k, on_bline n k f ? perp i l l f
I0123 12:18:09.665600 140516435906560 ddar.py:60] Depth 1/1000 time = 1.4459192752838135
I0123 12:18:15.894039 140516435906560 ddar.py:60] Depth 2/1000 time = 6.228194236755371
I0123 12:18:30.914520 140516435906560 ddar.py:60] Depth 3/1000 time = 15.020214796066284
I0123 12:18:42.775850 140516435906560 ddar.py:60] Depth 4/1000 time = 11.860998630523682
I0123 12:18:55.612318 140516435906560 ddar.py:60] Depth 5/1000 time = 12.836189985275269
I0123 12:19:08.530917 140516435906560 ddar.py:60] Depth 6/1000 time = 12.917685747146606
I0123 12:19:20.526227 140516435906560 ddar.py:60] Depth 7/1000 time = 11.83705735206604
I0123 12:19:20.526458 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:19:20.526513 140516435906560 alphageometry.py:566] LM output (score=-3.020802): "n : T a e a n 20 ;"
I0123 12:19:20.526547 140516435906560 alphageometry.py:567] Translation: "n = on_tline n a a e"

I0123 12:19:20.526586 140516435906560 alphageometry.py:576] Solving: "a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a c, on_bline m c a; n = on_tline n a a e ? perp i l l f"
I0123 12:19:20.526748 140516435906560 graph.py:498] 
I0123 12:19:20.526808 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a c, on_bline m c a; n = on_tline n a a e ? perp i l l f
I0123 12:19:23.027784 140516435906560 ddar.py:60] Depth 1/1000 time = 2.444490432739258
I0123 12:19:29.623512 140516435906560 ddar.py:60] Depth 2/1000 time = 6.595519304275513
I0123 12:19:42.457515 140516435906560 ddar.py:60] Depth 3/1000 time = 12.833715200424194
I0123 12:19:55.059223 140516435906560 ddar.py:60] Depth 4/1000 time = 12.601416826248169
I0123 12:20:06.503425 140516435906560 ddar.py:60] Depth 5/1000 time = 11.4438316822052
I0123 12:20:19.093550 140516435906560 ddar.py:60] Depth 6/1000 time = 12.589377164840698
I0123 12:20:32.045467 140516435906560 ddar.py:60] Depth 7/1000 time = 12.799744129180908
I0123 12:20:32.045859 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:20:32.046012 140516435906560 alphageometry.py:549] Decoding from {S} a : ; b : ; c : ; d : C a c d 00 T a c b d 01 ; e : C b c e 02 T a e b c 03 ; f : C a e f 04 C b d f 05 ; g : D a g b g 06 D b g c g 07 ; h : C a b h 08 D a h b h 09 ; i : C c h i 10 D c g g i 11 ; j : D d j e j 12 D e j i j 13 ; k : C g j k 14 T g j i k 15 ; l : C i k l 16 D i k k l 17 ? T i l l f {F1} x00 m : C a e m 18 D a m e m 19 ; x00
I0123 12:20:40.238210 140516435906560 alphageometry.py:566] LM output (score=-0.320038): "n : C b e n 20 D b n e n 21 ;"
I0123 12:20:40.238350 140516435906560 alphageometry.py:567] Translation: "n = on_line n b e, on_bline n e b"

I0123 12:20:40.238393 140516435906560 alphageometry.py:576] Solving: "a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a e, on_bline m e a; n = on_line n b e, on_bline n e b ? perp i l l f"
I0123 12:20:40.238618 140516435906560 graph.py:498] 
I0123 12:20:40.238681 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a e, on_bline m e a; n = on_line n b e, on_bline n e b ? perp i l l f
I0123 12:20:43.171386 140516435906560 ddar.py:60] Depth 1/1000 time = 2.8670520782470703
I0123 12:20:49.525950 140516435906560 ddar.py:60] Depth 2/1000 time = 6.354375600814819
I0123 12:21:05.809485 140516435906560 ddar.py:60] Depth 3/1000 time = 16.28334879875183
I0123 12:21:22.357734 140516435906560 ddar.py:60] Depth 4/1000 time = 16.54797339439392
I0123 12:21:38.997938 140516435906560 ddar.py:60] Depth 5/1000 time = 16.639851570129395
I0123 12:21:56.874711 140516435906560 ddar.py:60] Depth 6/1000 time = 17.87600350379944
I0123 12:22:14.017195 140516435906560 ddar.py:60] Depth 7/1000 time = 16.9870183467865
I0123 12:22:14.032402 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:22:14.032465 140516435906560 alphageometry.py:566] LM output (score=-1.443451): "n : C d e n 20 D d n e n 21 ;"
I0123 12:22:14.032500 140516435906560 alphageometry.py:567] Translation: "n = on_line n d e, on_bline n e d"

I0123 12:22:14.032539 140516435906560 alphageometry.py:576] Solving: "a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a e, on_bline m e a; n = on_line n d e, on_bline n e d ? perp i l l f"
I0123 12:22:14.032701 140516435906560 graph.py:498] 
I0123 12:22:14.032758 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a e, on_bline m e a; n = on_line n d e, on_bline n e d ? perp i l l f
I0123 12:22:15.536796 140516435906560 ddar.py:60] Depth 1/1000 time = 1.4384312629699707
I0123 12:22:21.541980 140516435906560 ddar.py:60] Depth 2/1000 time = 6.0049660205841064
I0123 12:22:39.401726 140516435906560 ddar.py:60] Depth 3/1000 time = 17.859394311904907
I0123 12:22:52.529727 140516435906560 ddar.py:60] Depth 4/1000 time = 13.127695798873901
I0123 12:23:06.775840 140516435906560 ddar.py:60] Depth 5/1000 time = 14.245809316635132
I0123 12:23:21.399092 140516435906560 ddar.py:60] Depth 6/1000 time = 14.622824668884277
I0123 12:23:36.085002 140516435906560 ddar.py:60] Depth 7/1000 time = 14.685555219650269
I0123 12:23:49.491952 140516435906560 ddar.py:60] Depth 8/1000 time = 13.406683683395386
I0123 12:24:04.076579 140516435906560 ddar.py:60] Depth 9/1000 time = 14.584288835525513
I0123 12:24:18.668678 140516435906560 ddar.py:60] Depth 10/1000 time = 14.591181516647339
I0123 12:24:33.314286 140516435906560 ddar.py:60] Depth 11/1000 time = 14.48561978340149
I0123 12:24:47.999547 140516435906560 ddar.py:60] Depth 12/1000 time = 14.614495992660522
I0123 12:24:47.999885 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:24:47.999948 140516435906560 alphageometry.py:566] LM output (score=-1.685022): "n : C e g n 20 D e n g n 21 ;"
I0123 12:24:47.999983 140516435906560 alphageometry.py:567] Translation: "n = on_line n e g, on_bline n g e"

I0123 12:24:48.000030 140516435906560 alphageometry.py:576] Solving: "a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a e, on_bline m e a; n = on_line n e g, on_bline n g e ? perp i l l f"
I0123 12:24:48.000203 140516435906560 graph.py:498] 
I0123 12:24:48.000263 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a e, on_bline m e a; n = on_line n e g, on_bline n g e ? perp i l l f
I0123 12:24:50.587781 140516435906560 ddar.py:60] Depth 1/1000 time = 2.521185874938965
I0123 12:24:56.277998 140516435906560 ddar.py:60] Depth 2/1000 time = 5.6900506019592285
I0123 12:25:06.700132 140516435906560 ddar.py:60] Depth 3/1000 time = 10.421948671340942
I0123 12:25:16.670608 140516435906560 ddar.py:60] Depth 4/1000 time = 9.970227718353271
I0123 12:25:27.897595 140516435906560 ddar.py:60] Depth 5/1000 time = 11.226732969284058
I0123 12:25:37.914844 140516435906560 ddar.py:60] Depth 6/1000 time = 10.016571760177612
I0123 12:25:49.394373 140516435906560 ddar.py:60] Depth 7/1000 time = 11.34199595451355
I0123 12:25:49.394584 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:25:49.394637 140516435906560 alphageometry.py:566] LM output (score=-1.724650): "n : C e h n 20 D e n h n 21 ;"
I0123 12:25:49.394670 140516435906560 alphageometry.py:567] Translation: "n = on_line n e h, on_bline n h e"

I0123 12:25:49.394705 140516435906560 alphageometry.py:576] Solving: "a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a e, on_bline m e a; n = on_line n e h, on_bline n h e ? perp i l l f"
I0123 12:25:49.394864 140516435906560 graph.py:498] 
I0123 12:25:49.394923 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a e, on_bline m e a; n = on_line n e h, on_bline n h e ? perp i l l f
I0123 12:25:50.859439 140516435906560 ddar.py:60] Depth 1/1000 time = 1.3990800380706787
I0123 12:25:56.219796 140516435906560 ddar.py:60] Depth 2/1000 time = 5.360169172286987
I0123 12:26:06.940258 140516435906560 ddar.py:60] Depth 3/1000 time = 10.720216274261475
I0123 12:26:18.602240 140516435906560 ddar.py:60] Depth 4/1000 time = 11.661705493927002
I0123 12:26:29.096669 140516435906560 ddar.py:60] Depth 5/1000 time = 10.494099855422974
I0123 12:26:40.844840 140516435906560 ddar.py:60] Depth 6/1000 time = 11.747519493103027
I0123 12:26:51.270107 140516435906560 ddar.py:60] Depth 7/1000 time = 10.424345970153809
I0123 12:27:02.048285 140516435906560 ddar.py:60] Depth 8/1000 time = 10.64115047454834
I0123 12:27:02.048640 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:27:02.048709 140516435906560 alphageometry.py:566] LM output (score=-1.936619): "n : C e k n 20 D e n k n 21 ;"
I0123 12:27:02.048745 140516435906560 alphageometry.py:567] Translation: "n = on_line n e k, on_bline n k e"

I0123 12:27:02.048793 140516435906560 alphageometry.py:576] Solving: "a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a e, on_bline m e a; n = on_line n e k, on_bline n k e ? perp i l l f"
I0123 12:27:02.048967 140516435906560 graph.py:498] 
I0123 12:27:02.049026 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a e, on_bline m e a; n = on_line n e k, on_bline n k e ? perp i l l f
I0123 12:27:04.656821 140516435906560 ddar.py:60] Depth 1/1000 time = 2.541046380996704
I0123 12:27:10.608192 140516435906560 ddar.py:60] Depth 2/1000 time = 5.95120644569397
I0123 12:27:22.974815 140516435906560 ddar.py:60] Depth 3/1000 time = 12.366349220275879
I0123 12:27:33.131478 140516435906560 ddar.py:60] Depth 4/1000 time = 10.156318187713623
I0123 12:27:44.500812 140516435906560 ddar.py:60] Depth 5/1000 time = 11.369088888168335
I0123 12:27:54.639762 140516435906560 ddar.py:60] Depth 6/1000 time = 10.13825011253357
I0123 12:28:04.898320 140516435906560 ddar.py:60] Depth 7/1000 time = 10.126734972000122
I0123 12:28:04.898545 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:28:04.898595 140516435906560 alphageometry.py:566] LM output (score=-2.025849): "n : C b m n 20 C e h n 21 ;"
I0123 12:28:04.898629 140516435906560 alphageometry.py:567] Translation: "n = on_line n b m, on_line n e h"

I0123 12:28:04.898667 140516435906560 alphageometry.py:576] Solving: "a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a e, on_bline m e a; n = on_line n b m, on_line n e h ? perp i l l f"
I0123 12:28:04.898830 140516435906560 graph.py:498] 
I0123 12:28:04.898890 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a e, on_bline m e a; n = on_line n b m, on_line n e h ? perp i l l f
I0123 12:28:07.532793 140516435906560 ddar.py:60] Depth 1/1000 time = 2.5820610523223877
I0123 12:28:13.229656 140516435906560 ddar.py:60] Depth 2/1000 time = 5.696662187576294
I0123 12:28:24.097228 140516435906560 ddar.py:60] Depth 3/1000 time = 10.86733341217041
I0123 12:28:34.320390 140516435906560 ddar.py:60] Depth 4/1000 time = 10.222883462905884
I0123 12:28:45.856897 140516435906560 ddar.py:60] Depth 5/1000 time = 11.536172151565552
I0123 12:28:56.084446 140516435906560 ddar.py:60] Depth 6/1000 time = 10.22688341140747
I0123 12:29:06.500723 140516435906560 ddar.py:60] Depth 7/1000 time = 10.286983489990234
I0123 12:29:06.500965 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:29:06.501021 140516435906560 alphageometry.py:566] LM output (score=-2.081316): "n : C k l n 20 D k n l n 21 ;"
I0123 12:29:06.501055 140516435906560 alphageometry.py:567] Translation: "n = on_line n k l, on_bline n l k"

I0123 12:29:06.501094 140516435906560 alphageometry.py:576] Solving: "a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a e, on_bline m e a; n = on_line n k l, on_bline n l k ? perp i l l f"
I0123 12:29:06.501283 140516435906560 graph.py:498] 
I0123 12:29:06.501349 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a e, on_bline m e a; n = on_line n k l, on_bline n l k ? perp i l l f
I0123 12:29:09.268010 140516435906560 ddar.py:60] Depth 1/1000 time = 2.6982290744781494
I0123 12:29:15.221489 140516435906560 ddar.py:60] Depth 2/1000 time = 5.953242063522339
I0123 12:29:27.523886 140516435906560 ddar.py:60] Depth 3/1000 time = 12.30208444595337
I0123 12:29:38.997328 140516435906560 ddar.py:60] Depth 4/1000 time = 11.473189115524292
I0123 12:29:49.157470 140516435906560 ddar.py:60] Depth 5/1000 time = 10.159833908081055
I0123 12:30:00.562551 140516435906560 ddar.py:60] Depth 6/1000 time = 11.403352737426758
I0123 12:30:12.210910 140516435906560 ddar.py:60] Depth 7/1000 time = 11.523379802703857
I0123 12:30:12.211261 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:30:12.211342 140516435906560 alphageometry.py:566] LM output (score=-2.167105): "n : C c e n 20 D c n e n 21 ;"
I0123 12:30:12.211377 140516435906560 alphageometry.py:567] Translation: "n = on_line n c e, on_bline n e c"

I0123 12:30:12.211427 140516435906560 alphageometry.py:576] Solving: "a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a e, on_bline m e a; n = on_line n c e, on_bline n e c ? perp i l l f"
I0123 12:30:12.211612 140516435906560 graph.py:498] 
I0123 12:30:12.211669 140516435906560 graph.py:499] a b c = triangle a b c; d = foot d b a c; e = foot e a b c; f = on_line f b d, on_line f a e; g = circle g c b a; h = midpoint h b a; i = on_circle i g c, on_line i h c; j = circle j d e i; k = foot k i g j; l = mirror l i k; m = on_line m a e, on_bline m e a; n = on_line n c e, on_bline n e c ? perp i l l f
I0123 12:30:13.954020 140516435906560 ddar.py:60] Depth 1/1000 time = 1.6765069961547852
I0123 12:30:21.561864 140516435906560 ddar.py:60] Depth 2/1000 time = 7.607649087905884
I0123 12:30:35.202476 140516435906560 ddar.py:60] Depth 3/1000 time = 13.640352010726929
I0123 12:30:48.687232 140516435906560 ddar.py:60] Depth 4/1000 time = 13.484420537948608
I0123 12:31:02.444045 140516435906560 ddar.py:60] Depth 5/1000 time = 13.756580829620361
I0123 12:31:02.444894 140516435906560 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:31:02.444938 140516435906560 alphageometry.py:585] Timeout.
