I0123 22:39:09.793004 140703190876160 inference_utils.py:69] Parsing gin configuration.
I0123 22:39:09.793198 140703190876160 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0123 22:39:09.793461 140703190876160 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0123 22:39:09.793499 140703190876160 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0123 22:39:09.793531 140703190876160 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0123 22:39:09.793561 140703190876160 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0123 22:39:09.793589 140703190876160 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0123 22:39:09.793617 140703190876160 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0123 22:39:09.793667 140703190876160 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0123 22:39:09.793700 140703190876160 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=32
I0123 22:39:09.793730 140703190876160 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0123 22:39:09.793758 140703190876160 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0123 22:39:09.793823 140703190876160 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0123 22:39:09.794034 140703190876160 resource_reader.py:55] Path not found: base_htrans.gin
I0123 22:39:09.794331 140703190876160 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0123 22:39:09.794452 140703190876160 resource_reader.py:55] Path not found: trainer_configuration.gin
I0123 22:39:09.801019 140703190876160 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0123 22:39:09.801165 140703190876160 resource_reader.py:55] Path not found: size/medium_150M.gin
I0123 22:39:09.801497 140703190876160 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0123 22:39:09.801609 140703190876160 resource_reader.py:55] Path not found: options/positions_t5.gin
I0123 22:39:09.801914 140703190876160 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0123 22:39:09.802023 140703190876160 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0123 22:39:09.802436 140703190876160 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0123 22:39:09.802542 140703190876160 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0123 22:39:09.806382 140703190876160 training_loop.py:334] ==== Training loop: initializing model ====
I0123 22:39:09.914245 140703190876160 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0123 22:39:09.915209 140703190876160 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0123 22:39:09.921899 140703190876160 training_loop.py:335] Process 0 of 1
I0123 22:39:09.921964 140703190876160 training_loop.py:336] Local device count = 1
I0123 22:39:09.922009 140703190876160 training_loop.py:337] Number of replicas = 1
I0123 22:39:09.922043 140703190876160 training_loop.py:339] Using random number seed 42
I0123 22:39:10.416667 140703190876160 training_loop.py:359] Initializing the model.
I0123 22:39:10.865236 140703190876160 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:10.865920 140703190876160 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 22:39:10.866040 140703190876160 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 22:39:10.866127 140703190876160 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 22:39:10.866211 140703190876160 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 22:39:10.866307 140703190876160 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 22:39:10.866384 140703190876160 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 22:39:10.866461 140703190876160 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 22:39:10.866535 140703190876160 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 22:39:10.866611 140703190876160 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 22:39:10.866684 140703190876160 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 22:39:10.866757 140703190876160 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 22:39:10.866830 140703190876160 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 22:39:10.866903 140703190876160 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 22:39:10.866945 140703190876160 attention.py:418] Single window, no scan.
I0123 22:39:10.866994 140703190876160 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 22:39:10.867118 140703190876160 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:39:10.867163 140703190876160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:39:10.867198 140703190876160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:39:10.869272 140703190876160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:10.874967 140703190876160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:39:10.886035 140703190876160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:10.886350 140703190876160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:39:10.890889 140703190876160 transformer_layer.py:213] tlayer: windowed attention.
I0123 22:39:10.902212 140703190876160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:39:10.902282 140703190876160 attention.py:418] Single window, no scan.
I0123 22:39:10.902324 140703190876160 transformer_layer.py:389] tlayer: self-attention.
I0123 22:39:10.902360 140703190876160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:10.902429 140703190876160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:10.903757 140703190876160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:10.903840 140703190876160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:10.904567 140703190876160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:10.907107 140703190876160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:10.913039 140703190876160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:10.914819 140703190876160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:10.914913 140703190876160 transformer_layer.py:468] tlayer: End windows.
I0123 22:39:10.914952 140703190876160 transformer_layer.py:472] tlayer: final FFN.
I0123 22:39:10.915019 140703190876160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:10.915157 140703190876160 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:39:10.915509 140703190876160 nn_components.py:325] mlp: activation = None
I0123 22:39:10.915565 140703190876160 nn_components.py:329] mlp: final activation = None
I0123 22:39:10.917501 140703190876160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:10.917610 140703190876160 nn_components.py:261] mlp: residual
I0123 22:39:10.920605 140703190876160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:10.920692 140703190876160 transformer_base.py:443] tbase: final FFN
I0123 22:39:10.921196 140703190876160 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:39:10.931619 140703190876160 nn_components.py:329] mlp: final activation = None
I0123 22:39:10.940470 140703190876160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:10.940576 140703190876160 nn_components.py:261] mlp: residual
I0123 22:39:10.940883 140703190876160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:10.940969 140703190876160 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 22:39:10.941082 140703190876160 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:39:10.941124 140703190876160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:39:10.941157 140703190876160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:39:10.943029 140703190876160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:10.945513 140703190876160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:39:10.951217 140703190876160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:10.951487 140703190876160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:39:10.954158 140703190876160 transformer_layer.py:213] tlayer: windowed attention.
I0123 22:39:10.957989 140703190876160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:39:10.958051 140703190876160 attention.py:418] Single window, no scan.
I0123 22:39:10.958090 140703190876160 transformer_layer.py:389] tlayer: self-attention.
I0123 22:39:10.958123 140703190876160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:10.958185 140703190876160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:10.958753 140703190876160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:10.958833 140703190876160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:10.959208 140703190876160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:10.959999 140703190876160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:10.962727 140703190876160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:10.963356 140703190876160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:10.963441 140703190876160 transformer_layer.py:468] tlayer: End windows.
I0123 22:39:10.963479 140703190876160 transformer_layer.py:472] tlayer: final FFN.
I0123 22:39:10.963541 140703190876160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:10.963672 140703190876160 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:39:10.964001 140703190876160 nn_components.py:325] mlp: activation = None
I0123 22:39:10.964049 140703190876160 nn_components.py:329] mlp: final activation = None
I0123 22:39:10.966016 140703190876160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:10.966117 140703190876160 nn_components.py:261] mlp: residual
I0123 22:39:10.968671 140703190876160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:10.968759 140703190876160 transformer_base.py:443] tbase: final FFN
I0123 22:39:10.969193 140703190876160 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:39:10.971575 140703190876160 nn_components.py:329] mlp: final activation = None
I0123 22:39:10.973493 140703190876160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:10.973593 140703190876160 nn_components.py:261] mlp: residual
I0123 22:39:10.973903 140703190876160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:10.973990 140703190876160 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 22:39:10.974104 140703190876160 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:39:10.974148 140703190876160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:39:10.974182 140703190876160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:39:10.976093 140703190876160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:10.978483 140703190876160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:39:10.984456 140703190876160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:10.984725 140703190876160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:39:10.987409 140703190876160 transformer_layer.py:213] tlayer: windowed attention.
I0123 22:39:10.991260 140703190876160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:39:10.991321 140703190876160 attention.py:418] Single window, no scan.
I0123 22:39:10.991360 140703190876160 transformer_layer.py:389] tlayer: self-attention.
I0123 22:39:10.991393 140703190876160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:10.991457 140703190876160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:10.992024 140703190876160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:10.992104 140703190876160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:10.992470 140703190876160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:10.993237 140703190876160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:10.995773 140703190876160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:10.996457 140703190876160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:10.996538 140703190876160 transformer_layer.py:468] tlayer: End windows.
I0123 22:39:10.996576 140703190876160 transformer_layer.py:472] tlayer: final FFN.
I0123 22:39:10.996639 140703190876160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:10.996773 140703190876160 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:39:10.997109 140703190876160 nn_components.py:325] mlp: activation = None
I0123 22:39:10.997158 140703190876160 nn_components.py:329] mlp: final activation = None
I0123 22:39:10.999095 140703190876160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:10.999194 140703190876160 nn_components.py:261] mlp: residual
I0123 22:39:11.001749 140703190876160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.002102 140703190876160 transformer_base.py:443] tbase: final FFN
I0123 22:39:11.002596 140703190876160 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:39:11.004897 140703190876160 nn_components.py:329] mlp: final activation = None
I0123 22:39:11.006837 140703190876160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.006936 140703190876160 nn_components.py:261] mlp: residual
I0123 22:39:11.007237 140703190876160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.007322 140703190876160 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 22:39:11.007436 140703190876160 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:39:11.007481 140703190876160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:39:11.007514 140703190876160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:39:11.009438 140703190876160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.011892 140703190876160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:39:11.017770 140703190876160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.018038 140703190876160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:39:11.020727 140703190876160 transformer_layer.py:213] tlayer: windowed attention.
I0123 22:39:11.024513 140703190876160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:39:11.024574 140703190876160 attention.py:418] Single window, no scan.
I0123 22:39:11.024612 140703190876160 transformer_layer.py:389] tlayer: self-attention.
I0123 22:39:11.024645 140703190876160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.024708 140703190876160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.025273 140703190876160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.025352 140703190876160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.025746 140703190876160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.026543 140703190876160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.029134 140703190876160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.029767 140703190876160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.029850 140703190876160 transformer_layer.py:468] tlayer: End windows.
I0123 22:39:11.029887 140703190876160 transformer_layer.py:472] tlayer: final FFN.
I0123 22:39:11.029950 140703190876160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.030081 140703190876160 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:39:11.030417 140703190876160 nn_components.py:325] mlp: activation = None
I0123 22:39:11.030465 140703190876160 nn_components.py:329] mlp: final activation = None
I0123 22:39:11.032372 140703190876160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.032469 140703190876160 nn_components.py:261] mlp: residual
I0123 22:39:11.035066 140703190876160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.035161 140703190876160 transformer_base.py:443] tbase: final FFN
I0123 22:39:11.035597 140703190876160 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:39:11.037876 140703190876160 nn_components.py:329] mlp: final activation = None
I0123 22:39:11.039811 140703190876160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.039910 140703190876160 nn_components.py:261] mlp: residual
I0123 22:39:11.040216 140703190876160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.040301 140703190876160 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 22:39:11.040411 140703190876160 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:39:11.040455 140703190876160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:39:11.040488 140703190876160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:39:11.042413 140703190876160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.044838 140703190876160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:39:11.050510 140703190876160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.050779 140703190876160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:39:11.053478 140703190876160 transformer_layer.py:213] tlayer: windowed attention.
I0123 22:39:11.057195 140703190876160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:39:11.057255 140703190876160 attention.py:418] Single window, no scan.
I0123 22:39:11.057294 140703190876160 transformer_layer.py:389] tlayer: self-attention.
I0123 22:39:11.057327 140703190876160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.057391 140703190876160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.057971 140703190876160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.058054 140703190876160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.058423 140703190876160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.059200 140703190876160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.062088 140703190876160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.062716 140703190876160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.062797 140703190876160 transformer_layer.py:468] tlayer: End windows.
I0123 22:39:11.062834 140703190876160 transformer_layer.py:472] tlayer: final FFN.
I0123 22:39:11.062897 140703190876160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.063039 140703190876160 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:39:11.063363 140703190876160 nn_components.py:325] mlp: activation = None
I0123 22:39:11.063411 140703190876160 nn_components.py:329] mlp: final activation = None
I0123 22:39:11.065323 140703190876160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.065421 140703190876160 nn_components.py:261] mlp: residual
I0123 22:39:11.068001 140703190876160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.068085 140703190876160 transformer_base.py:443] tbase: final FFN
I0123 22:39:11.068523 140703190876160 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:39:11.070818 140703190876160 nn_components.py:329] mlp: final activation = None
I0123 22:39:11.072788 140703190876160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.072888 140703190876160 nn_components.py:261] mlp: residual
I0123 22:39:11.073190 140703190876160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.073276 140703190876160 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 22:39:11.073390 140703190876160 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:39:11.073434 140703190876160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:39:11.073467 140703190876160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:39:11.075338 140703190876160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.077749 140703190876160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:39:11.083409 140703190876160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.083673 140703190876160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:39:11.086401 140703190876160 transformer_layer.py:213] tlayer: windowed attention.
I0123 22:39:11.090127 140703190876160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:39:11.090190 140703190876160 attention.py:418] Single window, no scan.
I0123 22:39:11.090229 140703190876160 transformer_layer.py:389] tlayer: self-attention.
I0123 22:39:11.090263 140703190876160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.090329 140703190876160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.090938 140703190876160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.091018 140703190876160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.091389 140703190876160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.092173 140703190876160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.094685 140703190876160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.095318 140703190876160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.095399 140703190876160 transformer_layer.py:468] tlayer: End windows.
I0123 22:39:11.095436 140703190876160 transformer_layer.py:472] tlayer: final FFN.
I0123 22:39:11.095496 140703190876160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.095630 140703190876160 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:39:11.095955 140703190876160 nn_components.py:325] mlp: activation = None
I0123 22:39:11.096003 140703190876160 nn_components.py:329] mlp: final activation = None
I0123 22:39:11.097910 140703190876160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.098009 140703190876160 nn_components.py:261] mlp: residual
I0123 22:39:11.100573 140703190876160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.100656 140703190876160 transformer_base.py:443] tbase: final FFN
I0123 22:39:11.101093 140703190876160 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:39:11.103427 140703190876160 nn_components.py:329] mlp: final activation = None
I0123 22:39:11.105345 140703190876160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.105444 140703190876160 nn_components.py:261] mlp: residual
I0123 22:39:11.105751 140703190876160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.105836 140703190876160 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 22:39:11.105948 140703190876160 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:39:11.105989 140703190876160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:39:11.106022 140703190876160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:39:11.107872 140703190876160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.110341 140703190876160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:39:11.116191 140703190876160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.116461 140703190876160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:39:11.119110 140703190876160 transformer_layer.py:213] tlayer: windowed attention.
I0123 22:39:11.122887 140703190876160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:39:11.122946 140703190876160 attention.py:418] Single window, no scan.
I0123 22:39:11.122986 140703190876160 transformer_layer.py:389] tlayer: self-attention.
I0123 22:39:11.123018 140703190876160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.123081 140703190876160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.123647 140703190876160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.123727 140703190876160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.124095 140703190876160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.124873 140703190876160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.127407 140703190876160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.128039 140703190876160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.128121 140703190876160 transformer_layer.py:468] tlayer: End windows.
I0123 22:39:11.128158 140703190876160 transformer_layer.py:472] tlayer: final FFN.
I0123 22:39:11.128219 140703190876160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.128354 140703190876160 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:39:11.128680 140703190876160 nn_components.py:325] mlp: activation = None
I0123 22:39:11.128728 140703190876160 nn_components.py:329] mlp: final activation = None
I0123 22:39:11.130732 140703190876160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.130830 140703190876160 nn_components.py:261] mlp: residual
I0123 22:39:11.133340 140703190876160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.133427 140703190876160 transformer_base.py:443] tbase: final FFN
I0123 22:39:11.133872 140703190876160 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:39:11.136564 140703190876160 nn_components.py:329] mlp: final activation = None
I0123 22:39:11.138487 140703190876160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.138619 140703190876160 nn_components.py:261] mlp: residual
I0123 22:39:11.138923 140703190876160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.139009 140703190876160 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 22:39:11.139123 140703190876160 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:39:11.139167 140703190876160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:39:11.139201 140703190876160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:39:11.282840 140703190876160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.286197 140703190876160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:39:11.292171 140703190876160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.292488 140703190876160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:39:11.295252 140703190876160 transformer_layer.py:213] tlayer: windowed attention.
I0123 22:39:11.299243 140703190876160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:39:11.299307 140703190876160 attention.py:418] Single window, no scan.
I0123 22:39:11.299347 140703190876160 transformer_layer.py:389] tlayer: self-attention.
I0123 22:39:11.299383 140703190876160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.299451 140703190876160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.300082 140703190876160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.300164 140703190876160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.300536 140703190876160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.301339 140703190876160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.303959 140703190876160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.304607 140703190876160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.304690 140703190876160 transformer_layer.py:468] tlayer: End windows.
I0123 22:39:11.304728 140703190876160 transformer_layer.py:472] tlayer: final FFN.
I0123 22:39:11.304792 140703190876160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.304928 140703190876160 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:39:11.305274 140703190876160 nn_components.py:325] mlp: activation = None
I0123 22:39:11.305322 140703190876160 nn_components.py:329] mlp: final activation = None
I0123 22:39:11.307269 140703190876160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.307373 140703190876160 nn_components.py:261] mlp: residual
I0123 22:39:11.309980 140703190876160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.310066 140703190876160 transformer_base.py:443] tbase: final FFN
I0123 22:39:11.310516 140703190876160 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:39:11.312864 140703190876160 nn_components.py:329] mlp: final activation = None
I0123 22:39:11.314805 140703190876160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.314932 140703190876160 nn_components.py:261] mlp: residual
I0123 22:39:11.315232 140703190876160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.315318 140703190876160 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 22:39:11.315432 140703190876160 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:39:11.315475 140703190876160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:39:11.315509 140703190876160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:39:11.317460 140703190876160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.319885 140703190876160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:39:11.325584 140703190876160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.325864 140703190876160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:39:11.328581 140703190876160 transformer_layer.py:213] tlayer: windowed attention.
I0123 22:39:11.332387 140703190876160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:39:11.332448 140703190876160 attention.py:418] Single window, no scan.
I0123 22:39:11.332487 140703190876160 transformer_layer.py:389] tlayer: self-attention.
I0123 22:39:11.332520 140703190876160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.332584 140703190876160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.333152 140703190876160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.333232 140703190876160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.333593 140703190876160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.334378 140703190876160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.336971 140703190876160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.337596 140703190876160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.337687 140703190876160 transformer_layer.py:468] tlayer: End windows.
I0123 22:39:11.337725 140703190876160 transformer_layer.py:472] tlayer: final FFN.
I0123 22:39:11.337787 140703190876160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.337919 140703190876160 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:39:11.338245 140703190876160 nn_components.py:325] mlp: activation = None
I0123 22:39:11.338293 140703190876160 nn_components.py:329] mlp: final activation = None
I0123 22:39:11.340217 140703190876160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.340316 140703190876160 nn_components.py:261] mlp: residual
I0123 22:39:11.342916 140703190876160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.343004 140703190876160 transformer_base.py:443] tbase: final FFN
I0123 22:39:11.343437 140703190876160 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:39:11.345742 140703190876160 nn_components.py:329] mlp: final activation = None
I0123 22:39:11.347909 140703190876160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.348009 140703190876160 nn_components.py:261] mlp: residual
I0123 22:39:11.348304 140703190876160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.348400 140703190876160 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 22:39:11.348517 140703190876160 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:39:11.348563 140703190876160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:39:11.348597 140703190876160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:39:11.350466 140703190876160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.352931 140703190876160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:39:11.358651 140703190876160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.358922 140703190876160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:39:11.362003 140703190876160 transformer_layer.py:213] tlayer: windowed attention.
I0123 22:39:11.365766 140703190876160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:39:11.365826 140703190876160 attention.py:418] Single window, no scan.
I0123 22:39:11.365866 140703190876160 transformer_layer.py:389] tlayer: self-attention.
I0123 22:39:11.365901 140703190876160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.365965 140703190876160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.366568 140703190876160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.366649 140703190876160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.367021 140703190876160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.367809 140703190876160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.370320 140703190876160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.370955 140703190876160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.371037 140703190876160 transformer_layer.py:468] tlayer: End windows.
I0123 22:39:11.371074 140703190876160 transformer_layer.py:472] tlayer: final FFN.
I0123 22:39:11.371137 140703190876160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.371275 140703190876160 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:39:11.371601 140703190876160 nn_components.py:325] mlp: activation = None
I0123 22:39:11.371649 140703190876160 nn_components.py:329] mlp: final activation = None
I0123 22:39:11.373554 140703190876160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.373660 140703190876160 nn_components.py:261] mlp: residual
I0123 22:39:11.376234 140703190876160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.376321 140703190876160 transformer_base.py:443] tbase: final FFN
I0123 22:39:11.376757 140703190876160 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:39:11.379102 140703190876160 nn_components.py:329] mlp: final activation = None
I0123 22:39:11.381028 140703190876160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.381129 140703190876160 nn_components.py:261] mlp: residual
I0123 22:39:11.381427 140703190876160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.381524 140703190876160 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 22:39:11.381647 140703190876160 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:39:11.381693 140703190876160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:39:11.381728 140703190876160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:39:11.383573 140703190876160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.386057 140703190876160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:39:11.391759 140703190876160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.392034 140703190876160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:39:11.394720 140703190876160 transformer_layer.py:213] tlayer: windowed attention.
I0123 22:39:11.398535 140703190876160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:39:11.398596 140703190876160 attention.py:418] Single window, no scan.
I0123 22:39:11.398636 140703190876160 transformer_layer.py:389] tlayer: self-attention.
I0123 22:39:11.398670 140703190876160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.398734 140703190876160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.399308 140703190876160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.399391 140703190876160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.399765 140703190876160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.400547 140703190876160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.403100 140703190876160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.403738 140703190876160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.403821 140703190876160 transformer_layer.py:468] tlayer: End windows.
I0123 22:39:11.403859 140703190876160 transformer_layer.py:472] tlayer: final FFN.
I0123 22:39:11.403921 140703190876160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.404056 140703190876160 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:39:11.404389 140703190876160 nn_components.py:325] mlp: activation = None
I0123 22:39:11.404438 140703190876160 nn_components.py:329] mlp: final activation = None
I0123 22:39:11.406426 140703190876160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.406526 140703190876160 nn_components.py:261] mlp: residual
I0123 22:39:11.409350 140703190876160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.409436 140703190876160 transformer_base.py:443] tbase: final FFN
I0123 22:39:11.409883 140703190876160 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:39:11.412252 140703190876160 nn_components.py:329] mlp: final activation = None
I0123 22:39:11.414211 140703190876160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.414312 140703190876160 nn_components.py:261] mlp: residual
I0123 22:39:11.414613 140703190876160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.414702 140703190876160 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 22:39:11.414826 140703190876160 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:39:11.414871 140703190876160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:39:11.414906 140703190876160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:39:11.416832 140703190876160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.419257 140703190876160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:39:11.424935 140703190876160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.425203 140703190876160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:39:11.427885 140703190876160 transformer_layer.py:213] tlayer: windowed attention.
I0123 22:39:11.431718 140703190876160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:39:11.431780 140703190876160 attention.py:418] Single window, no scan.
I0123 22:39:11.431819 140703190876160 transformer_layer.py:389] tlayer: self-attention.
I0123 22:39:11.431854 140703190876160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.431919 140703190876160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.432496 140703190876160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.432579 140703190876160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.432952 140703190876160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.433751 140703190876160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.436294 140703190876160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.437283 140703190876160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.437368 140703190876160 transformer_layer.py:468] tlayer: End windows.
I0123 22:39:11.437407 140703190876160 transformer_layer.py:472] tlayer: final FFN.
I0123 22:39:11.437474 140703190876160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.437610 140703190876160 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:39:11.437944 140703190876160 nn_components.py:325] mlp: activation = None
I0123 22:39:11.437993 140703190876160 nn_components.py:329] mlp: final activation = None
I0123 22:39:11.439916 140703190876160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.440015 140703190876160 nn_components.py:261] mlp: residual
I0123 22:39:11.442538 140703190876160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.442624 140703190876160 transformer_base.py:443] tbase: final FFN
I0123 22:39:11.443119 140703190876160 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:39:11.445414 140703190876160 nn_components.py:329] mlp: final activation = None
I0123 22:39:11.447371 140703190876160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.447474 140703190876160 nn_components.py:261] mlp: residual
I0123 22:39:11.447777 140703190876160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.448066 140703190876160 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 22:39:11.448142 140703190876160 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 22:39:11.448213 140703190876160 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 22:39:11.448274 140703190876160 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 22:39:11.448333 140703190876160 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 22:39:11.448391 140703190876160 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 22:39:11.448448 140703190876160 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 22:39:11.448504 140703190876160 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 22:39:11.448560 140703190876160 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 22:39:11.448616 140703190876160 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 22:39:11.448671 140703190876160 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 22:39:11.448726 140703190876160 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 22:39:11.448766 140703190876160 decoder_stack.py:344] dstack: Final layernorm.
I0123 22:39:11.452329 140703190876160 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:39:11.500928 140703190876160 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.501021 140703190876160 decoder_stack.py:333] dstack: autoregressive generator.
I0123 22:39:11.501079 140703190876160 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 22:39:11.501186 140703190876160 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:39:11.501229 140703190876160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:39:11.501263 140703190876160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:39:11.501336 140703190876160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.503839 140703190876160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:39:11.509417 140703190876160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.509695 140703190876160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:39:11.512399 140703190876160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:39:11.529258 140703190876160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:39:11.529322 140703190876160 attention.py:418] Single window, no scan.
I0123 22:39:11.529361 140703190876160 transformer_layer.py:389] tlayer: self-attention.
I0123 22:39:11.529396 140703190876160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.529461 140703190876160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.530621 140703190876160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.530705 140703190876160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.531427 140703190876160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.533465 140703190876160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.538343 140703190876160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.539663 140703190876160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.539755 140703190876160 transformer_layer.py:468] tlayer: End windows.
I0123 22:39:11.539796 140703190876160 transformer_layer.py:472] tlayer: final FFN.
I0123 22:39:11.539860 140703190876160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.539999 140703190876160 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:39:11.540120 140703190876160 nn_components.py:325] mlp: activation = None
I0123 22:39:11.540165 140703190876160 nn_components.py:329] mlp: final activation = None
I0123 22:39:11.542125 140703190876160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.542226 140703190876160 nn_components.py:261] mlp: residual
I0123 22:39:11.544717 140703190876160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.544806 140703190876160 transformer_base.py:443] tbase: final FFN
I0123 22:39:11.544921 140703190876160 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:39:11.547222 140703190876160 nn_components.py:329] mlp: final activation = None
I0123 22:39:11.549240 140703190876160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.549343 140703190876160 nn_components.py:261] mlp: residual
I0123 22:39:11.549653 140703190876160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.549742 140703190876160 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 22:39:11.549857 140703190876160 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:39:11.549899 140703190876160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:39:11.549934 140703190876160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:39:11.550003 140703190876160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.552323 140703190876160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:39:11.558035 140703190876160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.558308 140703190876160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:39:11.561399 140703190876160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:39:11.574657 140703190876160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:39:11.574720 140703190876160 attention.py:418] Single window, no scan.
I0123 22:39:11.574760 140703190876160 transformer_layer.py:389] tlayer: self-attention.
I0123 22:39:11.574794 140703190876160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.574858 140703190876160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.575429 140703190876160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.575512 140703190876160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.575876 140703190876160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.576581 140703190876160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.579125 140703190876160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.579751 140703190876160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.579833 140703190876160 transformer_layer.py:468] tlayer: End windows.
I0123 22:39:11.579878 140703190876160 transformer_layer.py:472] tlayer: final FFN.
I0123 22:39:11.579942 140703190876160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.580079 140703190876160 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:39:11.580194 140703190876160 nn_components.py:325] mlp: activation = None
I0123 22:39:11.580238 140703190876160 nn_components.py:329] mlp: final activation = None
I0123 22:39:11.582207 140703190876160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.582308 140703190876160 nn_components.py:261] mlp: residual
I0123 22:39:11.584741 140703190876160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.584825 140703190876160 transformer_base.py:443] tbase: final FFN
I0123 22:39:11.584938 140703190876160 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:39:11.587182 140703190876160 nn_components.py:329] mlp: final activation = None
I0123 22:39:11.589143 140703190876160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.589245 140703190876160 nn_components.py:261] mlp: residual
I0123 22:39:11.589541 140703190876160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.589627 140703190876160 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 22:39:11.589749 140703190876160 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:39:11.589792 140703190876160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:39:11.589826 140703190876160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:39:11.589893 140703190876160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.592167 140703190876160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:39:11.597676 140703190876160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.597944 140703190876160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:39:11.600651 140703190876160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:39:11.613359 140703190876160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:39:11.613422 140703190876160 attention.py:418] Single window, no scan.
I0123 22:39:11.613462 140703190876160 transformer_layer.py:389] tlayer: self-attention.
I0123 22:39:11.613497 140703190876160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.613562 140703190876160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.614134 140703190876160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.614215 140703190876160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.614584 140703190876160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.615296 140703190876160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.617875 140703190876160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.618512 140703190876160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.618595 140703190876160 transformer_layer.py:468] tlayer: End windows.
I0123 22:39:11.618633 140703190876160 transformer_layer.py:472] tlayer: final FFN.
I0123 22:39:11.618707 140703190876160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.618842 140703190876160 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:39:11.618957 140703190876160 nn_components.py:325] mlp: activation = None
I0123 22:39:11.619001 140703190876160 nn_components.py:329] mlp: final activation = None
I0123 22:39:11.620963 140703190876160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.621061 140703190876160 nn_components.py:261] mlp: residual
I0123 22:39:11.623530 140703190876160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.623613 140703190876160 transformer_base.py:443] tbase: final FFN
I0123 22:39:11.623723 140703190876160 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:39:11.625982 140703190876160 nn_components.py:329] mlp: final activation = None
I0123 22:39:11.627923 140703190876160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.628024 140703190876160 nn_components.py:261] mlp: residual
I0123 22:39:11.628316 140703190876160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.628403 140703190876160 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 22:39:11.628516 140703190876160 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:39:11.628560 140703190876160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:39:11.628594 140703190876160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:39:11.628662 140703190876160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.630930 140703190876160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:39:11.636458 140703190876160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.636723 140703190876160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:39:11.639436 140703190876160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:39:11.652112 140703190876160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:39:11.652173 140703190876160 attention.py:418] Single window, no scan.
I0123 22:39:11.652213 140703190876160 transformer_layer.py:389] tlayer: self-attention.
I0123 22:39:11.652248 140703190876160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.652312 140703190876160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.652874 140703190876160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.652956 140703190876160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.653319 140703190876160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.654039 140703190876160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.656565 140703190876160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.657198 140703190876160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.657281 140703190876160 transformer_layer.py:468] tlayer: End windows.
I0123 22:39:11.657320 140703190876160 transformer_layer.py:472] tlayer: final FFN.
I0123 22:39:11.657383 140703190876160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.657527 140703190876160 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:39:11.657650 140703190876160 nn_components.py:325] mlp: activation = None
I0123 22:39:11.657697 140703190876160 nn_components.py:329] mlp: final activation = None
I0123 22:39:11.659821 140703190876160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.660076 140703190876160 nn_components.py:261] mlp: residual
I0123 22:39:11.662504 140703190876160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.662588 140703190876160 transformer_base.py:443] tbase: final FFN
I0123 22:39:11.662703 140703190876160 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:39:11.664952 140703190876160 nn_components.py:329] mlp: final activation = None
I0123 22:39:11.666843 140703190876160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.666944 140703190876160 nn_components.py:261] mlp: residual
I0123 22:39:11.667236 140703190876160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.667321 140703190876160 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 22:39:11.667433 140703190876160 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:39:11.667477 140703190876160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:39:11.667510 140703190876160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:39:11.667578 140703190876160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.670189 140703190876160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:39:11.675709 140703190876160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.675978 140703190876160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:39:11.678612 140703190876160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:39:11.691364 140703190876160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:39:11.691426 140703190876160 attention.py:418] Single window, no scan.
I0123 22:39:11.691466 140703190876160 transformer_layer.py:389] tlayer: self-attention.
I0123 22:39:11.691500 140703190876160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.691566 140703190876160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.692131 140703190876160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.692212 140703190876160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.692575 140703190876160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.693282 140703190876160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.695841 140703190876160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.696472 140703190876160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.696554 140703190876160 transformer_layer.py:468] tlayer: End windows.
I0123 22:39:11.696591 140703190876160 transformer_layer.py:472] tlayer: final FFN.
I0123 22:39:11.696651 140703190876160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.696797 140703190876160 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:39:11.696911 140703190876160 nn_components.py:325] mlp: activation = None
I0123 22:39:11.696955 140703190876160 nn_components.py:329] mlp: final activation = None
I0123 22:39:11.698883 140703190876160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.698983 140703190876160 nn_components.py:261] mlp: residual
I0123 22:39:11.701414 140703190876160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.701498 140703190876160 transformer_base.py:443] tbase: final FFN
I0123 22:39:11.701607 140703190876160 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:39:11.703920 140703190876160 nn_components.py:329] mlp: final activation = None
I0123 22:39:11.705810 140703190876160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.705911 140703190876160 nn_components.py:261] mlp: residual
I0123 22:39:11.706207 140703190876160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.706293 140703190876160 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 22:39:11.706406 140703190876160 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:39:11.706450 140703190876160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:39:11.706484 140703190876160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:39:11.706551 140703190876160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.708817 140703190876160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:39:11.714310 140703190876160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.714571 140703190876160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:39:11.717282 140703190876160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:39:11.729998 140703190876160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:39:11.730059 140703190876160 attention.py:418] Single window, no scan.
I0123 22:39:11.730099 140703190876160 transformer_layer.py:389] tlayer: self-attention.
I0123 22:39:11.730132 140703190876160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.730197 140703190876160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.730768 140703190876160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.730849 140703190876160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.731213 140703190876160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.731920 140703190876160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.734438 140703190876160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.735068 140703190876160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.735152 140703190876160 transformer_layer.py:468] tlayer: End windows.
I0123 22:39:11.735189 140703190876160 transformer_layer.py:472] tlayer: final FFN.
I0123 22:39:11.735251 140703190876160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.735388 140703190876160 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:39:11.735510 140703190876160 nn_components.py:325] mlp: activation = None
I0123 22:39:11.735556 140703190876160 nn_components.py:329] mlp: final activation = None
I0123 22:39:11.737524 140703190876160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.737622 140703190876160 nn_components.py:261] mlp: residual
I0123 22:39:11.740044 140703190876160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.740127 140703190876160 transformer_base.py:443] tbase: final FFN
I0123 22:39:11.740238 140703190876160 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:39:11.742484 140703190876160 nn_components.py:329] mlp: final activation = None
I0123 22:39:11.744361 140703190876160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.744462 140703190876160 nn_components.py:261] mlp: residual
I0123 22:39:11.744753 140703190876160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.744840 140703190876160 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 22:39:11.744952 140703190876160 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:39:11.744997 140703190876160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:39:11.745032 140703190876160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:39:11.745098 140703190876160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.747364 140703190876160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:39:11.752954 140703190876160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.753223 140703190876160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:39:11.755870 140703190876160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:39:11.768637 140703190876160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:39:11.768698 140703190876160 attention.py:418] Single window, no scan.
I0123 22:39:11.768737 140703190876160 transformer_layer.py:389] tlayer: self-attention.
I0123 22:39:11.768771 140703190876160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.768834 140703190876160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.769426 140703190876160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.769508 140703190876160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.769885 140703190876160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.770583 140703190876160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.773269 140703190876160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.774271 140703190876160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.774356 140703190876160 transformer_layer.py:468] tlayer: End windows.
I0123 22:39:11.774393 140703190876160 transformer_layer.py:472] tlayer: final FFN.
I0123 22:39:11.774455 140703190876160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.774587 140703190876160 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:39:11.774702 140703190876160 nn_components.py:325] mlp: activation = None
I0123 22:39:11.774752 140703190876160 nn_components.py:329] mlp: final activation = None
I0123 22:39:11.776670 140703190876160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.776769 140703190876160 nn_components.py:261] mlp: residual
I0123 22:39:11.779197 140703190876160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.779284 140703190876160 transformer_base.py:443] tbase: final FFN
I0123 22:39:11.779395 140703190876160 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:39:11.781647 140703190876160 nn_components.py:329] mlp: final activation = None
I0123 22:39:11.783584 140703190876160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.783684 140703190876160 nn_components.py:261] mlp: residual
I0123 22:39:11.783977 140703190876160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.784063 140703190876160 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 22:39:11.784174 140703190876160 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:39:11.784218 140703190876160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:39:11.784252 140703190876160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:39:11.784317 140703190876160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.786576 140703190876160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:39:11.792047 140703190876160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.792322 140703190876160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:39:11.795007 140703190876160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:39:11.807608 140703190876160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:39:11.807670 140703190876160 attention.py:418] Single window, no scan.
I0123 22:39:11.807709 140703190876160 transformer_layer.py:389] tlayer: self-attention.
I0123 22:39:11.807742 140703190876160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.807806 140703190876160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.808413 140703190876160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.808494 140703190876160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.808862 140703190876160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.809558 140703190876160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.812068 140703190876160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.812700 140703190876160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.812782 140703190876160 transformer_layer.py:468] tlayer: End windows.
I0123 22:39:11.812820 140703190876160 transformer_layer.py:472] tlayer: final FFN.
I0123 22:39:11.812882 140703190876160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.813020 140703190876160 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:39:11.813135 140703190876160 nn_components.py:325] mlp: activation = None
I0123 22:39:11.813185 140703190876160 nn_components.py:329] mlp: final activation = None
I0123 22:39:11.815096 140703190876160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.815195 140703190876160 nn_components.py:261] mlp: residual
I0123 22:39:11.817666 140703190876160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.817751 140703190876160 transformer_base.py:443] tbase: final FFN
I0123 22:39:11.817863 140703190876160 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:39:11.820083 140703190876160 nn_components.py:329] mlp: final activation = None
I0123 22:39:11.821969 140703190876160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.822070 140703190876160 nn_components.py:261] mlp: residual
I0123 22:39:11.822362 140703190876160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.822449 140703190876160 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 22:39:11.822561 140703190876160 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:39:11.822604 140703190876160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:39:11.822638 140703190876160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:39:11.822705 140703190876160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.824946 140703190876160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:39:11.830466 140703190876160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.830730 140703190876160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:39:11.833375 140703190876160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:39:11.846095 140703190876160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:39:11.846157 140703190876160 attention.py:418] Single window, no scan.
I0123 22:39:11.846197 140703190876160 transformer_layer.py:389] tlayer: self-attention.
I0123 22:39:11.846231 140703190876160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.846296 140703190876160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.846866 140703190876160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.846946 140703190876160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.847314 140703190876160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.848021 140703190876160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.850543 140703190876160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.851223 140703190876160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.851305 140703190876160 transformer_layer.py:468] tlayer: End windows.
I0123 22:39:11.851343 140703190876160 transformer_layer.py:472] tlayer: final FFN.
I0123 22:39:11.851408 140703190876160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.851543 140703190876160 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:39:11.851657 140703190876160 nn_components.py:325] mlp: activation = None
I0123 22:39:11.851700 140703190876160 nn_components.py:329] mlp: final activation = None
I0123 22:39:11.853597 140703190876160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.853704 140703190876160 nn_components.py:261] mlp: residual
I0123 22:39:11.856093 140703190876160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.856176 140703190876160 transformer_base.py:443] tbase: final FFN
I0123 22:39:11.856286 140703190876160 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:39:11.858530 140703190876160 nn_components.py:329] mlp: final activation = None
I0123 22:39:11.860522 140703190876160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.860622 140703190876160 nn_components.py:261] mlp: residual
I0123 22:39:11.860915 140703190876160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.861002 140703190876160 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 22:39:11.861114 140703190876160 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:39:11.861157 140703190876160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:39:11.861191 140703190876160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:39:11.861258 140703190876160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.863544 140703190876160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:39:11.868986 140703190876160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.869250 140703190876160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:39:11.872039 140703190876160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:39:11.885074 140703190876160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:39:11.885138 140703190876160 attention.py:418] Single window, no scan.
I0123 22:39:11.885177 140703190876160 transformer_layer.py:389] tlayer: self-attention.
I0123 22:39:11.885210 140703190876160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.885275 140703190876160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.885903 140703190876160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.885986 140703190876160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.886352 140703190876160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.887063 140703190876160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.889563 140703190876160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.890207 140703190876160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.890290 140703190876160 transformer_layer.py:468] tlayer: End windows.
I0123 22:39:11.890328 140703190876160 transformer_layer.py:472] tlayer: final FFN.
I0123 22:39:11.890388 140703190876160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.890519 140703190876160 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:39:11.890632 140703190876160 nn_components.py:325] mlp: activation = None
I0123 22:39:11.890675 140703190876160 nn_components.py:329] mlp: final activation = None
I0123 22:39:11.892570 140703190876160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.892677 140703190876160 nn_components.py:261] mlp: residual
I0123 22:39:11.895163 140703190876160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.895250 140703190876160 transformer_base.py:443] tbase: final FFN
I0123 22:39:11.895362 140703190876160 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:39:11.897600 140703190876160 nn_components.py:329] mlp: final activation = None
I0123 22:39:11.899486 140703190876160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.899589 140703190876160 nn_components.py:261] mlp: residual
I0123 22:39:11.899880 140703190876160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.899965 140703190876160 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 22:39:11.900077 140703190876160 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:39:11.900121 140703190876160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:39:11.900155 140703190876160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:39:11.900221 140703190876160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.902489 140703190876160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:39:11.908081 140703190876160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.908348 140703190876160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:39:11.911005 140703190876160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:39:11.923822 140703190876160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:39:11.923885 140703190876160 attention.py:418] Single window, no scan.
I0123 22:39:11.923925 140703190876160 transformer_layer.py:389] tlayer: self-attention.
I0123 22:39:11.923963 140703190876160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.924029 140703190876160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.924601 140703190876160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.924683 140703190876160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.925050 140703190876160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.925762 140703190876160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.928287 140703190876160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.928953 140703190876160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.929037 140703190876160 transformer_layer.py:468] tlayer: End windows.
I0123 22:39:11.929075 140703190876160 transformer_layer.py:472] tlayer: final FFN.
I0123 22:39:11.929136 140703190876160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.929272 140703190876160 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:39:11.929391 140703190876160 nn_components.py:325] mlp: activation = None
I0123 22:39:11.929436 140703190876160 nn_components.py:329] mlp: final activation = None
I0123 22:39:11.931351 140703190876160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.931459 140703190876160 nn_components.py:261] mlp: residual
I0123 22:39:11.933919 140703190876160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.934003 140703190876160 transformer_base.py:443] tbase: final FFN
I0123 22:39:11.934114 140703190876160 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:39:11.936710 140703190876160 nn_components.py:329] mlp: final activation = None
I0123 22:39:11.938664 140703190876160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.938765 140703190876160 nn_components.py:261] mlp: residual
I0123 22:39:11.939060 140703190876160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.939146 140703190876160 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 22:39:11.939258 140703190876160 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:39:11.939302 140703190876160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:39:11.939336 140703190876160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:39:11.939403 140703190876160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.941679 140703190876160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:39:11.947219 140703190876160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.947482 140703190876160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:39:11.950202 140703190876160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:39:11.962804 140703190876160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:39:11.962866 140703190876160 attention.py:418] Single window, no scan.
I0123 22:39:11.962913 140703190876160 transformer_layer.py:389] tlayer: self-attention.
I0123 22:39:11.962948 140703190876160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.963012 140703190876160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.963582 140703190876160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.963663 140703190876160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.964026 140703190876160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.964778 140703190876160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.967324 140703190876160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.967956 140703190876160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.968038 140703190876160 transformer_layer.py:468] tlayer: End windows.
I0123 22:39:11.968076 140703190876160 transformer_layer.py:472] tlayer: final FFN.
I0123 22:39:11.968138 140703190876160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.968271 140703190876160 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:39:11.968389 140703190876160 nn_components.py:325] mlp: activation = None
I0123 22:39:11.968433 140703190876160 nn_components.py:329] mlp: final activation = None
I0123 22:39:11.970365 140703190876160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.970465 140703190876160 nn_components.py:261] mlp: residual
I0123 22:39:11.972910 140703190876160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.972994 140703190876160 transformer_base.py:443] tbase: final FFN
I0123 22:39:11.973105 140703190876160 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:39:11.975425 140703190876160 nn_components.py:329] mlp: final activation = None
I0123 22:39:11.977323 140703190876160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.977422 140703190876160 nn_components.py:261] mlp: residual
I0123 22:39:11.977726 140703190876160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:11.977819 140703190876160 decoder_stack.py:344] dstack: Final layernorm.
I0123 22:39:11.980694 140703190876160 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:39:12.036242 140703190876160 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.036339 140703190876160 decoder_stack.py:333] dstack: autoregressive generator.
I0123 22:39:12.036396 140703190876160 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 22:39:12.036503 140703190876160 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:39:12.036548 140703190876160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:39:12.036584 140703190876160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:39:12.036653 140703190876160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.039363 140703190876160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:39:12.044823 140703190876160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.045093 140703190876160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:39:12.047740 140703190876160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:39:12.060185 140703190876160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:39:12.060248 140703190876160 attention.py:418] Single window, no scan.
I0123 22:39:12.060288 140703190876160 transformer_layer.py:389] tlayer: self-attention.
I0123 22:39:12.060323 140703190876160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.060389 140703190876160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.060952 140703190876160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.061034 140703190876160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.061399 140703190876160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.062101 140703190876160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.064634 140703190876160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.065257 140703190876160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.065340 140703190876160 transformer_layer.py:468] tlayer: End windows.
I0123 22:39:12.065378 140703190876160 transformer_layer.py:472] tlayer: final FFN.
I0123 22:39:12.065441 140703190876160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.065575 140703190876160 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:39:12.065705 140703190876160 nn_components.py:325] mlp: activation = None
I0123 22:39:12.065748 140703190876160 nn_components.py:329] mlp: final activation = None
I0123 22:39:12.067619 140703190876160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.067718 140703190876160 nn_components.py:261] mlp: residual
I0123 22:39:12.070140 140703190876160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.070226 140703190876160 transformer_base.py:443] tbase: final FFN
I0123 22:39:12.070339 140703190876160 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:39:12.072620 140703190876160 nn_components.py:329] mlp: final activation = None
I0123 22:39:12.074501 140703190876160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.074603 140703190876160 nn_components.py:261] mlp: residual
I0123 22:39:12.074898 140703190876160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.074986 140703190876160 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 22:39:12.075097 140703190876160 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:39:12.075139 140703190876160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:39:12.075174 140703190876160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:39:12.075241 140703190876160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.077499 140703190876160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:39:12.083060 140703190876160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.083324 140703190876160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:39:12.085981 140703190876160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:39:12.098224 140703190876160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:39:12.098286 140703190876160 attention.py:418] Single window, no scan.
I0123 22:39:12.098325 140703190876160 transformer_layer.py:389] tlayer: self-attention.
I0123 22:39:12.098360 140703190876160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.098424 140703190876160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.098982 140703190876160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.099063 140703190876160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.099425 140703190876160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.100111 140703190876160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.102639 140703190876160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.103261 140703190876160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.103345 140703190876160 transformer_layer.py:468] tlayer: End windows.
I0123 22:39:12.103383 140703190876160 transformer_layer.py:472] tlayer: final FFN.
I0123 22:39:12.103445 140703190876160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.103578 140703190876160 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:39:12.103692 140703190876160 nn_components.py:325] mlp: activation = None
I0123 22:39:12.103746 140703190876160 nn_components.py:329] mlp: final activation = None
I0123 22:39:12.105614 140703190876160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.105721 140703190876160 nn_components.py:261] mlp: residual
I0123 22:39:12.108123 140703190876160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.108208 140703190876160 transformer_base.py:443] tbase: final FFN
I0123 22:39:12.108321 140703190876160 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:39:12.110591 140703190876160 nn_components.py:329] mlp: final activation = None
I0123 22:39:12.112441 140703190876160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.112542 140703190876160 nn_components.py:261] mlp: residual
I0123 22:39:12.112833 140703190876160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.112919 140703190876160 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 22:39:12.113029 140703190876160 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:39:12.113071 140703190876160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:39:12.113104 140703190876160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:39:12.113171 140703190876160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.115419 140703190876160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:39:12.120793 140703190876160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.121060 140703190876160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:39:12.123733 140703190876160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:39:12.135957 140703190876160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:39:12.136019 140703190876160 attention.py:418] Single window, no scan.
I0123 22:39:12.136059 140703190876160 transformer_layer.py:389] tlayer: self-attention.
I0123 22:39:12.136093 140703190876160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.136158 140703190876160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.136716 140703190876160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.136797 140703190876160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.137158 140703190876160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.137855 140703190876160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.140374 140703190876160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.140991 140703190876160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.141075 140703190876160 transformer_layer.py:468] tlayer: End windows.
I0123 22:39:12.141113 140703190876160 transformer_layer.py:472] tlayer: final FFN.
I0123 22:39:12.141175 140703190876160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.141309 140703190876160 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:39:12.141422 140703190876160 nn_components.py:325] mlp: activation = None
I0123 22:39:12.141466 140703190876160 nn_components.py:329] mlp: final activation = None
I0123 22:39:12.143336 140703190876160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.143435 140703190876160 nn_components.py:261] mlp: residual
I0123 22:39:12.145837 140703190876160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.145922 140703190876160 transformer_base.py:443] tbase: final FFN
I0123 22:39:12.146034 140703190876160 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:39:12.148726 140703190876160 nn_components.py:329] mlp: final activation = None
I0123 22:39:12.150588 140703190876160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.150690 140703190876160 nn_components.py:261] mlp: residual
I0123 22:39:12.150982 140703190876160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.151068 140703190876160 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 22:39:12.151180 140703190876160 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:39:12.151223 140703190876160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:39:12.151257 140703190876160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:39:12.151323 140703190876160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.153547 140703190876160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:39:12.158932 140703190876160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.159198 140703190876160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:39:12.161879 140703190876160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:39:12.174213 140703190876160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:39:12.174273 140703190876160 attention.py:418] Single window, no scan.
I0123 22:39:12.174314 140703190876160 transformer_layer.py:389] tlayer: self-attention.
I0123 22:39:12.174355 140703190876160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.174420 140703190876160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.174985 140703190876160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.175066 140703190876160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.175431 140703190876160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.176120 140703190876160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.178797 140703190876160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.179572 140703190876160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.179654 140703190876160 transformer_layer.py:468] tlayer: End windows.
I0123 22:39:12.179690 140703190876160 transformer_layer.py:472] tlayer: final FFN.
I0123 22:39:12.179751 140703190876160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.179883 140703190876160 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:39:12.179996 140703190876160 nn_components.py:325] mlp: activation = None
I0123 22:39:12.180040 140703190876160 nn_components.py:329] mlp: final activation = None
I0123 22:39:12.181926 140703190876160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.182024 140703190876160 nn_components.py:261] mlp: residual
I0123 22:39:12.184413 140703190876160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.184499 140703190876160 transformer_base.py:443] tbase: final FFN
I0123 22:39:12.184610 140703190876160 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:39:12.186932 140703190876160 nn_components.py:329] mlp: final activation = None
I0123 22:39:12.188813 140703190876160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.188911 140703190876160 nn_components.py:261] mlp: residual
I0123 22:39:12.189200 140703190876160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.189284 140703190876160 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 22:39:12.189392 140703190876160 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:39:12.189435 140703190876160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:39:12.189467 140703190876160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:39:12.189532 140703190876160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.191771 140703190876160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:39:12.197201 140703190876160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.197460 140703190876160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:39:12.200164 140703190876160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:39:12.212589 140703190876160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:39:12.212649 140703190876160 attention.py:418] Single window, no scan.
I0123 22:39:12.212687 140703190876160 transformer_layer.py:389] tlayer: self-attention.
I0123 22:39:12.212719 140703190876160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.212783 140703190876160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.213345 140703190876160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.213424 140703190876160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.213795 140703190876160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.214493 140703190876160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.217051 140703190876160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.217686 140703190876160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.217767 140703190876160 transformer_layer.py:468] tlayer: End windows.
I0123 22:39:12.217804 140703190876160 transformer_layer.py:472] tlayer: final FFN.
I0123 22:39:12.217866 140703190876160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.217998 140703190876160 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:39:12.218112 140703190876160 nn_components.py:325] mlp: activation = None
I0123 22:39:12.218155 140703190876160 nn_components.py:329] mlp: final activation = None
I0123 22:39:12.220041 140703190876160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.220147 140703190876160 nn_components.py:261] mlp: residual
I0123 22:39:12.222571 140703190876160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.222653 140703190876160 transformer_base.py:443] tbase: final FFN
I0123 22:39:12.222763 140703190876160 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:39:12.225050 140703190876160 nn_components.py:329] mlp: final activation = None
I0123 22:39:12.226932 140703190876160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.227032 140703190876160 nn_components.py:261] mlp: residual
I0123 22:39:12.227322 140703190876160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.227407 140703190876160 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 22:39:12.227515 140703190876160 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:39:12.227555 140703190876160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:39:12.227586 140703190876160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:39:12.227650 140703190876160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.229900 140703190876160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:39:12.235344 140703190876160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.235613 140703190876160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:39:12.238328 140703190876160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:39:12.250831 140703190876160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:39:12.250891 140703190876160 attention.py:418] Single window, no scan.
I0123 22:39:12.250930 140703190876160 transformer_layer.py:389] tlayer: self-attention.
I0123 22:39:12.250962 140703190876160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.251026 140703190876160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.251582 140703190876160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.251660 140703190876160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.252020 140703190876160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.252715 140703190876160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.255283 140703190876160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.255914 140703190876160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.255994 140703190876160 transformer_layer.py:468] tlayer: End windows.
I0123 22:39:12.256031 140703190876160 transformer_layer.py:472] tlayer: final FFN.
I0123 22:39:12.256091 140703190876160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.256222 140703190876160 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:39:12.256334 140703190876160 nn_components.py:325] mlp: activation = None
I0123 22:39:12.256376 140703190876160 nn_components.py:329] mlp: final activation = None
I0123 22:39:12.258270 140703190876160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.258374 140703190876160 nn_components.py:261] mlp: residual
I0123 22:39:12.260780 140703190876160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.260862 140703190876160 transformer_base.py:443] tbase: final FFN
I0123 22:39:12.260972 140703190876160 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:39:12.263677 140703190876160 nn_components.py:329] mlp: final activation = None
I0123 22:39:12.265548 140703190876160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.265651 140703190876160 nn_components.py:261] mlp: residual
I0123 22:39:12.265944 140703190876160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.266027 140703190876160 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 22:39:12.266134 140703190876160 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:39:12.266174 140703190876160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:39:12.266206 140703190876160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:39:12.266271 140703190876160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.268508 140703190876160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:39:12.273947 140703190876160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.274208 140703190876160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:39:12.276914 140703190876160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:39:12.289587 140703190876160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:39:12.289652 140703190876160 attention.py:418] Single window, no scan.
I0123 22:39:12.289692 140703190876160 transformer_layer.py:389] tlayer: self-attention.
I0123 22:39:12.289725 140703190876160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.289789 140703190876160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.290354 140703190876160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.290435 140703190876160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.290798 140703190876160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.291493 140703190876160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.294048 140703190876160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.294680 140703190876160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.294759 140703190876160 transformer_layer.py:468] tlayer: End windows.
I0123 22:39:12.294796 140703190876160 transformer_layer.py:472] tlayer: final FFN.
I0123 22:39:12.294858 140703190876160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.294989 140703190876160 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:39:12.295100 140703190876160 nn_components.py:325] mlp: activation = None
I0123 22:39:12.295142 140703190876160 nn_components.py:329] mlp: final activation = None
I0123 22:39:12.297031 140703190876160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.297129 140703190876160 nn_components.py:261] mlp: residual
I0123 22:39:12.299541 140703190876160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.299628 140703190876160 transformer_base.py:443] tbase: final FFN
I0123 22:39:12.299741 140703190876160 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:39:12.302037 140703190876160 nn_components.py:329] mlp: final activation = None
I0123 22:39:12.303917 140703190876160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.304014 140703190876160 nn_components.py:261] mlp: residual
I0123 22:39:12.304304 140703190876160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.304388 140703190876160 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 22:39:12.304497 140703190876160 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:39:12.304540 140703190876160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:39:12.304572 140703190876160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:39:12.304637 140703190876160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.306896 140703190876160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:39:12.312366 140703190876160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.312628 140703190876160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:39:12.315338 140703190876160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:39:12.327855 140703190876160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:39:12.327915 140703190876160 attention.py:418] Single window, no scan.
I0123 22:39:12.327954 140703190876160 transformer_layer.py:389] tlayer: self-attention.
I0123 22:39:12.327986 140703190876160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.328050 140703190876160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.328613 140703190876160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.328692 140703190876160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.329054 140703190876160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.329749 140703190876160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.332310 140703190876160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.332940 140703190876160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.333020 140703190876160 transformer_layer.py:468] tlayer: End windows.
I0123 22:39:12.333056 140703190876160 transformer_layer.py:472] tlayer: final FFN.
I0123 22:39:12.333119 140703190876160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.333251 140703190876160 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:39:12.333368 140703190876160 nn_components.py:325] mlp: activation = None
I0123 22:39:12.333412 140703190876160 nn_components.py:329] mlp: final activation = None
I0123 22:39:12.335285 140703190876160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.335381 140703190876160 nn_components.py:261] mlp: residual
I0123 22:39:12.337784 140703190876160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.337872 140703190876160 transformer_base.py:443] tbase: final FFN
I0123 22:39:12.337983 140703190876160 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:39:12.340272 140703190876160 nn_components.py:329] mlp: final activation = None
I0123 22:39:12.342160 140703190876160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.342257 140703190876160 nn_components.py:261] mlp: residual
I0123 22:39:12.342548 140703190876160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.342633 140703190876160 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 22:39:12.342741 140703190876160 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:39:12.342781 140703190876160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:39:12.342814 140703190876160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:39:12.342879 140703190876160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.345130 140703190876160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:39:12.350583 140703190876160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.350843 140703190876160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:39:12.353534 140703190876160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:39:12.366037 140703190876160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:39:12.366097 140703190876160 attention.py:418] Single window, no scan.
I0123 22:39:12.366135 140703190876160 transformer_layer.py:389] tlayer: self-attention.
I0123 22:39:12.366167 140703190876160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.366231 140703190876160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.366800 140703190876160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.366879 140703190876160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.367237 140703190876160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.367924 140703190876160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.370508 140703190876160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.371134 140703190876160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.371215 140703190876160 transformer_layer.py:468] tlayer: End windows.
I0123 22:39:12.371251 140703190876160 transformer_layer.py:472] tlayer: final FFN.
I0123 22:39:12.371313 140703190876160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.371446 140703190876160 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:39:12.371564 140703190876160 nn_components.py:325] mlp: activation = None
I0123 22:39:12.371607 140703190876160 nn_components.py:329] mlp: final activation = None
I0123 22:39:12.373490 140703190876160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.373586 140703190876160 nn_components.py:261] mlp: residual
I0123 22:39:12.375988 140703190876160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.376077 140703190876160 transformer_base.py:443] tbase: final FFN
I0123 22:39:12.376189 140703190876160 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:39:12.378877 140703190876160 nn_components.py:329] mlp: final activation = None
I0123 22:39:12.380935 140703190876160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.381035 140703190876160 nn_components.py:261] mlp: residual
I0123 22:39:12.381327 140703190876160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.381412 140703190876160 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 22:39:12.381521 140703190876160 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:39:12.381564 140703190876160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:39:12.381596 140703190876160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:39:12.381667 140703190876160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.384049 140703190876160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:39:12.389533 140703190876160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.389806 140703190876160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:39:12.392522 140703190876160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:39:12.405025 140703190876160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:39:12.405086 140703190876160 attention.py:418] Single window, no scan.
I0123 22:39:12.405123 140703190876160 transformer_layer.py:389] tlayer: self-attention.
I0123 22:39:12.405155 140703190876160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.405218 140703190876160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.405800 140703190876160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.405882 140703190876160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.406252 140703190876160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.406956 140703190876160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.409541 140703190876160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.410185 140703190876160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.410266 140703190876160 transformer_layer.py:468] tlayer: End windows.
I0123 22:39:12.410302 140703190876160 transformer_layer.py:472] tlayer: final FFN.
I0123 22:39:12.410361 140703190876160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.410490 140703190876160 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:39:12.410602 140703190876160 nn_components.py:325] mlp: activation = None
I0123 22:39:12.410645 140703190876160 nn_components.py:329] mlp: final activation = None
I0123 22:39:12.412887 140703190876160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.412985 140703190876160 nn_components.py:261] mlp: residual
I0123 22:39:12.415383 140703190876160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.415467 140703190876160 transformer_base.py:443] tbase: final FFN
I0123 22:39:12.415585 140703190876160 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:39:12.417863 140703190876160 nn_components.py:329] mlp: final activation = None
I0123 22:39:12.419722 140703190876160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.419820 140703190876160 nn_components.py:261] mlp: residual
I0123 22:39:12.420111 140703190876160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.420196 140703190876160 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 22:39:12.420302 140703190876160 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:39:12.420342 140703190876160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:39:12.420375 140703190876160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:39:12.420439 140703190876160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.422669 140703190876160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:39:12.428114 140703190876160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.428377 140703190876160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:39:12.431080 140703190876160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:39:12.443546 140703190876160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:39:12.443606 140703190876160 attention.py:418] Single window, no scan.
I0123 22:39:12.443643 140703190876160 transformer_layer.py:389] tlayer: self-attention.
I0123 22:39:12.443676 140703190876160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.443739 140703190876160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.444311 140703190876160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.444392 140703190876160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.444751 140703190876160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.445451 140703190876160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.448017 140703190876160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.448645 140703190876160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.448725 140703190876160 transformer_layer.py:468] tlayer: End windows.
I0123 22:39:12.448761 140703190876160 transformer_layer.py:472] tlayer: final FFN.
I0123 22:39:12.448820 140703190876160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.448956 140703190876160 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:39:12.449068 140703190876160 nn_components.py:325] mlp: activation = None
I0123 22:39:12.449110 140703190876160 nn_components.py:329] mlp: final activation = None
I0123 22:39:12.451002 140703190876160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.451099 140703190876160 nn_components.py:261] mlp: residual
I0123 22:39:12.453486 140703190876160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.453566 140703190876160 transformer_base.py:443] tbase: final FFN
I0123 22:39:12.453682 140703190876160 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:39:12.455956 140703190876160 nn_components.py:329] mlp: final activation = None
I0123 22:39:12.457822 140703190876160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.457921 140703190876160 nn_components.py:261] mlp: residual
I0123 22:39:12.458210 140703190876160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.458295 140703190876160 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 22:39:12.458403 140703190876160 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:39:12.458446 140703190876160 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:39:12.458478 140703190876160 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:39:12.458544 140703190876160 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.460770 140703190876160 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:39:12.466208 140703190876160 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.466470 140703190876160 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:39:12.469174 140703190876160 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:39:12.481580 140703190876160 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:39:12.481647 140703190876160 attention.py:418] Single window, no scan.
I0123 22:39:12.481880 140703190876160 transformer_layer.py:389] tlayer: self-attention.
I0123 22:39:12.481914 140703190876160 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.481976 140703190876160 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.482537 140703190876160 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.482617 140703190876160 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.482977 140703190876160 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.483679 140703190876160 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.486311 140703190876160 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.486933 140703190876160 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.487016 140703190876160 transformer_layer.py:468] tlayer: End windows.
I0123 22:39:12.487052 140703190876160 transformer_layer.py:472] tlayer: final FFN.
I0123 22:39:12.487111 140703190876160 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.487240 140703190876160 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:39:12.487356 140703190876160 nn_components.py:325] mlp: activation = None
I0123 22:39:12.487397 140703190876160 nn_components.py:329] mlp: final activation = None
I0123 22:39:12.489266 140703190876160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.489362 140703190876160 nn_components.py:261] mlp: residual
I0123 22:39:12.491765 140703190876160 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.491848 140703190876160 transformer_base.py:443] tbase: final FFN
I0123 22:39:12.491958 140703190876160 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:39:12.494628 140703190876160 nn_components.py:329] mlp: final activation = None
I0123 22:39:12.496506 140703190876160 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.496605 140703190876160 nn_components.py:261] mlp: residual
I0123 22:39:12.496900 140703190876160 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:12.496989 140703190876160 decoder_stack.py:344] dstack: Final layernorm.
I0123 22:39:12.499858 140703190876160 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:39:16.929273 140703190876160 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
I0123 22:39:17.462740 140703190876160 training_loop.py:409] No working directory specified.
I0123 22:39:17.462892 140703190876160 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0123 22:39:17.463756 140703190876160 checkpoints.py:1062] Restoring legacy Flax checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0123 22:39:20.725493 140703190876160 training_loop.py:447] Only restoring trainable parameters.
I0123 22:39:20.726409 140703190876160 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0123 22:39:20.726475 140703190876160 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0123 22:39:20.726523 140703190876160 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 22:39:20.726567 140703190876160 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 22:39:20.726611 140703190876160 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:39:20.726653 140703190876160 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 22:39:20.726695 140703190876160 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:39:20.726735 140703190876160 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:39:20.726774 140703190876160 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 22:39:20.726814 140703190876160 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 22:39:20.726852 140703190876160 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:39:20.726891 140703190876160 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 22:39:20.726929 140703190876160 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 22:39:20.726968 140703190876160 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 22:39:20.727007 140703190876160 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:39:20.727046 140703190876160 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 22:39:20.727084 140703190876160 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:39:20.727123 140703190876160 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:39:20.727161 140703190876160 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 22:39:20.727200 140703190876160 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 22:39:20.727266 140703190876160 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:39:20.727307 140703190876160 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 22:39:20.727348 140703190876160 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 22:39:20.727387 140703190876160 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 22:39:20.727427 140703190876160 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:39:20.727467 140703190876160 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 22:39:20.727506 140703190876160 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:39:20.727544 140703190876160 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:39:20.727583 140703190876160 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 22:39:20.727621 140703190876160 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 22:39:20.727659 140703190876160 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:39:20.727697 140703190876160 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 22:39:20.727736 140703190876160 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 22:39:20.727774 140703190876160 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 22:39:20.727813 140703190876160 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:39:20.727850 140703190876160 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 22:39:20.727889 140703190876160 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:39:20.727927 140703190876160 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:39:20.727965 140703190876160 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 22:39:20.728003 140703190876160 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 22:39:20.728040 140703190876160 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:39:20.728077 140703190876160 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 22:39:20.728115 140703190876160 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 22:39:20.728152 140703190876160 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 22:39:20.728189 140703190876160 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:39:20.728227 140703190876160 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 22:39:20.728271 140703190876160 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:39:20.728311 140703190876160 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:39:20.728349 140703190876160 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 22:39:20.728388 140703190876160 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 22:39:20.728426 140703190876160 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:39:20.728466 140703190876160 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 22:39:20.728504 140703190876160 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 22:39:20.728542 140703190876160 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 22:39:20.728579 140703190876160 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:39:20.728618 140703190876160 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 22:39:20.728656 140703190876160 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:39:20.728694 140703190876160 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:39:20.728731 140703190876160 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 22:39:20.728769 140703190876160 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 22:39:20.728805 140703190876160 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:39:20.728843 140703190876160 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 22:39:20.728882 140703190876160 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 22:39:20.728920 140703190876160 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 22:39:20.728958 140703190876160 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:39:20.728996 140703190876160 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 22:39:20.729033 140703190876160 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:39:20.729071 140703190876160 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:39:20.729109 140703190876160 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 22:39:20.729146 140703190876160 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 22:39:20.729183 140703190876160 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:39:20.729222 140703190876160 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 22:39:20.729261 140703190876160 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 22:39:20.729306 140703190876160 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 22:39:20.729346 140703190876160 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:39:20.729384 140703190876160 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 22:39:20.729422 140703190876160 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:39:20.729460 140703190876160 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:39:20.729498 140703190876160 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 22:39:20.729535 140703190876160 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 22:39:20.729572 140703190876160 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:39:20.729610 140703190876160 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 22:39:20.729658 140703190876160 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 22:39:20.729701 140703190876160 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 22:39:20.729739 140703190876160 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:39:20.729778 140703190876160 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 22:39:20.729816 140703190876160 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:39:20.729855 140703190876160 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:39:20.729892 140703190876160 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 22:39:20.729931 140703190876160 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 22:39:20.729969 140703190876160 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:39:20.730006 140703190876160 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 22:39:20.730044 140703190876160 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 22:39:20.730082 140703190876160 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 22:39:20.730120 140703190876160 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:39:20.730157 140703190876160 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 22:39:20.730195 140703190876160 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:39:20.730232 140703190876160 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:39:20.730270 140703190876160 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 22:39:20.730307 140703190876160 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 22:39:20.730351 140703190876160 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:39:20.730391 140703190876160 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 22:39:20.730429 140703190876160 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 22:39:20.730467 140703190876160 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 22:39:20.730504 140703190876160 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:39:20.730542 140703190876160 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 22:39:20.730579 140703190876160 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:39:20.730617 140703190876160 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:39:20.730655 140703190876160 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 22:39:20.730693 140703190876160 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 22:39:20.730731 140703190876160 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:39:20.730768 140703190876160 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 22:39:20.730805 140703190876160 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 22:39:20.730843 140703190876160 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 22:39:20.730880 140703190876160 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:39:20.730917 140703190876160 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 22:39:20.730955 140703190876160 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:39:20.730992 140703190876160 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:39:20.731029 140703190876160 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 22:39:20.731067 140703190876160 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 22:39:20.731104 140703190876160 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:39:20.731142 140703190876160 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 22:39:20.731172 140703190876160 training_loop.py:725] Total parameters: 152072288
I0123 22:39:20.731409 140703190876160 training_loop.py:739] Total state size: 0
I0123 22:39:20.754776 140703190876160 training_loop.py:492] Training loop: creating task for mode beam_search
I0123 22:39:20.755052 140703190876160 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0123 22:39:20.755658 140703190876160 training_loop.py:652] Compiling mode beam_search with jit.
I0123 22:39:20.756260 140703190876160 training_loop.py:89] registering functions: dict_keys([])
I0123 22:39:20.778157 140703190876160 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = midpoint e a b; f = angle_bisector f a c b, on_line f a b; g = on_line g c e; h = foot h g c f; i = mirror i g h; j = circle j c a i; k = circle k c b i; l = on_line l c d, on_line l j k ? cong l j l k
I0123 22:39:21.655262 140703190876160 ddar.py:60] Depth 1/1000 time = 0.8256001472473145
I0123 22:39:23.399008 140703190876160 ddar.py:60] Depth 2/1000 time = 1.7435197830200195
I0123 22:39:25.609840 140703190876160 ddar.py:60] Depth 3/1000 time = 2.2105958461761475
I0123 22:39:27.695798 140703190876160 ddar.py:60] Depth 4/1000 time = 2.085667610168457
I0123 22:39:30.075799 140703190876160 ddar.py:60] Depth 5/1000 time = 2.379592180252075
I0123 22:39:32.788942 140703190876160 ddar.py:60] Depth 6/1000 time = 2.683027744293213
I0123 22:39:35.570453 140703190876160 ddar.py:60] Depth 7/1000 time = 2.781175374984741
I0123 22:39:35.581008 140703190876160 alphageometry.py:191] 
==========================
 * From theorem premises:
A B C D E F G H I J K L : Points
DB = DC [00]
DA = DB [01]
AC:EC = AC:EC [02]
E,A,B are collinear [03]
EA = EB [04]
F,A,B are collinear [05]
∠ACF = ∠FCB [06]
E,C,G are collinear [07]
GH ⟂ CF [08]
F,H,C are collinear [09]
HG = HI [10]
H,I,G are collinear [11]
JA = JI [12]
JC = JA [13]
KC = KB [14]
KB = KI [15]
L,D,C are collinear [16]
J,L,K are collinear [17]

 * Auxiliary Constructions:
: Points


 * Proof steps:
001. DB = DC [00] & KC = KB [14] ⇒  BC ⟂ DK [18]
002. DA = DB [01] & EA = EB [04] ⇒  AB ⟂ DE [19]
003. F,A,B are collinear [05] & E,A,B are collinear [03] & BC ⟂ DK [18] & AB ⟂ DE [19] ⇒  ∠FED = ∠(DK-CB) [20]
004. DB = DC [00] ⇒  ∠DBC = ∠BCD [21]
005. DB = DC [00] & DA = DB [01] ⇒  D is the circumcenter of \Delta CBA [22]
006. DB = DC [00] & DA = DB [01] ⇒  DA = DC [23]
007. E,A,B are collinear [03] & EA = EB [04] ⇒  E is midpoint of BA [24]
008. D is the circumcenter of \Delta CBA [22] & E is midpoint of BA [24] ⇒  ∠DBC = ∠(DE-AC) [25]
009. D is the circumcenter of \Delta CBA [22] & E is midpoint of BA [24] ⇒  ∠DAC = ∠(DE-BC) [26]
010. L,D,C are collinear [16] & ∠DBC = ∠BCD [21] & ∠DBC = ∠(DE-AC) [25] ⇒  ∠(ED-AC) = ∠(CB-LD) [27]
011. ∠FED = ∠(DK-CB) [20] & ∠(ED-AC) = ∠(CB-LD) [27] ⇒  ∠(EF-AC) = ∠KDL [28]
012. L,D,C are collinear [16] & E,A,B are collinear [03] & ∠(EF-AC) = ∠KDL [28] & F,A,B are collinear [05] ⇒  ∠KDL = ∠EAC [29]
013. DA = DB [01] ⇒  ∠DBA = ∠BAD [30]
014. DA = DC [23] ⇒  ∠DAC = ∠ACD [31]
015. JA = JI [12] & JC = JA [13] ⇒  JC = JI [32]
016. KB = KI [15] & KC = KB [14] ⇒  KC = KI [33]
017. JC = JI [32] & KC = KI [33] ⇒  CI ⟂ JK [34]
018. H,F,C are collinear [09] & H,I,G are collinear [11] & GH ⟂ CF [08] ⇒  ∠GHC = ∠CHI [35]
019. HG = HI [10] & ∠GHC = ∠CHI [35] (SAS)⇒  ∠HGC = ∠CIH [36]
020. ∠HGC = ∠CIH [36] & E,C,G are collinear [07] & H,I,G are collinear [11] ⇒  ∠(GH-CE) = ∠(CI-GH) [37]
021. ∠ACF = ∠FCB [06] & GH ⟂ CF [08] & ∠DBA = ∠BAD [30] & ∠DBC = ∠BCD [21] & ∠DAC = ∠ACD [31] & CI ⟂ JK [34] & ∠(GH-CE) = ∠(CI-GH) [37] (Angle chase)⇒  ∠(AB-CE) = ∠(JK-CD) [38]
022. J,L,K are collinear [17] & L,D,C are collinear [16] & E,A,B are collinear [03] & ∠(JK-CD) = ∠(AB-CE) [38] ⇒  ∠KLD = ∠AEC [39]
023. ∠KDL = ∠EAC [29] & ∠KLD = ∠AEC [39] (Similar Triangles)⇒  LK:EC = LD:EA [40]
024. DA = DC [23] & JC = JA [13] ⇒  AC ⟂ DJ [41]
025. F,A,B are collinear [05] & E,A,B are collinear [03] & AC ⟂ DJ [41] & AB ⟂ DE [19] ⇒  ∠FED = ∠(JD-AC) [42]
026. L,D,C are collinear [16] & ∠DAC = ∠ACD [31] & ∠DAC = ∠(DE-BC) [26] ⇒  ∠(ED-CB) = ∠(AC-LD) [43]
027. ∠FED = ∠(JD-AC) [42] & ∠(ED-CB) = ∠(AC-LD) [43] ⇒  ∠(EF-CB) = ∠JDL [44]
028. E,A,B are collinear [03] & L,D,C are collinear [16] & ∠(EF-CB) = ∠JDL [44] & F,A,B are collinear [05] ⇒  ∠EBC = ∠JDL [45]
029. ∠ACF = ∠FCB [06] & GH ⟂ CF [08] & ∠(GH-CE) = ∠(CI-GH) [37] (Angle chase)⇒  ∠BCE = ∠ICA [46]
030. J,L,K are collinear [17] & CI ⟂ JK [34] ⇒  IC ⟂ JL [47]
031. IC ⟂ JL [47] & AC ⟂ DJ [41] ⇒  ∠ICA = ∠LJD [48]
032. J,L,K are collinear [17] & ∠BCE = ∠ICA [46] & ∠ICA = ∠LJD [48] ⇒  ∠LJD = ∠BCE [49]
033. ∠EBC = ∠JDL [45] & ∠LJD = ∠BCE [49] (Similar Triangles)⇒  EB:LD = EC:JL [50]
034. LK:EC = LD:EA [40] & EA = EB [04] & EB:LD = EC:JL [50] ⇒  EC:JL = EC:LK [51]
035. AC:EC = AC:EC [02] & EC:JL = EC:LK [51] ⇒  JL = LK
==========================

