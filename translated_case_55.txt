I0123 22:43:56.635098 140066259943424 inference_utils.py:69] Parsing gin configuration.
I0123 22:43:56.635196 140066259943424 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0123 22:43:56.635404 140066259943424 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0123 22:43:56.635438 140066259943424 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0123 22:43:56.635469 140066259943424 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0123 22:43:56.635497 140066259943424 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0123 22:43:56.635524 140066259943424 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0123 22:43:56.635551 140066259943424 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0123 22:43:56.635578 140066259943424 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0123 22:43:56.635605 140066259943424 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=32
I0123 22:43:56.635632 140066259943424 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0123 22:43:56.635659 140066259943424 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0123 22:43:56.635707 140066259943424 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0123 22:43:56.635834 140066259943424 resource_reader.py:55] Path not found: base_htrans.gin
I0123 22:43:56.636024 140066259943424 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0123 22:43:56.636130 140066259943424 resource_reader.py:55] Path not found: trainer_configuration.gin
I0123 22:43:56.642319 140066259943424 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0123 22:43:56.642445 140066259943424 resource_reader.py:55] Path not found: size/medium_150M.gin
I0123 22:43:56.642764 140066259943424 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0123 22:43:56.642873 140066259943424 resource_reader.py:55] Path not found: options/positions_t5.gin
I0123 22:43:56.643146 140066259943424 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0123 22:43:56.643247 140066259943424 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0123 22:43:56.643649 140066259943424 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0123 22:43:56.643751 140066259943424 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0123 22:43:56.647376 140066259943424 training_loop.py:334] ==== Training loop: initializing model ====
I0123 22:43:56.742765 140066259943424 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0123 22:43:56.743488 140066259943424 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0123 22:43:56.750694 140066259943424 training_loop.py:335] Process 0 of 1
I0123 22:43:56.750755 140066259943424 training_loop.py:336] Local device count = 1
I0123 22:43:56.750797 140066259943424 training_loop.py:337] Number of replicas = 1
I0123 22:43:56.750830 140066259943424 training_loop.py:339] Using random number seed 42
I0123 22:43:57.200937 140066259943424 training_loop.py:359] Initializing the model.
I0123 22:43:57.597774 140066259943424 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.598049 140066259943424 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 22:43:57.598155 140066259943424 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 22:43:57.598235 140066259943424 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 22:43:57.598316 140066259943424 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 22:43:57.598401 140066259943424 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 22:43:57.598472 140066259943424 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 22:43:57.598543 140066259943424 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 22:43:57.598611 140066259943424 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 22:43:57.598679 140066259943424 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 22:43:57.598747 140066259943424 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 22:43:57.598815 140066259943424 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 22:43:57.598883 140066259943424 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 22:43:57.598950 140066259943424 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 22:43:57.598989 140066259943424 attention.py:418] Single window, no scan.
I0123 22:43:57.599035 140066259943424 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 22:43:57.599151 140066259943424 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:43:57.599191 140066259943424 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:43:57.599223 140066259943424 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:43:57.601228 140066259943424 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.606487 140066259943424 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:43:57.617124 140066259943424 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.617401 140066259943424 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:43:57.621937 140066259943424 transformer_layer.py:213] tlayer: windowed attention.
I0123 22:43:57.632535 140066259943424 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:43:57.632598 140066259943424 attention.py:418] Single window, no scan.
I0123 22:43:57.632638 140066259943424 transformer_layer.py:389] tlayer: self-attention.
I0123 22:43:57.632671 140066259943424 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.632737 140066259943424 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.633941 140066259943424 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.634023 140066259943424 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.634730 140066259943424 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.637194 140066259943424 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.643356 140066259943424 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.644692 140066259943424 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.644777 140066259943424 transformer_layer.py:468] tlayer: End windows.
I0123 22:43:57.644814 140066259943424 transformer_layer.py:472] tlayer: final FFN.
I0123 22:43:57.644877 140066259943424 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.645009 140066259943424 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:43:57.645338 140066259943424 nn_components.py:325] mlp: activation = None
I0123 22:43:57.645389 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:43:57.647303 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.647610 140066259943424 nn_components.py:261] mlp: residual
I0123 22:43:57.650493 140066259943424 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.650581 140066259943424 transformer_base.py:443] tbase: final FFN
I0123 22:43:57.651090 140066259943424 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:43:57.661380 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:43:57.670114 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.670219 140066259943424 nn_components.py:261] mlp: residual
I0123 22:43:57.670516 140066259943424 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.670602 140066259943424 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 22:43:57.670713 140066259943424 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:43:57.670754 140066259943424 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:43:57.670787 140066259943424 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:43:57.672639 140066259943424 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.675124 140066259943424 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:43:57.680673 140066259943424 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.680941 140066259943424 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:43:57.683573 140066259943424 transformer_layer.py:213] tlayer: windowed attention.
I0123 22:43:57.687403 140066259943424 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:43:57.687464 140066259943424 attention.py:418] Single window, no scan.
I0123 22:43:57.687501 140066259943424 transformer_layer.py:389] tlayer: self-attention.
I0123 22:43:57.687533 140066259943424 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.687596 140066259943424 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.688160 140066259943424 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.688241 140066259943424 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.688601 140066259943424 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.689368 140066259943424 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.691844 140066259943424 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.692462 140066259943424 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.692543 140066259943424 transformer_layer.py:468] tlayer: End windows.
I0123 22:43:57.692579 140066259943424 transformer_layer.py:472] tlayer: final FFN.
I0123 22:43:57.692638 140066259943424 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.692765 140066259943424 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:43:57.693089 140066259943424 nn_components.py:325] mlp: activation = None
I0123 22:43:57.693135 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:43:57.695073 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.695171 140066259943424 nn_components.py:261] mlp: residual
I0123 22:43:57.697667 140066259943424 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.697751 140066259943424 transformer_base.py:443] tbase: final FFN
I0123 22:43:57.698191 140066259943424 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:43:57.700502 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:43:57.702405 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.702507 140066259943424 nn_components.py:261] mlp: residual
I0123 22:43:57.702797 140066259943424 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.702881 140066259943424 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 22:43:57.702990 140066259943424 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:43:57.703031 140066259943424 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:43:57.703062 140066259943424 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:43:57.705284 140066259943424 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.707632 140066259943424 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:43:57.713152 140066259943424 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.713421 140066259943424 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:43:57.716066 140066259943424 transformer_layer.py:213] tlayer: windowed attention.
I0123 22:43:57.719894 140066259943424 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:43:57.719953 140066259943424 attention.py:418] Single window, no scan.
I0123 22:43:57.719990 140066259943424 transformer_layer.py:389] tlayer: self-attention.
I0123 22:43:57.720022 140066259943424 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.720086 140066259943424 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.720641 140066259943424 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.720719 140066259943424 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.721078 140066259943424 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.721853 140066259943424 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.724321 140066259943424 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.724989 140066259943424 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.725070 140066259943424 transformer_layer.py:468] tlayer: End windows.
I0123 22:43:57.725106 140066259943424 transformer_layer.py:472] tlayer: final FFN.
I0123 22:43:57.725166 140066259943424 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.725297 140066259943424 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:43:57.725617 140066259943424 nn_components.py:325] mlp: activation = None
I0123 22:43:57.726134 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:43:57.728336 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.728444 140066259943424 nn_components.py:261] mlp: residual
I0123 22:43:57.730963 140066259943424 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.731057 140066259943424 transformer_base.py:443] tbase: final FFN
I0123 22:43:57.731540 140066259943424 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:43:57.733818 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:43:57.735740 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.735839 140066259943424 nn_components.py:261] mlp: residual
I0123 22:43:57.736130 140066259943424 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.736216 140066259943424 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 22:43:57.736327 140066259943424 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:43:57.736368 140066259943424 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:43:57.736399 140066259943424 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:43:57.738291 140066259943424 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.740684 140066259943424 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:43:57.746329 140066259943424 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.746603 140066259943424 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:43:57.749228 140066259943424 transformer_layer.py:213] tlayer: windowed attention.
I0123 22:43:57.753082 140066259943424 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:43:57.753144 140066259943424 attention.py:418] Single window, no scan.
I0123 22:43:57.753183 140066259943424 transformer_layer.py:389] tlayer: self-attention.
I0123 22:43:57.753216 140066259943424 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.753278 140066259943424 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.753859 140066259943424 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.753938 140066259943424 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.754300 140066259943424 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.755073 140066259943424 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.757600 140066259943424 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.758246 140066259943424 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.758327 140066259943424 transformer_layer.py:468] tlayer: End windows.
I0123 22:43:57.758364 140066259943424 transformer_layer.py:472] tlayer: final FFN.
I0123 22:43:57.758425 140066259943424 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.758557 140066259943424 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:43:57.758882 140066259943424 nn_components.py:325] mlp: activation = None
I0123 22:43:57.758932 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:43:57.760833 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.760931 140066259943424 nn_components.py:261] mlp: residual
I0123 22:43:57.763500 140066259943424 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.763592 140066259943424 transformer_base.py:443] tbase: final FFN
I0123 22:43:57.764024 140066259943424 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:43:57.766283 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:43:57.768192 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.768290 140066259943424 nn_components.py:261] mlp: residual
I0123 22:43:57.768585 140066259943424 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.768669 140066259943424 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 22:43:57.768781 140066259943424 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:43:57.768822 140066259943424 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:43:57.768853 140066259943424 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:43:57.770752 140066259943424 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.773142 140066259943424 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:43:57.778753 140066259943424 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.779019 140066259943424 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:43:57.782018 140066259943424 transformer_layer.py:213] tlayer: windowed attention.
I0123 22:43:57.785769 140066259943424 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:43:57.785828 140066259943424 attention.py:418] Single window, no scan.
I0123 22:43:57.785866 140066259943424 transformer_layer.py:389] tlayer: self-attention.
I0123 22:43:57.785898 140066259943424 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.785960 140066259943424 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.786531 140066259943424 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.786611 140066259943424 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.786982 140066259943424 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.787757 140066259943424 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.790297 140066259943424 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.790925 140066259943424 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.791005 140066259943424 transformer_layer.py:468] tlayer: End windows.
I0123 22:43:57.791040 140066259943424 transformer_layer.py:472] tlayer: final FFN.
I0123 22:43:57.791100 140066259943424 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.791234 140066259943424 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:43:57.791559 140066259943424 nn_components.py:325] mlp: activation = None
I0123 22:43:57.791607 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:43:57.793496 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.793593 140066259943424 nn_components.py:261] mlp: residual
I0123 22:43:57.796125 140066259943424 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.796210 140066259943424 transformer_base.py:443] tbase: final FFN
I0123 22:43:57.796640 140066259943424 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:43:57.798908 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:43:57.800859 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.800958 140066259943424 nn_components.py:261] mlp: residual
I0123 22:43:57.801256 140066259943424 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.801342 140066259943424 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 22:43:57.801453 140066259943424 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:43:57.801494 140066259943424 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:43:57.801526 140066259943424 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:43:57.803384 140066259943424 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.805752 140066259943424 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:43:57.811295 140066259943424 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.811561 140066259943424 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:43:57.814211 140066259943424 transformer_layer.py:213] tlayer: windowed attention.
I0123 22:43:57.817919 140066259943424 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:43:57.817977 140066259943424 attention.py:418] Single window, no scan.
I0123 22:43:57.818015 140066259943424 transformer_layer.py:389] tlayer: self-attention.
I0123 22:43:57.818048 140066259943424 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.818114 140066259943424 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.818722 140066259943424 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.818801 140066259943424 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.819159 140066259943424 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.819942 140066259943424 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.822431 140066259943424 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.823056 140066259943424 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.823137 140066259943424 transformer_layer.py:468] tlayer: End windows.
I0123 22:43:57.823173 140066259943424 transformer_layer.py:472] tlayer: final FFN.
I0123 22:43:57.823231 140066259943424 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.823358 140066259943424 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:43:57.823686 140066259943424 nn_components.py:325] mlp: activation = None
I0123 22:43:57.823732 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:43:57.825609 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.825716 140066259943424 nn_components.py:261] mlp: residual
I0123 22:43:57.828267 140066259943424 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.828350 140066259943424 transformer_base.py:443] tbase: final FFN
I0123 22:43:57.828786 140066259943424 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:43:57.831118 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:43:57.833031 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.833135 140066259943424 nn_components.py:261] mlp: residual
I0123 22:43:57.833428 140066259943424 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.833513 140066259943424 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 22:43:57.833623 140066259943424 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:43:57.833671 140066259943424 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:43:57.833702 140066259943424 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:43:57.835535 140066259943424 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.837982 140066259943424 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:43:57.843591 140066259943424 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.843856 140066259943424 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:43:57.846484 140066259943424 transformer_layer.py:213] tlayer: windowed attention.
I0123 22:43:57.850298 140066259943424 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:43:57.850358 140066259943424 attention.py:418] Single window, no scan.
I0123 22:43:57.850399 140066259943424 transformer_layer.py:389] tlayer: self-attention.
I0123 22:43:57.850434 140066259943424 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.850499 140066259943424 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.851070 140066259943424 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.851282 140066259943424 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.851643 140066259943424 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.852417 140066259943424 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.854896 140066259943424 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.855523 140066259943424 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.855602 140066259943424 transformer_layer.py:468] tlayer: End windows.
I0123 22:43:57.855638 140066259943424 transformer_layer.py:472] tlayer: final FFN.
I0123 22:43:57.855699 140066259943424 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.855829 140066259943424 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:43:57.856149 140066259943424 nn_components.py:325] mlp: activation = None
I0123 22:43:57.856194 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:43:57.858456 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.858557 140066259943424 nn_components.py:261] mlp: residual
I0123 22:43:57.861023 140066259943424 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:57.861105 140066259943424 transformer_base.py:443] tbase: final FFN
I0123 22:43:57.861536 140066259943424 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:43:58.002466 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:43:58.004634 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.004791 140066259943424 nn_components.py:261] mlp: residual
I0123 22:43:58.005107 140066259943424 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.005204 140066259943424 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 22:43:58.005317 140066259943424 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:43:58.005359 140066259943424 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:43:58.005391 140066259943424 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:43:58.007398 140066259943424 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.009882 140066259943424 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:43:58.015558 140066259943424 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.015833 140066259943424 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:43:58.018518 140066259943424 transformer_layer.py:213] tlayer: windowed attention.
I0123 22:43:58.022446 140066259943424 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:43:58.022506 140066259943424 attention.py:418] Single window, no scan.
I0123 22:43:58.022544 140066259943424 transformer_layer.py:389] tlayer: self-attention.
I0123 22:43:58.022578 140066259943424 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.022642 140066259943424 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.023257 140066259943424 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.023338 140066259943424 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.023705 140066259943424 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.024504 140066259943424 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.027120 140066259943424 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.027765 140066259943424 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.027848 140066259943424 transformer_layer.py:468] tlayer: End windows.
I0123 22:43:58.027884 140066259943424 transformer_layer.py:472] tlayer: final FFN.
I0123 22:43:58.027945 140066259943424 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.028074 140066259943424 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:43:58.028406 140066259943424 nn_components.py:325] mlp: activation = None
I0123 22:43:58.028451 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:43:58.030347 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.030446 140066259943424 nn_components.py:261] mlp: residual
I0123 22:43:58.032946 140066259943424 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.033030 140066259943424 transformer_base.py:443] tbase: final FFN
I0123 22:43:58.033516 140066259943424 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:43:58.035814 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:43:58.037742 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.037852 140066259943424 nn_components.py:261] mlp: residual
I0123 22:43:58.038148 140066259943424 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.038233 140066259943424 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 22:43:58.038347 140066259943424 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:43:58.038389 140066259943424 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:43:58.038422 140066259943424 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:43:58.040327 140066259943424 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.042710 140066259943424 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:43:58.048334 140066259943424 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.048608 140066259943424 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:43:58.051469 140066259943424 transformer_layer.py:213] tlayer: windowed attention.
I0123 22:43:58.055453 140066259943424 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:43:58.055512 140066259943424 attention.py:418] Single window, no scan.
I0123 22:43:58.055549 140066259943424 transformer_layer.py:389] tlayer: self-attention.
I0123 22:43:58.055582 140066259943424 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.055643 140066259943424 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.056208 140066259943424 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.056287 140066259943424 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.056649 140066259943424 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.057413 140066259943424 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.059956 140066259943424 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.060573 140066259943424 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.060652 140066259943424 transformer_layer.py:468] tlayer: End windows.
I0123 22:43:58.060689 140066259943424 transformer_layer.py:472] tlayer: final FFN.
I0123 22:43:58.060747 140066259943424 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.060876 140066259943424 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:43:58.061199 140066259943424 nn_components.py:325] mlp: activation = None
I0123 22:43:58.061244 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:43:58.063143 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.063239 140066259943424 nn_components.py:261] mlp: residual
I0123 22:43:58.065779 140066259943424 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.065863 140066259943424 transformer_base.py:443] tbase: final FFN
I0123 22:43:58.066294 140066259943424 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:43:58.068567 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:43:58.070473 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.070570 140066259943424 nn_components.py:261] mlp: residual
I0123 22:43:58.070856 140066259943424 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.070947 140066259943424 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 22:43:58.071059 140066259943424 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:43:58.071100 140066259943424 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:43:58.071131 140066259943424 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:43:58.073020 140066259943424 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.075377 140066259943424 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:43:58.081307 140066259943424 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.081573 140066259943424 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:43:58.084262 140066259943424 transformer_layer.py:213] tlayer: windowed attention.
I0123 22:43:58.088023 140066259943424 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:43:58.088083 140066259943424 attention.py:418] Single window, no scan.
I0123 22:43:58.088120 140066259943424 transformer_layer.py:389] tlayer: self-attention.
I0123 22:43:58.088152 140066259943424 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.088215 140066259943424 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.088778 140066259943424 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.088861 140066259943424 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.089229 140066259943424 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.090054 140066259943424 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.092541 140066259943424 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.093173 140066259943424 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.093253 140066259943424 transformer_layer.py:468] tlayer: End windows.
I0123 22:43:58.093289 140066259943424 transformer_layer.py:472] tlayer: final FFN.
I0123 22:43:58.093348 140066259943424 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.093479 140066259943424 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:43:58.093808 140066259943424 nn_components.py:325] mlp: activation = None
I0123 22:43:58.093855 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:43:58.095745 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.095842 140066259943424 nn_components.py:261] mlp: residual
I0123 22:43:58.098396 140066259943424 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.098480 140066259943424 transformer_base.py:443] tbase: final FFN
I0123 22:43:58.098914 140066259943424 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:43:58.101186 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:43:58.103127 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.103227 140066259943424 nn_components.py:261] mlp: residual
I0123 22:43:58.103518 140066259943424 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.103613 140066259943424 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 22:43:58.103727 140066259943424 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:43:58.103767 140066259943424 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:43:58.103798 140066259943424 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:43:58.105625 140066259943424 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.108048 140066259943424 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:43:58.113575 140066259943424 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.113849 140066259943424 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:43:58.116514 140066259943424 transformer_layer.py:213] tlayer: windowed attention.
I0123 22:43:58.120234 140066259943424 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:43:58.120295 140066259943424 attention.py:418] Single window, no scan.
I0123 22:43:58.120333 140066259943424 transformer_layer.py:389] tlayer: self-attention.
I0123 22:43:58.120364 140066259943424 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.120467 140066259943424 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.121028 140066259943424 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.121107 140066259943424 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.121471 140066259943424 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.122251 140066259943424 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.124726 140066259943424 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.125344 140066259943424 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.125423 140066259943424 transformer_layer.py:468] tlayer: End windows.
I0123 22:43:58.125458 140066259943424 transformer_layer.py:472] tlayer: final FFN.
I0123 22:43:58.125517 140066259943424 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.125650 140066259943424 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:43:58.125972 140066259943424 nn_components.py:325] mlp: activation = None
I0123 22:43:58.126018 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:43:58.127969 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.128068 140066259943424 nn_components.py:261] mlp: residual
I0123 22:43:58.130810 140066259943424 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.130894 140066259943424 transformer_base.py:443] tbase: final FFN
I0123 22:43:58.131334 140066259943424 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:43:58.133652 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:43:58.135537 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.135635 140066259943424 nn_components.py:261] mlp: residual
I0123 22:43:58.135919 140066259943424 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.136002 140066259943424 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 22:43:58.136120 140066259943424 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:43:58.136161 140066259943424 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:43:58.136192 140066259943424 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:43:58.138019 140066259943424 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.140430 140066259943424 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:43:58.145990 140066259943424 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.146256 140066259943424 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:43:58.148856 140066259943424 transformer_layer.py:213] tlayer: windowed attention.
I0123 22:43:58.153117 140066259943424 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:43:58.153177 140066259943424 attention.py:418] Single window, no scan.
I0123 22:43:58.153215 140066259943424 transformer_layer.py:389] tlayer: self-attention.
I0123 22:43:58.153248 140066259943424 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.153311 140066259943424 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.153873 140066259943424 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.153953 140066259943424 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.154309 140066259943424 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.155081 140066259943424 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.157536 140066259943424 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.158156 140066259943424 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.158240 140066259943424 transformer_layer.py:468] tlayer: End windows.
I0123 22:43:58.158276 140066259943424 transformer_layer.py:472] tlayer: final FFN.
I0123 22:43:58.158341 140066259943424 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.158474 140066259943424 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:43:58.158793 140066259943424 nn_components.py:325] mlp: activation = None
I0123 22:43:58.158839 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:43:58.160769 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.160865 140066259943424 nn_components.py:261] mlp: residual
I0123 22:43:58.163380 140066259943424 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.163463 140066259943424 transformer_base.py:443] tbase: final FFN
I0123 22:43:58.163907 140066259943424 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:43:58.166209 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:43:58.168093 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.168192 140066259943424 nn_components.py:261] mlp: residual
I0123 22:43:58.168485 140066259943424 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.168775 140066259943424 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 22:43:58.168855 140066259943424 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 22:43:58.168923 140066259943424 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 22:43:58.168981 140066259943424 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 22:43:58.169036 140066259943424 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 22:43:58.169090 140066259943424 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 22:43:58.169145 140066259943424 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 22:43:58.169199 140066259943424 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 22:43:58.169252 140066259943424 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 22:43:58.169306 140066259943424 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 22:43:58.169360 140066259943424 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 22:43:58.169414 140066259943424 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 22:43:58.169453 140066259943424 decoder_stack.py:344] dstack: Final layernorm.
I0123 22:43:58.172933 140066259943424 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:43:58.220371 140066259943424 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.220462 140066259943424 decoder_stack.py:333] dstack: autoregressive generator.
I0123 22:43:58.220518 140066259943424 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 22:43:58.220622 140066259943424 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:43:58.220662 140066259943424 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:43:58.220692 140066259943424 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:43:58.220756 140066259943424 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.223188 140066259943424 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:43:58.228669 140066259943424 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.228932 140066259943424 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:43:58.231676 140066259943424 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:43:58.248262 140066259943424 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:43:58.248323 140066259943424 attention.py:418] Single window, no scan.
I0123 22:43:58.248363 140066259943424 transformer_layer.py:389] tlayer: self-attention.
I0123 22:43:58.248397 140066259943424 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.248461 140066259943424 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.249590 140066259943424 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.249682 140066259943424 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.250394 140066259943424 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.252398 140066259943424 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.257137 140066259943424 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.258449 140066259943424 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.258542 140066259943424 transformer_layer.py:468] tlayer: End windows.
I0123 22:43:58.258581 140066259943424 transformer_layer.py:472] tlayer: final FFN.
I0123 22:43:58.258642 140066259943424 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.258777 140066259943424 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:43:58.258890 140066259943424 nn_components.py:325] mlp: activation = None
I0123 22:43:58.258931 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:43:58.260824 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.260922 140066259943424 nn_components.py:261] mlp: residual
I0123 22:43:58.263345 140066259943424 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.263432 140066259943424 transformer_base.py:443] tbase: final FFN
I0123 22:43:58.263545 140066259943424 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:43:58.265779 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:43:58.267727 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.267827 140066259943424 nn_components.py:261] mlp: residual
I0123 22:43:58.268120 140066259943424 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.268206 140066259943424 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 22:43:58.268316 140066259943424 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:43:58.268355 140066259943424 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:43:58.268387 140066259943424 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:43:58.268450 140066259943424 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.270713 140066259943424 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:43:58.276184 140066259943424 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.276445 140066259943424 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:43:58.279126 140066259943424 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:43:58.292208 140066259943424 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:43:58.292268 140066259943424 attention.py:418] Single window, no scan.
I0123 22:43:58.292305 140066259943424 transformer_layer.py:389] tlayer: self-attention.
I0123 22:43:58.292338 140066259943424 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.292400 140066259943424 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.292959 140066259943424 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.293040 140066259943424 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.293409 140066259943424 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.294115 140066259943424 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.296609 140066259943424 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.297230 140066259943424 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.297313 140066259943424 transformer_layer.py:468] tlayer: End windows.
I0123 22:43:58.297357 140066259943424 transformer_layer.py:472] tlayer: final FFN.
I0123 22:43:58.297417 140066259943424 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.297547 140066259943424 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:43:58.297666 140066259943424 nn_components.py:325] mlp: activation = None
I0123 22:43:58.297707 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:43:58.299631 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.299728 140066259943424 nn_components.py:261] mlp: residual
I0123 22:43:58.302129 140066259943424 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.302214 140066259943424 transformer_base.py:443] tbase: final FFN
I0123 22:43:58.302324 140066259943424 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:43:58.304530 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:43:58.306460 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.306560 140066259943424 nn_components.py:261] mlp: residual
I0123 22:43:58.306849 140066259943424 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.306935 140066259943424 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 22:43:58.307044 140066259943424 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:43:58.307085 140066259943424 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:43:58.307117 140066259943424 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:43:58.307179 140066259943424 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.309418 140066259943424 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:43:58.314869 140066259943424 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.315130 140066259943424 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:43:58.317809 140066259943424 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:43:58.330469 140066259943424 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:43:58.330528 140066259943424 attention.py:418] Single window, no scan.
I0123 22:43:58.330565 140066259943424 transformer_layer.py:389] tlayer: self-attention.
I0123 22:43:58.330597 140066259943424 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.330660 140066259943424 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.331222 140066259943424 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.331303 140066259943424 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.331667 140066259943424 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.332358 140066259943424 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.334837 140066259943424 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.335467 140066259943424 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.335548 140066259943424 transformer_layer.py:468] tlayer: End windows.
I0123 22:43:58.335585 140066259943424 transformer_layer.py:472] tlayer: final FFN.
I0123 22:43:58.335650 140066259943424 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.335780 140066259943424 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:43:58.335890 140066259943424 nn_components.py:325] mlp: activation = None
I0123 22:43:58.335931 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:43:58.337847 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.337944 140066259943424 nn_components.py:261] mlp: residual
I0123 22:43:58.340350 140066259943424 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.340433 140066259943424 transformer_base.py:443] tbase: final FFN
I0123 22:43:58.340542 140066259943424 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:43:58.342745 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:43:58.344652 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.344751 140066259943424 nn_components.py:261] mlp: residual
I0123 22:43:58.345037 140066259943424 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.345122 140066259943424 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 22:43:58.345232 140066259943424 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:43:58.345273 140066259943424 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:43:58.345306 140066259943424 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:43:58.345369 140066259943424 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.347603 140066259943424 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:43:58.353022 140066259943424 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.353284 140066259943424 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:43:58.355979 140066259943424 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:43:58.368707 140066259943424 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:43:58.368766 140066259943424 attention.py:418] Single window, no scan.
I0123 22:43:58.368803 140066259943424 transformer_layer.py:389] tlayer: self-attention.
I0123 22:43:58.368836 140066259943424 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.368899 140066259943424 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.369456 140066259943424 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.369535 140066259943424 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.369901 140066259943424 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.370601 140066259943424 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.373055 140066259943424 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.373680 140066259943424 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.373762 140066259943424 transformer_layer.py:468] tlayer: End windows.
I0123 22:43:58.373798 140066259943424 transformer_layer.py:472] tlayer: final FFN.
I0123 22:43:58.373859 140066259943424 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.374001 140066259943424 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:43:58.374113 140066259943424 nn_components.py:325] mlp: activation = None
I0123 22:43:58.374152 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:43:58.376355 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.376453 140066259943424 nn_components.py:261] mlp: residual
I0123 22:43:58.378844 140066259943424 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.378928 140066259943424 transformer_base.py:443] tbase: final FFN
I0123 22:43:58.379037 140066259943424 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:43:58.381250 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:43:58.383125 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.383224 140066259943424 nn_components.py:261] mlp: residual
I0123 22:43:58.383511 140066259943424 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.383596 140066259943424 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 22:43:58.383706 140066259943424 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:43:58.383747 140066259943424 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:43:58.383778 140066259943424 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:43:58.383840 140066259943424 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.389899 140066259943424 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:43:58.395554 140066259943424 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.395845 140066259943424 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:43:58.398525 140066259943424 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:43:58.411449 140066259943424 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:43:58.411511 140066259943424 attention.py:418] Single window, no scan.
I0123 22:43:58.411550 140066259943424 transformer_layer.py:389] tlayer: self-attention.
I0123 22:43:58.411582 140066259943424 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.411645 140066259943424 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.412245 140066259943424 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.412324 140066259943424 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.412690 140066259943424 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.413388 140066259943424 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.415978 140066259943424 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.416605 140066259943424 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.416685 140066259943424 transformer_layer.py:468] tlayer: End windows.
I0123 22:43:58.416721 140066259943424 transformer_layer.py:472] tlayer: final FFN.
I0123 22:43:58.416781 140066259943424 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.416920 140066259943424 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:43:58.417036 140066259943424 nn_components.py:325] mlp: activation = None
I0123 22:43:58.417077 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:43:58.418951 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.419048 140066259943424 nn_components.py:261] mlp: residual
I0123 22:43:58.421480 140066259943424 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.421563 140066259943424 transformer_base.py:443] tbase: final FFN
I0123 22:43:58.421684 140066259943424 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:43:58.423987 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:43:58.425868 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.425966 140066259943424 nn_components.py:261] mlp: residual
I0123 22:43:58.426252 140066259943424 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.426337 140066259943424 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 22:43:58.426446 140066259943424 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:43:58.426489 140066259943424 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:43:58.426521 140066259943424 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:43:58.426583 140066259943424 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.428844 140066259943424 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:43:58.434253 140066259943424 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.434512 140066259943424 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:43:58.437176 140066259943424 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:43:58.449811 140066259943424 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:43:58.449870 140066259943424 attention.py:418] Single window, no scan.
I0123 22:43:58.449907 140066259943424 transformer_layer.py:389] tlayer: self-attention.
I0123 22:43:58.449939 140066259943424 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.450000 140066259943424 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.450563 140066259943424 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.450645 140066259943424 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.451006 140066259943424 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.451704 140066259943424 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.454177 140066259943424 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.454800 140066259943424 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.454882 140066259943424 transformer_layer.py:468] tlayer: End windows.
I0123 22:43:58.454918 140066259943424 transformer_layer.py:472] tlayer: final FFN.
I0123 22:43:58.454978 140066259943424 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.455106 140066259943424 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:43:58.455223 140066259943424 nn_components.py:325] mlp: activation = None
I0123 22:43:58.455265 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:43:58.457363 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.457460 140066259943424 nn_components.py:261] mlp: residual
I0123 22:43:58.459869 140066259943424 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.459952 140066259943424 transformer_base.py:443] tbase: final FFN
I0123 22:43:58.460067 140066259943424 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:43:58.462288 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:43:58.464154 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.464252 140066259943424 nn_components.py:261] mlp: residual
I0123 22:43:58.464539 140066259943424 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.464624 140066259943424 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 22:43:58.464732 140066259943424 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:43:58.464771 140066259943424 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:43:58.464804 140066259943424 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:43:58.464866 140066259943424 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.467096 140066259943424 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:43:58.472623 140066259943424 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.472880 140066259943424 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:43:58.475497 140066259943424 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:43:58.488567 140066259943424 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:43:58.488629 140066259943424 attention.py:418] Single window, no scan.
I0123 22:43:58.488665 140066259943424 transformer_layer.py:389] tlayer: self-attention.
I0123 22:43:58.488698 140066259943424 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.488764 140066259943424 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.489324 140066259943424 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.489403 140066259943424 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.489768 140066259943424 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.490458 140066259943424 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.492922 140066259943424 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.493582 140066259943424 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.493669 140066259943424 transformer_layer.py:468] tlayer: End windows.
I0123 22:43:58.493705 140066259943424 transformer_layer.py:472] tlayer: final FFN.
I0123 22:43:58.493765 140066259943424 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.493897 140066259943424 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:43:58.494006 140066259943424 nn_components.py:325] mlp: activation = None
I0123 22:43:58.494051 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:43:58.495930 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.496027 140066259943424 nn_components.py:261] mlp: residual
I0123 22:43:58.498428 140066259943424 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.498513 140066259943424 transformer_base.py:443] tbase: final FFN
I0123 22:43:58.498621 140066259943424 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:43:58.500810 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:43:58.502751 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.502850 140066259943424 nn_components.py:261] mlp: residual
I0123 22:43:58.503136 140066259943424 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.503221 140066259943424 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 22:43:58.503331 140066259943424 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:43:58.503373 140066259943424 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:43:58.503405 140066259943424 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:43:58.503470 140066259943424 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.505699 140066259943424 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:43:58.511125 140066259943424 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.511394 140066259943424 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:43:58.514077 140066259943424 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:43:58.526679 140066259943424 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:43:58.526740 140066259943424 attention.py:418] Single window, no scan.
I0123 22:43:58.526778 140066259943424 transformer_layer.py:389] tlayer: self-attention.
I0123 22:43:58.526811 140066259943424 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.526873 140066259943424 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.527475 140066259943424 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.527554 140066259943424 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.527914 140066259943424 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.528604 140066259943424 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.531064 140066259943424 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.531691 140066259943424 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.531771 140066259943424 transformer_layer.py:468] tlayer: End windows.
I0123 22:43:58.531806 140066259943424 transformer_layer.py:472] tlayer: final FFN.
I0123 22:43:58.531865 140066259943424 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.531992 140066259943424 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:43:58.532106 140066259943424 nn_components.py:325] mlp: activation = None
I0123 22:43:58.532153 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:43:58.534027 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.534125 140066259943424 nn_components.py:261] mlp: residual
I0123 22:43:58.536572 140066259943424 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.536656 140066259943424 transformer_base.py:443] tbase: final FFN
I0123 22:43:58.536765 140066259943424 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:43:58.538971 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:43:58.540824 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.540922 140066259943424 nn_components.py:261] mlp: residual
I0123 22:43:58.541208 140066259943424 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.541294 140066259943424 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 22:43:58.541404 140066259943424 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:43:58.541444 140066259943424 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:43:58.541476 140066259943424 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:43:58.541539 140066259943424 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.543755 140066259943424 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:43:58.549306 140066259943424 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.549567 140066259943424 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:43:58.552264 140066259943424 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:43:58.565109 140066259943424 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:43:58.565168 140066259943424 attention.py:418] Single window, no scan.
I0123 22:43:58.565205 140066259943424 transformer_layer.py:389] tlayer: self-attention.
I0123 22:43:58.565235 140066259943424 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.565297 140066259943424 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.565863 140066259943424 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.565943 140066259943424 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.566299 140066259943424 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.566983 140066259943424 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.569463 140066259943424 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.570139 140066259943424 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.570220 140066259943424 transformer_layer.py:468] tlayer: End windows.
I0123 22:43:58.570256 140066259943424 transformer_layer.py:472] tlayer: final FFN.
I0123 22:43:58.570317 140066259943424 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.570444 140066259943424 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:43:58.570552 140066259943424 nn_components.py:325] mlp: activation = None
I0123 22:43:58.570591 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:43:58.572479 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.572576 140066259943424 nn_components.py:261] mlp: residual
I0123 22:43:58.574967 140066259943424 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.575050 140066259943424 transformer_base.py:443] tbase: final FFN
I0123 22:43:58.575160 140066259943424 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:43:58.577354 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:43:58.579284 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.579384 140066259943424 nn_components.py:261] mlp: residual
I0123 22:43:58.579668 140066259943424 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.579752 140066259943424 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 22:43:58.579860 140066259943424 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:43:58.579900 140066259943424 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:43:58.579931 140066259943424 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:43:58.579993 140066259943424 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.582244 140066259943424 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:43:58.587607 140066259943424 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.587867 140066259943424 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:43:58.590808 140066259943424 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:43:58.603244 140066259943424 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:43:58.603302 140066259943424 attention.py:418] Single window, no scan.
I0123 22:43:58.603338 140066259943424 transformer_layer.py:389] tlayer: self-attention.
I0123 22:43:58.603370 140066259943424 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.603431 140066259943424 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.604030 140066259943424 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.604110 140066259943424 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.604469 140066259943424 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.605150 140066259943424 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.607594 140066259943424 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.608213 140066259943424 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.608293 140066259943424 transformer_layer.py:468] tlayer: End windows.
I0123 22:43:58.608329 140066259943424 transformer_layer.py:472] tlayer: final FFN.
I0123 22:43:58.608388 140066259943424 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.608521 140066259943424 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:43:58.608629 140066259943424 nn_components.py:325] mlp: activation = None
I0123 22:43:58.608669 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:43:58.610568 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.610679 140066259943424 nn_components.py:261] mlp: residual
I0123 22:43:58.613160 140066259943424 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.613243 140066259943424 transformer_base.py:443] tbase: final FFN
I0123 22:43:58.613351 140066259943424 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:43:58.615606 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:43:58.617451 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.617550 140066259943424 nn_components.py:261] mlp: residual
I0123 22:43:58.617838 140066259943424 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.617924 140066259943424 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 22:43:58.618033 140066259943424 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:43:58.618074 140066259943424 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:43:58.618105 140066259943424 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:43:58.618168 140066259943424 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.620410 140066259943424 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:43:58.625883 140066259943424 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.626145 140066259943424 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:43:58.628823 140066259943424 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:43:58.641387 140066259943424 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:43:58.641447 140066259943424 attention.py:418] Single window, no scan.
I0123 22:43:58.641484 140066259943424 transformer_layer.py:389] tlayer: self-attention.
I0123 22:43:58.641516 140066259943424 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.641578 140066259943424 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.642151 140066259943424 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.642231 140066259943424 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.642590 140066259943424 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.643281 140066259943424 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.645744 140066259943424 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.646404 140066259943424 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.646483 140066259943424 transformer_layer.py:468] tlayer: End windows.
I0123 22:43:58.646519 140066259943424 transformer_layer.py:472] tlayer: final FFN.
I0123 22:43:58.646578 140066259943424 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.646709 140066259943424 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:43:58.646818 140066259943424 nn_components.py:325] mlp: activation = None
I0123 22:43:58.646858 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:43:58.648711 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.648815 140066259943424 nn_components.py:261] mlp: residual
I0123 22:43:58.651219 140066259943424 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.651304 140066259943424 transformer_base.py:443] tbase: final FFN
I0123 22:43:58.651414 140066259943424 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:43:58.653602 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:43:58.655509 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.655610 140066259943424 nn_components.py:261] mlp: residual
I0123 22:43:58.655896 140066259943424 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.655980 140066259943424 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 22:43:58.656089 140066259943424 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:43:58.656130 140066259943424 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:43:58.656162 140066259943424 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:43:58.656223 140066259943424 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.658638 140066259943424 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:43:58.664025 140066259943424 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.664287 140066259943424 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:43:58.666874 140066259943424 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:43:58.679452 140066259943424 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:43:58.679514 140066259943424 attention.py:418] Single window, no scan.
I0123 22:43:58.679552 140066259943424 transformer_layer.py:389] tlayer: self-attention.
I0123 22:43:58.679586 140066259943424 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.679650 140066259943424 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.680212 140066259943424 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.680291 140066259943424 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.680646 140066259943424 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.681335 140066259943424 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.683863 140066259943424 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.684482 140066259943424 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.684563 140066259943424 transformer_layer.py:468] tlayer: End windows.
I0123 22:43:58.684600 140066259943424 transformer_layer.py:472] tlayer: final FFN.
I0123 22:43:58.684659 140066259943424 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.684791 140066259943424 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:43:58.684905 140066259943424 nn_components.py:325] mlp: activation = None
I0123 22:43:58.684945 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:43:58.686964 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.687062 140066259943424 nn_components.py:261] mlp: residual
I0123 22:43:58.689451 140066259943424 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.689534 140066259943424 transformer_base.py:443] tbase: final FFN
I0123 22:43:58.689649 140066259943424 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:43:58.692220 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:43:58.694072 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.694171 140066259943424 nn_components.py:261] mlp: residual
I0123 22:43:58.694454 140066259943424 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.694548 140066259943424 decoder_stack.py:344] dstack: Final layernorm.
I0123 22:43:58.697399 140066259943424 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:43:58.752581 140066259943424 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.752673 140066259943424 decoder_stack.py:333] dstack: autoregressive generator.
I0123 22:43:58.752729 140066259943424 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 22:43:58.752838 140066259943424 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:43:58.752879 140066259943424 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:43:58.752910 140066259943424 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:43:58.752973 140066259943424 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.755312 140066259943424 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:43:58.760664 140066259943424 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.760923 140066259943424 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:43:58.763475 140066259943424 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:43:58.775752 140066259943424 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:43:58.775810 140066259943424 attention.py:418] Single window, no scan.
I0123 22:43:58.775847 140066259943424 transformer_layer.py:389] tlayer: self-attention.
I0123 22:43:58.775880 140066259943424 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.775942 140066259943424 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.776496 140066259943424 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.776573 140066259943424 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.776925 140066259943424 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.777599 140066259943424 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.780066 140066259943424 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.780674 140066259943424 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.780754 140066259943424 transformer_layer.py:468] tlayer: End windows.
I0123 22:43:58.780789 140066259943424 transformer_layer.py:472] tlayer: final FFN.
I0123 22:43:58.780847 140066259943424 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.780972 140066259943424 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:43:58.781090 140066259943424 nn_components.py:325] mlp: activation = None
I0123 22:43:58.781131 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:43:58.782957 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.783055 140066259943424 nn_components.py:261] mlp: residual
I0123 22:43:58.785600 140066259943424 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.785690 140066259943424 transformer_base.py:443] tbase: final FFN
I0123 22:43:58.785800 140066259943424 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:43:58.788178 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:43:58.790028 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.790128 140066259943424 nn_components.py:261] mlp: residual
I0123 22:43:58.790414 140066259943424 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.790499 140066259943424 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 22:43:58.790608 140066259943424 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:43:58.790649 140066259943424 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:43:58.790681 140066259943424 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:43:58.790745 140066259943424 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.792962 140066259943424 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:43:58.798305 140066259943424 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.798568 140066259943424 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:43:58.801197 140066259943424 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:43:58.813430 140066259943424 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:43:58.813490 140066259943424 attention.py:418] Single window, no scan.
I0123 22:43:58.813527 140066259943424 transformer_layer.py:389] tlayer: self-attention.
I0123 22:43:58.813559 140066259943424 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.813622 140066259943424 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.814206 140066259943424 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.814285 140066259943424 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.814642 140066259943424 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.815325 140066259943424 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.817831 140066259943424 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.818450 140066259943424 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.818531 140066259943424 transformer_layer.py:468] tlayer: End windows.
I0123 22:43:58.818567 140066259943424 transformer_layer.py:472] tlayer: final FFN.
I0123 22:43:58.818627 140066259943424 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.818756 140066259943424 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:43:58.818865 140066259943424 nn_components.py:325] mlp: activation = None
I0123 22:43:58.818913 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:43:58.820752 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.820849 140066259943424 nn_components.py:261] mlp: residual
I0123 22:43:58.823245 140066259943424 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.823328 140066259943424 transformer_base.py:443] tbase: final FFN
I0123 22:43:58.823438 140066259943424 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:43:58.825678 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:43:58.827497 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.827595 140066259943424 nn_components.py:261] mlp: residual
I0123 22:43:58.827878 140066259943424 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.827963 140066259943424 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 22:43:58.828073 140066259943424 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:43:58.828113 140066259943424 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:43:58.828145 140066259943424 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:43:58.828208 140066259943424 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.830407 140066259943424 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:43:58.835720 140066259943424 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.835977 140066259943424 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:43:58.838591 140066259943424 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:43:58.850792 140066259943424 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:43:58.850853 140066259943424 attention.py:418] Single window, no scan.
I0123 22:43:58.850890 140066259943424 transformer_layer.py:389] tlayer: self-attention.
I0123 22:43:58.850923 140066259943424 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.850985 140066259943424 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.851534 140066259943424 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.851612 140066259943424 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.851965 140066259943424 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.852641 140066259943424 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.855557 140066259943424 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.856172 140066259943424 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.856252 140066259943424 transformer_layer.py:468] tlayer: End windows.
I0123 22:43:58.856289 140066259943424 transformer_layer.py:472] tlayer: final FFN.
I0123 22:43:58.856347 140066259943424 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.856475 140066259943424 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:43:58.856584 140066259943424 nn_components.py:325] mlp: activation = None
I0123 22:43:58.856623 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:43:58.858455 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.858553 140066259943424 nn_components.py:261] mlp: residual
I0123 22:43:58.860923 140066259943424 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.861007 140066259943424 transformer_base.py:443] tbase: final FFN
I0123 22:43:58.861116 140066259943424 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:43:58.863405 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:43:58.865243 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.865341 140066259943424 nn_components.py:261] mlp: residual
I0123 22:43:58.865628 140066259943424 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.865722 140066259943424 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 22:43:58.865833 140066259943424 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:43:58.865874 140066259943424 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:43:58.865906 140066259943424 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:43:58.865968 140066259943424 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.868172 140066259943424 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:43:58.873551 140066259943424 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.873817 140066259943424 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:43:58.876458 140066259943424 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:43:58.889014 140066259943424 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:43:58.889075 140066259943424 attention.py:418] Single window, no scan.
I0123 22:43:58.889115 140066259943424 transformer_layer.py:389] tlayer: self-attention.
I0123 22:43:58.889162 140066259943424 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.889228 140066259943424 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.889788 140066259943424 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.889868 140066259943424 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.890227 140066259943424 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.890932 140066259943424 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.893467 140066259943424 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.894093 140066259943424 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.894178 140066259943424 transformer_layer.py:468] tlayer: End windows.
I0123 22:43:58.894214 140066259943424 transformer_layer.py:472] tlayer: final FFN.
I0123 22:43:58.894273 140066259943424 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.894408 140066259943424 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:43:58.894521 140066259943424 nn_components.py:325] mlp: activation = None
I0123 22:43:58.894564 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:43:58.896461 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.896557 140066259943424 nn_components.py:261] mlp: residual
I0123 22:43:58.898966 140066259943424 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.899052 140066259943424 transformer_base.py:443] tbase: final FFN
I0123 22:43:58.899328 140066259943424 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:43:58.901668 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:43:58.903582 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.903682 140066259943424 nn_components.py:261] mlp: residual
I0123 22:43:58.903970 140066259943424 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.904053 140066259943424 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 22:43:58.904161 140066259943424 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:43:58.904201 140066259943424 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:43:58.904232 140066259943424 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:43:58.904294 140066259943424 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.906527 140066259943424 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:43:58.911936 140066259943424 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.912196 140066259943424 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:43:58.914885 140066259943424 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:43:58.927358 140066259943424 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:43:58.927415 140066259943424 attention.py:418] Single window, no scan.
I0123 22:43:58.927451 140066259943424 transformer_layer.py:389] tlayer: self-attention.
I0123 22:43:58.927482 140066259943424 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.927544 140066259943424 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.928098 140066259943424 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.928176 140066259943424 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.928531 140066259943424 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.929213 140066259943424 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.931745 140066259943424 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.932363 140066259943424 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.932442 140066259943424 transformer_layer.py:468] tlayer: End windows.
I0123 22:43:58.932476 140066259943424 transformer_layer.py:472] tlayer: final FFN.
I0123 22:43:58.932534 140066259943424 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.932661 140066259943424 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:43:58.932770 140066259943424 nn_components.py:325] mlp: activation = None
I0123 22:43:58.932808 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:43:58.934672 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.934774 140066259943424 nn_components.py:261] mlp: residual
I0123 22:43:58.937155 140066259943424 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.937235 140066259943424 transformer_base.py:443] tbase: final FFN
I0123 22:43:58.937344 140066259943424 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:43:58.939612 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:43:58.941465 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.941562 140066259943424 nn_components.py:261] mlp: residual
I0123 22:43:58.941856 140066259943424 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.941943 140066259943424 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 22:43:58.942052 140066259943424 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:43:58.942091 140066259943424 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:43:58.942122 140066259943424 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:43:58.942185 140066259943424 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.944425 140066259943424 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:43:58.949805 140066259943424 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.950065 140066259943424 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:43:58.952715 140066259943424 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:43:58.965125 140066259943424 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:43:58.965184 140066259943424 attention.py:418] Single window, no scan.
I0123 22:43:58.965220 140066259943424 transformer_layer.py:389] tlayer: self-attention.
I0123 22:43:58.965250 140066259943424 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.965317 140066259943424 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.965879 140066259943424 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.965958 140066259943424 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.966315 140066259943424 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.967010 140066259943424 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.969932 140066259943424 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.970546 140066259943424 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.970625 140066259943424 transformer_layer.py:468] tlayer: End windows.
I0123 22:43:58.970660 140066259943424 transformer_layer.py:472] tlayer: final FFN.
I0123 22:43:58.970717 140066259943424 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.970844 140066259943424 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:43:58.970951 140066259943424 nn_components.py:325] mlp: activation = None
I0123 22:43:58.970990 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:43:58.972849 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.972951 140066259943424 nn_components.py:261] mlp: residual
I0123 22:43:58.975335 140066259943424 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.975417 140066259943424 transformer_base.py:443] tbase: final FFN
I0123 22:43:58.975525 140066259943424 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:43:58.977782 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:43:58.979655 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.979750 140066259943424 nn_components.py:261] mlp: residual
I0123 22:43:58.980035 140066259943424 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.980119 140066259943424 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 22:43:58.980226 140066259943424 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:43:58.980264 140066259943424 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:43:58.980294 140066259943424 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:43:58.980355 140066259943424 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.982583 140066259943424 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:43:58.987928 140066259943424 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:58.988189 140066259943424 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:43:58.990857 140066259943424 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:43:59.003445 140066259943424 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:43:59.003503 140066259943424 attention.py:418] Single window, no scan.
I0123 22:43:59.003538 140066259943424 transformer_layer.py:389] tlayer: self-attention.
I0123 22:43:59.003569 140066259943424 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.003631 140066259943424 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.004202 140066259943424 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.004281 140066259943424 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.004638 140066259943424 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.005324 140066259943424 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.007836 140066259943424 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.008459 140066259943424 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.008538 140066259943424 transformer_layer.py:468] tlayer: End windows.
I0123 22:43:59.008572 140066259943424 transformer_layer.py:472] tlayer: final FFN.
I0123 22:43:59.008629 140066259943424 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.008754 140066259943424 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:43:59.008861 140066259943424 nn_components.py:325] mlp: activation = None
I0123 22:43:59.008900 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:43:59.010748 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.010844 140066259943424 nn_components.py:261] mlp: residual
I0123 22:43:59.013215 140066259943424 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.013298 140066259943424 transformer_base.py:443] tbase: final FFN
I0123 22:43:59.013406 140066259943424 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:43:59.015671 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:43:59.017514 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.017610 140066259943424 nn_components.py:261] mlp: residual
I0123 22:43:59.017901 140066259943424 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.017989 140066259943424 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 22:43:59.018098 140066259943424 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:43:59.018138 140066259943424 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:43:59.018168 140066259943424 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:43:59.018230 140066259943424 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.020455 140066259943424 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:43:59.025839 140066259943424 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.026101 140066259943424 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:43:59.028769 140066259943424 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:43:59.041213 140066259943424 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:43:59.041271 140066259943424 attention.py:418] Single window, no scan.
I0123 22:43:59.041307 140066259943424 transformer_layer.py:389] tlayer: self-attention.
I0123 22:43:59.041338 140066259943424 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.041399 140066259943424 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.041966 140066259943424 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.042046 140066259943424 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.042401 140066259943424 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.043095 140066259943424 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.045609 140066259943424 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.046246 140066259943424 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.046326 140066259943424 transformer_layer.py:468] tlayer: End windows.
I0123 22:43:59.046360 140066259943424 transformer_layer.py:472] tlayer: final FFN.
I0123 22:43:59.046417 140066259943424 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.046544 140066259943424 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:43:59.046651 140066259943424 nn_components.py:325] mlp: activation = None
I0123 22:43:59.046689 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:43:59.048532 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.048628 140066259943424 nn_components.py:261] mlp: residual
I0123 22:43:59.051010 140066259943424 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.051098 140066259943424 transformer_base.py:443] tbase: final FFN
I0123 22:43:59.051207 140066259943424 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:43:59.053455 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:43:59.055306 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.055405 140066259943424 nn_components.py:261] mlp: residual
I0123 22:43:59.055689 140066259943424 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.055775 140066259943424 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 22:43:59.055884 140066259943424 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:43:59.055923 140066259943424 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:43:59.055955 140066259943424 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:43:59.056016 140066259943424 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.058231 140066259943424 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:43:59.063597 140066259943424 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.063859 140066259943424 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:43:59.066516 140066259943424 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:43:59.078966 140066259943424 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:43:59.079026 140066259943424 attention.py:418] Single window, no scan.
I0123 22:43:59.079062 140066259943424 transformer_layer.py:389] tlayer: self-attention.
I0123 22:43:59.079092 140066259943424 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.079155 140066259943424 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.079715 140066259943424 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.079794 140066259943424 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.080155 140066259943424 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.080836 140066259943424 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.083766 140066259943424 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.084384 140066259943424 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.084465 140066259943424 transformer_layer.py:468] tlayer: End windows.
I0123 22:43:59.084500 140066259943424 transformer_layer.py:472] tlayer: final FFN.
I0123 22:43:59.084558 140066259943424 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.084685 140066259943424 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:43:59.084792 140066259943424 nn_components.py:325] mlp: activation = None
I0123 22:43:59.084831 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:43:59.086698 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.086794 140066259943424 nn_components.py:261] mlp: residual
I0123 22:43:59.089175 140066259943424 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.089263 140066259943424 transformer_base.py:443] tbase: final FFN
I0123 22:43:59.089373 140066259943424 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:43:59.091641 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:43:59.093474 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.093572 140066259943424 nn_components.py:261] mlp: residual
I0123 22:43:59.093863 140066259943424 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.093949 140066259943424 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 22:43:59.094056 140066259943424 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:43:59.094096 140066259943424 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:43:59.094128 140066259943424 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:43:59.094190 140066259943424 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.096398 140066259943424 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:43:59.101783 140066259943424 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.102044 140066259943424 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:43:59.104668 140066259943424 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:43:59.117101 140066259943424 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:43:59.117158 140066259943424 attention.py:418] Single window, no scan.
I0123 22:43:59.117193 140066259943424 transformer_layer.py:389] tlayer: self-attention.
I0123 22:43:59.117224 140066259943424 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.117285 140066259943424 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.117852 140066259943424 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.117931 140066259943424 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.118289 140066259943424 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.118978 140066259943424 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.121517 140066259943424 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.122144 140066259943424 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.122225 140066259943424 transformer_layer.py:468] tlayer: End windows.
I0123 22:43:59.122259 140066259943424 transformer_layer.py:472] tlayer: final FFN.
I0123 22:43:59.122318 140066259943424 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.122446 140066259943424 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:43:59.122553 140066259943424 nn_components.py:325] mlp: activation = None
I0123 22:43:59.122592 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:43:59.124937 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.125036 140066259943424 nn_components.py:261] mlp: residual
I0123 22:43:59.127398 140066259943424 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.127481 140066259943424 transformer_base.py:443] tbase: final FFN
I0123 22:43:59.127597 140066259943424 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:43:59.129826 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:43:59.131631 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.131728 140066259943424 nn_components.py:261] mlp: residual
I0123 22:43:59.132010 140066259943424 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.132096 140066259943424 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 22:43:59.132201 140066259943424 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:43:59.132240 140066259943424 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:43:59.132271 140066259943424 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:43:59.132332 140066259943424 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.134537 140066259943424 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:43:59.139868 140066259943424 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.140126 140066259943424 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:43:59.142775 140066259943424 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:43:59.155463 140066259943424 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:43:59.155522 140066259943424 attention.py:418] Single window, no scan.
I0123 22:43:59.155558 140066259943424 transformer_layer.py:389] tlayer: self-attention.
I0123 22:43:59.155589 140066259943424 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.155652 140066259943424 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.156214 140066259943424 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.156292 140066259943424 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.156649 140066259943424 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.157329 140066259943424 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.159854 140066259943424 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.160473 140066259943424 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.160552 140066259943424 transformer_layer.py:468] tlayer: End windows.
I0123 22:43:59.160587 140066259943424 transformer_layer.py:472] tlayer: final FFN.
I0123 22:43:59.160644 140066259943424 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.160775 140066259943424 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:43:59.160885 140066259943424 nn_components.py:325] mlp: activation = None
I0123 22:43:59.160924 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:43:59.162763 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.162857 140066259943424 nn_components.py:261] mlp: residual
I0123 22:43:59.165209 140066259943424 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.165294 140066259943424 transformer_base.py:443] tbase: final FFN
I0123 22:43:59.165401 140066259943424 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:43:59.167638 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:43:59.169459 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.169554 140066259943424 nn_components.py:261] mlp: residual
I0123 22:43:59.169845 140066259943424 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.169932 140066259943424 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 22:43:59.170038 140066259943424 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:43:59.170076 140066259943424 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:43:59.170106 140066259943424 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:43:59.170168 140066259943424 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.172384 140066259943424 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:43:59.177747 140066259943424 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.178011 140066259943424 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:43:59.180666 140066259943424 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:43:59.193092 140066259943424 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:43:59.193152 140066259943424 attention.py:418] Single window, no scan.
I0123 22:43:59.193188 140066259943424 transformer_layer.py:389] tlayer: self-attention.
I0123 22:43:59.193218 140066259943424 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.193280 140066259943424 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.193842 140066259943424 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.193921 140066259943424 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.194276 140066259943424 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.194957 140066259943424 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.197820 140066259943424 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.198436 140066259943424 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.198518 140066259943424 transformer_layer.py:468] tlayer: End windows.
I0123 22:43:59.198554 140066259943424 transformer_layer.py:472] tlayer: final FFN.
I0123 22:43:59.198611 140066259943424 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.198737 140066259943424 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:43:59.198849 140066259943424 nn_components.py:325] mlp: activation = None
I0123 22:43:59.198889 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:43:59.200710 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.200805 140066259943424 nn_components.py:261] mlp: residual
I0123 22:43:59.203163 140066259943424 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.203249 140066259943424 transformer_base.py:443] tbase: final FFN
I0123 22:43:59.203357 140066259943424 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:43:59.205595 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:43:59.207451 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.207549 140066259943424 nn_components.py:261] mlp: residual
I0123 22:43:59.207833 140066259943424 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:43:59.207921 140066259943424 decoder_stack.py:344] dstack: Final layernorm.
I0123 22:43:59.210720 140066259943424 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:44:03.582269 140066259943424 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
I0123 22:44:04.095384 140066259943424 training_loop.py:409] No working directory specified.
I0123 22:44:04.095516 140066259943424 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0123 22:44:04.096279 140066259943424 checkpoints.py:1062] Restoring legacy Flax checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0123 22:44:07.194046 140066259943424 training_loop.py:447] Only restoring trainable parameters.
I0123 22:44:07.194785 140066259943424 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0123 22:44:07.194846 140066259943424 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0123 22:44:07.194892 140066259943424 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 22:44:07.194935 140066259943424 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 22:44:07.194977 140066259943424 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:44:07.195020 140066259943424 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 22:44:07.195060 140066259943424 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:44:07.195100 140066259943424 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:44:07.195140 140066259943424 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 22:44:07.195179 140066259943424 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 22:44:07.195218 140066259943424 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:44:07.195257 140066259943424 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 22:44:07.195296 140066259943424 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 22:44:07.195333 140066259943424 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 22:44:07.195370 140066259943424 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:44:07.195407 140066259943424 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 22:44:07.195445 140066259943424 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:44:07.195482 140066259943424 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:44:07.195521 140066259943424 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 22:44:07.195558 140066259943424 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 22:44:07.195611 140066259943424 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:44:07.195651 140066259943424 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 22:44:07.195689 140066259943424 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 22:44:07.195726 140066259943424 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 22:44:07.195764 140066259943424 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:44:07.195801 140066259943424 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 22:44:07.195838 140066259943424 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:44:07.195875 140066259943424 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:44:07.195912 140066259943424 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 22:44:07.195949 140066259943424 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 22:44:07.195987 140066259943424 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:44:07.196024 140066259943424 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 22:44:07.196062 140066259943424 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 22:44:07.196100 140066259943424 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 22:44:07.196136 140066259943424 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:44:07.196174 140066259943424 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 22:44:07.196211 140066259943424 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:44:07.196248 140066259943424 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:44:07.196285 140066259943424 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 22:44:07.196322 140066259943424 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 22:44:07.196359 140066259943424 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:44:07.196397 140066259943424 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 22:44:07.196435 140066259943424 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 22:44:07.196473 140066259943424 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 22:44:07.196514 140066259943424 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:44:07.196552 140066259943424 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 22:44:07.196597 140066259943424 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:44:07.196636 140066259943424 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:44:07.196674 140066259943424 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 22:44:07.196710 140066259943424 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 22:44:07.196748 140066259943424 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:44:07.196786 140066259943424 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 22:44:07.196823 140066259943424 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 22:44:07.196861 140066259943424 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 22:44:07.196897 140066259943424 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:44:07.196935 140066259943424 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 22:44:07.196971 140066259943424 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:44:07.197008 140066259943424 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:44:07.197045 140066259943424 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 22:44:07.197081 140066259943424 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 22:44:07.197117 140066259943424 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:44:07.197154 140066259943424 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 22:44:07.197191 140066259943424 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 22:44:07.197227 140066259943424 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 22:44:07.197263 140066259943424 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:44:07.197300 140066259943424 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 22:44:07.197336 140066259943424 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:44:07.197373 140066259943424 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:44:07.197410 140066259943424 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 22:44:07.197445 140066259943424 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 22:44:07.197481 140066259943424 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:44:07.197517 140066259943424 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 22:44:07.197554 140066259943424 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 22:44:07.197596 140066259943424 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 22:44:07.197634 140066259943424 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:44:07.197680 140066259943424 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 22:44:07.197718 140066259943424 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:44:07.197755 140066259943424 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:44:07.197793 140066259943424 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 22:44:07.197830 140066259943424 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 22:44:07.197866 140066259943424 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:44:07.197903 140066259943424 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 22:44:07.197939 140066259943424 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 22:44:07.197976 140066259943424 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 22:44:07.198012 140066259943424 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:44:07.198049 140066259943424 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 22:44:07.198085 140066259943424 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:44:07.198122 140066259943424 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:44:07.198159 140066259943424 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 22:44:07.198195 140066259943424 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 22:44:07.198232 140066259943424 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:44:07.198268 140066259943424 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 22:44:07.198306 140066259943424 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 22:44:07.198341 140066259943424 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 22:44:07.198379 140066259943424 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:44:07.198416 140066259943424 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 22:44:07.198454 140066259943424 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:44:07.198491 140066259943424 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:44:07.198528 140066259943424 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 22:44:07.198565 140066259943424 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 22:44:07.198610 140066259943424 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:44:07.198649 140066259943424 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 22:44:07.198686 140066259943424 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 22:44:07.198723 140066259943424 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 22:44:07.198759 140066259943424 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:44:07.198795 140066259943424 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 22:44:07.198831 140066259943424 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:44:07.198868 140066259943424 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:44:07.198904 140066259943424 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 22:44:07.198940 140066259943424 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 22:44:07.198976 140066259943424 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:44:07.199013 140066259943424 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 22:44:07.199049 140066259943424 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 22:44:07.199085 140066259943424 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 22:44:07.199122 140066259943424 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:44:07.199157 140066259943424 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 22:44:07.199194 140066259943424 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:44:07.199230 140066259943424 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:44:07.199267 140066259943424 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 22:44:07.199304 140066259943424 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 22:44:07.199341 140066259943424 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 22:44:07.199377 140066259943424 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 22:44:07.199406 140066259943424 training_loop.py:725] Total parameters: 152072288
I0123 22:44:07.199629 140066259943424 training_loop.py:739] Total state size: 0
I0123 22:44:07.222138 140066259943424 training_loop.py:492] Training loop: creating task for mode beam_search
I0123 22:44:07.222412 140066259943424 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0123 22:44:07.222787 140066259943424 training_loop.py:652] Compiling mode beam_search with jit.
I0123 22:44:07.223141 140066259943424 training_loop.py:89] registering functions: dict_keys([])
I0123 22:44:07.239963 140066259943424 graph.py:499] a b c = triangle a b c; d = circle d a b c; e = mirror e b d; f = on_circle f d a; g = lc_tangent g c d, on_line g e d; h = lc_tangent h e d, on_line h f d; i = lc_tangent i f d, on_line i a d; j = lc_tangent j a d, on_line j c d; k = on_line k c g, on_line k a j; l = on_line l e h, on_line l f i; m = on_line m c g, on_line m f i; n = on_line n a j, on_line n e h; o = incenter o m k n; p = foot p o m n; q = incenter q m l n; r = foot r q m n; s = foot s d m n; t = on_circle t d c, on_line t d s; u = on_circle u d c, on_line u d s; v = mirror v p o ? coll u v k
I0123 22:44:29.981014 140066259943424 ddar.py:60] Depth 1/1000 time = 22.487545490264893
I0123 22:45:19.667068 140066259943424 ddar.py:60] Depth 2/1000 time = 49.685678482055664
I0123 22:46:18.680324 140066259943424 ddar.py:60] Depth 3/1000 time = 59.012895345687866
I0123 22:47:18.789873 140066259943424 ddar.py:60] Depth 4/1000 time = 60.109169244766235
I0123 22:48:17.746908 140066259943424 ddar.py:60] Depth 5/1000 time = 58.95659780502319
I0123 22:49:18.272663 140066259943424 ddar.py:60] Depth 6/1000 time = 60.524346113204956
I0123 22:50:22.352492 140066259943424 ddar.py:60] Depth 7/1000 time = 64.02894639968872
I0123 22:51:25.216163 140066259943424 ddar.py:60] Depth 8/1000 time = 62.86328172683716
I0123 22:52:29.596180 140066259943424 ddar.py:60] Depth 9/1000 time = 64.37967300415039
I0123 22:53:34.000857 140066259943424 ddar.py:60] Depth 10/1000 time = 64.40051579475403
I0123 22:54:39.845940 140066259943424 ddar.py:60] Depth 11/1000 time = 65.24517750740051
I0123 22:54:39.846324 140066259943424 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 22:54:39.846438 140066259943424 alphageometry.py:540] Depth 0. There are 1 nodes to expand:
I0123 22:54:39.846482 140066259943424 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b d e 02 D b d d e 03 ; f : D a d d f 04 ; g : C d e g 05 T c d c g 06 ; h : C d f h 07 T d e e h 08 ; i : C a d i 09 T d f f i 10 ; j : C c d j 11 T a d a j 12 ; k : C a j k 13 C c g k 14 ; l : C e h l 15 C f i l 16 ; m : C c g m 17 C f i m 18 ; n : C a j n 19 C e h n 20 ; o : ^ m k m o m o m n 21 ^ n k n o n o n m 22 ; p : C m n p 23 T m n o p 24 ; q : ^ m l m q m q m n 25 ^ n l n q n q n m 26 ; r : C m n r 27 T m n q r 28 ; s : C m n s 29 T d s m n 30 ; t : C d s t 31 D c d d t 32 ; u : C d s u 33 D c d d u 34 ; v : C o p v 35 D o p o v 36 ? C u v k {F1} x00
I0123 22:54:39.846519 140066259943424 alphageometry.py:549] Decoding from {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C b d e 02 D b d d e 03 ; f : D a d d f 04 ; g : C d e g 05 T c d c g 06 ; h : C d f h 07 T d e e h 08 ; i : C a d i 09 T d f f i 10 ; j : C c d j 11 T a d a j 12 ; k : C a j k 13 C c g k 14 ; l : C e h l 15 C f i l 16 ; m : C c g m 17 C f i m 18 ; n : C a j n 19 C e h n 20 ; o : ^ m k m o m o m n 21 ^ n k n o n o n m 22 ; p : C m n p 23 T m n o p 24 ; q : ^ m l m q m q m n 25 ^ n l n q n q n m 26 ; r : C m n r 27 T m n q r 28 ; s : C m n s 29 T d s m n 30 ; t : C d s t 31 D c d d t 32 ; u : C d s u 33 D c d d u 34 ; v : C o p v 35 D o p o v 36 ? C u v k {F1} x00
I0123 22:54:39.996900 140066259943424 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:39.997097 140066259943424 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 22:54:39.997201 140066259943424 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 22:54:39.997276 140066259943424 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 22:54:39.997347 140066259943424 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 22:54:39.997414 140066259943424 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 22:54:39.997482 140066259943424 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 22:54:39.997549 140066259943424 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 22:54:39.997629 140066259943424 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 22:54:39.997706 140066259943424 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 22:54:39.997775 140066259943424 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 22:54:39.997841 140066259943424 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 22:54:39.997907 140066259943424 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 22:54:39.997975 140066259943424 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 22:54:39.998018 140066259943424 attention.py:418] Single window, no scan.
I0123 22:54:39.998064 140066259943424 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 22:54:39.998177 140066259943424 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:54:39.998220 140066259943424 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:54:39.998251 140066259943424 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:54:40.000233 140066259943424 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.002734 140066259943424 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:54:40.008400 140066259943424 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.008680 140066259943424 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:54:40.011744 140066259943424 transformer_layer.py:213] tlayer: windowed attention.
I0123 22:54:40.015645 140066259943424 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:54:40.015707 140066259943424 attention.py:418] Single window, no scan.
I0123 22:54:40.015747 140066259943424 transformer_layer.py:389] tlayer: self-attention.
I0123 22:54:40.015781 140066259943424 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.015847 140066259943424 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.016473 140066259943424 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.016556 140066259943424 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.016921 140066259943424 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.017717 140066259943424 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.020306 140066259943424 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.020944 140066259943424 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.021027 140066259943424 transformer_layer.py:468] tlayer: End windows.
I0123 22:54:40.021063 140066259943424 transformer_layer.py:472] tlayer: final FFN.
I0123 22:54:40.021122 140066259943424 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.021251 140066259943424 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:54:40.021578 140066259943424 nn_components.py:325] mlp: activation = None
I0123 22:54:40.021625 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:54:40.023517 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.023617 140066259943424 nn_components.py:261] mlp: residual
I0123 22:54:40.026065 140066259943424 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.026159 140066259943424 transformer_base.py:443] tbase: final FFN
I0123 22:54:40.026585 140066259943424 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:54:40.029040 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:54:40.031043 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.031147 140066259943424 nn_components.py:261] mlp: residual
I0123 22:54:40.031440 140066259943424 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.031526 140066259943424 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 22:54:40.031635 140066259943424 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:54:40.031677 140066259943424 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:54:40.031708 140066259943424 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:54:40.033484 140066259943424 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.035838 140066259943424 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:54:40.041419 140066259943424 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.041688 140066259943424 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:54:40.044337 140066259943424 transformer_layer.py:213] tlayer: windowed attention.
I0123 22:54:40.048050 140066259943424 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:54:40.048111 140066259943424 attention.py:418] Single window, no scan.
I0123 22:54:40.048149 140066259943424 transformer_layer.py:389] tlayer: self-attention.
I0123 22:54:40.048180 140066259943424 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.048244 140066259943424 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.048852 140066259943424 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.048934 140066259943424 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.049293 140066259943424 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.050067 140066259943424 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.052510 140066259943424 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.053127 140066259943424 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.053209 140066259943424 transformer_layer.py:468] tlayer: End windows.
I0123 22:54:40.053244 140066259943424 transformer_layer.py:472] tlayer: final FFN.
I0123 22:54:40.053302 140066259943424 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.053430 140066259943424 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:54:40.053754 140066259943424 nn_components.py:325] mlp: activation = None
I0123 22:54:40.053802 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:54:40.055747 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.055845 140066259943424 nn_components.py:261] mlp: residual
I0123 22:54:40.058283 140066259943424 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.058378 140066259943424 transformer_base.py:443] tbase: final FFN
I0123 22:54:40.058807 140066259943424 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:54:40.061218 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:54:40.063151 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.063253 140066259943424 nn_components.py:261] mlp: residual
I0123 22:54:40.063545 140066259943424 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.063635 140066259943424 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 22:54:40.063743 140066259943424 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:54:40.063785 140066259943424 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:54:40.063815 140066259943424 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:54:40.065674 140066259943424 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.067970 140066259943424 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:54:40.073498 140066259943424 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.073769 140066259943424 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:54:40.076351 140066259943424 transformer_layer.py:213] tlayer: windowed attention.
I0123 22:54:40.080083 140066259943424 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:54:40.080144 140066259943424 attention.py:418] Single window, no scan.
I0123 22:54:40.080181 140066259943424 transformer_layer.py:389] tlayer: self-attention.
I0123 22:54:40.080213 140066259943424 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.080278 140066259943424 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.080840 140066259943424 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.080921 140066259943424 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.081280 140066259943424 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.082059 140066259943424 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.084520 140066259943424 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.085138 140066259943424 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.085222 140066259943424 transformer_layer.py:468] tlayer: End windows.
I0123 22:54:40.085257 140066259943424 transformer_layer.py:472] tlayer: final FFN.
I0123 22:54:40.085314 140066259943424 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.085626 140066259943424 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:54:40.086004 140066259943424 nn_components.py:325] mlp: activation = None
I0123 22:54:40.086052 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:54:40.087949 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.088047 140066259943424 nn_components.py:261] mlp: residual
I0123 22:54:40.090494 140066259943424 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.090579 140066259943424 transformer_base.py:443] tbase: final FFN
I0123 22:54:40.091015 140066259943424 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:54:40.093257 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:54:40.095272 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.095373 140066259943424 nn_components.py:261] mlp: residual
I0123 22:54:40.095662 140066259943424 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.095748 140066259943424 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 22:54:40.095857 140066259943424 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:54:40.095900 140066259943424 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:54:40.095932 140066259943424 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:54:40.097732 140066259943424 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.100023 140066259943424 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:54:40.106004 140066259943424 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.106268 140066259943424 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:54:40.108894 140066259943424 transformer_layer.py:213] tlayer: windowed attention.
I0123 22:54:40.112595 140066259943424 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:54:40.112656 140066259943424 attention.py:418] Single window, no scan.
I0123 22:54:40.112692 140066259943424 transformer_layer.py:389] tlayer: self-attention.
I0123 22:54:40.112723 140066259943424 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.112787 140066259943424 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.113389 140066259943424 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.113469 140066259943424 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.113837 140066259943424 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.114604 140066259943424 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.117035 140066259943424 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.117669 140066259943424 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.117753 140066259943424 transformer_layer.py:468] tlayer: End windows.
I0123 22:54:40.117789 140066259943424 transformer_layer.py:472] tlayer: final FFN.
I0123 22:54:40.117845 140066259943424 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.117977 140066259943424 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:54:40.118294 140066259943424 nn_components.py:325] mlp: activation = None
I0123 22:54:40.118341 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:54:40.120654 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.120754 140066259943424 nn_components.py:261] mlp: residual
I0123 22:54:40.123224 140066259943424 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.123310 140066259943424 transformer_base.py:443] tbase: final FFN
I0123 22:54:40.123736 140066259943424 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:54:40.126000 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:54:40.127927 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.128027 140066259943424 nn_components.py:261] mlp: residual
I0123 22:54:40.128314 140066259943424 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.128401 140066259943424 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 22:54:40.128509 140066259943424 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:54:40.128553 140066259943424 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:54:40.128586 140066259943424 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:54:40.130439 140066259943424 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.132750 140066259943424 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:54:40.138332 140066259943424 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.138594 140066259943424 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:54:40.141218 140066259943424 transformer_layer.py:213] tlayer: windowed attention.
I0123 22:54:40.144891 140066259943424 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:54:40.144952 140066259943424 attention.py:418] Single window, no scan.
I0123 22:54:40.144987 140066259943424 transformer_layer.py:389] tlayer: self-attention.
I0123 22:54:40.145018 140066259943424 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.145082 140066259943424 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.145645 140066259943424 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.145728 140066259943424 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.146091 140066259943424 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.146853 140066259943424 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.149304 140066259943424 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.149977 140066259943424 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.150061 140066259943424 transformer_layer.py:468] tlayer: End windows.
I0123 22:54:40.150097 140066259943424 transformer_layer.py:472] tlayer: final FFN.
I0123 22:54:40.150155 140066259943424 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.150288 140066259943424 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:54:40.150609 140066259943424 nn_components.py:325] mlp: activation = None
I0123 22:54:40.150657 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:54:40.152562 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.152661 140066259943424 nn_components.py:261] mlp: residual
I0123 22:54:40.155095 140066259943424 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.155180 140066259943424 transformer_base.py:443] tbase: final FFN
I0123 22:54:40.155603 140066259943424 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:54:40.157921 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:54:40.159831 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.159941 140066259943424 nn_components.py:261] mlp: residual
I0123 22:54:40.160234 140066259943424 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.160321 140066259943424 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 22:54:40.160429 140066259943424 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:54:40.160473 140066259943424 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:54:40.160504 140066259943424 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:54:40.162300 140066259943424 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.164608 140066259943424 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:54:40.170265 140066259943424 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.170527 140066259943424 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:54:40.173096 140066259943424 transformer_layer.py:213] tlayer: windowed attention.
I0123 22:54:40.176733 140066259943424 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:54:40.176794 140066259943424 attention.py:418] Single window, no scan.
I0123 22:54:40.176831 140066259943424 transformer_layer.py:389] tlayer: self-attention.
I0123 22:54:40.176862 140066259943424 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.176977 140066259943424 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.177531 140066259943424 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.177613 140066259943424 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.177981 140066259943424 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.178744 140066259943424 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.181182 140066259943424 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.181805 140066259943424 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.181886 140066259943424 transformer_layer.py:468] tlayer: End windows.
I0123 22:54:40.181922 140066259943424 transformer_layer.py:472] tlayer: final FFN.
I0123 22:54:40.181979 140066259943424 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.182128 140066259943424 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:54:40.182455 140066259943424 nn_components.py:325] mlp: activation = None
I0123 22:54:40.182502 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:54:40.184450 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.184548 140066259943424 nn_components.py:261] mlp: residual
I0123 22:54:40.186982 140066259943424 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.187067 140066259943424 transformer_base.py:443] tbase: final FFN
I0123 22:54:40.187490 140066259943424 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:54:40.189730 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:54:40.191623 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.191733 140066259943424 nn_components.py:261] mlp: residual
I0123 22:54:40.192021 140066259943424 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.192107 140066259943424 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 22:54:40.192216 140066259943424 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:54:40.192260 140066259943424 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:54:40.192292 140066259943424 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:54:40.194152 140066259943424 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.196466 140066259943424 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:54:40.202039 140066259943424 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.202304 140066259943424 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:54:40.204902 140066259943424 transformer_layer.py:213] tlayer: windowed attention.
I0123 22:54:40.208538 140066259943424 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:54:40.208598 140066259943424 attention.py:418] Single window, no scan.
I0123 22:54:40.208634 140066259943424 transformer_layer.py:389] tlayer: self-attention.
I0123 22:54:40.208665 140066259943424 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.208729 140066259943424 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.209280 140066259943424 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.209360 140066259943424 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.209718 140066259943424 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.210479 140066259943424 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.212921 140066259943424 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.213585 140066259943424 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.213673 140066259943424 transformer_layer.py:468] tlayer: End windows.
I0123 22:54:40.213710 140066259943424 transformer_layer.py:472] tlayer: final FFN.
I0123 22:54:40.213767 140066259943424 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.213900 140066259943424 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:54:40.214216 140066259943424 nn_components.py:325] mlp: activation = None
I0123 22:54:40.214264 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:54:40.216140 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.216236 140066259943424 nn_components.py:261] mlp: residual
I0123 22:54:40.218657 140066259943424 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.218742 140066259943424 transformer_base.py:443] tbase: final FFN
I0123 22:54:40.219171 140066259943424 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:54:40.221479 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:54:40.223397 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.223498 140066259943424 nn_components.py:261] mlp: residual
I0123 22:54:40.223800 140066259943424 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.223887 140066259943424 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 22:54:40.224072 140066259943424 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:54:40.224117 140066259943424 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:54:40.224149 140066259943424 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:54:40.225933 140066259943424 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.228213 140066259943424 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:54:40.234197 140066259943424 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.234456 140066259943424 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:54:40.237025 140066259943424 transformer_layer.py:213] tlayer: windowed attention.
I0123 22:54:40.240667 140066259943424 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:54:40.240727 140066259943424 attention.py:418] Single window, no scan.
I0123 22:54:40.240763 140066259943424 transformer_layer.py:389] tlayer: self-attention.
I0123 22:54:40.240794 140066259943424 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.240909 140066259943424 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.241469 140066259943424 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.241550 140066259943424 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.241915 140066259943424 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.242671 140066259943424 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.245107 140066259943424 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.245901 140066259943424 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.245984 140066259943424 transformer_layer.py:468] tlayer: End windows.
I0123 22:54:40.246019 140066259943424 transformer_layer.py:472] tlayer: final FFN.
I0123 22:54:40.246076 140066259943424 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.246209 140066259943424 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:54:40.246528 140066259943424 nn_components.py:325] mlp: activation = None
I0123 22:54:40.246576 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:54:40.248681 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.248780 140066259943424 nn_components.py:261] mlp: residual
I0123 22:54:40.251221 140066259943424 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.251307 140066259943424 transformer_base.py:443] tbase: final FFN
I0123 22:54:40.251730 140066259943424 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:54:40.254112 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:54:40.256026 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.256125 140066259943424 nn_components.py:261] mlp: residual
I0123 22:54:40.256412 140066259943424 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.256508 140066259943424 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 22:54:40.256619 140066259943424 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:54:40.256664 140066259943424 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:54:40.256697 140066259943424 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:54:40.258547 140066259943424 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.260836 140066259943424 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:54:40.266377 140066259943424 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.266637 140066259943424 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:54:40.269218 140066259943424 transformer_layer.py:213] tlayer: windowed attention.
I0123 22:54:40.272839 140066259943424 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:54:40.272899 140066259943424 attention.py:418] Single window, no scan.
I0123 22:54:40.272935 140066259943424 transformer_layer.py:389] tlayer: self-attention.
I0123 22:54:40.272965 140066259943424 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.273030 140066259943424 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.273584 140066259943424 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.273670 140066259943424 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.274028 140066259943424 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.274781 140066259943424 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.277212 140066259943424 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.277833 140066259943424 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.277915 140066259943424 transformer_layer.py:468] tlayer: End windows.
I0123 22:54:40.277950 140066259943424 transformer_layer.py:472] tlayer: final FFN.
I0123 22:54:40.278007 140066259943424 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.278138 140066259943424 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:54:40.278513 140066259943424 nn_components.py:325] mlp: activation = None
I0123 22:54:40.278562 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:54:40.280456 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.280554 140066259943424 nn_components.py:261] mlp: residual
I0123 22:54:40.283001 140066259943424 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.283087 140066259943424 transformer_base.py:443] tbase: final FFN
I0123 22:54:40.283514 140066259943424 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:54:40.285747 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:54:40.287699 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.287799 140066259943424 nn_components.py:261] mlp: residual
I0123 22:54:40.288090 140066259943424 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.288177 140066259943424 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 22:54:40.288298 140066259943424 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:54:40.288344 140066259943424 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:54:40.288376 140066259943424 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:54:40.290167 140066259943424 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.292457 140066259943424 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:54:40.298105 140066259943424 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.298363 140066259943424 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:54:40.300904 140066259943424 transformer_layer.py:213] tlayer: windowed attention.
I0123 22:54:40.304551 140066259943424 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:54:40.304611 140066259943424 attention.py:418] Single window, no scan.
I0123 22:54:40.304647 140066259943424 transformer_layer.py:389] tlayer: self-attention.
I0123 22:54:40.304679 140066259943424 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.304743 140066259943424 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.305358 140066259943424 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.305441 140066259943424 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.305808 140066259943424 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.306571 140066259943424 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.309007 140066259943424 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.309624 140066259943424 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.309715 140066259943424 transformer_layer.py:468] tlayer: End windows.
I0123 22:54:40.309751 140066259943424 transformer_layer.py:472] tlayer: final FFN.
I0123 22:54:40.309810 140066259943424 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.309946 140066259943424 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:54:40.310271 140066259943424 nn_components.py:325] mlp: activation = None
I0123 22:54:40.310317 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:54:40.312209 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.312307 140066259943424 nn_components.py:261] mlp: residual
I0123 22:54:40.314794 140066259943424 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.314879 140066259943424 transformer_base.py:443] tbase: final FFN
I0123 22:54:40.315300 140066259943424 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:54:40.317524 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:54:40.319426 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.319525 140066259943424 nn_components.py:261] mlp: residual
I0123 22:54:40.319814 140066259943424 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.319900 140066259943424 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 22:54:40.320007 140066259943424 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:54:40.320059 140066259943424 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:54:40.320092 140066259943424 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:54:40.321872 140066259943424 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.324253 140066259943424 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:54:40.329764 140066259943424 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.330024 140066259943424 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:54:40.332572 140066259943424 transformer_layer.py:213] tlayer: windowed attention.
I0123 22:54:40.336184 140066259943424 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:54:40.336245 140066259943424 attention.py:418] Single window, no scan.
I0123 22:54:40.336281 140066259943424 transformer_layer.py:389] tlayer: self-attention.
I0123 22:54:40.336312 140066259943424 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.336428 140066259943424 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.336986 140066259943424 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.337067 140066259943424 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.337425 140066259943424 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.338195 140066259943424 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.340632 140066259943424 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.341249 140066259943424 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.341331 140066259943424 transformer_layer.py:468] tlayer: End windows.
I0123 22:54:40.341367 140066259943424 transformer_layer.py:472] tlayer: final FFN.
I0123 22:54:40.341424 140066259943424 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.341558 140066259943424 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:54:40.341885 140066259943424 nn_components.py:325] mlp: activation = None
I0123 22:54:40.341933 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:54:40.343825 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.343922 140066259943424 nn_components.py:261] mlp: residual
I0123 22:54:40.346786 140066259943424 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.346871 140066259943424 transformer_base.py:443] tbase: final FFN
I0123 22:54:40.347289 140066259943424 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:54:40.349524 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:54:40.351425 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.351525 140066259943424 nn_components.py:261] mlp: residual
I0123 22:54:40.351814 140066259943424 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.351901 140066259943424 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 22:54:40.352010 140066259943424 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:54:40.352053 140066259943424 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:54:40.352094 140066259943424 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:54:40.353994 140066259943424 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.356380 140066259943424 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:54:40.361945 140066259943424 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.362208 140066259943424 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:54:40.364748 140066259943424 transformer_layer.py:213] tlayer: windowed attention.
I0123 22:54:40.368389 140066259943424 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:54:40.368450 140066259943424 attention.py:418] Single window, no scan.
I0123 22:54:40.368486 140066259943424 transformer_layer.py:389] tlayer: self-attention.
I0123 22:54:40.368517 140066259943424 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.368634 140066259943424 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.369195 140066259943424 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.369277 140066259943424 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.369634 140066259943424 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.370411 140066259943424 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.372869 140066259943424 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.373491 140066259943424 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.373575 140066259943424 transformer_layer.py:468] tlayer: End windows.
I0123 22:54:40.373611 140066259943424 transformer_layer.py:472] tlayer: final FFN.
I0123 22:54:40.373675 140066259943424 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.373813 140066259943424 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:54:40.374137 140066259943424 nn_components.py:325] mlp: activation = None
I0123 22:54:40.374183 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:54:40.376067 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.376167 140066259943424 nn_components.py:261] mlp: residual
I0123 22:54:40.378657 140066259943424 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.378742 140066259943424 transformer_base.py:443] tbase: final FFN
I0123 22:54:40.379167 140066259943424 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:54:40.381411 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:54:40.383306 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.383407 140066259943424 nn_components.py:261] mlp: residual
I0123 22:54:40.383697 140066259943424 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.383952 140066259943424 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 22:54:40.384026 140066259943424 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 22:54:40.384084 140066259943424 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 22:54:40.384150 140066259943424 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 22:54:40.384208 140066259943424 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 22:54:40.384262 140066259943424 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 22:54:40.384315 140066259943424 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 22:54:40.384368 140066259943424 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 22:54:40.384421 140066259943424 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 22:54:40.384475 140066259943424 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 22:54:40.384529 140066259943424 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 22:54:40.384584 140066259943424 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 22:54:40.384621 140066259943424 decoder_stack.py:344] dstack: Final layernorm.
I0123 22:54:40.387523 140066259943424 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 22:54:40.432037 140066259943424 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.432128 140066259943424 decoder_stack.py:333] dstack: autoregressive generator.
I0123 22:54:40.432183 140066259943424 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 22:54:40.432288 140066259943424 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:54:40.432329 140066259943424 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:54:40.432361 140066259943424 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:54:40.432421 140066259943424 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.434785 140066259943424 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:54:40.440140 140066259943424 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.440407 140066259943424 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:54:40.443002 140066259943424 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:54:40.455814 140066259943424 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:54:40.455876 140066259943424 attention.py:418] Single window, no scan.
I0123 22:54:40.455913 140066259943424 transformer_layer.py:389] tlayer: self-attention.
I0123 22:54:40.455945 140066259943424 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.456009 140066259943424 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.456577 140066259943424 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.456657 140066259943424 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.457025 140066259943424 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.457725 140066259943424 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.460269 140066259943424 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.460886 140066259943424 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.460967 140066259943424 transformer_layer.py:468] tlayer: End windows.
I0123 22:54:40.461012 140066259943424 transformer_layer.py:472] tlayer: final FFN.
I0123 22:54:40.461073 140066259943424 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.461205 140066259943424 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:54:40.461319 140066259943424 nn_components.py:325] mlp: activation = None
I0123 22:54:40.461362 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:54:40.463195 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.463294 140066259943424 nn_components.py:261] mlp: residual
I0123 22:54:40.465672 140066259943424 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.465756 140066259943424 transformer_base.py:443] tbase: final FFN
I0123 22:54:40.465866 140066259943424 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:54:40.468080 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:54:40.469913 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.470013 140066259943424 nn_components.py:261] mlp: residual
I0123 22:54:40.470301 140066259943424 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.470387 140066259943424 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 22:54:40.470495 140066259943424 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:54:40.470536 140066259943424 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:54:40.470569 140066259943424 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:54:40.470630 140066259943424 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.472837 140066259943424 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:54:40.478188 140066259943424 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.478454 140066259943424 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:54:40.481081 140066259943424 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:54:40.493394 140066259943424 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:54:40.493455 140066259943424 attention.py:418] Single window, no scan.
I0123 22:54:40.493493 140066259943424 transformer_layer.py:389] tlayer: self-attention.
I0123 22:54:40.493523 140066259943424 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.493587 140066259943424 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.494146 140066259943424 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.494227 140066259943424 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.494584 140066259943424 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.495319 140066259943424 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.497761 140066259943424 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.498378 140066259943424 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.498461 140066259943424 transformer_layer.py:468] tlayer: End windows.
I0123 22:54:40.498497 140066259943424 transformer_layer.py:472] tlayer: final FFN.
I0123 22:54:40.498563 140066259943424 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.498695 140066259943424 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:54:40.498809 140066259943424 nn_components.py:325] mlp: activation = None
I0123 22:54:40.498853 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:54:40.500738 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.500837 140066259943424 nn_components.py:261] mlp: residual
I0123 22:54:40.503236 140066259943424 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.503322 140066259943424 transformer_base.py:443] tbase: final FFN
I0123 22:54:40.503429 140066259943424 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:54:40.505644 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:54:40.507482 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.507581 140066259943424 nn_components.py:261] mlp: residual
I0123 22:54:40.507869 140066259943424 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.507955 140066259943424 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 22:54:40.508064 140066259943424 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:54:40.508105 140066259943424 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:54:40.508138 140066259943424 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:54:40.508200 140066259943424 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.510425 140066259943424 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:54:40.515746 140066259943424 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.516009 140066259943424 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:54:40.518659 140066259943424 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:54:40.531636 140066259943424 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:54:40.531697 140066259943424 attention.py:418] Single window, no scan.
I0123 22:54:40.531734 140066259943424 transformer_layer.py:389] tlayer: self-attention.
I0123 22:54:40.531765 140066259943424 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.531830 140066259943424 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.532382 140066259943424 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.532462 140066259943424 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.532814 140066259943424 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.533549 140066259943424 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.535983 140066259943424 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.536600 140066259943424 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.536684 140066259943424 transformer_layer.py:468] tlayer: End windows.
I0123 22:54:40.536719 140066259943424 transformer_layer.py:472] tlayer: final FFN.
I0123 22:54:40.536778 140066259943424 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.536919 140066259943424 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:54:40.537034 140066259943424 nn_components.py:325] mlp: activation = None
I0123 22:54:40.537078 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:54:40.538899 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.538998 140066259943424 nn_components.py:261] mlp: residual
I0123 22:54:40.541387 140066259943424 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.541471 140066259943424 transformer_base.py:443] tbase: final FFN
I0123 22:54:40.541580 140066259943424 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:54:40.543809 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:54:40.545627 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.545734 140066259943424 nn_components.py:261] mlp: residual
I0123 22:54:40.546025 140066259943424 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.546112 140066259943424 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 22:54:40.546221 140066259943424 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:54:40.546262 140066259943424 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:54:40.546295 140066259943424 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:54:40.546358 140066259943424 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.548557 140066259943424 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:54:40.553858 140066259943424 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.554122 140066259943424 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:54:40.556736 140066259943424 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:54:40.569051 140066259943424 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:54:40.569113 140066259943424 attention.py:418] Single window, no scan.
I0123 22:54:40.569149 140066259943424 transformer_layer.py:389] tlayer: self-attention.
I0123 22:54:40.569180 140066259943424 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.569244 140066259943424 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.569800 140066259943424 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.569881 140066259943424 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.570239 140066259943424 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.570970 140066259943424 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.573399 140066259943424 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.574023 140066259943424 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.574107 140066259943424 transformer_layer.py:468] tlayer: End windows.
I0123 22:54:40.574143 140066259943424 transformer_layer.py:472] tlayer: final FFN.
I0123 22:54:40.574201 140066259943424 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.574339 140066259943424 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:54:40.574453 140066259943424 nn_components.py:325] mlp: activation = None
I0123 22:54:40.574496 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:54:40.576326 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.576424 140066259943424 nn_components.py:261] mlp: residual
I0123 22:54:40.578818 140066259943424 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.578903 140066259943424 transformer_base.py:443] tbase: final FFN
I0123 22:54:40.579013 140066259943424 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:54:40.581252 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:54:40.583098 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.583199 140066259943424 nn_components.py:261] mlp: residual
I0123 22:54:40.583494 140066259943424 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.583580 140066259943424 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 22:54:40.583689 140066259943424 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:54:40.583730 140066259943424 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:54:40.583763 140066259943424 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:54:40.583824 140066259943424 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.586050 140066259943424 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:54:40.591410 140066259943424 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.591676 140066259943424 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:54:40.594325 140066259943424 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:54:40.606652 140066259943424 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:54:40.606713 140066259943424 attention.py:418] Single window, no scan.
I0123 22:54:40.606750 140066259943424 transformer_layer.py:389] tlayer: self-attention.
I0123 22:54:40.606781 140066259943424 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.606845 140066259943424 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.607396 140066259943424 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.607477 140066259943424 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.607836 140066259943424 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.608576 140066259943424 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.611029 140066259943424 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.611648 140066259943424 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.611731 140066259943424 transformer_layer.py:468] tlayer: End windows.
I0123 22:54:40.611766 140066259943424 transformer_layer.py:472] tlayer: final FFN.
I0123 22:54:40.611824 140066259943424 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.611955 140066259943424 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:54:40.612079 140066259943424 nn_components.py:325] mlp: activation = None
I0123 22:54:40.612123 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:54:40.613961 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.614059 140066259943424 nn_components.py:261] mlp: residual
I0123 22:54:40.616445 140066259943424 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.616528 140066259943424 transformer_base.py:443] tbase: final FFN
I0123 22:54:40.616635 140066259943424 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:54:40.618884 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:54:40.620709 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.620809 140066259943424 nn_components.py:261] mlp: residual
I0123 22:54:40.621100 140066259943424 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.621187 140066259943424 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 22:54:40.621295 140066259943424 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:54:40.621336 140066259943424 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:54:40.621368 140066259943424 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:54:40.621429 140066259943424 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.623749 140066259943424 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:54:40.629122 140066259943424 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.629386 140066259943424 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:54:40.632031 140066259943424 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:54:40.644775 140066259943424 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:54:40.644837 140066259943424 attention.py:418] Single window, no scan.
I0123 22:54:40.644875 140066259943424 transformer_layer.py:389] tlayer: self-attention.
I0123 22:54:40.644906 140066259943424 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.644972 140066259943424 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.645524 140066259943424 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.645605 140066259943424 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.645969 140066259943424 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.646708 140066259943424 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.649152 140066259943424 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.649776 140066259943424 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.649858 140066259943424 transformer_layer.py:468] tlayer: End windows.
I0123 22:54:40.649894 140066259943424 transformer_layer.py:472] tlayer: final FFN.
I0123 22:54:40.649951 140066259943424 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.650081 140066259943424 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:54:40.650194 140066259943424 nn_components.py:325] mlp: activation = None
I0123 22:54:40.650246 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:54:40.652091 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.652188 140066259943424 nn_components.py:261] mlp: residual
I0123 22:54:40.654558 140066259943424 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.654643 140066259943424 transformer_base.py:443] tbase: final FFN
I0123 22:54:40.654753 140066259943424 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:54:40.656975 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:54:40.658835 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.658937 140066259943424 nn_components.py:261] mlp: residual
I0123 22:54:40.659228 140066259943424 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.659315 140066259943424 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 22:54:40.659424 140066259943424 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:54:40.659464 140066259943424 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:54:40.659497 140066259943424 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:54:40.659559 140066259943424 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.661809 140066259943424 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:54:40.667203 140066259943424 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.667469 140066259943424 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:54:40.670106 140066259943424 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:54:40.682417 140066259943424 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:54:40.682479 140066259943424 attention.py:418] Single window, no scan.
I0123 22:54:40.682516 140066259943424 transformer_layer.py:389] tlayer: self-attention.
I0123 22:54:40.682547 140066259943424 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.682611 140066259943424 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.683161 140066259943424 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.683241 140066259943424 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.683596 140066259943424 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.684334 140066259943424 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.686786 140066259943424 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.687404 140066259943424 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.687486 140066259943424 transformer_layer.py:468] tlayer: End windows.
I0123 22:54:40.687523 140066259943424 transformer_layer.py:472] tlayer: final FFN.
I0123 22:54:40.687580 140066259943424 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.687710 140066259943424 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:54:40.687823 140066259943424 nn_components.py:325] mlp: activation = None
I0123 22:54:40.687867 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:54:40.689722 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.689821 140066259943424 nn_components.py:261] mlp: residual
I0123 22:54:40.692195 140066259943424 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.692279 140066259943424 transformer_base.py:443] tbase: final FFN
I0123 22:54:40.692387 140066259943424 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:54:40.694642 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:54:40.696490 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.696590 140066259943424 nn_components.py:261] mlp: residual
I0123 22:54:40.696884 140066259943424 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.696971 140066259943424 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 22:54:40.697080 140066259943424 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:54:40.697121 140066259943424 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:54:40.697154 140066259943424 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:54:40.697215 140066259943424 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.699439 140066259943424 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:54:40.704777 140066259943424 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.705051 140066259943424 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:54:40.707692 140066259943424 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:54:40.720000 140066259943424 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:54:40.720060 140066259943424 attention.py:418] Single window, no scan.
I0123 22:54:40.720097 140066259943424 transformer_layer.py:389] tlayer: self-attention.
I0123 22:54:40.720127 140066259943424 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.720192 140066259943424 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.720747 140066259943424 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.720827 140066259943424 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.721182 140066259943424 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.721928 140066259943424 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.724354 140066259943424 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.724967 140066259943424 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.725050 140066259943424 transformer_layer.py:468] tlayer: End windows.
I0123 22:54:40.725085 140066259943424 transformer_layer.py:472] tlayer: final FFN.
I0123 22:54:40.725143 140066259943424 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.725273 140066259943424 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:54:40.725385 140066259943424 nn_components.py:325] mlp: activation = None
I0123 22:54:40.725429 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:54:40.727256 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.727363 140066259943424 nn_components.py:261] mlp: residual
I0123 22:54:40.729750 140066259943424 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.729837 140066259943424 transformer_base.py:443] tbase: final FFN
I0123 22:54:40.729950 140066259943424 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:54:40.732189 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:54:40.734033 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.734133 140066259943424 nn_components.py:261] mlp: residual
I0123 22:54:40.734426 140066259943424 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.734513 140066259943424 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 22:54:40.734622 140066259943424 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:54:40.734663 140066259943424 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:54:40.734695 140066259943424 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:54:40.734756 140066259943424 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.736978 140066259943424 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:54:40.742370 140066259943424 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.742636 140066259943424 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:54:40.745280 140066259943424 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:54:40.757985 140066259943424 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:54:40.758045 140066259943424 attention.py:418] Single window, no scan.
I0123 22:54:40.758082 140066259943424 transformer_layer.py:389] tlayer: self-attention.
I0123 22:54:40.758113 140066259943424 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.758178 140066259943424 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.758735 140066259943424 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.758816 140066259943424 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.759177 140066259943424 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.759862 140066259943424 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.762370 140066259943424 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.762996 140066259943424 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.763079 140066259943424 transformer_layer.py:468] tlayer: End windows.
I0123 22:54:40.763114 140066259943424 transformer_layer.py:472] tlayer: final FFN.
I0123 22:54:40.763172 140066259943424 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.763306 140066259943424 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:54:40.763418 140066259943424 nn_components.py:325] mlp: activation = None
I0123 22:54:40.763462 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:54:40.765291 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.765400 140066259943424 nn_components.py:261] mlp: residual
I0123 22:54:40.767790 140066259943424 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.767876 140066259943424 transformer_base.py:443] tbase: final FFN
I0123 22:54:40.767987 140066259943424 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:54:40.770246 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:54:40.772073 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.772173 140066259943424 nn_components.py:261] mlp: residual
I0123 22:54:40.772462 140066259943424 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.772549 140066259943424 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 22:54:40.772656 140066259943424 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:54:40.772698 140066259943424 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:54:40.772729 140066259943424 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:54:40.772792 140066259943424 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.775023 140066259943424 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:54:40.780343 140066259943424 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.780605 140066259943424 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:54:40.783227 140066259943424 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:54:40.795498 140066259943424 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:54:40.795560 140066259943424 attention.py:418] Single window, no scan.
I0123 22:54:40.795597 140066259943424 transformer_layer.py:389] tlayer: self-attention.
I0123 22:54:40.795628 140066259943424 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.795691 140066259943424 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.796241 140066259943424 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.796323 140066259943424 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.796683 140066259943424 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.797371 140066259943424 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.799871 140066259943424 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.800487 140066259943424 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.800571 140066259943424 transformer_layer.py:468] tlayer: End windows.
I0123 22:54:40.800607 140066259943424 transformer_layer.py:472] tlayer: final FFN.
I0123 22:54:40.800664 140066259943424 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.800795 140066259943424 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:54:40.800909 140066259943424 nn_components.py:325] mlp: activation = None
I0123 22:54:40.800952 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:54:40.802794 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.802904 140066259943424 nn_components.py:261] mlp: residual
I0123 22:54:40.805274 140066259943424 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.805360 140066259943424 transformer_base.py:443] tbase: final FFN
I0123 22:54:40.805470 140066259943424 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:54:40.807706 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:54:40.809545 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.809651 140066259943424 nn_components.py:261] mlp: residual
I0123 22:54:40.809942 140066259943424 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.810030 140066259943424 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 22:54:40.810139 140066259943424 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:54:40.810181 140066259943424 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:54:40.810213 140066259943424 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:54:40.810274 140066259943424 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.812495 140066259943424 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:54:40.817837 140066259943424 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.818103 140066259943424 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:54:40.820745 140066259943424 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:54:40.833039 140066259943424 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:54:40.833100 140066259943424 attention.py:418] Single window, no scan.
I0123 22:54:40.833136 140066259943424 transformer_layer.py:389] tlayer: self-attention.
I0123 22:54:40.833168 140066259943424 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.833230 140066259943424 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.833789 140066259943424 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.833870 140066259943424 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.834230 140066259943424 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.834913 140066259943424 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.837397 140066259943424 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.838018 140066259943424 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.838103 140066259943424 transformer_layer.py:468] tlayer: End windows.
I0123 22:54:40.838138 140066259943424 transformer_layer.py:472] tlayer: final FFN.
I0123 22:54:40.838195 140066259943424 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.838323 140066259943424 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:54:40.838602 140066259943424 nn_components.py:325] mlp: activation = None
I0123 22:54:40.838642 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:54:40.840480 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.840579 140066259943424 nn_components.py:261] mlp: residual
I0123 22:54:40.842974 140066259943424 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.843061 140066259943424 transformer_base.py:443] tbase: final FFN
I0123 22:54:40.843171 140066259943424 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:54:40.845398 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:54:40.847240 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.847340 140066259943424 nn_components.py:261] mlp: residual
I0123 22:54:40.847626 140066259943424 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.847713 140066259943424 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 22:54:40.847823 140066259943424 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:54:40.847864 140066259943424 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:54:40.847895 140066259943424 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:54:40.847956 140066259943424 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.850188 140066259943424 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:54:40.855509 140066259943424 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.855772 140066259943424 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:54:40.858439 140066259943424 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:54:40.871256 140066259943424 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:54:40.871317 140066259943424 attention.py:418] Single window, no scan.
I0123 22:54:40.871355 140066259943424 transformer_layer.py:389] tlayer: self-attention.
I0123 22:54:40.871388 140066259943424 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.871450 140066259943424 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.872007 140066259943424 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.872089 140066259943424 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.872445 140066259943424 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.873127 140066259943424 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.875648 140066259943424 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.876270 140066259943424 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.876352 140066259943424 transformer_layer.py:468] tlayer: End windows.
I0123 22:54:40.876387 140066259943424 transformer_layer.py:472] tlayer: final FFN.
I0123 22:54:40.876444 140066259943424 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.876572 140066259943424 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:54:40.876683 140066259943424 nn_components.py:325] mlp: activation = None
I0123 22:54:40.876724 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:54:40.878565 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.878665 140066259943424 nn_components.py:261] mlp: residual
I0123 22:54:40.881054 140066259943424 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.881147 140066259943424 transformer_base.py:443] tbase: final FFN
I0123 22:54:40.881262 140066259943424 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:54:40.883575 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:54:40.885418 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.885519 140066259943424 nn_components.py:261] mlp: residual
I0123 22:54:40.885821 140066259943424 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.885914 140066259943424 decoder_stack.py:344] dstack: Final layernorm.
I0123 22:54:40.888743 140066259943424 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 22:54:40.939097 140066259943424 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:40.939190 140066259943424 decoder_stack.py:333] dstack: autoregressive generator.
I0123 22:54:40.939245 140066259943424 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 22:54:40.939348 140066259943424 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:54:40.939388 140066259943424 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:54:40.939418 140066259943424 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:54:40.939480 140066259943424 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:40.941899 140066259943424 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:54:40.947350 140066259943424 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:40.947615 140066259943424 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:54:40.950202 140066259943424 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:54:40.962695 140066259943424 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:54:40.962756 140066259943424 attention.py:418] Single window, no scan.
I0123 22:54:40.962793 140066259943424 transformer_layer.py:389] tlayer: self-attention.
I0123 22:54:40.962826 140066259943424 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:40.962888 140066259943424 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:40.963444 140066259943424 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:40.963525 140066259943424 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:40.963881 140066259943424 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:40.964568 140066259943424 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:40.967005 140066259943424 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:40.967619 140066259943424 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:40.967702 140066259943424 transformer_layer.py:468] tlayer: End windows.
I0123 22:54:40.967738 140066259943424 transformer_layer.py:472] tlayer: final FFN.
I0123 22:54:40.967796 140066259943424 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:40.967925 140066259943424 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:54:40.968036 140066259943424 nn_components.py:325] mlp: activation = None
I0123 22:54:40.968086 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:54:40.969987 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:40.970087 140066259943424 nn_components.py:261] mlp: residual
I0123 22:54:40.972465 140066259943424 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:40.972550 140066259943424 transformer_base.py:443] tbase: final FFN
I0123 22:54:40.972660 140066259943424 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:54:40.974864 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:54:40.976716 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:40.976817 140066259943424 nn_components.py:261] mlp: residual
I0123 22:54:40.977111 140066259943424 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:40.977200 140066259943424 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 22:54:40.977307 140066259943424 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:54:40.977349 140066259943424 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:54:40.977380 140066259943424 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:54:40.977443 140066259943424 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:40.979688 140066259943424 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:54:40.985178 140066259943424 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:40.985446 140066259943424 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:54:40.988037 140066259943424 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:54:41.000520 140066259943424 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:54:41.000582 140066259943424 attention.py:418] Single window, no scan.
I0123 22:54:41.000621 140066259943424 transformer_layer.py:389] tlayer: self-attention.
I0123 22:54:41.000653 140066259943424 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.000717 140066259943424 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.001270 140066259943424 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.001351 140066259943424 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.001714 140066259943424 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.002402 140066259943424 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.004829 140066259943424 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.005447 140066259943424 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.005530 140066259943424 transformer_layer.py:468] tlayer: End windows.
I0123 22:54:41.005567 140066259943424 transformer_layer.py:472] tlayer: final FFN.
I0123 22:54:41.005627 140066259943424 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.005764 140066259943424 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:54:41.005875 140066259943424 nn_components.py:325] mlp: activation = None
I0123 22:54:41.005915 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:54:41.007821 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.007919 140066259943424 nn_components.py:261] mlp: residual
I0123 22:54:41.010324 140066259943424 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.010410 140066259943424 transformer_base.py:443] tbase: final FFN
I0123 22:54:41.010522 140066259943424 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:54:41.012692 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:54:41.014533 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.014635 140066259943424 nn_components.py:261] mlp: residual
I0123 22:54:41.014930 140066259943424 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.015015 140066259943424 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 22:54:41.015123 140066259943424 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:54:41.015164 140066259943424 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:54:41.015194 140066259943424 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:54:41.015258 140066259943424 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.017493 140066259943424 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:54:41.022943 140066259943424 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.023212 140066259943424 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:54:41.025794 140066259943424 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:54:41.038214 140066259943424 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:54:41.038275 140066259943424 attention.py:418] Single window, no scan.
I0123 22:54:41.038312 140066259943424 transformer_layer.py:389] tlayer: self-attention.
I0123 22:54:41.038344 140066259943424 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.038406 140066259943424 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.038960 140066259943424 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.039041 140066259943424 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.039404 140066259943424 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.040090 140066259943424 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.042832 140066259943424 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.043449 140066259943424 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.043531 140066259943424 transformer_layer.py:468] tlayer: End windows.
I0123 22:54:41.043571 140066259943424 transformer_layer.py:472] tlayer: final FFN.
I0123 22:54:41.043630 140066259943424 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.043760 140066259943424 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:54:41.043870 140066259943424 nn_components.py:325] mlp: activation = None
I0123 22:54:41.043911 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:54:41.046265 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.046373 140066259943424 nn_components.py:261] mlp: residual
I0123 22:54:41.048742 140066259943424 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.048827 140066259943424 transformer_base.py:443] tbase: final FFN
I0123 22:54:41.048936 140066259943424 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:54:41.051102 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:54:41.052928 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.053028 140066259943424 nn_components.py:261] mlp: residual
I0123 22:54:41.053316 140066259943424 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.053403 140066259943424 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 22:54:41.053511 140066259943424 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:54:41.053552 140066259943424 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:54:41.053584 140066259943424 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:54:41.053653 140066259943424 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.055846 140066259943424 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:54:41.061203 140066259943424 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.061470 140066259943424 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:54:41.064043 140066259943424 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:54:41.076270 140066259943424 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:54:41.076331 140066259943424 attention.py:418] Single window, no scan.
I0123 22:54:41.076368 140066259943424 transformer_layer.py:389] tlayer: self-attention.
I0123 22:54:41.076400 140066259943424 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.076463 140066259943424 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.077014 140066259943424 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.077095 140066259943424 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.077454 140066259943424 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.078143 140066259943424 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.080546 140066259943424 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.081156 140066259943424 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.081239 140066259943424 transformer_layer.py:468] tlayer: End windows.
I0123 22:54:41.081275 140066259943424 transformer_layer.py:472] tlayer: final FFN.
I0123 22:54:41.081333 140066259943424 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.081464 140066259943424 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:54:41.081574 140066259943424 nn_components.py:325] mlp: activation = None
I0123 22:54:41.081615 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:54:41.083505 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.083613 140066259943424 nn_components.py:261] mlp: residual
I0123 22:54:41.085977 140066259943424 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.086064 140066259943424 transformer_base.py:443] tbase: final FFN
I0123 22:54:41.086172 140066259943424 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:54:41.088322 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:54:41.090143 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.090244 140066259943424 nn_components.py:261] mlp: residual
I0123 22:54:41.090533 140066259943424 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.090619 140066259943424 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 22:54:41.090728 140066259943424 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:54:41.090769 140066259943424 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:54:41.090801 140066259943424 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:54:41.090862 140066259943424 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.093050 140066259943424 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:54:41.098435 140066259943424 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.098696 140066259943424 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:54:41.101249 140066259943424 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:54:41.113446 140066259943424 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:54:41.113507 140066259943424 attention.py:418] Single window, no scan.
I0123 22:54:41.113543 140066259943424 transformer_layer.py:389] tlayer: self-attention.
I0123 22:54:41.113574 140066259943424 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.113636 140066259943424 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.114193 140066259943424 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.114274 140066259943424 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.114628 140066259943424 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.115305 140066259943424 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.117701 140066259943424 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.118313 140066259943424 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.118394 140066259943424 transformer_layer.py:468] tlayer: End windows.
I0123 22:54:41.118429 140066259943424 transformer_layer.py:472] tlayer: final FFN.
I0123 22:54:41.118485 140066259943424 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.118613 140066259943424 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:54:41.118720 140066259943424 nn_components.py:325] mlp: activation = None
I0123 22:54:41.118761 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:54:41.120644 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.120741 140066259943424 nn_components.py:261] mlp: residual
I0123 22:54:41.123122 140066259943424 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.123208 140066259943424 transformer_base.py:443] tbase: final FFN
I0123 22:54:41.123316 140066259943424 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:54:41.125458 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:54:41.127298 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.127399 140066259943424 nn_components.py:261] mlp: residual
I0123 22:54:41.127690 140066259943424 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.127778 140066259943424 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 22:54:41.127887 140066259943424 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:54:41.127928 140066259943424 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:54:41.127959 140066259943424 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:54:41.128020 140066259943424 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.130227 140066259943424 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:54:41.135621 140066259943424 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.135887 140066259943424 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:54:41.138433 140066259943424 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:54:41.151033 140066259943424 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:54:41.151093 140066259943424 attention.py:418] Single window, no scan.
I0123 22:54:41.151129 140066259943424 transformer_layer.py:389] tlayer: self-attention.
I0123 22:54:41.151161 140066259943424 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.151223 140066259943424 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.151776 140066259943424 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.151859 140066259943424 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.152218 140066259943424 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.152906 140066259943424 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.155355 140066259943424 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.155970 140066259943424 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.156052 140066259943424 transformer_layer.py:468] tlayer: End windows.
I0123 22:54:41.156088 140066259943424 transformer_layer.py:472] tlayer: final FFN.
I0123 22:54:41.156146 140066259943424 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.156276 140066259943424 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:54:41.156385 140066259943424 nn_components.py:325] mlp: activation = None
I0123 22:54:41.156426 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:54:41.158766 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.158866 140066259943424 nn_components.py:261] mlp: residual
I0123 22:54:41.161241 140066259943424 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.161334 140066259943424 transformer_base.py:443] tbase: final FFN
I0123 22:54:41.161446 140066259943424 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:54:41.163608 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:54:41.165438 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.165539 140066259943424 nn_components.py:261] mlp: residual
I0123 22:54:41.165841 140066259943424 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.165930 140066259943424 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 22:54:41.166039 140066259943424 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:54:41.166081 140066259943424 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:54:41.166113 140066259943424 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:54:41.166175 140066259943424 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.168373 140066259943424 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:54:41.173799 140066259943424 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.174064 140066259943424 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:54:41.176593 140066259943424 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:54:41.188814 140066259943424 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:54:41.188874 140066259943424 attention.py:418] Single window, no scan.
I0123 22:54:41.188911 140066259943424 transformer_layer.py:389] tlayer: self-attention.
I0123 22:54:41.188942 140066259943424 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.189004 140066259943424 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.189553 140066259943424 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.189634 140066259943424 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.189995 140066259943424 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.190674 140066259943424 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.193103 140066259943424 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.193722 140066259943424 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.193804 140066259943424 transformer_layer.py:468] tlayer: End windows.
I0123 22:54:41.193840 140066259943424 transformer_layer.py:472] tlayer: final FFN.
I0123 22:54:41.193896 140066259943424 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.194024 140066259943424 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:54:41.194133 140066259943424 nn_components.py:325] mlp: activation = None
I0123 22:54:41.194174 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:54:41.196080 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.196177 140066259943424 nn_components.py:261] mlp: residual
I0123 22:54:41.198552 140066259943424 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.198646 140066259943424 transformer_base.py:443] tbase: final FFN
I0123 22:54:41.198759 140066259943424 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:54:41.200917 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:54:41.202753 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.202854 140066259943424 nn_components.py:261] mlp: residual
I0123 22:54:41.203144 140066259943424 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.203231 140066259943424 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 22:54:41.203338 140066259943424 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:54:41.203379 140066259943424 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:54:41.203409 140066259943424 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:54:41.203470 140066259943424 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.205667 140066259943424 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:54:41.211066 140066259943424 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.211335 140066259943424 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:54:41.213912 140066259943424 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:54:41.226172 140066259943424 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:54:41.226232 140066259943424 attention.py:418] Single window, no scan.
I0123 22:54:41.226269 140066259943424 transformer_layer.py:389] tlayer: self-attention.
I0123 22:54:41.226300 140066259943424 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.226365 140066259943424 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.226917 140066259943424 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.226999 140066259943424 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.227359 140066259943424 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.228043 140066259943424 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.230459 140066259943424 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.231069 140066259943424 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.231150 140066259943424 transformer_layer.py:468] tlayer: End windows.
I0123 22:54:41.231185 140066259943424 transformer_layer.py:472] tlayer: final FFN.
I0123 22:54:41.231243 140066259943424 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.231372 140066259943424 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:54:41.231480 140066259943424 nn_components.py:325] mlp: activation = None
I0123 22:54:41.231520 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:54:41.233407 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.233504 140066259943424 nn_components.py:261] mlp: residual
I0123 22:54:41.235878 140066259943424 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.235972 140066259943424 transformer_base.py:443] tbase: final FFN
I0123 22:54:41.236084 140066259943424 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:54:41.238240 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:54:41.240067 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.240166 140066259943424 nn_components.py:261] mlp: residual
I0123 22:54:41.240456 140066259943424 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.240542 140066259943424 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 22:54:41.240651 140066259943424 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:54:41.240693 140066259943424 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:54:41.240724 140066259943424 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:54:41.240785 140066259943424 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.242996 140066259943424 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:54:41.248777 140066259943424 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.249039 140066259943424 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:54:41.251600 140066259943424 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:54:41.263874 140066259943424 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:54:41.263935 140066259943424 attention.py:418] Single window, no scan.
I0123 22:54:41.263971 140066259943424 transformer_layer.py:389] tlayer: self-attention.
I0123 22:54:41.264002 140066259943424 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.264063 140066259943424 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.264609 140066259943424 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.264688 140066259943424 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.265042 140066259943424 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.265727 140066259943424 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.268146 140066259943424 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.268761 140066259943424 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.268842 140066259943424 transformer_layer.py:468] tlayer: End windows.
I0123 22:54:41.268878 140066259943424 transformer_layer.py:472] tlayer: final FFN.
I0123 22:54:41.268936 140066259943424 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.269063 140066259943424 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:54:41.269171 140066259943424 nn_components.py:325] mlp: activation = None
I0123 22:54:41.269212 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:54:41.271522 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.271622 140066259943424 nn_components.py:261] mlp: residual
I0123 22:54:41.273980 140066259943424 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.274065 140066259943424 transformer_base.py:443] tbase: final FFN
I0123 22:54:41.274184 140066259943424 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:54:41.276333 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:54:41.278152 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.278252 140066259943424 nn_components.py:261] mlp: residual
I0123 22:54:41.278542 140066259943424 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.278630 140066259943424 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 22:54:41.278738 140066259943424 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:54:41.278779 140066259943424 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:54:41.278811 140066259943424 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:54:41.278871 140066259943424 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.281068 140066259943424 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:54:41.286435 140066259943424 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.286695 140066259943424 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:54:41.289234 140066259943424 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:54:41.301459 140066259943424 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:54:41.301520 140066259943424 attention.py:418] Single window, no scan.
I0123 22:54:41.301556 140066259943424 transformer_layer.py:389] tlayer: self-attention.
I0123 22:54:41.301587 140066259943424 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.301652 140066259943424 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.302209 140066259943424 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.302290 140066259943424 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.302646 140066259943424 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.303326 140066259943424 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.305730 140066259943424 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.306342 140066259943424 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.306423 140066259943424 transformer_layer.py:468] tlayer: End windows.
I0123 22:54:41.306459 140066259943424 transformer_layer.py:472] tlayer: final FFN.
I0123 22:54:41.306516 140066259943424 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.306645 140066259943424 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:54:41.306752 140066259943424 nn_components.py:325] mlp: activation = None
I0123 22:54:41.306792 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:54:41.308659 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.308756 140066259943424 nn_components.py:261] mlp: residual
I0123 22:54:41.311120 140066259943424 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.311204 140066259943424 transformer_base.py:443] tbase: final FFN
I0123 22:54:41.311312 140066259943424 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:54:41.313460 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:54:41.315288 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.315388 140066259943424 nn_components.py:261] mlp: residual
I0123 22:54:41.315675 140066259943424 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.315761 140066259943424 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 22:54:41.315869 140066259943424 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:54:41.315911 140066259943424 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:54:41.315942 140066259943424 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:54:41.316004 140066259943424 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.318196 140066259943424 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:54:41.323570 140066259943424 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.323833 140066259943424 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:54:41.326370 140066259943424 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:54:41.338576 140066259943424 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:54:41.338637 140066259943424 attention.py:418] Single window, no scan.
I0123 22:54:41.338673 140066259943424 transformer_layer.py:389] tlayer: self-attention.
I0123 22:54:41.338704 140066259943424 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.338764 140066259943424 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.339314 140066259943424 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.339394 140066259943424 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.339745 140066259943424 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.340423 140066259943424 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.342844 140066259943424 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.343453 140066259943424 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.343533 140066259943424 transformer_layer.py:468] tlayer: End windows.
I0123 22:54:41.343569 140066259943424 transformer_layer.py:472] tlayer: final FFN.
I0123 22:54:41.343627 140066259943424 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.343755 140066259943424 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:54:41.343861 140066259943424 nn_components.py:325] mlp: activation = None
I0123 22:54:41.343901 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:54:41.345783 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.345880 140066259943424 nn_components.py:261] mlp: residual
I0123 22:54:41.348443 140066259943424 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.348526 140066259943424 transformer_base.py:443] tbase: final FFN
I0123 22:54:41.348636 140066259943424 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:54:41.350782 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:54:41.352626 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.352726 140066259943424 nn_components.py:261] mlp: residual
I0123 22:54:41.353017 140066259943424 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.353104 140066259943424 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 22:54:41.353213 140066259943424 transformer_layer.py:154] tlayer: recurrent = False
I0123 22:54:41.353254 140066259943424 transformer_layer.py:155] tlayer: compute_importance = False
I0123 22:54:41.353285 140066259943424 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 22:54:41.353345 140066259943424 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.355554 140066259943424 transformer_base.py:161] kvq: pre_attn dropout.
I0123 22:54:41.360969 140066259943424 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.361233 140066259943424 transformer_base.py:194] kvq: normalize keys, queries.
I0123 22:54:41.363796 140066259943424 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 22:54:41.376110 140066259943424 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 22:54:41.376170 140066259943424 attention.py:418] Single window, no scan.
I0123 22:54:41.376207 140066259943424 transformer_layer.py:389] tlayer: self-attention.
I0123 22:54:41.376238 140066259943424 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.376298 140066259943424 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.376854 140066259943424 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.376935 140066259943424 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.377290 140066259943424 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.377976 140066259943424 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.380381 140066259943424 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.380993 140066259943424 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.381075 140066259943424 transformer_layer.py:468] tlayer: End windows.
I0123 22:54:41.381109 140066259943424 transformer_layer.py:472] tlayer: final FFN.
I0123 22:54:41.381166 140066259943424 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.381296 140066259943424 transformer_base.py:410] tbase: post-attention MLP.
I0123 22:54:41.381404 140066259943424 nn_components.py:325] mlp: activation = None
I0123 22:54:41.381444 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:54:41.383764 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.383862 140066259943424 nn_components.py:261] mlp: residual
I0123 22:54:41.386256 140066259943424 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.386341 140066259943424 transformer_base.py:443] tbase: final FFN
I0123 22:54:41.386450 140066259943424 nn_components.py:320] mlp: hidden 4096, relu
I0123 22:54:41.388618 140066259943424 nn_components.py:329] mlp: final activation = None
I0123 22:54:41.390441 140066259943424 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.390552 140066259943424 nn_components.py:261] mlp: residual
I0123 22:54:41.390847 140066259943424 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:41.390938 140066259943424 decoder_stack.py:344] dstack: Final layernorm.
I0123 22:54:41.393726 140066259943424 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 22:54:56.248340 140066259943424 alphageometry.py:566] LM output (score=-2.880431): "t : C l n t 37 T m n t q 38 ? H h t r {B*} * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ;"
I0123 22:54:56.248671 140066259943424 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 22:54:56.248728 140066259943424 alphageometry.py:566] LM output (score=-2.882952): "t : C l n t 37 T m n t q 38 ? H h t r {B*} * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ;"
I0123 22:54:56.248759 140066259943424 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 22:54:56.248787 140066259943424 alphageometry.py:566] LM output (score=-2.892967): "t : C l n t 37 T m n t q 38 ? H h t r {B*} * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ;"
I0123 22:54:56.248816 140066259943424 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 22:54:56.248846 140066259943424 alphageometry.py:566] LM output (score=-2.893068): "t : C l n t 37 T m n t q 38 ? H h t r {B*} * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ;"
I0123 22:54:56.248872 140066259943424 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 22:54:56.248897 140066259943424 alphageometry.py:566] LM output (score=-2.897490): "t : C l n t 37 T m n t q 38 ? H h t r {F*} * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ;"
I0123 22:54:56.248921 140066259943424 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 22:54:56.248948 140066259943424 alphageometry.py:566] LM output (score=-2.897549): "t : C l n t 37 T m n t q 38 ? H h t r {F*} * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ;"
I0123 22:54:56.248972 140066259943424 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 22:54:56.248996 140066259943424 alphageometry.py:566] LM output (score=-2.900542): "t : C l n t 37 T m n t q 38 ? H h t r {F*} * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ;"
I0123 22:54:56.249020 140066259943424 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 22:54:56.249044 140066259943424 alphageometry.py:566] LM output (score=-2.902480): "t : C l n t 37 T m n t q 38 ? H h t r {F*} * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ;"
I0123 22:54:56.249069 140066259943424 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 22:54:56.249093 140066259943424 alphageometry.py:566] LM output (score=-2.903235): "t : C l n t 37 T m n t q 38 ? H h t r {F*} * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ;"
I0123 22:54:56.249116 140066259943424 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 22:54:56.249140 140066259943424 alphageometry.py:566] LM output (score=-2.904362): "t : C l n t 37 T m n t q 38 ? H h t r {F*} * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ;"
I0123 22:54:56.249167 140066259943424 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 22:54:56.249196 140066259943424 alphageometry.py:566] LM output (score=-2.904922): "t : C l n t 37 T m n t q 38 ? H h t r {B*} * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ;"
I0123 22:54:56.249221 140066259943424 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 22:54:56.249246 140066259943424 alphageometry.py:566] LM output (score=-2.909571): "t : C l n t 37 T m n t q 38 ? H h t r {B*} * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ;"
I0123 22:54:56.249284 140066259943424 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 22:54:56.249313 140066259943424 alphageometry.py:566] LM output (score=-2.914506): "t : C l n t 37 T m n t q 38 ? H h t r {F*} * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ;"
I0123 22:54:56.249337 140066259943424 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 22:54:56.249361 140066259943424 alphageometry.py:566] LM output (score=-2.915045): "t : C l n t 37 T m n t q 38 ? H h t r {B*} * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ;"
I0123 22:54:56.249384 140066259943424 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 22:54:56.249407 140066259943424 alphageometry.py:566] LM output (score=-2.919315): "t : C l n t 37 T m n t q 38 ? H h t r {F*} * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ;"
I0123 22:54:56.249429 140066259943424 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 22:54:56.249455 140066259943424 alphageometry.py:566] LM output (score=-2.920704): "t : C l n t 37 T m n t q 38 ? H h t r {B*} * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ;"
I0123 22:54:56.249477 140066259943424 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 22:54:56.249500 140066259943424 alphageometry.py:566] LM output (score=-2.924559): "t : C l n t 37 T m n t q 38 ? H h t r {F*} * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ;"
I0123 22:54:56.249523 140066259943424 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 22:54:56.249547 140066259943424 alphageometry.py:566] LM output (score=-2.926162): "t : C l n t 37 T m n t q 38 ? H h t r {F*} * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ;"
I0123 22:54:56.249569 140066259943424 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 22:54:56.249596 140066259943424 alphageometry.py:566] LM output (score=-2.926319): "t : C l n t 37 T m n t q 38 ? H h t r {B*} * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ;"
I0123 22:54:56.249619 140066259943424 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 22:54:56.249648 140066259943424 alphageometry.py:566] LM output (score=-2.933604): "t : C l n t 37 T m n t q 38 ? H h t r {F*} * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ;"
I0123 22:54:56.249674 140066259943424 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 22:54:56.249698 140066259943424 alphageometry.py:566] LM output (score=-2.934803): "t : C l n t 37 T m n t q 38 ? H h t r {B*} * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ;"
I0123 22:54:56.249720 140066259943424 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 22:54:56.249746 140066259943424 alphageometry.py:566] LM output (score=-2.940625): "t : C l n t 37 T m n t q 38 ? H h t r {F*} * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ;"
I0123 22:54:56.249768 140066259943424 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 22:54:56.249790 140066259943424 alphageometry.py:566] LM output (score=-2.943238): "t : C l n t 37 T m n t q 38 ? H h t r {F*} * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ;"
I0123 22:54:56.249813 140066259943424 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 22:54:56.249836 140066259943424 alphageometry.py:566] LM output (score=-2.943999): "t : C l n t 37 T m n t q 38 ? H h t r {B*} * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ;"
I0123 22:54:56.249861 140066259943424 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 22:54:56.249886 140066259943424 alphageometry.py:566] LM output (score=-2.946512): "t : C l n t 37 T m n t q 38 ? H h t r {F*} * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ;"
I0123 22:54:56.249914 140066259943424 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 22:54:56.249939 140066259943424 alphageometry.py:566] LM output (score=-2.948576): "t : C l n t 37 T m n t q 38 ? H h t r {B*} * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ;"
I0123 22:54:56.249962 140066259943424 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 22:54:56.249985 140066259943424 alphageometry.py:566] LM output (score=-2.949878): "t : C l n t 37 T m n t q 38 ? H h t r {F*} * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ;"
I0123 22:54:56.250012 140066259943424 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 22:54:56.250036 140066259943424 alphageometry.py:566] LM output (score=-2.951818): "t : C l n t 37 T m n t q 38 ? H h t r {F*} * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ;"
I0123 22:54:56.250061 140066259943424 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 22:54:56.250086 140066259943424 alphageometry.py:566] LM output (score=-2.953226): "t : C l n t 37 T m n t q 38 ? H h t r {F*} * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ;"
I0123 22:54:56.250109 140066259943424 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 22:54:56.250135 140066259943424 alphageometry.py:566] LM output (score=-2.954144): "t : C l n t 37 T m n t q 38 ? H h t r {F*} * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ;"
I0123 22:54:56.250158 140066259943424 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 22:54:56.250182 140066259943424 alphageometry.py:566] LM output (score=-2.955098): "t : C l n t 37 T m n t q 38 ? H h t r {F*} * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ;"
I0123 22:54:56.250205 140066259943424 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 22:54:56.250228 140066259943424 alphageometry.py:566] LM output (score=-2.958253): "t : C l n t 37 T m n t q 38 ? H h t r {F*} * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ;"
I0123 22:54:56.250251 140066259943424 alphageometry.py:567] Translation: "ERROR: point t already exists."

I0123 22:54:56.250296 140066259943424 alphageometry.py:540] Depth 1. There are 0 nodes to expand:
I0123 22:54:56.250327 140066259943424 alphageometry.py:540] Depth 2. There are 0 nodes to expand:
I0123 22:54:56.250354 140066259943424 alphageometry.py:540] Depth 3. There are 0 nodes to expand:
I0123 22:54:56.250379 140066259943424 alphageometry.py:540] Depth 4. There are 0 nodes to expand:
I0123 22:54:56.250406 140066259943424 alphageometry.py:540] Depth 5. There are 0 nodes to expand:
I0123 22:54:56.250431 140066259943424 alphageometry.py:540] Depth 6. There are 0 nodes to expand:
I0123 22:54:56.250455 140066259943424 alphageometry.py:540] Depth 7. There are 0 nodes to expand:
I0123 22:54:56.250479 140066259943424 alphageometry.py:540] Depth 8. There are 0 nodes to expand:
I0123 22:54:56.250503 140066259943424 alphageometry.py:540] Depth 9. There are 0 nodes to expand:
I0123 22:54:56.250526 140066259943424 alphageometry.py:540] Depth 10. There are 0 nodes to expand:
I0123 22:54:56.250552 140066259943424 alphageometry.py:540] Depth 11. There are 0 nodes to expand:
I0123 22:54:56.250576 140066259943424 alphageometry.py:540] Depth 12. There are 0 nodes to expand:
I0123 22:54:56.250600 140066259943424 alphageometry.py:540] Depth 13. There are 0 nodes to expand:
I0123 22:54:56.250623 140066259943424 alphageometry.py:540] Depth 14. There are 0 nodes to expand:
I0123 22:54:56.250647 140066259943424 alphageometry.py:540] Depth 15. There are 0 nodes to expand:
