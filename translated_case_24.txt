I0123 14:32:37.390482 139841926008832 inference_utils.py:69] Parsing gin configuration.
I0123 14:32:37.390579 139841926008832 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0123 14:32:37.390773 139841926008832 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0123 14:32:37.390806 139841926008832 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0123 14:32:37.390833 139841926008832 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0123 14:32:37.390859 139841926008832 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0123 14:32:37.390883 139841926008832 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0123 14:32:37.390908 139841926008832 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0123 14:32:37.390932 139841926008832 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0123 14:32:37.390957 139841926008832 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=32
I0123 14:32:37.390981 139841926008832 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0123 14:32:37.391005 139841926008832 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0123 14:32:37.391048 139841926008832 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0123 14:32:37.391178 139841926008832 resource_reader.py:55] Path not found: base_htrans.gin
I0123 14:32:37.391376 139841926008832 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0123 14:32:37.391478 139841926008832 resource_reader.py:55] Path not found: trainer_configuration.gin
I0123 14:32:37.397710 139841926008832 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0123 14:32:37.397836 139841926008832 resource_reader.py:55] Path not found: size/medium_150M.gin
I0123 14:32:37.398151 139841926008832 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0123 14:32:37.398252 139841926008832 resource_reader.py:55] Path not found: options/positions_t5.gin
I0123 14:32:37.398524 139841926008832 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0123 14:32:37.398620 139841926008832 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0123 14:32:37.399020 139841926008832 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0123 14:32:37.399117 139841926008832 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0123 14:32:37.402744 139841926008832 training_loop.py:334] ==== Training loop: initializing model ====
I0123 14:32:37.521541 139841926008832 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0123 14:32:37.522264 139841926008832 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0123 14:32:37.528702 139841926008832 training_loop.py:335] Process 0 of 1
I0123 14:32:37.528754 139841926008832 training_loop.py:336] Local device count = 1
I0123 14:32:37.528792 139841926008832 training_loop.py:337] Number of replicas = 1
I0123 14:32:37.528821 139841926008832 training_loop.py:339] Using random number seed 42
I0123 14:32:38.021860 139841926008832 training_loop.py:359] Initializing the model.
I0123 14:32:38.441182 139841926008832 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.441440 139841926008832 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 14:32:38.441542 139841926008832 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 14:32:38.441620 139841926008832 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 14:32:38.441705 139841926008832 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 14:32:38.441800 139841926008832 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 14:32:38.441871 139841926008832 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 14:32:38.441941 139841926008832 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 14:32:38.442013 139841926008832 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 14:32:38.442082 139841926008832 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 14:32:38.442152 139841926008832 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 14:32:38.442220 139841926008832 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 14:32:38.442291 139841926008832 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 14:32:38.442360 139841926008832 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 14:32:38.442399 139841926008832 attention.py:418] Single window, no scan.
I0123 14:32:38.442446 139841926008832 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 14:32:38.442561 139841926008832 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:32:38.442600 139841926008832 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:32:38.442632 139841926008832 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:32:38.444659 139841926008832 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.449979 139841926008832 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:32:38.460723 139841926008832 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.461006 139841926008832 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:32:38.465425 139841926008832 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:32:38.476149 139841926008832 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:32:38.476206 139841926008832 attention.py:418] Single window, no scan.
I0123 14:32:38.476244 139841926008832 transformer_layer.py:389] tlayer: self-attention.
I0123 14:32:38.476278 139841926008832 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.476339 139841926008832 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.477521 139841926008832 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.477600 139841926008832 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.478313 139841926008832 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.480768 139841926008832 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.486532 139841926008832 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.488265 139841926008832 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.488346 139841926008832 transformer_layer.py:468] tlayer: End windows.
I0123 14:32:38.488382 139841926008832 transformer_layer.py:472] tlayer: final FFN.
I0123 14:32:38.488445 139841926008832 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.488574 139841926008832 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:32:38.488920 139841926008832 nn_components.py:325] mlp: activation = None
I0123 14:32:38.488970 139841926008832 nn_components.py:329] mlp: final activation = None
I0123 14:32:38.490915 139841926008832 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.491020 139841926008832 nn_components.py:261] mlp: residual
I0123 14:32:38.493923 139841926008832 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.494008 139841926008832 transformer_base.py:443] tbase: final FFN
I0123 14:32:38.494503 139841926008832 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:32:38.504679 139841926008832 nn_components.py:329] mlp: final activation = None
I0123 14:32:38.513489 139841926008832 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.513586 139841926008832 nn_components.py:261] mlp: residual
I0123 14:32:38.513895 139841926008832 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.513976 139841926008832 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 14:32:38.514086 139841926008832 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:32:38.514126 139841926008832 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:32:38.514158 139841926008832 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:32:38.516020 139841926008832 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.518497 139841926008832 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:32:38.524083 139841926008832 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.524343 139841926008832 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:32:38.526993 139841926008832 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:32:38.530825 139841926008832 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:32:38.530880 139841926008832 attention.py:418] Single window, no scan.
I0123 14:32:38.530916 139841926008832 transformer_layer.py:389] tlayer: self-attention.
I0123 14:32:38.530947 139841926008832 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.531008 139841926008832 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.531575 139841926008832 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.531651 139841926008832 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.532011 139841926008832 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.532781 139841926008832 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.535964 139841926008832 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.536696 139841926008832 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.536776 139841926008832 transformer_layer.py:468] tlayer: End windows.
I0123 14:32:38.536811 139841926008832 transformer_layer.py:472] tlayer: final FFN.
I0123 14:32:38.536874 139841926008832 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.537007 139841926008832 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:32:38.537365 139841926008832 nn_components.py:325] mlp: activation = None
I0123 14:32:38.537408 139841926008832 nn_components.py:329] mlp: final activation = None
I0123 14:32:38.539387 139841926008832 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.539481 139841926008832 nn_components.py:261] mlp: residual
I0123 14:32:38.541999 139841926008832 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.542083 139841926008832 transformer_base.py:443] tbase: final FFN
I0123 14:32:38.542526 139841926008832 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:32:38.544861 139841926008832 nn_components.py:329] mlp: final activation = None
I0123 14:32:38.546810 139841926008832 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.546910 139841926008832 nn_components.py:261] mlp: residual
I0123 14:32:38.547221 139841926008832 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.547304 139841926008832 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 14:32:38.547419 139841926008832 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:32:38.547460 139841926008832 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:32:38.547493 139841926008832 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:32:38.549433 139841926008832 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.551893 139841926008832 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:32:38.557933 139841926008832 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.558193 139841926008832 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:32:38.560873 139841926008832 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:32:38.564834 139841926008832 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:32:38.564893 139841926008832 attention.py:418] Single window, no scan.
I0123 14:32:38.564931 139841926008832 transformer_layer.py:389] tlayer: self-attention.
I0123 14:32:38.564963 139841926008832 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.565025 139841926008832 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.565594 139841926008832 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.565677 139841926008832 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.566042 139841926008832 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.566818 139841926008832 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.569346 139841926008832 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.570028 139841926008832 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.570107 139841926008832 transformer_layer.py:468] tlayer: End windows.
I0123 14:32:38.570142 139841926008832 transformer_layer.py:472] tlayer: final FFN.
I0123 14:32:38.570201 139841926008832 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.570332 139841926008832 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:32:38.570650 139841926008832 nn_components.py:325] mlp: activation = None
I0123 14:32:38.570693 139841926008832 nn_components.py:329] mlp: final activation = None
I0123 14:32:38.572609 139841926008832 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.572701 139841926008832 nn_components.py:261] mlp: residual
I0123 14:32:38.575238 139841926008832 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.575322 139841926008832 transformer_base.py:443] tbase: final FFN
I0123 14:32:38.575817 139841926008832 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:32:38.578108 139841926008832 nn_components.py:329] mlp: final activation = None
I0123 14:32:38.580043 139841926008832 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.580140 139841926008832 nn_components.py:261] mlp: residual
I0123 14:32:38.580434 139841926008832 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.580513 139841926008832 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 14:32:38.580624 139841926008832 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:32:38.580663 139841926008832 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:32:38.580694 139841926008832 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:32:38.582622 139841926008832 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.585050 139841926008832 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:32:38.590766 139841926008832 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.591030 139841926008832 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:32:38.593716 139841926008832 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:32:38.597578 139841926008832 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:32:38.597633 139841926008832 attention.py:418] Single window, no scan.
I0123 14:32:38.597676 139841926008832 transformer_layer.py:389] tlayer: self-attention.
I0123 14:32:38.597708 139841926008832 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.597769 139841926008832 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.598326 139841926008832 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.598402 139841926008832 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.598765 139841926008832 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.599554 139841926008832 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.602137 139841926008832 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.602766 139841926008832 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.602844 139841926008832 transformer_layer.py:468] tlayer: End windows.
I0123 14:32:38.602880 139841926008832 transformer_layer.py:472] tlayer: final FFN.
I0123 14:32:38.602939 139841926008832 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.603072 139841926008832 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:32:38.603400 139841926008832 nn_components.py:325] mlp: activation = None
I0123 14:32:38.603444 139841926008832 nn_components.py:329] mlp: final activation = None
I0123 14:32:38.605368 139841926008832 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.605463 139841926008832 nn_components.py:261] mlp: residual
I0123 14:32:38.608043 139841926008832 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.608130 139841926008832 transformer_base.py:443] tbase: final FFN
I0123 14:32:38.608565 139841926008832 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:32:38.610897 139841926008832 nn_components.py:329] mlp: final activation = None
I0123 14:32:38.612852 139841926008832 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.612951 139841926008832 nn_components.py:261] mlp: residual
I0123 14:32:38.613245 139841926008832 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.613325 139841926008832 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 14:32:38.613435 139841926008832 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:32:38.613475 139841926008832 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:32:38.613508 139841926008832 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:32:38.615448 139841926008832 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.617874 139841926008832 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:32:38.623554 139841926008832 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.623821 139841926008832 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:32:38.626544 139841926008832 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:32:38.630346 139841926008832 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:32:38.630401 139841926008832 attention.py:418] Single window, no scan.
I0123 14:32:38.630438 139841926008832 transformer_layer.py:389] tlayer: self-attention.
I0123 14:32:38.630471 139841926008832 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.630533 139841926008832 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.631098 139841926008832 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.631174 139841926008832 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.631538 139841926008832 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.632317 139841926008832 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.635215 139841926008832 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.635850 139841926008832 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.635931 139841926008832 transformer_layer.py:468] tlayer: End windows.
I0123 14:32:38.635966 139841926008832 transformer_layer.py:472] tlayer: final FFN.
I0123 14:32:38.636028 139841926008832 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.636161 139841926008832 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:32:38.636485 139841926008832 nn_components.py:325] mlp: activation = None
I0123 14:32:38.636527 139841926008832 nn_components.py:329] mlp: final activation = None
I0123 14:32:38.638459 139841926008832 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.638551 139841926008832 nn_components.py:261] mlp: residual
I0123 14:32:38.641113 139841926008832 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.641194 139841926008832 transformer_base.py:443] tbase: final FFN
I0123 14:32:38.641625 139841926008832 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:32:38.643938 139841926008832 nn_components.py:329] mlp: final activation = None
I0123 14:32:38.645941 139841926008832 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.646038 139841926008832 nn_components.py:261] mlp: residual
I0123 14:32:38.646338 139841926008832 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.646420 139841926008832 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 14:32:38.646532 139841926008832 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:32:38.646570 139841926008832 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:32:38.646603 139841926008832 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:32:38.648456 139841926008832 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.650873 139841926008832 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:32:38.656554 139841926008832 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.656818 139841926008832 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:32:38.659568 139841926008832 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:32:38.663362 139841926008832 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:32:38.663419 139841926008832 attention.py:418] Single window, no scan.
I0123 14:32:38.663455 139841926008832 transformer_layer.py:389] tlayer: self-attention.
I0123 14:32:38.663488 139841926008832 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.663550 139841926008832 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.664156 139841926008832 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.664232 139841926008832 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.664591 139841926008832 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.665380 139841926008832 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.667874 139841926008832 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.668506 139841926008832 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.668584 139841926008832 transformer_layer.py:468] tlayer: End windows.
I0123 14:32:38.668620 139841926008832 transformer_layer.py:472] tlayer: final FFN.
I0123 14:32:38.668681 139841926008832 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.668811 139841926008832 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:32:38.669137 139841926008832 nn_components.py:325] mlp: activation = None
I0123 14:32:38.669180 139841926008832 nn_components.py:329] mlp: final activation = None
I0123 14:32:38.671086 139841926008832 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.671180 139841926008832 nn_components.py:261] mlp: residual
I0123 14:32:38.673756 139841926008832 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.673835 139841926008832 transformer_base.py:443] tbase: final FFN
I0123 14:32:38.674269 139841926008832 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:32:38.676628 139841926008832 nn_components.py:329] mlp: final activation = None
I0123 14:32:38.678566 139841926008832 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.678663 139841926008832 nn_components.py:261] mlp: residual
I0123 14:32:38.678957 139841926008832 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.679036 139841926008832 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 14:32:38.679146 139841926008832 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:32:38.679185 139841926008832 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:32:38.679216 139841926008832 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:32:38.681062 139841926008832 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.683521 139841926008832 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:32:38.689165 139841926008832 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.689431 139841926008832 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:32:38.692084 139841926008832 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:32:38.695925 139841926008832 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:32:38.695980 139841926008832 attention.py:418] Single window, no scan.
I0123 14:32:38.696017 139841926008832 transformer_layer.py:389] tlayer: self-attention.
I0123 14:32:38.696049 139841926008832 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.696112 139841926008832 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.696683 139841926008832 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.696763 139841926008832 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.697126 139841926008832 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.697907 139841926008832 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.700396 139841926008832 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.701028 139841926008832 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.701105 139841926008832 transformer_layer.py:468] tlayer: End windows.
I0123 14:32:38.701140 139841926008832 transformer_layer.py:472] tlayer: final FFN.
I0123 14:32:38.701200 139841926008832 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.701327 139841926008832 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:32:38.701654 139841926008832 nn_components.py:325] mlp: activation = None
I0123 14:32:38.701703 139841926008832 nn_components.py:329] mlp: final activation = None
I0123 14:32:38.703686 139841926008832 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.703780 139841926008832 nn_components.py:261] mlp: residual
I0123 14:32:38.706292 139841926008832 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.706371 139841926008832 transformer_base.py:443] tbase: final FFN
I0123 14:32:38.706800 139841926008832 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:32:38.709454 139841926008832 nn_components.py:329] mlp: final activation = None
I0123 14:32:38.711382 139841926008832 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.711483 139841926008832 nn_components.py:261] mlp: residual
I0123 14:32:38.711782 139841926008832 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.711864 139841926008832 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 14:32:38.711974 139841926008832 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:32:38.712013 139841926008832 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:32:38.712045 139841926008832 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:32:38.853007 139841926008832 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.856121 139841926008832 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:32:38.862126 139841926008832 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.862425 139841926008832 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:32:38.865145 139841926008832 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:32:38.869219 139841926008832 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:32:38.869280 139841926008832 attention.py:418] Single window, no scan.
I0123 14:32:38.869319 139841926008832 transformer_layer.py:389] tlayer: self-attention.
I0123 14:32:38.869353 139841926008832 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.869420 139841926008832 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.870055 139841926008832 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.870137 139841926008832 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.870522 139841926008832 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.871347 139841926008832 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.873999 139841926008832 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.874646 139841926008832 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.874726 139841926008832 transformer_layer.py:468] tlayer: End windows.
I0123 14:32:38.874763 139841926008832 transformer_layer.py:472] tlayer: final FFN.
I0123 14:32:38.874825 139841926008832 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.874960 139841926008832 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:32:38.875298 139841926008832 nn_components.py:325] mlp: activation = None
I0123 14:32:38.875342 139841926008832 nn_components.py:329] mlp: final activation = None
I0123 14:32:38.877266 139841926008832 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.877364 139841926008832 nn_components.py:261] mlp: residual
I0123 14:32:38.880047 139841926008832 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.880127 139841926008832 transformer_base.py:443] tbase: final FFN
I0123 14:32:38.880568 139841926008832 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:32:38.882952 139841926008832 nn_components.py:329] mlp: final activation = None
I0123 14:32:38.884907 139841926008832 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.885015 139841926008832 nn_components.py:261] mlp: residual
I0123 14:32:38.885313 139841926008832 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.885396 139841926008832 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 14:32:38.885511 139841926008832 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:32:38.885551 139841926008832 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:32:38.885584 139841926008832 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:32:38.887614 139841926008832 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.890031 139841926008832 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:32:38.895703 139841926008832 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.895972 139841926008832 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:32:38.898679 139841926008832 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:32:38.902519 139841926008832 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:32:38.902578 139841926008832 attention.py:418] Single window, no scan.
I0123 14:32:38.902616 139841926008832 transformer_layer.py:389] tlayer: self-attention.
I0123 14:32:38.902648 139841926008832 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.902710 139841926008832 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.903282 139841926008832 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.903359 139841926008832 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.903721 139841926008832 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.904507 139841926008832 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.907083 139841926008832 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.907711 139841926008832 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.907788 139841926008832 transformer_layer.py:468] tlayer: End windows.
I0123 14:32:38.907824 139841926008832 transformer_layer.py:472] tlayer: final FFN.
I0123 14:32:38.907884 139841926008832 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.908016 139841926008832 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:32:38.908335 139841926008832 nn_components.py:325] mlp: activation = None
I0123 14:32:38.908378 139841926008832 nn_components.py:329] mlp: final activation = None
I0123 14:32:38.910287 139841926008832 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.910384 139841926008832 nn_components.py:261] mlp: residual
I0123 14:32:38.912929 139841926008832 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.913007 139841926008832 transformer_base.py:443] tbase: final FFN
I0123 14:32:38.913448 139841926008832 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:32:38.915732 139841926008832 nn_components.py:329] mlp: final activation = None
I0123 14:32:38.917717 139841926008832 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.917813 139841926008832 nn_components.py:261] mlp: residual
I0123 14:32:38.918104 139841926008832 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.918189 139841926008832 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 14:32:38.918303 139841926008832 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:32:38.918342 139841926008832 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:32:38.918374 139841926008832 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:32:38.920212 139841926008832 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.922826 139841926008832 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:32:38.928388 139841926008832 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.928651 139841926008832 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:32:38.931684 139841926008832 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:32:38.935458 139841926008832 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:32:38.935515 139841926008832 attention.py:418] Single window, no scan.
I0123 14:32:38.935551 139841926008832 transformer_layer.py:389] tlayer: self-attention.
I0123 14:32:38.935583 139841926008832 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.935643 139841926008832 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.936244 139841926008832 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.936320 139841926008832 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.936675 139841926008832 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.937447 139841926008832 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.939946 139841926008832 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.940571 139841926008832 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.940648 139841926008832 transformer_layer.py:468] tlayer: End windows.
I0123 14:32:38.940683 139841926008832 transformer_layer.py:472] tlayer: final FFN.
I0123 14:32:38.940743 139841926008832 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.940878 139841926008832 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:32:38.941199 139841926008832 nn_components.py:325] mlp: activation = None
I0123 14:32:38.941241 139841926008832 nn_components.py:329] mlp: final activation = None
I0123 14:32:38.943157 139841926008832 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.943254 139841926008832 nn_components.py:261] mlp: residual
I0123 14:32:38.945818 139841926008832 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.945898 139841926008832 transformer_base.py:443] tbase: final FFN
I0123 14:32:38.946331 139841926008832 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:32:38.948642 139841926008832 nn_components.py:329] mlp: final activation = None
I0123 14:32:38.950565 139841926008832 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.950661 139841926008832 nn_components.py:261] mlp: residual
I0123 14:32:38.950950 139841926008832 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.951037 139841926008832 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 14:32:38.951149 139841926008832 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:32:38.951189 139841926008832 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:32:38.951221 139841926008832 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:32:38.953061 139841926008832 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.955512 139841926008832 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:32:38.961128 139841926008832 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.961390 139841926008832 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:32:38.964100 139841926008832 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:32:38.967910 139841926008832 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:32:38.967965 139841926008832 attention.py:418] Single window, no scan.
I0123 14:32:38.968002 139841926008832 transformer_layer.py:389] tlayer: self-attention.
I0123 14:32:38.968034 139841926008832 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.968096 139841926008832 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.968653 139841926008832 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.968728 139841926008832 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.969091 139841926008832 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.969876 139841926008832 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.972387 139841926008832 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.973009 139841926008832 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.973087 139841926008832 transformer_layer.py:468] tlayer: End windows.
I0123 14:32:38.973122 139841926008832 transformer_layer.py:472] tlayer: final FFN.
I0123 14:32:38.973186 139841926008832 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.973319 139841926008832 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:32:38.973639 139841926008832 nn_components.py:325] mlp: activation = None
I0123 14:32:38.973691 139841926008832 nn_components.py:329] mlp: final activation = None
I0123 14:32:38.975638 139841926008832 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.975734 139841926008832 nn_components.py:261] mlp: residual
I0123 14:32:38.978478 139841926008832 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.978558 139841926008832 transformer_base.py:443] tbase: final FFN
I0123 14:32:38.978995 139841926008832 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:32:38.981316 139841926008832 nn_components.py:329] mlp: final activation = None
I0123 14:32:38.983210 139841926008832 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.983305 139841926008832 nn_components.py:261] mlp: residual
I0123 14:32:38.983595 139841926008832 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.983676 139841926008832 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 14:32:38.983794 139841926008832 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:32:38.983834 139841926008832 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:32:38.983866 139841926008832 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:32:38.985770 139841926008832 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.988143 139841926008832 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:32:38.993733 139841926008832 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:38.993991 139841926008832 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:32:38.996625 139841926008832 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:32:39.000443 139841926008832 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:32:39.000499 139841926008832 attention.py:418] Single window, no scan.
I0123 14:32:39.000535 139841926008832 transformer_layer.py:389] tlayer: self-attention.
I0123 14:32:39.000566 139841926008832 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:39.000628 139841926008832 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:39.001208 139841926008832 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:39.001284 139841926008832 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:39.001650 139841926008832 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:39.002430 139841926008832 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:39.004929 139841926008832 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:39.005928 139841926008832 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:39.006007 139841926008832 transformer_layer.py:468] tlayer: End windows.
I0123 14:32:39.006043 139841926008832 transformer_layer.py:472] tlayer: final FFN.
I0123 14:32:39.006103 139841926008832 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:39.006237 139841926008832 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:32:39.006564 139841926008832 nn_components.py:325] mlp: activation = None
I0123 14:32:39.006608 139841926008832 nn_components.py:329] mlp: final activation = None
I0123 14:32:39.008528 139841926008832 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:39.008626 139841926008832 nn_components.py:261] mlp: residual
I0123 14:32:39.011154 139841926008832 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:39.011234 139841926008832 transformer_base.py:443] tbase: final FFN
I0123 14:32:39.011725 139841926008832 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:32:39.014005 139841926008832 nn_components.py:329] mlp: final activation = None
I0123 14:32:39.015935 139841926008832 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:39.016031 139841926008832 nn_components.py:261] mlp: residual
I0123 14:32:39.016324 139841926008832 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:39.016608 139841926008832 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 14:32:39.016679 139841926008832 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 14:32:39.016745 139841926008832 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 14:32:39.016803 139841926008832 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 14:32:39.016860 139841926008832 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 14:32:39.016914 139841926008832 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 14:32:39.016968 139841926008832 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 14:32:39.017022 139841926008832 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 14:32:39.017075 139841926008832 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 14:32:39.017128 139841926008832 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 14:32:39.017181 139841926008832 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 14:32:39.017234 139841926008832 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 14:32:39.017272 139841926008832 decoder_stack.py:344] dstack: Final layernorm.
I0123 14:32:39.020807 139841926008832 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:32:39.069340 139841926008832 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.069426 139841926008832 decoder_stack.py:333] dstack: autoregressive generator.
I0123 14:32:39.069480 139841926008832 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 14:32:39.069584 139841926008832 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:32:39.069622 139841926008832 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:32:39.069660 139841926008832 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:32:39.069725 139841926008832 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.072170 139841926008832 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:32:39.077835 139841926008832 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.078096 139841926008832 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:32:39.080775 139841926008832 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:32:39.097592 139841926008832 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:32:39.097653 139841926008832 attention.py:418] Single window, no scan.
I0123 14:32:39.097692 139841926008832 transformer_layer.py:389] tlayer: self-attention.
I0123 14:32:39.097728 139841926008832 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.097792 139841926008832 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.098927 139841926008832 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.099010 139841926008832 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.099717 139841926008832 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.101749 139841926008832 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.106520 139841926008832 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.107849 139841926008832 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.107934 139841926008832 transformer_layer.py:468] tlayer: End windows.
I0123 14:32:39.107970 139841926008832 transformer_layer.py:472] tlayer: final FFN.
I0123 14:32:39.108032 139841926008832 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.108164 139841926008832 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:32:39.108274 139841926008832 nn_components.py:325] mlp: activation = None
I0123 14:32:39.108314 139841926008832 nn_components.py:329] mlp: final activation = None
I0123 14:32:39.110260 139841926008832 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.110357 139841926008832 nn_components.py:261] mlp: residual
I0123 14:32:39.112812 139841926008832 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.112895 139841926008832 transformer_base.py:443] tbase: final FFN
I0123 14:32:39.113008 139841926008832 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:32:39.115293 139841926008832 nn_components.py:329] mlp: final activation = None
I0123 14:32:39.117259 139841926008832 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.117357 139841926008832 nn_components.py:261] mlp: residual
I0123 14:32:39.117656 139841926008832 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.117749 139841926008832 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 14:32:39.117858 139841926008832 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:32:39.117898 139841926008832 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:32:39.117930 139841926008832 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:32:39.117994 139841926008832 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.120255 139841926008832 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:32:39.125781 139841926008832 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.126041 139841926008832 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:32:39.128759 139841926008832 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:32:39.142132 139841926008832 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:32:39.142189 139841926008832 attention.py:418] Single window, no scan.
I0123 14:32:39.142225 139841926008832 transformer_layer.py:389] tlayer: self-attention.
I0123 14:32:39.142257 139841926008832 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.142318 139841926008832 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.142880 139841926008832 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.142956 139841926008832 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.143314 139841926008832 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.144004 139841926008832 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.146525 139841926008832 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.147145 139841926008832 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.147222 139841926008832 transformer_layer.py:468] tlayer: End windows.
I0123 14:32:39.147263 139841926008832 transformer_layer.py:472] tlayer: final FFN.
I0123 14:32:39.147326 139841926008832 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.147458 139841926008832 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:32:39.147568 139841926008832 nn_components.py:325] mlp: activation = None
I0123 14:32:39.147608 139841926008832 nn_components.py:329] mlp: final activation = None
I0123 14:32:39.149562 139841926008832 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.149663 139841926008832 nn_components.py:261] mlp: residual
I0123 14:32:39.152109 139841926008832 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.152189 139841926008832 transformer_base.py:443] tbase: final FFN
I0123 14:32:39.152299 139841926008832 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:32:39.154571 139841926008832 nn_components.py:329] mlp: final activation = None
I0123 14:32:39.156533 139841926008832 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.156628 139841926008832 nn_components.py:261] mlp: residual
I0123 14:32:39.156919 139841926008832 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.157000 139841926008832 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 14:32:39.157108 139841926008832 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:32:39.157146 139841926008832 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:32:39.157178 139841926008832 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:32:39.157240 139841926008832 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.159509 139841926008832 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:32:39.165016 139841926008832 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.165277 139841926008832 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:32:39.167997 139841926008832 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:32:39.180927 139841926008832 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:32:39.180982 139841926008832 attention.py:418] Single window, no scan.
I0123 14:32:39.181018 139841926008832 transformer_layer.py:389] tlayer: self-attention.
I0123 14:32:39.181050 139841926008832 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.181112 139841926008832 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.181688 139841926008832 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.181766 139841926008832 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.182131 139841926008832 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.182824 139841926008832 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.185518 139841926008832 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.186167 139841926008832 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.186246 139841926008832 transformer_layer.py:468] tlayer: End windows.
I0123 14:32:39.186281 139841926008832 transformer_layer.py:472] tlayer: final FFN.
I0123 14:32:39.186345 139841926008832 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.186475 139841926008832 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:32:39.186583 139841926008832 nn_components.py:325] mlp: activation = None
I0123 14:32:39.186621 139841926008832 nn_components.py:329] mlp: final activation = None
I0123 14:32:39.188569 139841926008832 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.188663 139841926008832 nn_components.py:261] mlp: residual
I0123 14:32:39.195285 139841926008832 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.195407 139841926008832 transformer_base.py:443] tbase: final FFN
I0123 14:32:39.195525 139841926008832 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:32:39.197925 139841926008832 nn_components.py:329] mlp: final activation = None
I0123 14:32:39.199902 139841926008832 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.199996 139841926008832 nn_components.py:261] mlp: residual
I0123 14:32:39.200294 139841926008832 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.200379 139841926008832 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 14:32:39.200492 139841926008832 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:32:39.200534 139841926008832 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:32:39.200566 139841926008832 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:32:39.200632 139841926008832 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.202966 139841926008832 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:32:39.208496 139841926008832 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.208765 139841926008832 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:32:39.211547 139841926008832 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:32:39.224937 139841926008832 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:32:39.224996 139841926008832 attention.py:418] Single window, no scan.
I0123 14:32:39.225034 139841926008832 transformer_layer.py:389] tlayer: self-attention.
I0123 14:32:39.225066 139841926008832 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.225131 139841926008832 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.225723 139841926008832 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.225801 139841926008832 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.226208 139841926008832 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.226941 139841926008832 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.229460 139841926008832 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.230098 139841926008832 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.230176 139841926008832 transformer_layer.py:468] tlayer: End windows.
I0123 14:32:39.230211 139841926008832 transformer_layer.py:472] tlayer: final FFN.
I0123 14:32:39.230274 139841926008832 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.230408 139841926008832 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:32:39.230518 139841926008832 nn_components.py:325] mlp: activation = None
I0123 14:32:39.230557 139841926008832 nn_components.py:329] mlp: final activation = None
I0123 14:32:39.232500 139841926008832 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.232595 139841926008832 nn_components.py:261] mlp: residual
I0123 14:32:39.235026 139841926008832 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.235105 139841926008832 transformer_base.py:443] tbase: final FFN
I0123 14:32:39.235212 139841926008832 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:32:39.237446 139841926008832 nn_components.py:329] mlp: final activation = None
I0123 14:32:39.239344 139841926008832 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.239441 139841926008832 nn_components.py:261] mlp: residual
I0123 14:32:39.239725 139841926008832 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.239806 139841926008832 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 14:32:39.239914 139841926008832 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:32:39.239953 139841926008832 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:32:39.239985 139841926008832 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:32:39.240049 139841926008832 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.242641 139841926008832 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:32:39.248145 139841926008832 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.248412 139841926008832 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:32:39.251049 139841926008832 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:32:39.263984 139841926008832 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:32:39.264039 139841926008832 attention.py:418] Single window, no scan.
I0123 14:32:39.264075 139841926008832 transformer_layer.py:389] tlayer: self-attention.
I0123 14:32:39.264106 139841926008832 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.264168 139841926008832 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.264729 139841926008832 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.264806 139841926008832 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.265167 139841926008832 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.265874 139841926008832 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.268435 139841926008832 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.269068 139841926008832 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.269145 139841926008832 transformer_layer.py:468] tlayer: End windows.
I0123 14:32:39.269181 139841926008832 transformer_layer.py:472] tlayer: final FFN.
I0123 14:32:39.269240 139841926008832 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.269377 139841926008832 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:32:39.269487 139841926008832 nn_components.py:325] mlp: activation = None
I0123 14:32:39.269528 139841926008832 nn_components.py:329] mlp: final activation = None
I0123 14:32:39.271435 139841926008832 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.271529 139841926008832 nn_components.py:261] mlp: residual
I0123 14:32:39.273991 139841926008832 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.274072 139841926008832 transformer_base.py:443] tbase: final FFN
I0123 14:32:39.274180 139841926008832 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:32:39.276503 139841926008832 nn_components.py:329] mlp: final activation = None
I0123 14:32:39.278423 139841926008832 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.278520 139841926008832 nn_components.py:261] mlp: residual
I0123 14:32:39.278809 139841926008832 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.278889 139841926008832 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 14:32:39.278999 139841926008832 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:32:39.279038 139841926008832 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:32:39.279070 139841926008832 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:32:39.279135 139841926008832 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.281414 139841926008832 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:32:39.286974 139841926008832 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.287232 139841926008832 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:32:39.289967 139841926008832 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:32:39.302920 139841926008832 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:32:39.302975 139841926008832 attention.py:418] Single window, no scan.
I0123 14:32:39.303011 139841926008832 transformer_layer.py:389] tlayer: self-attention.
I0123 14:32:39.303042 139841926008832 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.303105 139841926008832 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.303675 139841926008832 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.303751 139841926008832 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.304113 139841926008832 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.304819 139841926008832 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.307340 139841926008832 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.307997 139841926008832 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.308096 139841926008832 transformer_layer.py:468] tlayer: End windows.
I0123 14:32:39.308132 139841926008832 transformer_layer.py:472] tlayer: final FFN.
I0123 14:32:39.308192 139841926008832 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.308323 139841926008832 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:32:39.308443 139841926008832 nn_components.py:325] mlp: activation = None
I0123 14:32:39.308484 139841926008832 nn_components.py:329] mlp: final activation = None
I0123 14:32:39.310495 139841926008832 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.310591 139841926008832 nn_components.py:261] mlp: residual
I0123 14:32:39.313020 139841926008832 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.313100 139841926008832 transformer_base.py:443] tbase: final FFN
I0123 14:32:39.313207 139841926008832 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:32:39.315450 139841926008832 nn_components.py:329] mlp: final activation = None
I0123 14:32:39.317322 139841926008832 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.317418 139841926008832 nn_components.py:261] mlp: residual
I0123 14:32:39.317712 139841926008832 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.317795 139841926008832 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 14:32:39.317904 139841926008832 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:32:39.317943 139841926008832 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:32:39.317975 139841926008832 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:32:39.318037 139841926008832 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.320473 139841926008832 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:32:39.326045 139841926008832 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.326307 139841926008832 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:32:39.328933 139841926008832 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:32:39.341960 139841926008832 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:32:39.342016 139841926008832 attention.py:418] Single window, no scan.
I0123 14:32:39.342053 139841926008832 transformer_layer.py:389] tlayer: self-attention.
I0123 14:32:39.342086 139841926008832 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.342148 139841926008832 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.342718 139841926008832 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.342795 139841926008832 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.343154 139841926008832 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.343850 139841926008832 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.346368 139841926008832 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.347372 139841926008832 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.347450 139841926008832 transformer_layer.py:468] tlayer: End windows.
I0123 14:32:39.347486 139841926008832 transformer_layer.py:472] tlayer: final FFN.
I0123 14:32:39.347546 139841926008832 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.347680 139841926008832 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:32:39.347791 139841926008832 nn_components.py:325] mlp: activation = None
I0123 14:32:39.347835 139841926008832 nn_components.py:329] mlp: final activation = None
I0123 14:32:39.349765 139841926008832 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.349859 139841926008832 nn_components.py:261] mlp: residual
I0123 14:32:39.352313 139841926008832 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.352396 139841926008832 transformer_base.py:443] tbase: final FFN
I0123 14:32:39.352508 139841926008832 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:32:39.354782 139841926008832 nn_components.py:329] mlp: final activation = None
I0123 14:32:39.356766 139841926008832 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.356863 139841926008832 nn_components.py:261] mlp: residual
I0123 14:32:39.357157 139841926008832 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.357239 139841926008832 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 14:32:39.357349 139841926008832 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:32:39.357389 139841926008832 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:32:39.357421 139841926008832 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:32:39.357485 139841926008832 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.359754 139841926008832 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:32:39.365255 139841926008832 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.365531 139841926008832 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:32:39.368265 139841926008832 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:32:39.381211 139841926008832 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:32:39.381268 139841926008832 attention.py:418] Single window, no scan.
I0123 14:32:39.381304 139841926008832 transformer_layer.py:389] tlayer: self-attention.
I0123 14:32:39.381335 139841926008832 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.381398 139841926008832 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.382011 139841926008832 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.382088 139841926008832 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.382446 139841926008832 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.383136 139841926008832 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.385614 139841926008832 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.386262 139841926008832 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.386340 139841926008832 transformer_layer.py:468] tlayer: End windows.
I0123 14:32:39.386375 139841926008832 transformer_layer.py:472] tlayer: final FFN.
I0123 14:32:39.386435 139841926008832 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.386562 139841926008832 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:32:39.386672 139841926008832 nn_components.py:325] mlp: activation = None
I0123 14:32:39.386717 139841926008832 nn_components.py:329] mlp: final activation = None
I0123 14:32:39.388617 139841926008832 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.388713 139841926008832 nn_components.py:261] mlp: residual
I0123 14:32:39.391222 139841926008832 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.391302 139841926008832 transformer_base.py:443] tbase: final FFN
I0123 14:32:39.391412 139841926008832 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:32:39.393661 139841926008832 nn_components.py:329] mlp: final activation = None
I0123 14:32:39.395541 139841926008832 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.395637 139841926008832 nn_components.py:261] mlp: residual
I0123 14:32:39.395923 139841926008832 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.396004 139841926008832 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 14:32:39.396114 139841926008832 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:32:39.396154 139841926008832 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:32:39.396186 139841926008832 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:32:39.396249 139841926008832 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.398509 139841926008832 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:32:39.404076 139841926008832 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.404338 139841926008832 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:32:39.406994 139841926008832 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:32:39.419987 139841926008832 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:32:39.420043 139841926008832 attention.py:418] Single window, no scan.
I0123 14:32:39.420079 139841926008832 transformer_layer.py:389] tlayer: self-attention.
I0123 14:32:39.420111 139841926008832 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.420171 139841926008832 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.420736 139841926008832 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.420925 139841926008832 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.421282 139841926008832 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.422000 139841926008832 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.424529 139841926008832 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.425208 139841926008832 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.425287 139841926008832 transformer_layer.py:468] tlayer: End windows.
I0123 14:32:39.425323 139841926008832 transformer_layer.py:472] tlayer: final FFN.
I0123 14:32:39.425383 139841926008832 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.425518 139841926008832 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:32:39.425631 139841926008832 nn_components.py:325] mlp: activation = None
I0123 14:32:39.425681 139841926008832 nn_components.py:329] mlp: final activation = None
I0123 14:32:39.427578 139841926008832 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.427673 139841926008832 nn_components.py:261] mlp: residual
I0123 14:32:39.430112 139841926008832 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.430193 139841926008832 transformer_base.py:443] tbase: final FFN
I0123 14:32:39.430302 139841926008832 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:32:39.432525 139841926008832 nn_components.py:329] mlp: final activation = None
I0123 14:32:39.434498 139841926008832 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.434596 139841926008832 nn_components.py:261] mlp: residual
I0123 14:32:39.434887 139841926008832 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.434970 139841926008832 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 14:32:39.435079 139841926008832 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:32:39.435118 139841926008832 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:32:39.435150 139841926008832 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:32:39.435214 139841926008832 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.437460 139841926008832 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:32:39.442929 139841926008832 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.443190 139841926008832 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:32:39.445922 139841926008832 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:32:39.459089 139841926008832 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:32:39.459146 139841926008832 attention.py:418] Single window, no scan.
I0123 14:32:39.459183 139841926008832 transformer_layer.py:389] tlayer: self-attention.
I0123 14:32:39.459214 139841926008832 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.459277 139841926008832 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.459887 139841926008832 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.459965 139841926008832 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.460326 139841926008832 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.461024 139841926008832 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.463526 139841926008832 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.464160 139841926008832 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.464238 139841926008832 transformer_layer.py:468] tlayer: End windows.
I0123 14:32:39.464274 139841926008832 transformer_layer.py:472] tlayer: final FFN.
I0123 14:32:39.464334 139841926008832 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.464469 139841926008832 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:32:39.464581 139841926008832 nn_components.py:325] mlp: activation = None
I0123 14:32:39.464620 139841926008832 nn_components.py:329] mlp: final activation = None
I0123 14:32:39.466522 139841926008832 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.466624 139841926008832 nn_components.py:261] mlp: residual
I0123 14:32:39.469115 139841926008832 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.469196 139841926008832 transformer_base.py:443] tbase: final FFN
I0123 14:32:39.469305 139841926008832 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:32:39.471562 139841926008832 nn_components.py:329] mlp: final activation = None
I0123 14:32:39.473449 139841926008832 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.473545 139841926008832 nn_components.py:261] mlp: residual
I0123 14:32:39.473849 139841926008832 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.473932 139841926008832 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 14:32:39.474042 139841926008832 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:32:39.474082 139841926008832 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:32:39.474114 139841926008832 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:32:39.474178 139841926008832 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.476445 139841926008832 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:32:39.482023 139841926008832 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.482283 139841926008832 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:32:39.484980 139841926008832 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:32:39.498034 139841926008832 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:32:39.498093 139841926008832 attention.py:418] Single window, no scan.
I0123 14:32:39.498129 139841926008832 transformer_layer.py:389] tlayer: self-attention.
I0123 14:32:39.498160 139841926008832 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.498222 139841926008832 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.498784 139841926008832 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.498859 139841926008832 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.499220 139841926008832 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.499916 139841926008832 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.502422 139841926008832 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.503098 139841926008832 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.503175 139841926008832 transformer_layer.py:468] tlayer: End windows.
I0123 14:32:39.503211 139841926008832 transformer_layer.py:472] tlayer: final FFN.
I0123 14:32:39.503272 139841926008832 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.503402 139841926008832 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:32:39.503512 139841926008832 nn_components.py:325] mlp: activation = None
I0123 14:32:39.503551 139841926008832 nn_components.py:329] mlp: final activation = None
I0123 14:32:39.505457 139841926008832 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.505558 139841926008832 nn_components.py:261] mlp: residual
I0123 14:32:39.508013 139841926008832 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.508093 139841926008832 transformer_base.py:443] tbase: final FFN
I0123 14:32:39.508200 139841926008832 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:32:39.510434 139841926008832 nn_components.py:329] mlp: final activation = None
I0123 14:32:39.512404 139841926008832 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.512500 139841926008832 nn_components.py:261] mlp: residual
I0123 14:32:39.512792 139841926008832 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.512874 139841926008832 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 14:32:39.512984 139841926008832 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:32:39.513024 139841926008832 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:32:39.513056 139841926008832 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:32:39.513119 139841926008832 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.515402 139841926008832 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:32:39.520916 139841926008832 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.521177 139841926008832 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:32:39.523924 139841926008832 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:32:39.536774 139841926008832 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:32:39.536831 139841926008832 attention.py:418] Single window, no scan.
I0123 14:32:39.536868 139841926008832 transformer_layer.py:389] tlayer: self-attention.
I0123 14:32:39.536900 139841926008832 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.536962 139841926008832 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.537515 139841926008832 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.537591 139841926008832 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.537955 139841926008832 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.538698 139841926008832 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.541208 139841926008832 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.541842 139841926008832 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.541919 139841926008832 transformer_layer.py:468] tlayer: End windows.
I0123 14:32:39.541956 139841926008832 transformer_layer.py:472] tlayer: final FFN.
I0123 14:32:39.542017 139841926008832 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.542147 139841926008832 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:32:39.542256 139841926008832 nn_components.py:325] mlp: activation = None
I0123 14:32:39.542294 139841926008832 nn_components.py:329] mlp: final activation = None
I0123 14:32:39.544178 139841926008832 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.544273 139841926008832 nn_components.py:261] mlp: residual
I0123 14:32:39.546710 139841926008832 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.546790 139841926008832 transformer_base.py:443] tbase: final FFN
I0123 14:32:39.546901 139841926008832 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:32:39.549196 139841926008832 nn_components.py:329] mlp: final activation = None
I0123 14:32:39.551074 139841926008832 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.551171 139841926008832 nn_components.py:261] mlp: residual
I0123 14:32:39.551459 139841926008832 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.551550 139841926008832 decoder_stack.py:344] dstack: Final layernorm.
I0123 14:32:39.554428 139841926008832 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:32:39.610455 139841926008832 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.610543 139841926008832 decoder_stack.py:333] dstack: autoregressive generator.
I0123 14:32:39.610597 139841926008832 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 14:32:39.610703 139841926008832 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:32:39.610742 139841926008832 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:32:39.610772 139841926008832 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:32:39.610835 139841926008832 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.613510 139841926008832 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:32:39.618926 139841926008832 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.619189 139841926008832 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:32:39.621785 139841926008832 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:32:39.634340 139841926008832 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:32:39.634397 139841926008832 attention.py:418] Single window, no scan.
I0123 14:32:39.634433 139841926008832 transformer_layer.py:389] tlayer: self-attention.
I0123 14:32:39.634464 139841926008832 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.634524 139841926008832 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.635079 139841926008832 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.635155 139841926008832 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.635509 139841926008832 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.636188 139841926008832 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.638701 139841926008832 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.639318 139841926008832 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.639395 139841926008832 transformer_layer.py:468] tlayer: End windows.
I0123 14:32:39.639430 139841926008832 transformer_layer.py:472] tlayer: final FFN.
I0123 14:32:39.639490 139841926008832 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.639620 139841926008832 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:32:39.639735 139841926008832 nn_components.py:325] mlp: activation = None
I0123 14:32:39.639775 139841926008832 nn_components.py:329] mlp: final activation = None
I0123 14:32:39.641618 139841926008832 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.641721 139841926008832 nn_components.py:261] mlp: residual
I0123 14:32:39.644128 139841926008832 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.644207 139841926008832 transformer_base.py:443] tbase: final FFN
I0123 14:32:39.644317 139841926008832 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:32:39.646579 139841926008832 nn_components.py:329] mlp: final activation = None
I0123 14:32:39.648459 139841926008832 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.648556 139841926008832 nn_components.py:261] mlp: residual
I0123 14:32:39.648844 139841926008832 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.648926 139841926008832 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 14:32:39.649035 139841926008832 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:32:39.649075 139841926008832 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:32:39.649106 139841926008832 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:32:39.649168 139841926008832 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.651428 139841926008832 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:32:39.656832 139841926008832 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.657095 139841926008832 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:32:39.659976 139841926008832 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:32:39.672636 139841926008832 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:32:39.672691 139841926008832 attention.py:418] Single window, no scan.
I0123 14:32:39.672728 139841926008832 transformer_layer.py:389] tlayer: self-attention.
I0123 14:32:39.672760 139841926008832 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.672822 139841926008832 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.673380 139841926008832 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.673457 139841926008832 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.673823 139841926008832 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.674515 139841926008832 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.677032 139841926008832 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.677658 139841926008832 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.677737 139841926008832 transformer_layer.py:468] tlayer: End windows.
I0123 14:32:39.677773 139841926008832 transformer_layer.py:472] tlayer: final FFN.
I0123 14:32:39.677834 139841926008832 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.677964 139841926008832 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:32:39.678074 139841926008832 nn_components.py:325] mlp: activation = None
I0123 14:32:39.678119 139841926008832 nn_components.py:329] mlp: final activation = None
I0123 14:32:39.680133 139841926008832 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.680227 139841926008832 nn_components.py:261] mlp: residual
I0123 14:32:39.682635 139841926008832 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.682715 139841926008832 transformer_base.py:443] tbase: final FFN
I0123 14:32:39.682825 139841926008832 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:32:39.685104 139841926008832 nn_components.py:329] mlp: final activation = None
I0123 14:32:39.686974 139841926008832 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.687072 139841926008832 nn_components.py:261] mlp: residual
I0123 14:32:39.687365 139841926008832 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.687448 139841926008832 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 14:32:39.687557 139841926008832 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:32:39.687597 139841926008832 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:32:39.687629 139841926008832 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:32:39.687693 139841926008832 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.689962 139841926008832 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:32:39.695372 139841926008832 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.695633 139841926008832 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:32:39.698312 139841926008832 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:32:39.710849 139841926008832 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:32:39.710906 139841926008832 attention.py:418] Single window, no scan.
I0123 14:32:39.710942 139841926008832 transformer_layer.py:389] tlayer: self-attention.
I0123 14:32:39.710974 139841926008832 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.711035 139841926008832 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.711588 139841926008832 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.711665 139841926008832 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.712022 139841926008832 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.712706 139841926008832 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.715255 139841926008832 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.715874 139841926008832 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.715952 139841926008832 transformer_layer.py:468] tlayer: End windows.
I0123 14:32:39.715988 139841926008832 transformer_layer.py:472] tlayer: final FFN.
I0123 14:32:39.716049 139841926008832 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.716178 139841926008832 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:32:39.716287 139841926008832 nn_components.py:325] mlp: activation = None
I0123 14:32:39.716327 139841926008832 nn_components.py:329] mlp: final activation = None
I0123 14:32:39.718354 139841926008832 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.718448 139841926008832 nn_components.py:261] mlp: residual
I0123 14:32:39.721031 139841926008832 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.721112 139841926008832 transformer_base.py:443] tbase: final FFN
I0123 14:32:39.721222 139841926008832 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:32:39.723974 139841926008832 nn_components.py:329] mlp: final activation = None
I0123 14:32:39.725862 139841926008832 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.725959 139841926008832 nn_components.py:261] mlp: residual
I0123 14:32:39.726249 139841926008832 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.726330 139841926008832 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 14:32:39.726439 139841926008832 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:32:39.726479 139841926008832 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:32:39.726511 139841926008832 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:32:39.726576 139841926008832 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.728832 139841926008832 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:32:39.734247 139841926008832 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.734507 139841926008832 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:32:39.737206 139841926008832 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:32:39.749906 139841926008832 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:32:39.749963 139841926008832 attention.py:418] Single window, no scan.
I0123 14:32:39.750001 139841926008832 transformer_layer.py:389] tlayer: self-attention.
I0123 14:32:39.750044 139841926008832 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.750108 139841926008832 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.750669 139841926008832 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.750744 139841926008832 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.751105 139841926008832 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.751797 139841926008832 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.754351 139841926008832 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.754971 139841926008832 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.755046 139841926008832 transformer_layer.py:468] tlayer: End windows.
I0123 14:32:39.755081 139841926008832 transformer_layer.py:472] tlayer: final FFN.
I0123 14:32:39.755140 139841926008832 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.755269 139841926008832 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:32:39.755376 139841926008832 nn_components.py:325] mlp: activation = None
I0123 14:32:39.755416 139841926008832 nn_components.py:329] mlp: final activation = None
I0123 14:32:39.757306 139841926008832 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.757399 139841926008832 nn_components.py:261] mlp: residual
I0123 14:32:39.759834 139841926008832 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.759914 139841926008832 transformer_base.py:443] tbase: final FFN
I0123 14:32:39.760022 139841926008832 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:32:39.762325 139841926008832 nn_components.py:329] mlp: final activation = None
I0123 14:32:39.764216 139841926008832 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.764310 139841926008832 nn_components.py:261] mlp: residual
I0123 14:32:39.764595 139841926008832 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.764675 139841926008832 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 14:32:39.764783 139841926008832 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:32:39.764821 139841926008832 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:32:39.764852 139841926008832 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:32:39.764914 139841926008832 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.767163 139841926008832 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:32:39.772612 139841926008832 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.772871 139841926008832 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:32:39.775603 139841926008832 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:32:39.788344 139841926008832 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:32:39.788399 139841926008832 attention.py:418] Single window, no scan.
I0123 14:32:39.788434 139841926008832 transformer_layer.py:389] tlayer: self-attention.
I0123 14:32:39.788464 139841926008832 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.788526 139841926008832 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.789077 139841926008832 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.789152 139841926008832 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.789506 139841926008832 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.790198 139841926008832 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.792733 139841926008832 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.793362 139841926008832 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.793438 139841926008832 transformer_layer.py:468] tlayer: End windows.
I0123 14:32:39.793473 139841926008832 transformer_layer.py:472] tlayer: final FFN.
I0123 14:32:39.793532 139841926008832 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.793665 139841926008832 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:32:39.793776 139841926008832 nn_components.py:325] mlp: activation = None
I0123 14:32:39.793813 139841926008832 nn_components.py:329] mlp: final activation = None
I0123 14:32:39.795683 139841926008832 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.795781 139841926008832 nn_components.py:261] mlp: residual
I0123 14:32:39.798186 139841926008832 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.798264 139841926008832 transformer_base.py:443] tbase: final FFN
I0123 14:32:39.798372 139841926008832 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:32:39.800666 139841926008832 nn_components.py:329] mlp: final activation = None
I0123 14:32:39.802519 139841926008832 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.802614 139841926008832 nn_components.py:261] mlp: residual
I0123 14:32:39.802901 139841926008832 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.802981 139841926008832 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 14:32:39.803088 139841926008832 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:32:39.803127 139841926008832 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:32:39.803157 139841926008832 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:32:39.803217 139841926008832 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.805462 139841926008832 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:32:39.810921 139841926008832 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.811180 139841926008832 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:32:39.813890 139841926008832 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:32:39.826914 139841926008832 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:32:39.826969 139841926008832 attention.py:418] Single window, no scan.
I0123 14:32:39.827003 139841926008832 transformer_layer.py:389] tlayer: self-attention.
I0123 14:32:39.827033 139841926008832 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.827094 139841926008832 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.827651 139841926008832 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.827727 139841926008832 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.828086 139841926008832 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.828767 139841926008832 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.831315 139841926008832 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.831930 139841926008832 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.832005 139841926008832 transformer_layer.py:468] tlayer: End windows.
I0123 14:32:39.832039 139841926008832 transformer_layer.py:472] tlayer: final FFN.
I0123 14:32:39.832097 139841926008832 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.832223 139841926008832 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:32:39.832333 139841926008832 nn_components.py:325] mlp: activation = None
I0123 14:32:39.832372 139841926008832 nn_components.py:329] mlp: final activation = None
I0123 14:32:39.834253 139841926008832 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.834353 139841926008832 nn_components.py:261] mlp: residual
I0123 14:32:39.836736 139841926008832 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.836815 139841926008832 transformer_base.py:443] tbase: final FFN
I0123 14:32:39.836928 139841926008832 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:32:39.839611 139841926008832 nn_components.py:329] mlp: final activation = None
I0123 14:32:39.841471 139841926008832 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.841565 139841926008832 nn_components.py:261] mlp: residual
I0123 14:32:39.841857 139841926008832 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.841938 139841926008832 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 14:32:39.842045 139841926008832 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:32:39.842083 139841926008832 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:32:39.842113 139841926008832 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:32:39.842175 139841926008832 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.844387 139841926008832 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:32:39.849801 139841926008832 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.850061 139841926008832 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:32:39.852770 139841926008832 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:32:39.865699 139841926008832 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:32:39.865753 139841926008832 attention.py:418] Single window, no scan.
I0123 14:32:39.865788 139841926008832 transformer_layer.py:389] tlayer: self-attention.
I0123 14:32:39.865818 139841926008832 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.865878 139841926008832 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.866468 139841926008832 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.866545 139841926008832 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.866912 139841926008832 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.867620 139841926008832 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.870193 139841926008832 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.870843 139841926008832 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.870922 139841926008832 transformer_layer.py:468] tlayer: End windows.
I0123 14:32:39.870957 139841926008832 transformer_layer.py:472] tlayer: final FFN.
I0123 14:32:39.871018 139841926008832 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.871150 139841926008832 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:32:39.871266 139841926008832 nn_components.py:325] mlp: activation = None
I0123 14:32:39.871304 139841926008832 nn_components.py:329] mlp: final activation = None
I0123 14:32:39.873170 139841926008832 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.873263 139841926008832 nn_components.py:261] mlp: residual
I0123 14:32:39.875741 139841926008832 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.875819 139841926008832 transformer_base.py:443] tbase: final FFN
I0123 14:32:39.875927 139841926008832 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:32:39.878241 139841926008832 nn_components.py:329] mlp: final activation = None
I0123 14:32:39.880165 139841926008832 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.880259 139841926008832 nn_components.py:261] mlp: residual
I0123 14:32:39.880544 139841926008832 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.880624 139841926008832 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 14:32:39.880731 139841926008832 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:32:39.880769 139841926008832 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:32:39.880800 139841926008832 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:32:39.880862 139841926008832 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.883171 139841926008832 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:32:39.888717 139841926008832 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.888978 139841926008832 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:32:39.891743 139841926008832 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:32:39.904494 139841926008832 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:32:39.904549 139841926008832 attention.py:418] Single window, no scan.
I0123 14:32:39.904584 139841926008832 transformer_layer.py:389] tlayer: self-attention.
I0123 14:32:39.904614 139841926008832 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.904674 139841926008832 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.905241 139841926008832 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.905315 139841926008832 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.905676 139841926008832 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.906363 139841926008832 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.908899 139841926008832 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.909524 139841926008832 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.909600 139841926008832 transformer_layer.py:468] tlayer: End windows.
I0123 14:32:39.909634 139841926008832 transformer_layer.py:472] tlayer: final FFN.
I0123 14:32:39.909707 139841926008832 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.909836 139841926008832 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:32:39.909949 139841926008832 nn_components.py:325] mlp: activation = None
I0123 14:32:39.909987 139841926008832 nn_components.py:329] mlp: final activation = None
I0123 14:32:39.911857 139841926008832 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.911950 139841926008832 nn_components.py:261] mlp: residual
I0123 14:32:39.914376 139841926008832 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.914463 139841926008832 transformer_base.py:443] tbase: final FFN
I0123 14:32:39.914573 139841926008832 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:32:39.916864 139841926008832 nn_components.py:329] mlp: final activation = None
I0123 14:32:39.918745 139841926008832 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.918840 139841926008832 nn_components.py:261] mlp: residual
I0123 14:32:39.919128 139841926008832 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.919208 139841926008832 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 14:32:39.919315 139841926008832 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:32:39.919353 139841926008832 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:32:39.919384 139841926008832 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:32:39.919446 139841926008832 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.922045 139841926008832 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:32:39.927485 139841926008832 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.927744 139841926008832 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:32:39.930471 139841926008832 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:32:39.943188 139841926008832 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:32:39.943243 139841926008832 attention.py:418] Single window, no scan.
I0123 14:32:39.943280 139841926008832 transformer_layer.py:389] tlayer: self-attention.
I0123 14:32:39.943311 139841926008832 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.943371 139841926008832 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.943932 139841926008832 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.944007 139841926008832 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.944363 139841926008832 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.945054 139841926008832 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.947647 139841926008832 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.948268 139841926008832 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.948343 139841926008832 transformer_layer.py:468] tlayer: End windows.
I0123 14:32:39.948377 139841926008832 transformer_layer.py:472] tlayer: final FFN.
I0123 14:32:39.948436 139841926008832 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.948563 139841926008832 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:32:39.948669 139841926008832 nn_components.py:325] mlp: activation = None
I0123 14:32:39.948707 139841926008832 nn_components.py:329] mlp: final activation = None
I0123 14:32:39.950592 139841926008832 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.950685 139841926008832 nn_components.py:261] mlp: residual
I0123 14:32:39.953096 139841926008832 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.953180 139841926008832 transformer_base.py:443] tbase: final FFN
I0123 14:32:39.953287 139841926008832 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:32:39.955972 139841926008832 nn_components.py:329] mlp: final activation = None
I0123 14:32:39.957840 139841926008832 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.957941 139841926008832 nn_components.py:261] mlp: residual
I0123 14:32:39.958453 139841926008832 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.958535 139841926008832 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 14:32:39.958644 139841926008832 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:32:39.958682 139841926008832 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:32:39.958712 139841926008832 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:32:39.958775 139841926008832 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.961008 139841926008832 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:32:39.966485 139841926008832 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.966761 139841926008832 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:32:39.969442 139841926008832 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:32:39.982139 139841926008832 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:32:39.982194 139841926008832 attention.py:418] Single window, no scan.
I0123 14:32:39.982229 139841926008832 transformer_layer.py:389] tlayer: self-attention.
I0123 14:32:39.982259 139841926008832 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.982319 139841926008832 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.982880 139841926008832 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.982954 139841926008832 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.983305 139841926008832 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.983986 139841926008832 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.986535 139841926008832 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.987151 139841926008832 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.987227 139841926008832 transformer_layer.py:468] tlayer: End windows.
I0123 14:32:39.987261 139841926008832 transformer_layer.py:472] tlayer: final FFN.
I0123 14:32:39.987317 139841926008832 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.987443 139841926008832 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:32:39.987549 139841926008832 nn_components.py:325] mlp: activation = None
I0123 14:32:39.987586 139841926008832 nn_components.py:329] mlp: final activation = None
I0123 14:32:39.989974 139841926008832 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.990070 139841926008832 nn_components.py:261] mlp: residual
I0123 14:32:39.992469 139841926008832 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.992547 139841926008832 transformer_base.py:443] tbase: final FFN
I0123 14:32:39.992663 139841926008832 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:32:39.994925 139841926008832 nn_components.py:329] mlp: final activation = None
I0123 14:32:39.996768 139841926008832 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.996863 139841926008832 nn_components.py:261] mlp: residual
I0123 14:32:39.997152 139841926008832 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.997233 139841926008832 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 14:32:39.997341 139841926008832 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:32:39.997380 139841926008832 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:32:39.997410 139841926008832 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:32:39.997473 139841926008832 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:39.999711 139841926008832 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:32:40.005153 139841926008832 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:40.005412 139841926008832 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:32:40.008102 139841926008832 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:32:40.020756 139841926008832 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:32:40.020811 139841926008832 attention.py:418] Single window, no scan.
I0123 14:32:40.020846 139841926008832 transformer_layer.py:389] tlayer: self-attention.
I0123 14:32:40.020876 139841926008832 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:40.020935 139841926008832 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:40.021500 139841926008832 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:40.021575 139841926008832 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:40.022090 139841926008832 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:40.022781 139841926008832 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:40.025440 139841926008832 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:40.026075 139841926008832 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:40.026153 139841926008832 transformer_layer.py:468] tlayer: End windows.
I0123 14:32:40.026186 139841926008832 transformer_layer.py:472] tlayer: final FFN.
I0123 14:32:40.026244 139841926008832 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:40.026373 139841926008832 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:32:40.026480 139841926008832 nn_components.py:325] mlp: activation = None
I0123 14:32:40.026518 139841926008832 nn_components.py:329] mlp: final activation = None
I0123 14:32:40.028382 139841926008832 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:40.028474 139841926008832 nn_components.py:261] mlp: residual
I0123 14:32:40.030884 139841926008832 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:40.030963 139841926008832 transformer_base.py:443] tbase: final FFN
I0123 14:32:40.031070 139841926008832 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:32:40.033375 139841926008832 nn_components.py:329] mlp: final activation = None
I0123 14:32:40.035219 139841926008832 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:40.035314 139841926008832 nn_components.py:261] mlp: residual
I0123 14:32:40.035597 139841926008832 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:40.035677 139841926008832 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 14:32:40.035784 139841926008832 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:32:40.035822 139841926008832 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:32:40.035853 139841926008832 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:32:40.035914 139841926008832 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:40.038153 139841926008832 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:32:40.043624 139841926008832 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:40.043884 139841926008832 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:32:40.046595 139841926008832 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:32:40.059531 139841926008832 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:32:40.059587 139841926008832 attention.py:418] Single window, no scan.
I0123 14:32:40.059623 139841926008832 transformer_layer.py:389] tlayer: self-attention.
I0123 14:32:40.059654 139841926008832 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:40.059716 139841926008832 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:40.060298 139841926008832 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:40.060379 139841926008832 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:40.060743 139841926008832 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:40.061436 139841926008832 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:40.064010 139841926008832 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:40.064631 139841926008832 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:40.064709 139841926008832 transformer_layer.py:468] tlayer: End windows.
I0123 14:32:40.064744 139841926008832 transformer_layer.py:472] tlayer: final FFN.
I0123 14:32:40.064801 139841926008832 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:40.064928 139841926008832 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:32:40.065039 139841926008832 nn_components.py:325] mlp: activation = None
I0123 14:32:40.065078 139841926008832 nn_components.py:329] mlp: final activation = None
I0123 14:32:40.066961 139841926008832 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:40.067054 139841926008832 nn_components.py:261] mlp: residual
I0123 14:32:40.069466 139841926008832 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:40.069544 139841926008832 transformer_base.py:443] tbase: final FFN
I0123 14:32:40.069658 139841926008832 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:32:40.072319 139841926008832 nn_components.py:329] mlp: final activation = None
I0123 14:32:40.074189 139841926008832 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:40.074284 139841926008832 nn_components.py:261] mlp: residual
I0123 14:32:40.074568 139841926008832 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:40.074653 139841926008832 decoder_stack.py:344] dstack: Final layernorm.
I0123 14:32:40.077480 139841926008832 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:32:44.515958 139841926008832 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
I0123 14:32:45.064881 139841926008832 training_loop.py:409] No working directory specified.
I0123 14:32:45.064994 139841926008832 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0123 14:32:45.065764 139841926008832 checkpoints.py:1062] Restoring legacy Flax checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0123 14:32:48.229418 139841926008832 training_loop.py:447] Only restoring trainable parameters.
I0123 14:32:48.230087 139841926008832 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0123 14:32:48.230145 139841926008832 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0123 14:32:48.230189 139841926008832 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 14:32:48.230229 139841926008832 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 14:32:48.230268 139841926008832 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:32:48.230306 139841926008832 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 14:32:48.230342 139841926008832 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:32:48.230379 139841926008832 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:32:48.230416 139841926008832 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 14:32:48.230452 139841926008832 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 14:32:48.230488 139841926008832 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:32:48.230527 139841926008832 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 14:32:48.230564 139841926008832 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 14:32:48.230599 139841926008832 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 14:32:48.230633 139841926008832 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:32:48.230668 139841926008832 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 14:32:48.230703 139841926008832 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:32:48.230738 139841926008832 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:32:48.230774 139841926008832 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 14:32:48.230808 139841926008832 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 14:32:48.230856 139841926008832 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:32:48.230893 139841926008832 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 14:32:48.230928 139841926008832 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 14:32:48.230962 139841926008832 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 14:32:48.230996 139841926008832 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:32:48.231030 139841926008832 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 14:32:48.231064 139841926008832 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:32:48.231098 139841926008832 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:32:48.231132 139841926008832 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 14:32:48.231166 139841926008832 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 14:32:48.231200 139841926008832 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:32:48.231235 139841926008832 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 14:32:48.231271 139841926008832 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 14:32:48.231306 139841926008832 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 14:32:48.231340 139841926008832 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:32:48.231374 139841926008832 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 14:32:48.231409 139841926008832 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:32:48.231443 139841926008832 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:32:48.231477 139841926008832 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 14:32:48.231511 139841926008832 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 14:32:48.231544 139841926008832 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:32:48.231578 139841926008832 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 14:32:48.231611 139841926008832 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 14:32:48.231645 139841926008832 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 14:32:48.231679 139841926008832 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:32:48.231712 139841926008832 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 14:32:48.231751 139841926008832 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:32:48.231786 139841926008832 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:32:48.231821 139841926008832 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 14:32:48.231854 139841926008832 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 14:32:48.231889 139841926008832 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:32:48.231922 139841926008832 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 14:32:48.231956 139841926008832 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 14:32:48.231990 139841926008832 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 14:32:48.232023 139841926008832 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:32:48.232057 139841926008832 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 14:32:48.232090 139841926008832 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:32:48.232123 139841926008832 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:32:48.232156 139841926008832 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 14:32:48.232190 139841926008832 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 14:32:48.232223 139841926008832 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:32:48.232257 139841926008832 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 14:32:48.232290 139841926008832 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 14:32:48.232324 139841926008832 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 14:32:48.232358 139841926008832 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:32:48.232392 139841926008832 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 14:32:48.232426 139841926008832 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:32:48.232459 139841926008832 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:32:48.232494 139841926008832 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 14:32:48.232527 139841926008832 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 14:32:48.232561 139841926008832 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:32:48.232595 139841926008832 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 14:32:48.232629 139841926008832 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 14:32:48.232667 139841926008832 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 14:32:48.232702 139841926008832 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:32:48.232736 139841926008832 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 14:32:48.232769 139841926008832 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:32:48.232803 139841926008832 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:32:48.232836 139841926008832 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 14:32:48.232869 139841926008832 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 14:32:48.232903 139841926008832 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:32:48.232937 139841926008832 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 14:32:48.232972 139841926008832 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 14:32:48.233005 139841926008832 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 14:32:48.233039 139841926008832 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:32:48.233073 139841926008832 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 14:32:48.233107 139841926008832 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:32:48.233141 139841926008832 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:32:48.233174 139841926008832 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 14:32:48.233208 139841926008832 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 14:32:48.233241 139841926008832 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:32:48.233275 139841926008832 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 14:32:48.233309 139841926008832 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 14:32:48.233343 139841926008832 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 14:32:48.233377 139841926008832 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:32:48.233410 139841926008832 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 14:32:48.233444 139841926008832 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:32:48.233478 139841926008832 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:32:48.233512 139841926008832 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 14:32:48.233546 139841926008832 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 14:32:48.233585 139841926008832 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:32:48.233620 139841926008832 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 14:32:48.233661 139841926008832 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 14:32:48.233699 139841926008832 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 14:32:48.233734 139841926008832 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:32:48.233769 139841926008832 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 14:32:48.233803 139841926008832 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:32:48.233837 139841926008832 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:32:48.233870 139841926008832 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 14:32:48.233904 139841926008832 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 14:32:48.233937 139841926008832 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:32:48.233971 139841926008832 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 14:32:48.234004 139841926008832 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 14:32:48.234038 139841926008832 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 14:32:48.234072 139841926008832 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:32:48.234105 139841926008832 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 14:32:48.234139 139841926008832 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:32:48.234172 139841926008832 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:32:48.234206 139841926008832 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 14:32:48.234240 139841926008832 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 14:32:48.234273 139841926008832 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:32:48.234307 139841926008832 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 14:32:48.234333 139841926008832 training_loop.py:725] Total parameters: 152072288
I0123 14:32:48.234540 139841926008832 training_loop.py:739] Total state size: 0
I0123 14:32:48.254873 139841926008832 training_loop.py:492] Training loop: creating task for mode beam_search
I0123 14:32:48.255126 139841926008832 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0123 14:32:48.255617 139841926008832 training_loop.py:652] Compiling mode beam_search with jit.
I0123 14:32:48.255931 139841926008832 training_loop.py:89] registering functions: dict_keys([])
I0123 14:32:48.272264 139841926008832 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = foot e c b a; f = foot f b c a; g = on_line g c e, on_line g b f; h = circle h c g a; i = angle_bisector i b c a, on_line i b a; j = on_circle j h c, on_line j i c; k = circle k c j b; l = foot l a c j; m = foot m j c a; n = on_line n j m, on_line n l a ? cong k n d n
I0123 14:32:51.789039 139841926008832 ddar.py:60] Depth 1/1000 time = 3.4727816581726074
I0123 14:32:57.992684 139841926008832 ddar.py:60] Depth 2/1000 time = 6.203486442565918
I0123 14:33:07.206824 139841926008832 ddar.py:60] Depth 3/1000 time = 9.213937997817993
I0123 14:33:16.478833 139841926008832 ddar.py:60] Depth 4/1000 time = 9.271777153015137
I0123 14:33:26.323524 139841926008832 ddar.py:60] Depth 5/1000 time = 9.844428300857544
I0123 14:33:36.608232 139841926008832 ddar.py:60] Depth 6/1000 time = 10.278292417526245
I0123 14:33:51.052784 139841926008832 ddar.py:60] Depth 7/1000 time = 14.444323062896729
I0123 14:34:10.556104 139841926008832 ddar.py:60] Depth 8/1000 time = 19.503057718276978
I0123 14:34:10.654568 139841926008832 alphageometry.py:191] 
==========================
 * From theorem premises:
A B C D E F G H I J K L M N : Points
CB:CA = CB:CA [00]
DB = DA [01]
DC = DB [02]
E,A,B are collinear [03]
CE  AB [04]
BF  AC [05]
C,F,A are collinear [06]
E,C,G are collinear [07]
G,F,B are collinear [08]
HG = HA [09]
HC = HG [10]
I,A,B are collinear [11]
ICB = ACI [12]
HJ = HC [13]
C,I,J are collinear [14]
KC = KJ [15]
KJ = KB [16]
LA  CJ [17]
C,L,J are collinear [18]
C,A,M are collinear [19]
CA  MJ [20]
L,A,N are collinear [21]
M,J,N are collinear [22]

 * Auxiliary Constructions:
: Points


 * Proof steps:
001. DB = DA [01] & DC = DB [02]   DC = DA [23]
002. HG = HA [09] & HC = HG [10]   HC = HA [24]
003. DC = DA [23] & HC = HA [24] (SSS)  CDH = HDA [25]
004. DC = DA [23] & HC = HA [24]   CA  DH [26]
005. LA  CJ [17] & C,I,J are collinear [14]   AL  CI [27]
006. HC = HG [10] & HJ = HC [13] & HG = HA [09]   G,C,J,A are concyclic [28]
007. G,C,J,A are concyclic [28]   GJC = GAC [29]
008. G,C,J,A are concyclic [28]   GCJ = GAJ [30]
009. GJC = GAC [29] & C,I,J are collinear [14]   (GJ-CI) = GAC [31]
010. GCJ = GAJ [30] & E,C,G are collinear [07] & C,I,J are collinear [14]   ECI = GAJ [32]
011. HJ = HC [13]   HCJ = CJH [33]
012. HCJ = CJH [33] & C,I,J are collinear [14]   HCI = (CI-HJ) [34]
013. HC = HG [10]   HCG = CGH [35]
014. HCG = CGH [35] & E,C,G are collinear [07]   HCE = (CE-GH) [36]
015. HC = HG [10] & HJ = HC [13]   HJ = HG [37]
016. HJ = HG [37]   HJG = JGH [38]
017. C,M,A are collinear [19] & C,L,J are collinear [18] & C,I,J are collinear [14] & AL  CI [27] & BF  AC [05] & CA  MJ [20]   AMJ = ALJ [39]
018. AMJ = ALJ [39]   L,J,A,M are concyclic [40]
019. L,J,A,M are concyclic [40]   LAJ = LMJ [41]
020. L,J,A,M are concyclic [40]   LJA = LMA [42]
021. LAJ = LMJ [41] & CA  MJ [20] & BF  AC [05]   LAJ = (LM-BF) [43]
022. BF  AC [05] & AL  CI [27] & (GJ-CI) = GAC [31] & ECI = GAJ [32] & HCI = (CI-HJ) [34] & HCE = (CE-GH) [36] & HJG = JGH [38] & LAJ = (LM-BF) [43] (Angle chase)  (CH-LM) = 1_PI/2 [44]
023. E,A,B are collinear [03] & E,G,C are collinear [07] & C,F,A are collinear [06] & G,F,B are collinear [08] & BF  AC [05] & CE  AB [04]   AEG = AFG [45]
024. AEG = AFG [45]   E,G,F,A are concyclic [46]
025. E,G,F,A are concyclic [46]   EGA = EFA [47]
026. E,G,F,A are concyclic [46]   EFG = EAG [48]
027. C,F,A are collinear [06] & LJA = LMA [42] & C,L,J are collinear [18] & C,I,J are collinear [14] & C,A,M are collinear [19] & GCJ = GAJ [30] & E,C,G are collinear [07] & EGA = EFA [47]   EFC = (LM-CF) [49]
028. EFC = (LM-CF) [49]   EF  LM [50]
029. DC = DB [02]   DBC = BCD [51]
030. DB = DA [01]   DBA = BAD [52]
031. DC = DA [23]   DCA = CAD [53]
032. EFG = EAG [48] & G,F,B are collinear [08] & E,A,B are collinear [03]   EFB = BAG [54]
033. E,A,B are collinear [03] & C,F,A are collinear [06] & BF  AC [05] & CE  AB [04]   BEC = BFC [55]
034. BEC = BFC [55]   E,C,F,B are concyclic [56]
035. E,C,F,B are concyclic [56]   EFC = EBC [57]
036. E,C,F,B are concyclic [56]   CEF = CBF [58]
037. EFC = EBC [57] & C,F,A are collinear [06] & E,A,B are collinear [03]   (EF-AC) = ABC [59]
038. CE  AB [04] & AL  CI [27] & ECI = GAJ [32] & DBC = BCD [51] & DBA = BAD [52] & DCA = CAD [53] & LAJ = (LM-BF) [43] & EFB = BAG [54] & (EF-AC) = ABC [59] (Angle chase)  (AD-LM) = 1_PI/2 [60]
039. CDH = HDA [25] & CA  DH [26] & BF  AC [05] & (CH-LM) = 1_PI/2 [44] & EF  LM [50] & (AD-LM) = 1_PI/2 [60]   CDH = DHC [61]
040. CDH = DHC [61]   CD = CH [62]
041. HJ = HC [13] & CD = CH [62] & DC = DB [02]   DB = HJ [63]
042. E,A,B are collinear [03] & C,F,A are collinear [06] & G,F,B are collinear [08] & BF  AC [05] & CE  AB [04]   AEC = CFG [64]
043. C,F,A are collinear [06] & G,C,E are collinear [07]   ACE = FCG [65]
044. AEC = CFG [64] & ACE = FCG [65] (Similar Triangles)  EA:GF = CA:GC [66]
045. E,A,B are collinear [03] & E,G,C are collinear [07] & G,F,B are collinear [08] & C,F,A are collinear [06] & BF  AC [05] & CE  AB [04]   BEG = GFC [67]
046. G,F,B are collinear [08] & E,G,C are collinear [07]   BGE = FGC [68]
047. BEG = GFC [67] & BGE = FGC [68] (Similar Triangles)  GB:GC = EG:GF [69]
048. EA:GF = CA:GC [66] & GB:GC = EG:GF [69]   EA:EG = CA:GB [70]
049. I,A,B are collinear [11] & E,A,B are collinear [03] & CE  AB [04]   EC  EI [71]
050. EC  EI [71] & AL  CI [27]   ECI = (EI-LA) [72]
051. C,L,J are collinear [18] & C,I,J are collinear [14] & E,A,B are collinear [03] & E,G,C are collinear [07] & ECI = (EI-LA) [72] & I,A,B are collinear [11]   ALJ = AEG [73]
052. C,L,J are collinear [18] & C,I,J are collinear [14] & E,G,C are collinear [07] & GCJ = GAJ [30]   AJL = AGE [74]
053. ALJ = AEG [73] & AJL = AGE [74] (Similar Triangles)  LA:LJ = EA:EG [75]
054. C,L,J are collinear [18] & C,I,J are collinear [14] & L,A,N are collinear [21] & AL  CI [27]   CLA = NLJ [76]
055. G,F,B are collinear [08] & C,F,A are collinear [06] & BF  AC [05]   GF  CF [77]
056. GF  CF [77] & AL  CI [27]   (GF-CI) = (CF-LA) [78]
057. M,J,N are collinear [22] & C,L,J are collinear [18] & C,I,J are collinear [14] & (GF-CI) = (CF-LA) [78] & G,F,B are collinear [08] & C,F,A are collinear [06] & CA  MJ [20] & BF  AC [05]   CAL = NJL [79]
058. CLA = NLJ [76] & CAL = NJL [79] (Similar Triangles)  CA:JN = LA:LJ [80]
059. EA:EG = CA:GB [70] & LA:LJ = EA:EG [75] & CA:JN = LA:LJ [80]   CA:JN = CA:GB [81]
060. CB:CA = CB:CA [00] & CA:JN = CA:GB [81]   JN = GB [82]
061. M,J,N are collinear [22] & G,F,B are collinear [08] & CA  MJ [20] & BF  AC [05]   JNG = BGN [83]
062. JN = GB [82] & JNG = BGN [83] (SAS)  GJ = NB [84]
063. JN = GB [82] & JNG = BGN [83] (SAS)  NJG = GBN [85]
064. CE  AB [04] & BF  AC [05] & (GJ-CI) = GAC [31] & DBC = BCD [51] & DBA = BAD [52] & DCA = CAD [53] & HCI = (CI-HJ) [34] & HCE = (CE-GH) [36] & HJG = JGH [38] & EFB = BAG [54] & (EF-AC) = ABC [59] (Angle chase)  BD  GH [86]
065. HGJ = GJH [38] & GH  BD [86] & NJG = GBN [85] & M,J,N are collinear [22] & G,F,B are collinear [08] & CA  MJ [20] & BF  AC [05]   DBN = GJH [87]
066. DB = HJ [63] & GJ = NB [84] & DBN = GJH [87] (SAS)  ND = GH [88]
067. KC = KJ [15] & KJ = KB [16]   KB = KC [89]
068. DC = DB [02] & KB = KC [89]   BC  DK [90]
069. BC  DK [90] & AL  CI [27]   (LA-CI) = (DK-CB) [91]
070. G,F,B are collinear [08] & C,F,A are collinear [06] & AL  CI [27] & BF  AC [05]   GFC = (LA-CI) [92]
071. C,F,A are collinear [06] & ACI = ICB [12]   FCI = ICB [93]
072. GFC = (LA-CI) [92] & FCI = ICB [93]   (GF-CI) = (LA-CB) [94]
073. (GF-CI) = (LA-CB) [94] & G,F,B are collinear [08] & CA  DH [26] & BF  AC [05]   (DH-CI) = (LA-CB) [95]
074. (LA-CI) = (DK-CB) [91] & (DH-CI) = (LA-CB) [95]   (AL-DH) = (DK-AL) [96]
075. HJ = HC [13] & KC = KJ [15]   CJ  HK [97]
076. (AL-DH) = (DK-AL) [96] & CA  DH [26] & BF  AC [05] & CJ  HK [97] & AL  CI [27] & C,I,J are collinear [14]   DHK = HKD [98]
077. DHK = HKD [98]   HD = DK [99]
078. ND = GH [88] & HC = HG [10] & HD = DK [99]   HC:HD = DN:DK [100]
079. AL  CI [27] & (AD-LM) = 1_PI/2 [60] & EF  LM [50]   (DA-EF) = (CI-LA) [101]
080. HCI = (CI-HJ) [34] & (CH-LM) = 1_PI/2 [44] & EF  LM [50] & (AD-LM) = 1_PI/2 [60]   (DA-CI) = (CI-HJ) [102]
081. (DA-EF) = (CI-LA) [101] & (DA-CI) = (CI-HJ) [102]   (EF-CI) = (AL-HJ) [103]
082. C,F,A are collinear [06] & (EF-CI) = (AL-HJ) [103] & EF  LM [50] & LJA = LMA [42] & C,L,J are collinear [18] & C,I,J are collinear [14] & C,A,M are collinear [19]   (CF-JA) = (LA-HJ) [104]
083. (LA-CI) = (DK-CB) [91] & FCI = ICB [93]   (LA-CF) = (DK-CI) [105]
084. C,F,A are collinear [06] & DH  AC [26]   DH  CF [106]
085. DH  CF [106] & BC  DK [90]   (DH-CB) = (CF-DK) [107]
086. C,F,A are collinear [06] & (DH-CB) = (CF-DK) [107] & CA  DH [26] & BF  AC [05] & CEF = CBF [58] & EGA = EFA [47] & E,C,G are collinear [07]   (GA-CF) = (DK-CF) [108]
087. (GA-CF) = (DK-CF) [108]   GA  DK [109]
088. C,F,A are collinear [06] & (LA-CF) = (DK-CI) [105] & DK  AG [109]   FCI = LAG [110]
089. (CF-JA) = (LA-HJ) [104] & FCI = LAG [110]   (AG-HJ) = (CI-AJ) [111]
090. C,F,A are collinear [06] & BF  AC [05] & (AD-LM) = 1_PI/2 [60] & EF  LM [50] & CA  DH [26]   (DA-EF) = (DH-CF) [112]
091. C,F,A are collinear [06] & LJA = LMA [42] & C,L,J are collinear [18] & C,I,J are collinear [14] & C,A,M are collinear [19] & LM  EF [50]   (JA-CF) = (CI-EF) [113]
092. (DA-EF) = (DH-CF) [112] & (JA-CF) = (CI-EF) [113]   ADH = (CI-AJ) [114]
093. HC = HG [10] & CD = CH [62] & DC = DB [02]   DB = HG [115]
094. GH  BD [86] & NJG = GBN [85] & M,J,N are collinear [22] & G,F,B are collinear [08] & CA  MJ [20] & BF  AC [05]   DBN = HGJ [116]
095. DB = HG [115] & GJ = NB [84] & DBN = HGJ [116] (SAS)  (BD-GH) = (DN-HJ) [117]
096. (AG-HJ) = (CI-AJ) [111] & ADH = (CI-AJ) [114] & (CH-LM) = 1_PI/2 [44] & EF  LM [50] & (AD-LM) = 1_PI/2 [60] & CA  DH [26] & BF  AC [05] & AG  DK [109] & (BD-GH) = (DN-HJ) [117] & BD  GH [86]   CHD = KDN [118]
097. HC:HD = DN:DK [100] & CHD = KDN [118] (Similar Triangles)  CH:CD = ND:NK [119]
098. CH:CD = ND:NK [119] & CD = CH [62]   ND = NK
==========================

