I0123 11:42:19.356713 139918641729536 inference_utils.py:69] Parsing gin configuration.
I0123 11:42:19.356811 139918641729536 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0123 11:42:19.357010 139918641729536 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0123 11:42:19.357043 139918641729536 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0123 11:42:19.357070 139918641729536 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0123 11:42:19.357096 139918641729536 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0123 11:42:19.357123 139918641729536 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0123 11:42:19.357149 139918641729536 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0123 11:42:19.357175 139918641729536 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0123 11:42:19.357200 139918641729536 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=32
I0123 11:42:19.357227 139918641729536 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0123 11:42:19.357251 139918641729536 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0123 11:42:19.357296 139918641729536 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0123 11:42:19.357432 139918641729536 resource_reader.py:55] Path not found: base_htrans.gin
I0123 11:42:19.357633 139918641729536 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0123 11:42:19.357756 139918641729536 resource_reader.py:55] Path not found: trainer_configuration.gin
I0123 11:42:19.364036 139918641729536 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0123 11:42:19.364161 139918641729536 resource_reader.py:55] Path not found: size/medium_150M.gin
I0123 11:42:19.364483 139918641729536 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0123 11:42:19.364585 139918641729536 resource_reader.py:55] Path not found: options/positions_t5.gin
I0123 11:42:19.364862 139918641729536 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0123 11:42:19.364961 139918641729536 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0123 11:42:19.365366 139918641729536 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0123 11:42:19.365464 139918641729536 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0123 11:42:19.369169 139918641729536 training_loop.py:334] ==== Training loop: initializing model ====
I0123 11:42:19.467484 139918641729536 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0123 11:42:19.468205 139918641729536 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0123 11:42:19.474772 139918641729536 training_loop.py:335] Process 0 of 1
I0123 11:42:19.474826 139918641729536 training_loop.py:336] Local device count = 1
I0123 11:42:19.474865 139918641729536 training_loop.py:337] Number of replicas = 1
I0123 11:42:19.474895 139918641729536 training_loop.py:339] Using random number seed 42
I0123 11:42:19.943526 139918641729536 training_loop.py:359] Initializing the model.
I0123 11:42:20.354635 139918641729536 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.355066 139918641729536 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 11:42:20.355169 139918641729536 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:42:20.355248 139918641729536 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:42:20.355325 139918641729536 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:42:20.355405 139918641729536 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:42:20.355476 139918641729536 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:42:20.355545 139918641729536 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:42:20.355612 139918641729536 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:42:20.355679 139918641729536 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:42:20.355746 139918641729536 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:42:20.355812 139918641729536 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:42:20.355879 139918641729536 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:42:20.355947 139918641729536 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:42:20.355986 139918641729536 attention.py:418] Single window, no scan.
I0123 11:42:20.356030 139918641729536 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 11:42:20.356142 139918641729536 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:42:20.356183 139918641729536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:42:20.356215 139918641729536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:42:20.358273 139918641729536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.363661 139918641729536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:42:20.374455 139918641729536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.374731 139918641729536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:42:20.379137 139918641729536 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:42:20.389784 139918641729536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:42:20.389840 139918641729536 attention.py:418] Single window, no scan.
I0123 11:42:20.389877 139918641729536 transformer_layer.py:389] tlayer: self-attention.
I0123 11:42:20.389909 139918641729536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.389971 139918641729536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.391144 139918641729536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.391221 139918641729536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.391936 139918641729536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.394404 139918641729536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.400573 139918641729536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.401890 139918641729536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.401970 139918641729536 transformer_layer.py:468] tlayer: End windows.
I0123 11:42:20.402005 139918641729536 transformer_layer.py:472] tlayer: final FFN.
I0123 11:42:20.402065 139918641729536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.402193 139918641729536 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:42:20.402538 139918641729536 nn_components.py:325] mlp: activation = None
I0123 11:42:20.402585 139918641729536 nn_components.py:329] mlp: final activation = None
I0123 11:42:20.404517 139918641729536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.404617 139918641729536 nn_components.py:261] mlp: residual
I0123 11:42:20.407504 139918641729536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.407589 139918641729536 transformer_base.py:443] tbase: final FFN
I0123 11:42:20.408080 139918641729536 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:42:20.418346 139918641729536 nn_components.py:329] mlp: final activation = None
I0123 11:42:20.427263 139918641729536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.427363 139918641729536 nn_components.py:261] mlp: residual
I0123 11:42:20.427664 139918641729536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.427746 139918641729536 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 11:42:20.427856 139918641729536 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:42:20.427896 139918641729536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:42:20.427927 139918641729536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:42:20.429800 139918641729536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.432344 139918641729536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:42:20.437927 139918641729536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.438189 139918641729536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:42:20.440825 139918641729536 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:42:20.444600 139918641729536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:42:20.444656 139918641729536 attention.py:418] Single window, no scan.
I0123 11:42:20.444692 139918641729536 transformer_layer.py:389] tlayer: self-attention.
I0123 11:42:20.444723 139918641729536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.444785 139918641729536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.445351 139918641729536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.445425 139918641729536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.446282 139918641729536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.447229 139918641729536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.449764 139918641729536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.450427 139918641729536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.450504 139918641729536 transformer_layer.py:468] tlayer: End windows.
I0123 11:42:20.450540 139918641729536 transformer_layer.py:472] tlayer: final FFN.
I0123 11:42:20.450601 139918641729536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.450727 139918641729536 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:42:20.451072 139918641729536 nn_components.py:325] mlp: activation = None
I0123 11:42:20.451116 139918641729536 nn_components.py:329] mlp: final activation = None
I0123 11:42:20.453104 139918641729536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.453200 139918641729536 nn_components.py:261] mlp: residual
I0123 11:42:20.455905 139918641729536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.455984 139918641729536 transformer_base.py:443] tbase: final FFN
I0123 11:42:20.456413 139918641729536 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:42:20.458926 139918641729536 nn_components.py:329] mlp: final activation = None
I0123 11:42:20.460824 139918641729536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.460921 139918641729536 nn_components.py:261] mlp: residual
I0123 11:42:20.461217 139918641729536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.461297 139918641729536 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 11:42:20.461406 139918641729536 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:42:20.461445 139918641729536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:42:20.461476 139918641729536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:42:20.463732 139918641729536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.466107 139918641729536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:42:20.471688 139918641729536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.471953 139918641729536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:42:20.474590 139918641729536 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:42:20.478450 139918641729536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:42:20.478507 139918641729536 attention.py:418] Single window, no scan.
I0123 11:42:20.478544 139918641729536 transformer_layer.py:389] tlayer: self-attention.
I0123 11:42:20.478575 139918641729536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.478636 139918641729536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.479197 139918641729536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.479272 139918641729536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.479632 139918641729536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.480398 139918641729536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.482916 139918641729536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.483582 139918641729536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.483658 139918641729536 transformer_layer.py:468] tlayer: End windows.
I0123 11:42:20.483693 139918641729536 transformer_layer.py:472] tlayer: final FFN.
I0123 11:42:20.483750 139918641729536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.483874 139918641729536 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:42:20.484195 139918641729536 nn_components.py:325] mlp: activation = None
I0123 11:42:20.484239 139918641729536 nn_components.py:329] mlp: final activation = None
I0123 11:42:20.486162 139918641729536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.486253 139918641729536 nn_components.py:261] mlp: residual
I0123 11:42:20.488757 139918641729536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.488840 139918641729536 transformer_base.py:443] tbase: final FFN
I0123 11:42:20.489323 139918641729536 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:42:20.491610 139918641729536 nn_components.py:329] mlp: final activation = None
I0123 11:42:20.493527 139918641729536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.493621 139918641729536 nn_components.py:261] mlp: residual
I0123 11:42:20.493927 139918641729536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.494008 139918641729536 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 11:42:20.494117 139918641729536 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:42:20.494156 139918641729536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:42:20.494188 139918641729536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:42:20.496081 139918641729536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.498490 139918641729536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:42:20.504134 139918641729536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.504395 139918641729536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:42:20.507057 139918641729536 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:42:20.511122 139918641729536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:42:20.511176 139918641729536 attention.py:418] Single window, no scan.
I0123 11:42:20.511212 139918641729536 transformer_layer.py:389] tlayer: self-attention.
I0123 11:42:20.511243 139918641729536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.511308 139918641729536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.511863 139918641729536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.511942 139918641729536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.512304 139918641729536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.513074 139918641729536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.515640 139918641729536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.516259 139918641729536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.516339 139918641729536 transformer_layer.py:468] tlayer: End windows.
I0123 11:42:20.516377 139918641729536 transformer_layer.py:472] tlayer: final FFN.
I0123 11:42:20.516438 139918641729536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.516566 139918641729536 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:42:20.516891 139918641729536 nn_components.py:325] mlp: activation = None
I0123 11:42:20.516935 139918641729536 nn_components.py:329] mlp: final activation = None
I0123 11:42:20.518851 139918641729536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.518944 139918641729536 nn_components.py:261] mlp: residual
I0123 11:42:20.521517 139918641729536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.521600 139918641729536 transformer_base.py:443] tbase: final FFN
I0123 11:42:20.522041 139918641729536 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:42:20.524317 139918641729536 nn_components.py:329] mlp: final activation = None
I0123 11:42:20.526244 139918641729536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.526340 139918641729536 nn_components.py:261] mlp: residual
I0123 11:42:20.526636 139918641729536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.526718 139918641729536 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 11:42:20.526827 139918641729536 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:42:20.526866 139918641729536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:42:20.526897 139918641729536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:42:20.528802 139918641729536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.531213 139918641729536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:42:20.536910 139918641729536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.537164 139918641729536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:42:20.540228 139918641729536 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:42:20.543983 139918641729536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:42:20.544037 139918641729536 attention.py:418] Single window, no scan.
I0123 11:42:20.544074 139918641729536 transformer_layer.py:389] tlayer: self-attention.
I0123 11:42:20.544106 139918641729536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.544167 139918641729536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.544737 139918641729536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.544814 139918641729536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.545178 139918641729536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.545955 139918641729536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.548541 139918641729536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.549160 139918641729536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.549237 139918641729536 transformer_layer.py:468] tlayer: End windows.
I0123 11:42:20.549273 139918641729536 transformer_layer.py:472] tlayer: final FFN.
I0123 11:42:20.549335 139918641729536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.549473 139918641729536 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:42:20.549806 139918641729536 nn_components.py:325] mlp: activation = None
I0123 11:42:20.549851 139918641729536 nn_components.py:329] mlp: final activation = None
I0123 11:42:20.551776 139918641729536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.551868 139918641729536 nn_components.py:261] mlp: residual
I0123 11:42:20.554448 139918641729536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.554528 139918641729536 transformer_base.py:443] tbase: final FFN
I0123 11:42:20.554962 139918641729536 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:42:20.557236 139918641729536 nn_components.py:329] mlp: final activation = None
I0123 11:42:20.559364 139918641729536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.559459 139918641729536 nn_components.py:261] mlp: residual
I0123 11:42:20.559754 139918641729536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.559834 139918641729536 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 11:42:20.559943 139918641729536 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:42:20.559984 139918641729536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:42:20.560015 139918641729536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:42:20.562019 139918641729536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.564495 139918641729536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:42:20.570201 139918641729536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.570459 139918641729536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:42:20.573179 139918641729536 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:42:20.576905 139918641729536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:42:20.576960 139918641729536 attention.py:418] Single window, no scan.
I0123 11:42:20.576996 139918641729536 transformer_layer.py:389] tlayer: self-attention.
I0123 11:42:20.577027 139918641729536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.577089 139918641729536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.577692 139918641729536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.577768 139918641729536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.578134 139918641729536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.578906 139918641729536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.581439 139918641729536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.582070 139918641729536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.582147 139918641729536 transformer_layer.py:468] tlayer: End windows.
I0123 11:42:20.582183 139918641729536 transformer_layer.py:472] tlayer: final FFN.
I0123 11:42:20.582242 139918641729536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.582372 139918641729536 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:42:20.582703 139918641729536 nn_components.py:325] mlp: activation = None
I0123 11:42:20.582747 139918641729536 nn_components.py:329] mlp: final activation = None
I0123 11:42:20.584672 139918641729536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.584764 139918641729536 nn_components.py:261] mlp: residual
I0123 11:42:20.587339 139918641729536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.587419 139918641729536 transformer_base.py:443] tbase: final FFN
I0123 11:42:20.587867 139918641729536 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:42:20.590211 139918641729536 nn_components.py:329] mlp: final activation = None
I0123 11:42:20.592134 139918641729536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.592227 139918641729536 nn_components.py:261] mlp: residual
I0123 11:42:20.592520 139918641729536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.592600 139918641729536 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 11:42:20.592707 139918641729536 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:42:20.592746 139918641729536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:42:20.592777 139918641729536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:42:20.594616 139918641729536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.597061 139918641729536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:42:20.602726 139918641729536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.602989 139918641729536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:42:20.605626 139918641729536 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:42:20.609405 139918641729536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:42:20.609459 139918641729536 attention.py:418] Single window, no scan.
I0123 11:42:20.609494 139918641729536 transformer_layer.py:389] tlayer: self-attention.
I0123 11:42:20.609526 139918641729536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.609589 139918641729536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.610169 139918641729536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.610246 139918641729536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.610606 139918641729536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.611388 139918641729536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.613891 139918641729536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.614517 139918641729536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.614593 139918641729536 transformer_layer.py:468] tlayer: End windows.
I0123 11:42:20.614627 139918641729536 transformer_layer.py:472] tlayer: final FFN.
I0123 11:42:20.614682 139918641729536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.614808 139918641729536 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:42:20.615133 139918641729536 nn_components.py:325] mlp: activation = None
I0123 11:42:20.615177 139918641729536 nn_components.py:329] mlp: final activation = None
I0123 11:42:20.617462 139918641729536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.617556 139918641729536 nn_components.py:261] mlp: residual
I0123 11:42:20.620088 139918641729536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.620169 139918641729536 transformer_base.py:443] tbase: final FFN
I0123 11:42:20.620601 139918641729536 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:42:20.760561 139918641729536 nn_components.py:329] mlp: final activation = None
I0123 11:42:20.762765 139918641729536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.762917 139918641729536 nn_components.py:261] mlp: residual
I0123 11:42:20.763243 139918641729536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.763336 139918641729536 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 11:42:20.763450 139918641729536 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:42:20.763490 139918641729536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:42:20.763522 139918641729536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:42:20.765573 139918641729536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.768105 139918641729536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:42:20.773856 139918641729536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.774131 139918641729536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:42:20.776834 139918641729536 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:42:20.780756 139918641729536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:42:20.780815 139918641729536 attention.py:418] Single window, no scan.
I0123 11:42:20.780852 139918641729536 transformer_layer.py:389] tlayer: self-attention.
I0123 11:42:20.780884 139918641729536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.780950 139918641729536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.781561 139918641729536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.781637 139918641729536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.782012 139918641729536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.782794 139918641729536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.785407 139918641729536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.786041 139918641729536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.786119 139918641729536 transformer_layer.py:468] tlayer: End windows.
I0123 11:42:20.786154 139918641729536 transformer_layer.py:472] tlayer: final FFN.
I0123 11:42:20.786213 139918641729536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.786340 139918641729536 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:42:20.786672 139918641729536 nn_components.py:325] mlp: activation = None
I0123 11:42:20.786716 139918641729536 nn_components.py:329] mlp: final activation = None
I0123 11:42:20.788637 139918641729536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.788729 139918641729536 nn_components.py:261] mlp: residual
I0123 11:42:20.791280 139918641729536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.791360 139918641729536 transformer_base.py:443] tbase: final FFN
I0123 11:42:20.791849 139918641729536 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:42:20.794170 139918641729536 nn_components.py:329] mlp: final activation = None
I0123 11:42:20.796093 139918641729536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.796194 139918641729536 nn_components.py:261] mlp: residual
I0123 11:42:20.796493 139918641729536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.796574 139918641729536 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 11:42:20.796685 139918641729536 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:42:20.796725 139918641729536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:42:20.796756 139918641729536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:42:20.798681 139918641729536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.801088 139918641729536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:42:20.806768 139918641729536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.807025 139918641729536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:42:20.809729 139918641729536 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:42:20.813495 139918641729536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:42:20.813548 139918641729536 attention.py:418] Single window, no scan.
I0123 11:42:20.813585 139918641729536 transformer_layer.py:389] tlayer: self-attention.
I0123 11:42:20.813616 139918641729536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.813693 139918641729536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.814256 139918641729536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.814332 139918641729536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.814696 139918641729536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.815462 139918641729536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.818021 139918641729536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.818678 139918641729536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.818754 139918641729536 transformer_layer.py:468] tlayer: End windows.
I0123 11:42:20.818788 139918641729536 transformer_layer.py:472] tlayer: final FFN.
I0123 11:42:20.818845 139918641729536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.818969 139918641729536 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:42:20.819293 139918641729536 nn_components.py:325] mlp: activation = None
I0123 11:42:20.819335 139918641729536 nn_components.py:329] mlp: final activation = None
I0123 11:42:20.821400 139918641729536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.821493 139918641729536 nn_components.py:261] mlp: residual
I0123 11:42:20.824059 139918641729536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.824140 139918641729536 transformer_base.py:443] tbase: final FFN
I0123 11:42:20.824573 139918641729536 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:42:20.826858 139918641729536 nn_components.py:329] mlp: final activation = None
I0123 11:42:20.828763 139918641729536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.828856 139918641729536 nn_components.py:261] mlp: residual
I0123 11:42:20.829140 139918641729536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.829226 139918641729536 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 11:42:20.829338 139918641729536 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:42:20.829377 139918641729536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:42:20.829408 139918641729536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:42:20.831304 139918641729536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.833693 139918641729536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:42:20.839644 139918641729536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.839905 139918641729536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:42:20.842621 139918641729536 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:42:20.846328 139918641729536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:42:20.846382 139918641729536 attention.py:418] Single window, no scan.
I0123 11:42:20.846422 139918641729536 transformer_layer.py:389] tlayer: self-attention.
I0123 11:42:20.846453 139918641729536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.846516 139918641729536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.847081 139918641729536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.847157 139918641729536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.847522 139918641729536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.848334 139918641729536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.850829 139918641729536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.851456 139918641729536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.851533 139918641729536 transformer_layer.py:468] tlayer: End windows.
I0123 11:42:20.851568 139918641729536 transformer_layer.py:472] tlayer: final FFN.
I0123 11:42:20.851625 139918641729536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.851749 139918641729536 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:42:20.852077 139918641729536 nn_components.py:325] mlp: activation = None
I0123 11:42:20.852122 139918641729536 nn_components.py:329] mlp: final activation = None
I0123 11:42:20.854039 139918641729536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.854132 139918641729536 nn_components.py:261] mlp: residual
I0123 11:42:20.856675 139918641729536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.856753 139918641729536 transformer_base.py:443] tbase: final FFN
I0123 11:42:20.857187 139918641729536 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:42:20.859464 139918641729536 nn_components.py:329] mlp: final activation = None
I0123 11:42:20.861426 139918641729536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.861520 139918641729536 nn_components.py:261] mlp: residual
I0123 11:42:20.861828 139918641729536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.861915 139918641729536 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 11:42:20.862026 139918641729536 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:42:20.862066 139918641729536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:42:20.862097 139918641729536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:42:20.863932 139918641729536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.866384 139918641729536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:42:20.871976 139918641729536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.872239 139918641729536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:42:20.874929 139918641729536 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:42:20.878653 139918641729536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:42:20.878709 139918641729536 attention.py:418] Single window, no scan.
I0123 11:42:20.878744 139918641729536 transformer_layer.py:389] tlayer: self-attention.
I0123 11:42:20.878776 139918641729536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.878880 139918641729536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.879449 139918641729536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.879524 139918641729536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.879889 139918641729536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.880663 139918641729536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.883173 139918641729536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.883800 139918641729536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.883876 139918641729536 transformer_layer.py:468] tlayer: End windows.
I0123 11:42:20.883914 139918641729536 transformer_layer.py:472] tlayer: final FFN.
I0123 11:42:20.883972 139918641729536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.884097 139918641729536 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:42:20.884421 139918641729536 nn_components.py:325] mlp: activation = None
I0123 11:42:20.884465 139918641729536 nn_components.py:329] mlp: final activation = None
I0123 11:42:20.886438 139918641729536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.886532 139918641729536 nn_components.py:261] mlp: residual
I0123 11:42:20.889272 139918641729536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.889351 139918641729536 transformer_base.py:443] tbase: final FFN
I0123 11:42:20.889792 139918641729536 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:42:20.892116 139918641729536 nn_components.py:329] mlp: final activation = None
I0123 11:42:20.894050 139918641729536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.894145 139918641729536 nn_components.py:261] mlp: residual
I0123 11:42:20.894439 139918641729536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.894520 139918641729536 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 11:42:20.894637 139918641729536 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:42:20.894677 139918641729536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:42:20.894709 139918641729536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:42:20.896538 139918641729536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.898991 139918641729536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:42:20.904598 139918641729536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.904857 139918641729536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:42:20.907495 139918641729536 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:42:20.911581 139918641729536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:42:20.911636 139918641729536 attention.py:418] Single window, no scan.
I0123 11:42:20.911672 139918641729536 transformer_layer.py:389] tlayer: self-attention.
I0123 11:42:20.911704 139918641729536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.911767 139918641729536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.912331 139918641729536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.912407 139918641729536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.912769 139918641729536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.913543 139918641729536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.916064 139918641729536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.916688 139918641729536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.916768 139918641729536 transformer_layer.py:468] tlayer: End windows.
I0123 11:42:20.916804 139918641729536 transformer_layer.py:472] tlayer: final FFN.
I0123 11:42:20.916862 139918641729536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.916987 139918641729536 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:42:20.917309 139918641729536 nn_components.py:325] mlp: activation = None
I0123 11:42:20.917354 139918641729536 nn_components.py:329] mlp: final activation = None
I0123 11:42:20.919315 139918641729536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.919544 139918641729536 nn_components.py:261] mlp: residual
I0123 11:42:20.922234 139918641729536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.922317 139918641729536 transformer_base.py:443] tbase: final FFN
I0123 11:42:20.922745 139918641729536 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:42:20.925050 139918641729536 nn_components.py:329] mlp: final activation = None
I0123 11:42:20.926964 139918641729536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.927063 139918641729536 nn_components.py:261] mlp: residual
I0123 11:42:20.927354 139918641729536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.927628 139918641729536 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:42:20.927697 139918641729536 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:42:20.927762 139918641729536 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:42:20.927818 139918641729536 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:42:20.927871 139918641729536 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:42:20.927923 139918641729536 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:42:20.927975 139918641729536 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:42:20.928026 139918641729536 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:42:20.928076 139918641729536 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:42:20.928126 139918641729536 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:42:20.928176 139918641729536 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:42:20.928227 139918641729536 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:42:20.928263 139918641729536 decoder_stack.py:344] dstack: Final layernorm.
I0123 11:42:20.931782 139918641729536 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:42:20.979099 139918641729536 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:20.979184 139918641729536 decoder_stack.py:333] dstack: autoregressive generator.
I0123 11:42:20.979238 139918641729536 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 11:42:20.979343 139918641729536 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:42:20.979381 139918641729536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:42:20.979412 139918641729536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:42:20.979473 139918641729536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:20.981905 139918641729536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:42:20.987383 139918641729536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:20.987637 139918641729536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:42:20.990283 139918641729536 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:42:21.006614 139918641729536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:42:21.006669 139918641729536 attention.py:418] Single window, no scan.
I0123 11:42:21.006706 139918641729536 transformer_layer.py:389] tlayer: self-attention.
I0123 11:42:21.006737 139918641729536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.006799 139918641729536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.007923 139918641729536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.008001 139918641729536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.008713 139918641729536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.010718 139918641729536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.015453 139918641729536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.016754 139918641729536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.016837 139918641729536 transformer_layer.py:468] tlayer: End windows.
I0123 11:42:21.016873 139918641729536 transformer_layer.py:472] tlayer: final FFN.
I0123 11:42:21.016931 139918641729536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.017059 139918641729536 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:42:21.017169 139918641729536 nn_components.py:325] mlp: activation = None
I0123 11:42:21.017207 139918641729536 nn_components.py:329] mlp: final activation = None
I0123 11:42:21.019114 139918641729536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.019210 139918641729536 nn_components.py:261] mlp: residual
I0123 11:42:21.021749 139918641729536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.021830 139918641729536 transformer_base.py:443] tbase: final FFN
I0123 11:42:21.021937 139918641729536 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:42:21.024277 139918641729536 nn_components.py:329] mlp: final activation = None
I0123 11:42:21.026234 139918641729536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.026331 139918641729536 nn_components.py:261] mlp: residual
I0123 11:42:21.026625 139918641729536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.026708 139918641729536 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 11:42:21.026818 139918641729536 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:42:21.026858 139918641729536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:42:21.026888 139918641729536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:42:21.026952 139918641729536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.029203 139918641729536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:42:21.034697 139918641729536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.034956 139918641729536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:42:21.037621 139918641729536 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:42:21.050657 139918641729536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:42:21.050713 139918641729536 attention.py:418] Single window, no scan.
I0123 11:42:21.050750 139918641729536 transformer_layer.py:389] tlayer: self-attention.
I0123 11:42:21.050781 139918641729536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.050844 139918641729536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.051395 139918641729536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.051472 139918641729536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.051830 139918641729536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.052515 139918641729536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.055010 139918641729536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.055624 139918641729536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.055701 139918641729536 transformer_layer.py:468] tlayer: End windows.
I0123 11:42:21.055741 139918641729536 transformer_layer.py:472] tlayer: final FFN.
I0123 11:42:21.055803 139918641729536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.055931 139918641729536 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:42:21.056040 139918641729536 nn_components.py:325] mlp: activation = None
I0123 11:42:21.056080 139918641729536 nn_components.py:329] mlp: final activation = None
I0123 11:42:21.058005 139918641729536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.058099 139918641729536 nn_components.py:261] mlp: residual
I0123 11:42:21.060497 139918641729536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.060575 139918641729536 transformer_base.py:443] tbase: final FFN
I0123 11:42:21.060682 139918641729536 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:42:21.062895 139918641729536 nn_components.py:329] mlp: final activation = None
I0123 11:42:21.064800 139918641729536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.064894 139918641729536 nn_components.py:261] mlp: residual
I0123 11:42:21.065181 139918641729536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.065262 139918641729536 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 11:42:21.065372 139918641729536 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:42:21.065411 139918641729536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:42:21.065443 139918641729536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:42:21.065504 139918641729536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.067743 139918641729536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:42:21.073157 139918641729536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.073412 139918641729536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:42:21.076118 139918641729536 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:42:21.088746 139918641729536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:42:21.088801 139918641729536 attention.py:418] Single window, no scan.
I0123 11:42:21.088837 139918641729536 transformer_layer.py:389] tlayer: self-attention.
I0123 11:42:21.088868 139918641729536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.088930 139918641729536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.089478 139918641729536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.089557 139918641729536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.089927 139918641729536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.090613 139918641729536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.093095 139918641729536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.093717 139918641729536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.093794 139918641729536 transformer_layer.py:468] tlayer: End windows.
I0123 11:42:21.093828 139918641729536 transformer_layer.py:472] tlayer: final FFN.
I0123 11:42:21.093891 139918641729536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.094018 139918641729536 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:42:21.094125 139918641729536 nn_components.py:325] mlp: activation = None
I0123 11:42:21.094162 139918641729536 nn_components.py:329] mlp: final activation = None
I0123 11:42:21.096101 139918641729536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.096195 139918641729536 nn_components.py:261] mlp: residual
I0123 11:42:21.098617 139918641729536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.098696 139918641729536 transformer_base.py:443] tbase: final FFN
I0123 11:42:21.098804 139918641729536 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:42:21.101016 139918641729536 nn_components.py:329] mlp: final activation = None
I0123 11:42:21.102938 139918641729536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.103032 139918641729536 nn_components.py:261] mlp: residual
I0123 11:42:21.103318 139918641729536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.103399 139918641729536 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 11:42:21.103507 139918641729536 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:42:21.103547 139918641729536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:42:21.103578 139918641729536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:42:21.103641 139918641729536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.109617 139918641729536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:42:21.115371 139918641729536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.115653 139918641729536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:42:21.118393 139918641729536 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:42:21.132005 139918641729536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:42:21.132062 139918641729536 attention.py:418] Single window, no scan.
I0123 11:42:21.132100 139918641729536 transformer_layer.py:389] tlayer: self-attention.
I0123 11:42:21.132131 139918641729536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.132192 139918641729536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.132776 139918641729536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.132856 139918641729536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.133219 139918641729536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.133934 139918641729536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.136661 139918641729536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.137279 139918641729536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.137358 139918641729536 transformer_layer.py:468] tlayer: End windows.
I0123 11:42:21.137392 139918641729536 transformer_layer.py:472] tlayer: final FFN.
I0123 11:42:21.137453 139918641729536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.137586 139918641729536 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:42:21.137706 139918641729536 nn_components.py:325] mlp: activation = None
I0123 11:42:21.137747 139918641729536 nn_components.py:329] mlp: final activation = None
I0123 11:42:21.139983 139918641729536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.140077 139918641729536 nn_components.py:261] mlp: residual
I0123 11:42:21.142541 139918641729536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.142620 139918641729536 transformer_base.py:443] tbase: final FFN
I0123 11:42:21.142729 139918641729536 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:42:21.144967 139918641729536 nn_components.py:329] mlp: final activation = None
I0123 11:42:21.146847 139918641729536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.146942 139918641729536 nn_components.py:261] mlp: residual
I0123 11:42:21.147229 139918641729536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.147309 139918641729536 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 11:42:21.147420 139918641729536 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:42:21.147462 139918641729536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:42:21.147493 139918641729536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:42:21.147557 139918641729536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.149885 139918641729536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:42:21.155371 139918641729536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.155632 139918641729536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:42:21.158266 139918641729536 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:42:21.170967 139918641729536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:42:21.171022 139918641729536 attention.py:418] Single window, no scan.
I0123 11:42:21.171058 139918641729536 transformer_layer.py:389] tlayer: self-attention.
I0123 11:42:21.171088 139918641729536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.171153 139918641729536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.171710 139918641729536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.171785 139918641729536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.172136 139918641729536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.172828 139918641729536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.175397 139918641729536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.176015 139918641729536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.176092 139918641729536 transformer_layer.py:468] tlayer: End windows.
I0123 11:42:21.176127 139918641729536 transformer_layer.py:472] tlayer: final FFN.
I0123 11:42:21.176185 139918641729536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.176318 139918641729536 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:42:21.176428 139918641729536 nn_components.py:325] mlp: activation = None
I0123 11:42:21.176467 139918641729536 nn_components.py:329] mlp: final activation = None
I0123 11:42:21.178351 139918641729536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.178445 139918641729536 nn_components.py:261] mlp: residual
I0123 11:42:21.180855 139918641729536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.180935 139918641729536 transformer_base.py:443] tbase: final FFN
I0123 11:42:21.181044 139918641729536 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:42:21.183350 139918641729536 nn_components.py:329] mlp: final activation = None
I0123 11:42:21.185212 139918641729536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.185307 139918641729536 nn_components.py:261] mlp: residual
I0123 11:42:21.185594 139918641729536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.185681 139918641729536 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 11:42:21.185791 139918641729536 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:42:21.185830 139918641729536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:42:21.185862 139918641729536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:42:21.185924 139918641729536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.188164 139918641729536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:42:21.193630 139918641729536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.193892 139918641729536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:42:21.196591 139918641729536 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:42:21.209270 139918641729536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:42:21.209325 139918641729536 attention.py:418] Single window, no scan.
I0123 11:42:21.209362 139918641729536 transformer_layer.py:389] tlayer: self-attention.
I0123 11:42:21.209393 139918641729536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.209455 139918641729536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.210026 139918641729536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.210103 139918641729536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.210461 139918641729536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.211156 139918641729536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.213631 139918641729536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.214251 139918641729536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.214328 139918641729536 transformer_layer.py:468] tlayer: End windows.
I0123 11:42:21.214362 139918641729536 transformer_layer.py:472] tlayer: final FFN.
I0123 11:42:21.214421 139918641729536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.214551 139918641729536 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:42:21.214666 139918641729536 nn_components.py:325] mlp: activation = None
I0123 11:42:21.214706 139918641729536 nn_components.py:329] mlp: final activation = None
I0123 11:42:21.216638 139918641729536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.216731 139918641729536 nn_components.py:261] mlp: residual
I0123 11:42:21.219140 139918641729536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.219222 139918641729536 transformer_base.py:443] tbase: final FFN
I0123 11:42:21.219331 139918641729536 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:42:21.221559 139918641729536 nn_components.py:329] mlp: final activation = None
I0123 11:42:21.223423 139918641729536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.223518 139918641729536 nn_components.py:261] mlp: residual
I0123 11:42:21.223803 139918641729536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.223884 139918641729536 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 11:42:21.223994 139918641729536 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:42:21.224033 139918641729536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:42:21.224065 139918641729536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:42:21.224127 139918641729536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.226368 139918641729536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:42:21.231897 139918641729536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.232157 139918641729536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:42:21.234903 139918641729536 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:42:21.248008 139918641729536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:42:21.248063 139918641729536 attention.py:418] Single window, no scan.
I0123 11:42:21.248099 139918641729536 transformer_layer.py:389] tlayer: self-attention.
I0123 11:42:21.248132 139918641729536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.248193 139918641729536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.248757 139918641729536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.248832 139918641729536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.249185 139918641729536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.249892 139918641729536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.252400 139918641729536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.253074 139918641729536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.253152 139918641729536 transformer_layer.py:468] tlayer: End windows.
I0123 11:42:21.253187 139918641729536 transformer_layer.py:472] tlayer: final FFN.
I0123 11:42:21.253246 139918641729536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.253375 139918641729536 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:42:21.253484 139918641729536 nn_components.py:325] mlp: activation = None
I0123 11:42:21.253528 139918641729536 nn_components.py:329] mlp: final activation = None
I0123 11:42:21.255414 139918641729536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.255508 139918641729536 nn_components.py:261] mlp: residual
I0123 11:42:21.257902 139918641729536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.257980 139918641729536 transformer_base.py:443] tbase: final FFN
I0123 11:42:21.258088 139918641729536 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:42:21.260288 139918641729536 nn_components.py:329] mlp: final activation = None
I0123 11:42:21.262225 139918641729536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.262320 139918641729536 nn_components.py:261] mlp: residual
I0123 11:42:21.262608 139918641729536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.262689 139918641729536 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 11:42:21.262798 139918641729536 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:42:21.262837 139918641729536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:42:21.262869 139918641729536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:42:21.262931 139918641729536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.265170 139918641729536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:42:21.270610 139918641729536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.270879 139918641729536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:42:21.273544 139918641729536 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:42:21.286138 139918641729536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:42:21.286195 139918641729536 attention.py:418] Single window, no scan.
I0123 11:42:21.286230 139918641729536 transformer_layer.py:389] tlayer: self-attention.
I0123 11:42:21.286261 139918641729536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.286325 139918641729536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.286939 139918641729536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.287017 139918641729536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.287387 139918641729536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.288104 139918641729536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.290592 139918641729536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.291217 139918641729536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.291293 139918641729536 transformer_layer.py:468] tlayer: End windows.
I0123 11:42:21.291328 139918641729536 transformer_layer.py:472] tlayer: final FFN.
I0123 11:42:21.291386 139918641729536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.291514 139918641729536 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:42:21.291626 139918641729536 nn_components.py:325] mlp: activation = None
I0123 11:42:21.291670 139918641729536 nn_components.py:329] mlp: final activation = None
I0123 11:42:21.293532 139918641729536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.293625 139918641729536 nn_components.py:261] mlp: residual
I0123 11:42:21.296094 139918641729536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.296172 139918641729536 transformer_base.py:443] tbase: final FFN
I0123 11:42:21.296278 139918641729536 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:42:21.298512 139918641729536 nn_components.py:329] mlp: final activation = None
I0123 11:42:21.300361 139918641729536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.300456 139918641729536 nn_components.py:261] mlp: residual
I0123 11:42:21.300742 139918641729536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.300824 139918641729536 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 11:42:21.300933 139918641729536 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:42:21.300971 139918641729536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:42:21.301002 139918641729536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:42:21.301064 139918641729536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.303297 139918641729536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:42:21.308804 139918641729536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.309062 139918641729536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:42:21.311673 139918641729536 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:42:21.324266 139918641729536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:42:21.324321 139918641729536 attention.py:418] Single window, no scan.
I0123 11:42:21.324357 139918641729536 transformer_layer.py:389] tlayer: self-attention.
I0123 11:42:21.324389 139918641729536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.324450 139918641729536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.325006 139918641729536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.325082 139918641729536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.325438 139918641729536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.326138 139918641729536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.328632 139918641729536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.329295 139918641729536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.329373 139918641729536 transformer_layer.py:468] tlayer: End windows.
I0123 11:42:21.329408 139918641729536 transformer_layer.py:472] tlayer: final FFN.
I0123 11:42:21.329466 139918641729536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.329597 139918641729536 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:42:21.329715 139918641729536 nn_components.py:325] mlp: activation = None
I0123 11:42:21.329756 139918641729536 nn_components.py:329] mlp: final activation = None
I0123 11:42:21.331638 139918641729536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.331730 139918641729536 nn_components.py:261] mlp: residual
I0123 11:42:21.334122 139918641729536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.334201 139918641729536 transformer_base.py:443] tbase: final FFN
I0123 11:42:21.334308 139918641729536 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:42:21.336505 139918641729536 nn_components.py:329] mlp: final activation = None
I0123 11:42:21.338723 139918641729536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.338818 139918641729536 nn_components.py:261] mlp: residual
I0123 11:42:21.339104 139918641729536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.339185 139918641729536 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 11:42:21.339293 139918641729536 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:42:21.339333 139918641729536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:42:21.339364 139918641729536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:42:21.339424 139918641729536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.341658 139918641729536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:42:21.347040 139918641729536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.347292 139918641729536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:42:21.350257 139918641729536 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:42:21.362772 139918641729536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:42:21.362827 139918641729536 attention.py:418] Single window, no scan.
I0123 11:42:21.362863 139918641729536 transformer_layer.py:389] tlayer: self-attention.
I0123 11:42:21.362894 139918641729536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.362956 139918641729536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.363559 139918641729536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.363636 139918641729536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.363995 139918641729536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.364681 139918641729536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.367172 139918641729536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.367789 139918641729536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.367865 139918641729536 transformer_layer.py:468] tlayer: End windows.
I0123 11:42:21.367900 139918641729536 transformer_layer.py:472] tlayer: final FFN.
I0123 11:42:21.367960 139918641729536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.368092 139918641729536 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:42:21.368200 139918641729536 nn_components.py:325] mlp: activation = None
I0123 11:42:21.368239 139918641729536 nn_components.py:329] mlp: final activation = None
I0123 11:42:21.370135 139918641729536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.370234 139918641729536 nn_components.py:261] mlp: residual
I0123 11:42:21.372696 139918641729536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.372775 139918641729536 transformer_base.py:443] tbase: final FFN
I0123 11:42:21.372883 139918641729536 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:42:21.375102 139918641729536 nn_components.py:329] mlp: final activation = None
I0123 11:42:21.376948 139918641729536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.377042 139918641729536 nn_components.py:261] mlp: residual
I0123 11:42:21.377326 139918641729536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.377405 139918641729536 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 11:42:21.377512 139918641729536 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:42:21.377550 139918641729536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:42:21.377581 139918641729536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:42:21.377647 139918641729536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.379869 139918641729536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:42:21.385346 139918641729536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.385603 139918641729536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:42:21.388223 139918641729536 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:42:21.400791 139918641729536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:42:21.400846 139918641729536 attention.py:418] Single window, no scan.
I0123 11:42:21.400882 139918641729536 transformer_layer.py:389] tlayer: self-attention.
I0123 11:42:21.400913 139918641729536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.400974 139918641729536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.401526 139918641729536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.401606 139918641729536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.401973 139918641729536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.402673 139918641729536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.405159 139918641729536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.405830 139918641729536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.405909 139918641729536 transformer_layer.py:468] tlayer: End windows.
I0123 11:42:21.405944 139918641729536 transformer_layer.py:472] tlayer: final FFN.
I0123 11:42:21.406001 139918641729536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.406127 139918641729536 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:42:21.406233 139918641729536 nn_components.py:325] mlp: activation = None
I0123 11:42:21.406272 139918641729536 nn_components.py:329] mlp: final activation = None
I0123 11:42:21.408157 139918641729536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.408254 139918641729536 nn_components.py:261] mlp: residual
I0123 11:42:21.410675 139918641729536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.410753 139918641729536 transformer_base.py:443] tbase: final FFN
I0123 11:42:21.410861 139918641729536 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:42:21.413068 139918641729536 nn_components.py:329] mlp: final activation = None
I0123 11:42:21.415008 139918641729536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.415103 139918641729536 nn_components.py:261] mlp: residual
I0123 11:42:21.415390 139918641729536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.415471 139918641729536 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 11:42:21.415580 139918641729536 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:42:21.415619 139918641729536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:42:21.415651 139918641729536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:42:21.415714 139918641729536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.417925 139918641729536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:42:21.423358 139918641729536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.423615 139918641729536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:42:21.426239 139918641729536 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:42:21.439071 139918641729536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:42:21.439127 139918641729536 attention.py:418] Single window, no scan.
I0123 11:42:21.439163 139918641729536 transformer_layer.py:389] tlayer: self-attention.
I0123 11:42:21.439194 139918641729536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.439255 139918641729536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.439807 139918641729536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.439883 139918641729536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.440243 139918641729536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.440936 139918641729536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.443493 139918641729536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.444113 139918641729536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.444190 139918641729536 transformer_layer.py:468] tlayer: End windows.
I0123 11:42:21.444225 139918641729536 transformer_layer.py:472] tlayer: final FFN.
I0123 11:42:21.444282 139918641729536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.444411 139918641729536 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:42:21.444521 139918641729536 nn_components.py:325] mlp: activation = None
I0123 11:42:21.444560 139918641729536 nn_components.py:329] mlp: final activation = None
I0123 11:42:21.446732 139918641729536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.446825 139918641729536 nn_components.py:261] mlp: residual
I0123 11:42:21.449214 139918641729536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.449291 139918641729536 transformer_base.py:443] tbase: final FFN
I0123 11:42:21.449402 139918641729536 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:42:21.452066 139918641729536 nn_components.py:329] mlp: final activation = None
I0123 11:42:21.453940 139918641729536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.454037 139918641729536 nn_components.py:261] mlp: residual
I0123 11:42:21.454334 139918641729536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.454425 139918641729536 decoder_stack.py:344] dstack: Final layernorm.
I0123 11:42:21.457300 139918641729536 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:42:21.512270 139918641729536 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.512356 139918641729536 decoder_stack.py:333] dstack: autoregressive generator.
I0123 11:42:21.512410 139918641729536 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 11:42:21.512514 139918641729536 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:42:21.512552 139918641729536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:42:21.512583 139918641729536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:42:21.512644 139918641729536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.514989 139918641729536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:42:21.520339 139918641729536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.520599 139918641729536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:42:21.523128 139918641729536 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:42:21.535345 139918641729536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:42:21.535401 139918641729536 attention.py:418] Single window, no scan.
I0123 11:42:21.535437 139918641729536 transformer_layer.py:389] tlayer: self-attention.
I0123 11:42:21.535468 139918641729536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.535529 139918641729536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.536076 139918641729536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.536153 139918641729536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.536503 139918641729536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.537173 139918641729536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.539669 139918641729536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.540278 139918641729536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.540354 139918641729536 transformer_layer.py:468] tlayer: End windows.
I0123 11:42:21.540388 139918641729536 transformer_layer.py:472] tlayer: final FFN.
I0123 11:42:21.540446 139918641729536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.540575 139918641729536 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:42:21.540689 139918641729536 nn_components.py:325] mlp: activation = None
I0123 11:42:21.540729 139918641729536 nn_components.py:329] mlp: final activation = None
I0123 11:42:21.542561 139918641729536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.542654 139918641729536 nn_components.py:261] mlp: residual
I0123 11:42:21.545006 139918641729536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.545084 139918641729536 transformer_base.py:443] tbase: final FFN
I0123 11:42:21.545192 139918641729536 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:42:21.547541 139918641729536 nn_components.py:329] mlp: final activation = None
I0123 11:42:21.549572 139918641729536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.549673 139918641729536 nn_components.py:261] mlp: residual
I0123 11:42:21.549959 139918641729536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.550040 139918641729536 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 11:42:21.550146 139918641729536 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:42:21.550185 139918641729536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:42:21.550216 139918641729536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:42:21.550279 139918641729536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.552477 139918641729536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:42:21.557810 139918641729536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.558065 139918641729536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:42:21.560675 139918641729536 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:42:21.572829 139918641729536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:42:21.572884 139918641729536 attention.py:418] Single window, no scan.
I0123 11:42:21.572920 139918641729536 transformer_layer.py:389] tlayer: self-attention.
I0123 11:42:21.572952 139918641729536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.573014 139918641729536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.573571 139918641729536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.573655 139918641729536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.574016 139918641729536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.574694 139918641729536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.577182 139918641729536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.577800 139918641729536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.577878 139918641729536 transformer_layer.py:468] tlayer: End windows.
I0123 11:42:21.577913 139918641729536 transformer_layer.py:472] tlayer: final FFN.
I0123 11:42:21.577972 139918641729536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.578100 139918641729536 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:42:21.578210 139918641729536 nn_components.py:325] mlp: activation = None
I0123 11:42:21.578254 139918641729536 nn_components.py:329] mlp: final activation = None
I0123 11:42:21.580091 139918641729536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.580185 139918641729536 nn_components.py:261] mlp: residual
I0123 11:42:21.582547 139918641729536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.582626 139918641729536 transformer_base.py:443] tbase: final FFN
I0123 11:42:21.582734 139918641729536 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:42:21.584952 139918641729536 nn_components.py:329] mlp: final activation = None
I0123 11:42:21.586798 139918641729536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.586893 139918641729536 nn_components.py:261] mlp: residual
I0123 11:42:21.587177 139918641729536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.587258 139918641729536 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 11:42:21.587365 139918641729536 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:42:21.587404 139918641729536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:42:21.587435 139918641729536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:42:21.587496 139918641729536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.589712 139918641729536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:42:21.595032 139918641729536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.595288 139918641729536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:42:21.597929 139918641729536 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:42:21.610094 139918641729536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:42:21.610150 139918641729536 attention.py:418] Single window, no scan.
I0123 11:42:21.610185 139918641729536 transformer_layer.py:389] tlayer: self-attention.
I0123 11:42:21.610216 139918641729536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.610277 139918641729536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.610825 139918641729536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.610900 139918641729536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.611251 139918641729536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.611917 139918641729536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.614844 139918641729536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.615457 139918641729536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.615533 139918641729536 transformer_layer.py:468] tlayer: End windows.
I0123 11:42:21.615568 139918641729536 transformer_layer.py:472] tlayer: final FFN.
I0123 11:42:21.615627 139918641729536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.615752 139918641729536 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:42:21.615859 139918641729536 nn_components.py:325] mlp: activation = None
I0123 11:42:21.615897 139918641729536 nn_components.py:329] mlp: final activation = None
I0123 11:42:21.617745 139918641729536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.617839 139918641729536 nn_components.py:261] mlp: residual
I0123 11:42:21.620211 139918641729536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.620290 139918641729536 transformer_base.py:443] tbase: final FFN
I0123 11:42:21.620398 139918641729536 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:42:21.622640 139918641729536 nn_components.py:329] mlp: final activation = None
I0123 11:42:21.624471 139918641729536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.624565 139918641729536 nn_components.py:261] mlp: residual
I0123 11:42:21.624848 139918641729536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.624929 139918641729536 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 11:42:21.625037 139918641729536 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:42:21.625075 139918641729536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:42:21.625106 139918641729536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:42:21.625167 139918641729536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.627374 139918641729536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:42:21.632655 139918641729536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.632909 139918641729536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:42:21.635559 139918641729536 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:42:21.647778 139918641729536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:42:21.647832 139918641729536 attention.py:418] Single window, no scan.
I0123 11:42:21.647870 139918641729536 transformer_layer.py:389] tlayer: self-attention.
I0123 11:42:21.647908 139918641729536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.647972 139918641729536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.648596 139918641729536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.648670 139918641729536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.649028 139918641729536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.649715 139918641729536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.652244 139918641729536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.652847 139918641729536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.652921 139918641729536 transformer_layer.py:468] tlayer: End windows.
I0123 11:42:21.652954 139918641729536 transformer_layer.py:472] tlayer: final FFN.
I0123 11:42:21.653010 139918641729536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.653133 139918641729536 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:42:21.653239 139918641729536 nn_components.py:325] mlp: activation = None
I0123 11:42:21.653277 139918641729536 nn_components.py:329] mlp: final activation = None
I0123 11:42:21.655138 139918641729536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.655228 139918641729536 nn_components.py:261] mlp: residual
I0123 11:42:21.657588 139918641729536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.657672 139918641729536 transformer_base.py:443] tbase: final FFN
I0123 11:42:21.657780 139918641729536 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:42:21.660028 139918641729536 nn_components.py:329] mlp: final activation = None
I0123 11:42:21.661875 139918641729536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.661969 139918641729536 nn_components.py:261] mlp: residual
I0123 11:42:21.662251 139918641729536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.662329 139918641729536 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 11:42:21.662434 139918641729536 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:42:21.662472 139918641729536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:42:21.662501 139918641729536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:42:21.662560 139918641729536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.664756 139918641729536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:42:21.670122 139918641729536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.670380 139918641729536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:42:21.673022 139918641729536 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:42:21.685374 139918641729536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:42:21.685427 139918641729536 attention.py:418] Single window, no scan.
I0123 11:42:21.685461 139918641729536 transformer_layer.py:389] tlayer: self-attention.
I0123 11:42:21.685491 139918641729536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.685552 139918641729536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.686117 139918641729536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.686192 139918641729536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.686549 139918641729536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.687237 139918641729536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.689775 139918641729536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.690386 139918641729536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.690460 139918641729536 transformer_layer.py:468] tlayer: End windows.
I0123 11:42:21.690493 139918641729536 transformer_layer.py:472] tlayer: final FFN.
I0123 11:42:21.690549 139918641729536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.690673 139918641729536 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:42:21.690779 139918641729536 nn_components.py:325] mlp: activation = None
I0123 11:42:21.690816 139918641729536 nn_components.py:329] mlp: final activation = None
I0123 11:42:21.692679 139918641729536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.692776 139918641729536 nn_components.py:261] mlp: residual
I0123 11:42:21.695168 139918641729536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.695244 139918641729536 transformer_base.py:443] tbase: final FFN
I0123 11:42:21.695350 139918641729536 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:42:21.697594 139918641729536 nn_components.py:329] mlp: final activation = None
I0123 11:42:21.699445 139918641729536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.699539 139918641729536 nn_components.py:261] mlp: residual
I0123 11:42:21.699822 139918641729536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.699901 139918641729536 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 11:42:21.700006 139918641729536 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:42:21.700043 139918641729536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:42:21.700073 139918641729536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:42:21.700133 139918641729536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.702358 139918641729536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:42:21.707745 139918641729536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.708002 139918641729536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:42:21.710690 139918641729536 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:42:21.723070 139918641729536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:42:21.723124 139918641729536 attention.py:418] Single window, no scan.
I0123 11:42:21.723158 139918641729536 transformer_layer.py:389] tlayer: self-attention.
I0123 11:42:21.723187 139918641729536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.723247 139918641729536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.723805 139918641729536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.723878 139918641729536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.724234 139918641729536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.724910 139918641729536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.727853 139918641729536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.728463 139918641729536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.728538 139918641729536 transformer_layer.py:468] tlayer: End windows.
I0123 11:42:21.728570 139918641729536 transformer_layer.py:472] tlayer: final FFN.
I0123 11:42:21.728626 139918641729536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.728748 139918641729536 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:42:21.728857 139918641729536 nn_components.py:325] mlp: activation = None
I0123 11:42:21.728895 139918641729536 nn_components.py:329] mlp: final activation = None
I0123 11:42:21.730751 139918641729536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.730849 139918641729536 nn_components.py:261] mlp: residual
I0123 11:42:21.733219 139918641729536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.733299 139918641729536 transformer_base.py:443] tbase: final FFN
I0123 11:42:21.733408 139918641729536 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:42:21.735679 139918641729536 nn_components.py:329] mlp: final activation = None
I0123 11:42:21.737538 139918641729536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.737631 139918641729536 nn_components.py:261] mlp: residual
I0123 11:42:21.737925 139918641729536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.738005 139918641729536 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 11:42:21.738112 139918641729536 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:42:21.738149 139918641729536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:42:21.738178 139918641729536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:42:21.738238 139918641729536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.740457 139918641729536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:42:21.746039 139918641729536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.746296 139918641729536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:42:21.748979 139918641729536 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:42:21.761371 139918641729536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:42:21.761424 139918641729536 attention.py:418] Single window, no scan.
I0123 11:42:21.761458 139918641729536 transformer_layer.py:389] tlayer: self-attention.
I0123 11:42:21.761488 139918641729536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.761548 139918641729536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.762113 139918641729536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.762187 139918641729536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.762539 139918641729536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.763221 139918641729536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.765777 139918641729536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.766394 139918641729536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.766468 139918641729536 transformer_layer.py:468] tlayer: End windows.
I0123 11:42:21.766500 139918641729536 transformer_layer.py:472] tlayer: final FFN.
I0123 11:42:21.766556 139918641729536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.766678 139918641729536 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:42:21.766787 139918641729536 nn_components.py:325] mlp: activation = None
I0123 11:42:21.766823 139918641729536 nn_components.py:329] mlp: final activation = None
I0123 11:42:21.768670 139918641729536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.768760 139918641729536 nn_components.py:261] mlp: residual
I0123 11:42:21.771167 139918641729536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.771244 139918641729536 transformer_base.py:443] tbase: final FFN
I0123 11:42:21.771348 139918641729536 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:42:21.773604 139918641729536 nn_components.py:329] mlp: final activation = None
I0123 11:42:21.775467 139918641729536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.775560 139918641729536 nn_components.py:261] mlp: residual
I0123 11:42:21.775844 139918641729536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.775923 139918641729536 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 11:42:21.776028 139918641729536 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:42:21.776065 139918641729536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:42:21.776094 139918641729536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:42:21.776155 139918641729536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.778353 139918641729536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:42:21.783714 139918641729536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.783967 139918641729536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:42:21.786612 139918641729536 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:42:21.799002 139918641729536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:42:21.799055 139918641729536 attention.py:418] Single window, no scan.
I0123 11:42:21.799089 139918641729536 transformer_layer.py:389] tlayer: self-attention.
I0123 11:42:21.799118 139918641729536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.799179 139918641729536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.799729 139918641729536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.799802 139918641729536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.800157 139918641729536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.800840 139918641729536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.803364 139918641729536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.803977 139918641729536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.804052 139918641729536 transformer_layer.py:468] tlayer: End windows.
I0123 11:42:21.804085 139918641729536 transformer_layer.py:472] tlayer: final FFN.
I0123 11:42:21.804142 139918641729536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.804264 139918641729536 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:42:21.804368 139918641729536 nn_components.py:325] mlp: activation = None
I0123 11:42:21.804404 139918641729536 nn_components.py:329] mlp: final activation = None
I0123 11:42:21.806252 139918641729536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.806344 139918641729536 nn_components.py:261] mlp: residual
I0123 11:42:21.808698 139918641729536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.808779 139918641729536 transformer_base.py:443] tbase: final FFN
I0123 11:42:21.808886 139918641729536 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:42:21.811141 139918641729536 nn_components.py:329] mlp: final activation = None
I0123 11:42:21.812972 139918641729536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.813064 139918641729536 nn_components.py:261] mlp: residual
I0123 11:42:21.813349 139918641729536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.813429 139918641729536 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 11:42:21.813534 139918641729536 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:42:21.813571 139918641729536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:42:21.813601 139918641729536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:42:21.813671 139918641729536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.815877 139918641729536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:42:21.821244 139918641729536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.821501 139918641729536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:42:21.824160 139918641729536 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:42:21.836496 139918641729536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:42:21.836549 139918641729536 attention.py:418] Single window, no scan.
I0123 11:42:21.836583 139918641729536 transformer_layer.py:389] tlayer: self-attention.
I0123 11:42:21.836612 139918641729536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.836674 139918641729536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.837229 139918641729536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.837302 139918641729536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.837660 139918641729536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.838344 139918641729536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.841270 139918641729536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.841901 139918641729536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.841977 139918641729536 transformer_layer.py:468] tlayer: End windows.
I0123 11:42:21.842010 139918641729536 transformer_layer.py:472] tlayer: final FFN.
I0123 11:42:21.842066 139918641729536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.842189 139918641729536 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:42:21.842295 139918641729536 nn_components.py:325] mlp: activation = None
I0123 11:42:21.842332 139918641729536 nn_components.py:329] mlp: final activation = None
I0123 11:42:21.844216 139918641729536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.844306 139918641729536 nn_components.py:261] mlp: residual
I0123 11:42:21.846678 139918641729536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.846760 139918641729536 transformer_base.py:443] tbase: final FFN
I0123 11:42:21.846868 139918641729536 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:42:21.849149 139918641729536 nn_components.py:329] mlp: final activation = None
I0123 11:42:21.850984 139918641729536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.851077 139918641729536 nn_components.py:261] mlp: residual
I0123 11:42:21.851358 139918641729536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.851437 139918641729536 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 11:42:21.851542 139918641729536 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:42:21.851579 139918641729536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:42:21.851608 139918641729536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:42:21.851668 139918641729536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.853887 139918641729536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:42:21.859277 139918641729536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.859529 139918641729536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:42:21.862179 139918641729536 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:42:21.874527 139918641729536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:42:21.874581 139918641729536 attention.py:418] Single window, no scan.
I0123 11:42:21.874615 139918641729536 transformer_layer.py:389] tlayer: self-attention.
I0123 11:42:21.874644 139918641729536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.874703 139918641729536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.875258 139918641729536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.875329 139918641729536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.875684 139918641729536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.876358 139918641729536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.878908 139918641729536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.879523 139918641729536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.879597 139918641729536 transformer_layer.py:468] tlayer: End windows.
I0123 11:42:21.879630 139918641729536 transformer_layer.py:472] tlayer: final FFN.
I0123 11:42:21.879685 139918641729536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.879813 139918641729536 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:42:21.879920 139918641729536 nn_components.py:325] mlp: activation = None
I0123 11:42:21.879957 139918641729536 nn_components.py:329] mlp: final activation = None
I0123 11:42:21.882306 139918641729536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.882399 139918641729536 nn_components.py:261] mlp: residual
I0123 11:42:21.884756 139918641729536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.884831 139918641729536 transformer_base.py:443] tbase: final FFN
I0123 11:42:21.884942 139918641729536 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:42:21.887176 139918641729536 nn_components.py:329] mlp: final activation = None
I0123 11:42:21.888983 139918641729536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.889074 139918641729536 nn_components.py:261] mlp: residual
I0123 11:42:21.889363 139918641729536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.889441 139918641729536 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 11:42:21.889545 139918641729536 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:42:21.889582 139918641729536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:42:21.889611 139918641729536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:42:21.889679 139918641729536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.891880 139918641729536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:42:21.897241 139918641729536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.897494 139918641729536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:42:21.900139 139918641729536 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:42:21.912745 139918641729536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:42:21.912798 139918641729536 attention.py:418] Single window, no scan.
I0123 11:42:21.912831 139918641729536 transformer_layer.py:389] tlayer: self-attention.
I0123 11:42:21.912860 139918641729536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.912920 139918641729536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.913479 139918641729536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.913553 139918641729536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.913913 139918641729536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.914593 139918641729536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.917109 139918641729536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.917727 139918641729536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.917803 139918641729536 transformer_layer.py:468] tlayer: End windows.
I0123 11:42:21.917837 139918641729536 transformer_layer.py:472] tlayer: final FFN.
I0123 11:42:21.917892 139918641729536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.918014 139918641729536 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:42:21.918119 139918641729536 nn_components.py:325] mlp: activation = None
I0123 11:42:21.918156 139918641729536 nn_components.py:329] mlp: final activation = None
I0123 11:42:21.920007 139918641729536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.920097 139918641729536 nn_components.py:261] mlp: residual
I0123 11:42:21.922450 139918641729536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.922528 139918641729536 transformer_base.py:443] tbase: final FFN
I0123 11:42:21.922633 139918641729536 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:42:21.924887 139918641729536 nn_components.py:329] mlp: final activation = None
I0123 11:42:21.926709 139918641729536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.926801 139918641729536 nn_components.py:261] mlp: residual
I0123 11:42:21.927084 139918641729536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.927163 139918641729536 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 11:42:21.927267 139918641729536 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:42:21.927304 139918641729536 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:42:21.927333 139918641729536 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:42:21.927392 139918641729536 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.929595 139918641729536 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:42:21.934973 139918641729536 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.935230 139918641729536 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:42:21.937889 139918641729536 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:42:21.950189 139918641729536 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:42:21.950243 139918641729536 attention.py:418] Single window, no scan.
I0123 11:42:21.950277 139918641729536 transformer_layer.py:389] tlayer: self-attention.
I0123 11:42:21.950307 139918641729536 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.950368 139918641729536 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.950917 139918641729536 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.950989 139918641729536 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.951339 139918641729536 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.952015 139918641729536 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.954892 139918641729536 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.955504 139918641729536 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.955579 139918641729536 transformer_layer.py:468] tlayer: End windows.
I0123 11:42:21.955611 139918641729536 transformer_layer.py:472] tlayer: final FFN.
I0123 11:42:21.955665 139918641729536 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.955789 139918641729536 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:42:21.955902 139918641729536 nn_components.py:325] mlp: activation = None
I0123 11:42:21.955940 139918641729536 nn_components.py:329] mlp: final activation = None
I0123 11:42:21.957779 139918641729536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.957870 139918641729536 nn_components.py:261] mlp: residual
I0123 11:42:21.960223 139918641729536 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.960299 139918641729536 transformer_base.py:443] tbase: final FFN
I0123 11:42:21.960405 139918641729536 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:42:21.962667 139918641729536 nn_components.py:329] mlp: final activation = None
I0123 11:42:21.964507 139918641729536 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.964598 139918641729536 nn_components.py:261] mlp: residual
I0123 11:42:21.964880 139918641729536 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:21.964962 139918641729536 decoder_stack.py:344] dstack: Final layernorm.
I0123 11:42:21.967737 139918641729536 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:42:26.364975 139918641729536 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
I0123 11:42:26.857531 139918641729536 training_loop.py:409] No working directory specified.
I0123 11:42:26.857669 139918641729536 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0123 11:42:26.858466 139918641729536 checkpoints.py:1062] Restoring legacy Flax checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0123 11:42:29.881127 139918641729536 training_loop.py:447] Only restoring trainable parameters.
I0123 11:42:29.881840 139918641729536 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0123 11:42:29.881900 139918641729536 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0123 11:42:29.881948 139918641729536 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:42:29.881993 139918641729536 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:42:29.882034 139918641729536 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:29.882076 139918641729536 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:42:29.882119 139918641729536 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:29.882158 139918641729536 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:29.882196 139918641729536 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:42:29.882234 139918641729536 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:42:29.882272 139918641729536 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:29.882309 139918641729536 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:42:29.882347 139918641729536 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:42:29.882385 139918641729536 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:42:29.882423 139918641729536 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:29.882461 139918641729536 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:42:29.882499 139918641729536 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:29.882536 139918641729536 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:29.882574 139918641729536 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:42:29.882612 139918641729536 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:42:29.882663 139918641729536 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:29.882702 139918641729536 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:42:29.882741 139918641729536 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:42:29.882778 139918641729536 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:42:29.882815 139918641729536 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:29.882852 139918641729536 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:42:29.882889 139918641729536 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:29.882927 139918641729536 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:29.882963 139918641729536 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:42:29.883000 139918641729536 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:42:29.883036 139918641729536 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:29.883074 139918641729536 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:42:29.883111 139918641729536 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:42:29.883148 139918641729536 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:42:29.883185 139918641729536 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:29.883221 139918641729536 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:42:29.883256 139918641729536 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:29.883291 139918641729536 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:29.883326 139918641729536 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:42:29.883362 139918641729536 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:42:29.883398 139918641729536 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:29.883434 139918641729536 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:42:29.883470 139918641729536 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:42:29.883505 139918641729536 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:42:29.883540 139918641729536 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:29.883575 139918641729536 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:42:29.883615 139918641729536 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:29.883652 139918641729536 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:29.883688 139918641729536 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:42:29.883723 139918641729536 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:42:29.883757 139918641729536 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:29.883792 139918641729536 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:42:29.883827 139918641729536 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:42:29.883862 139918641729536 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:42:29.883897 139918641729536 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:29.883932 139918641729536 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:42:29.883966 139918641729536 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:29.884001 139918641729536 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:29.884036 139918641729536 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:42:29.884071 139918641729536 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:42:29.884106 139918641729536 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:29.884141 139918641729536 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:42:29.884176 139918641729536 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:42:29.884211 139918641729536 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:42:29.884246 139918641729536 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:29.884281 139918641729536 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:42:29.884316 139918641729536 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:29.884351 139918641729536 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:29.884386 139918641729536 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:42:29.884422 139918641729536 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:42:29.884458 139918641729536 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:29.884493 139918641729536 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:42:29.884528 139918641729536 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:42:29.884569 139918641729536 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:42:29.884606 139918641729536 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:29.884642 139918641729536 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:42:29.884678 139918641729536 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:29.884714 139918641729536 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:29.884749 139918641729536 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:42:29.884785 139918641729536 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:42:29.884820 139918641729536 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:29.884856 139918641729536 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:42:29.884892 139918641729536 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:42:29.884928 139918641729536 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:42:29.884963 139918641729536 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:29.884999 139918641729536 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:42:29.885035 139918641729536 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:29.885071 139918641729536 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:29.885107 139918641729536 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:42:29.885142 139918641729536 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:42:29.885177 139918641729536 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:29.885213 139918641729536 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:42:29.885249 139918641729536 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:42:29.885284 139918641729536 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:42:29.885319 139918641729536 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:29.885354 139918641729536 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:42:29.885389 139918641729536 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:29.885424 139918641729536 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:29.885460 139918641729536 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:42:29.885495 139918641729536 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:42:29.885535 139918641729536 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:29.885571 139918641729536 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:42:29.885606 139918641729536 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:42:29.885648 139918641729536 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:42:29.885687 139918641729536 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:29.885725 139918641729536 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:42:29.885761 139918641729536 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:29.885796 139918641729536 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:29.885831 139918641729536 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:42:29.885867 139918641729536 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:42:29.885903 139918641729536 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:29.885938 139918641729536 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:42:29.885976 139918641729536 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:42:29.886013 139918641729536 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:42:29.886049 139918641729536 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:29.886088 139918641729536 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:42:29.886124 139918641729536 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:29.886161 139918641729536 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:29.886198 139918641729536 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:42:29.886234 139918641729536 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:42:29.886270 139918641729536 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:42:29.886307 139918641729536 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:42:29.886336 139918641729536 training_loop.py:725] Total parameters: 152072288
I0123 11:42:29.886549 139918641729536 training_loop.py:739] Total state size: 0
I0123 11:42:29.906645 139918641729536 training_loop.py:492] Training loop: creating task for mode beam_search
I0123 11:42:29.906919 139918641729536 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0123 11:42:29.907426 139918641729536 training_loop.py:652] Compiling mode beam_search with jit.
I0123 11:42:29.907744 139918641729536 training_loop.py:89] registering functions: dict_keys([])
I0123 11:42:29.923523 139918641729536 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = lc_tangent e c d, on_line e a d; f = on_line f c e, on_line f b a; g = on_circle g f c, on_line g b a; h = on_circle h f c, on_line h b a; i = on_circle i f h; j = incenter j c i b; k = incenter k c i a; l = on_line l c i, on_line l b j ? coll a l k
I0123 11:42:31.029698 139918641729536 ddar.py:60] Depth 1/1000 time = 1.0776174068450928
I0123 11:42:32.727381 139918641729536 ddar.py:60] Depth 2/1000 time = 1.6975071430206299
I0123 11:42:34.376972 139918641729536 ddar.py:60] Depth 3/1000 time = 1.6494114398956299
I0123 11:42:36.214372 139918641729536 ddar.py:60] Depth 4/1000 time = 1.8372089862823486
I0123 11:42:37.772768 139918641729536 ddar.py:60] Depth 5/1000 time = 1.558210849761963
I0123 11:42:39.513875 139918641729536 ddar.py:60] Depth 6/1000 time = 1.7409241199493408
I0123 11:42:41.433504 139918641729536 ddar.py:60] Depth 7/1000 time = 1.9192728996276855
I0123 11:42:42.999061 139918641729536 ddar.py:60] Depth 8/1000 time = 1.5652077198028564
I0123 11:42:43.003165 139918641729536 alphageometry.py:191] 
==========================
 * From theorem premises:
A B C D E F H I J K L : Points
DB = DA [00]
DC = DB [01]
CE  CD [02]
A,B,F are collinear [03]
E,C,F are collinear [04]
FH = FC [05]
FI = FH [06]
JBC = IBJ [07]
CAK = KAI [08]
J,B,L are collinear [09]
I,C,L are collinear [10]

 * Auxiliary Constructions:
: Points


 * Proof steps:
001. J,B,L are collinear [09] & IBJ = JBC [07]   IBL = LBC [11]
002. IBL = LBC [11] & I,C,L are collinear [10]   IL:IB = CL:CB [12]
003. DB = DA [00] & DC = DB [01]   D is the circumcenter of \Delta CAB [13]
004. D is the circumcenter of \Delta CAB [13] & CE  CD [02]   ECA = CBA [14]
005. A,B,F are collinear [03] & E,C,F are collinear [04] & CBA = ECA [14]   CBF = FCA [15]
006. E,C,F are collinear [04] & A,B,F are collinear [03]   CFB = CFA [16]
007. CBF = FCA [15] & CFB = CFA [16] (Similar Triangles)  FC:FB = FA:FC [17]
008. CBF = FCA [15] & CFB = CFA [16] (Similar Triangles)  CB:AC = CF:AF [18]
009. FC:FB = FA:FC [17] & FI = FH [06] & FH = FC [05]   IF:BF = AF:IF [19]
010. A,B,F are collinear [03]   BFI = AFI [20]
011. IF:BF = AF:IF [19] & BFI = AFI [20] (Similar Triangles)  IB:AI = IF:AF [21]
012. IB:AI = IF:AF [21] & CB:AC = CF:AF [18] & FI = FH [06] & FH = FC [05]   CB:AC = IB:AI [22]
013. IL:IB = CL:CB [12] & CB:AC = IB:AI [22]   CL:IL = AC:AI [23]
014. CL:IL = AC:AI [23] & I,C,L are collinear [10]   IAL = LAC [24]
015. CAK = KAI [08] & IAL = LAC [24] (Angle chase)  AK  AL [25]
016. AK  AL [25]   A,K,L are collinear
==========================

