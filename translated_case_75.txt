I0123 14:00:02.904940 140227981185024 inference_utils.py:69] Parsing gin configuration.
I0123 14:00:02.905044 140227981185024 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0123 14:00:02.905252 140227981185024 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0123 14:00:02.905287 140227981185024 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0123 14:00:02.905316 140227981185024 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0123 14:00:02.905344 140227981185024 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0123 14:00:02.905371 140227981185024 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0123 14:00:02.905396 140227981185024 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0123 14:00:02.905421 140227981185024 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0123 14:00:02.905445 140227981185024 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=32
I0123 14:00:02.905470 140227981185024 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0123 14:00:02.905494 140227981185024 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0123 14:00:02.905539 140227981185024 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0123 14:00:02.905699 140227981185024 resource_reader.py:55] Path not found: base_htrans.gin
I0123 14:00:02.905906 140227981185024 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0123 14:00:02.906011 140227981185024 resource_reader.py:55] Path not found: trainer_configuration.gin
I0123 14:00:02.912369 140227981185024 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0123 14:00:02.912494 140227981185024 resource_reader.py:55] Path not found: size/medium_150M.gin
I0123 14:00:02.912815 140227981185024 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0123 14:00:02.912918 140227981185024 resource_reader.py:55] Path not found: options/positions_t5.gin
I0123 14:00:02.913199 140227981185024 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0123 14:00:02.913298 140227981185024 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0123 14:00:02.913710 140227981185024 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0123 14:00:02.913810 140227981185024 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0123 14:00:02.917490 140227981185024 training_loop.py:334] ==== Training loop: initializing model ====
I0123 14:00:03.012388 140227981185024 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0123 14:00:03.013101 140227981185024 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0123 14:00:03.019706 140227981185024 training_loop.py:335] Process 0 of 1
I0123 14:00:03.019758 140227981185024 training_loop.py:336] Local device count = 1
I0123 14:00:03.019796 140227981185024 training_loop.py:337] Number of replicas = 1
I0123 14:00:03.019825 140227981185024 training_loop.py:339] Using random number seed 42
I0123 14:00:03.512243 140227981185024 training_loop.py:359] Initializing the model.
I0123 14:00:03.917532 140227981185024 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:03.917782 140227981185024 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 14:00:03.917883 140227981185024 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 14:00:03.917959 140227981185024 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 14:00:03.918037 140227981185024 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 14:00:03.918119 140227981185024 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 14:00:03.918192 140227981185024 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 14:00:03.918263 140227981185024 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 14:00:03.918335 140227981185024 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 14:00:03.918406 140227981185024 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 14:00:03.918476 140227981185024 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 14:00:03.918547 140227981185024 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 14:00:03.918619 140227981185024 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 14:00:03.918689 140227981185024 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 14:00:03.918730 140227981185024 attention.py:418] Single window, no scan.
I0123 14:00:03.918776 140227981185024 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 14:00:03.918893 140227981185024 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:00:03.918934 140227981185024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:00:03.918965 140227981185024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:00:03.920986 140227981185024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:03.926374 140227981185024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:00:03.937280 140227981185024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:03.937558 140227981185024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:00:03.942125 140227981185024 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:00:03.952931 140227981185024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:00:03.952988 140227981185024 attention.py:418] Single window, no scan.
I0123 14:00:03.953026 140227981185024 transformer_layer.py:389] tlayer: self-attention.
I0123 14:00:03.953059 140227981185024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:03.953123 140227981185024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:03.954324 140227981185024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:03.954402 140227981185024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:03.955122 140227981185024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:03.957580 140227981185024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:03.964149 140227981185024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:03.965965 140227981185024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:03.966054 140227981185024 transformer_layer.py:468] tlayer: End windows.
I0123 14:00:03.966092 140227981185024 transformer_layer.py:472] tlayer: final FFN.
I0123 14:00:03.966155 140227981185024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:03.966300 140227981185024 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:00:03.966652 140227981185024 nn_components.py:325] mlp: activation = None
I0123 14:00:03.966700 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:00:03.968678 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:03.968778 140227981185024 nn_components.py:261] mlp: residual
I0123 14:00:03.971711 140227981185024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:03.971791 140227981185024 transformer_base.py:443] tbase: final FFN
I0123 14:00:03.972291 140227981185024 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:00:03.982756 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:00:03.991738 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:03.991838 140227981185024 nn_components.py:261] mlp: residual
I0123 14:00:03.992144 140227981185024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:03.992226 140227981185024 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 14:00:03.992338 140227981185024 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:00:03.992378 140227981185024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:00:03.992410 140227981185024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:00:03.994280 140227981185024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:03.996767 140227981185024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:00:04.002442 140227981185024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.002703 140227981185024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:00:04.005349 140227981185024 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:00:04.009228 140227981185024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:00:04.009285 140227981185024 attention.py:418] Single window, no scan.
I0123 14:00:04.009321 140227981185024 transformer_layer.py:389] tlayer: self-attention.
I0123 14:00:04.009352 140227981185024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.009415 140227981185024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.009992 140227981185024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.010068 140227981185024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.010431 140227981185024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.011199 140227981185024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.013720 140227981185024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.014348 140227981185024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.014428 140227981185024 transformer_layer.py:468] tlayer: End windows.
I0123 14:00:04.014463 140227981185024 transformer_layer.py:472] tlayer: final FFN.
I0123 14:00:04.014521 140227981185024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.014650 140227981185024 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:00:04.014968 140227981185024 nn_components.py:325] mlp: activation = None
I0123 14:00:04.015012 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:00:04.016957 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.017050 140227981185024 nn_components.py:261] mlp: residual
I0123 14:00:04.019586 140227981185024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.019670 140227981185024 transformer_base.py:443] tbase: final FFN
I0123 14:00:04.020100 140227981185024 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:00:04.022420 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:00:04.024347 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.024441 140227981185024 nn_components.py:261] mlp: residual
I0123 14:00:04.024736 140227981185024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.024820 140227981185024 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 14:00:04.024932 140227981185024 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:00:04.024971 140227981185024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:00:04.025002 140227981185024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:00:04.026913 140227981185024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.029298 140227981185024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:00:04.035342 140227981185024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.035604 140227981185024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:00:04.038315 140227981185024 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:00:04.042201 140227981185024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:00:04.042257 140227981185024 attention.py:418] Single window, no scan.
I0123 14:00:04.042293 140227981185024 transformer_layer.py:389] tlayer: self-attention.
I0123 14:00:04.042324 140227981185024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.042385 140227981185024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.042944 140227981185024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.043026 140227981185024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.043392 140227981185024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.044156 140227981185024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.046694 140227981185024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.047371 140227981185024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.047449 140227981185024 transformer_layer.py:468] tlayer: End windows.
I0123 14:00:04.047485 140227981185024 transformer_layer.py:472] tlayer: final FFN.
I0123 14:00:04.047545 140227981185024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.047672 140227981185024 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:00:04.048000 140227981185024 nn_components.py:325] mlp: activation = None
I0123 14:00:04.048043 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:00:04.049990 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.050086 140227981185024 nn_components.py:261] mlp: residual
I0123 14:00:04.052647 140227981185024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.052731 140227981185024 transformer_base.py:443] tbase: final FFN
I0123 14:00:04.053222 140227981185024 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:00:04.055522 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:00:04.057467 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.057564 140227981185024 nn_components.py:261] mlp: residual
I0123 14:00:04.057867 140227981185024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.057948 140227981185024 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 14:00:04.058058 140227981185024 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:00:04.058096 140227981185024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:00:04.058127 140227981185024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:00:04.060035 140227981185024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.062471 140227981185024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:00:04.068172 140227981185024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.068439 140227981185024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:00:04.071117 140227981185024 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:00:04.074959 140227981185024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:00:04.075015 140227981185024 attention.py:418] Single window, no scan.
I0123 14:00:04.075050 140227981185024 transformer_layer.py:389] tlayer: self-attention.
I0123 14:00:04.075081 140227981185024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.075142 140227981185024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.075708 140227981185024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.075784 140227981185024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.076154 140227981185024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.076942 140227981185024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.079823 140227981185024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.080452 140227981185024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.080529 140227981185024 transformer_layer.py:468] tlayer: End windows.
I0123 14:00:04.080564 140227981185024 transformer_layer.py:472] tlayer: final FFN.
I0123 14:00:04.080623 140227981185024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.080756 140227981185024 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:00:04.081080 140227981185024 nn_components.py:325] mlp: activation = None
I0123 14:00:04.081123 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:00:04.083072 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.083165 140227981185024 nn_components.py:261] mlp: residual
I0123 14:00:04.085776 140227981185024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.085860 140227981185024 transformer_base.py:443] tbase: final FFN
I0123 14:00:04.086292 140227981185024 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:00:04.088583 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:00:04.090527 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.090621 140227981185024 nn_components.py:261] mlp: residual
I0123 14:00:04.090922 140227981185024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.091003 140227981185024 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 14:00:04.091113 140227981185024 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:00:04.091153 140227981185024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:00:04.091184 140227981185024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:00:04.093107 140227981185024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.095545 140227981185024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:00:04.101282 140227981185024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.101540 140227981185024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:00:04.104306 140227981185024 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:00:04.108100 140227981185024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:00:04.108155 140227981185024 attention.py:418] Single window, no scan.
I0123 14:00:04.108191 140227981185024 transformer_layer.py:389] tlayer: self-attention.
I0123 14:00:04.108222 140227981185024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.108291 140227981185024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.108862 140227981185024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.108938 140227981185024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.109299 140227981185024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.110091 140227981185024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.112956 140227981185024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.113580 140227981185024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.113666 140227981185024 transformer_layer.py:468] tlayer: End windows.
I0123 14:00:04.113703 140227981185024 transformer_layer.py:472] tlayer: final FFN.
I0123 14:00:04.113764 140227981185024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.113903 140227981185024 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:00:04.114228 140227981185024 nn_components.py:325] mlp: activation = None
I0123 14:00:04.114272 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:00:04.116187 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.116280 140227981185024 nn_components.py:261] mlp: residual
I0123 14:00:04.118881 140227981185024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.118962 140227981185024 transformer_base.py:443] tbase: final FFN
I0123 14:00:04.119397 140227981185024 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:00:04.121693 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:00:04.123673 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.123769 140227981185024 nn_components.py:261] mlp: residual
I0123 14:00:04.124064 140227981185024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.124145 140227981185024 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 14:00:04.124256 140227981185024 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:00:04.124296 140227981185024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:00:04.124328 140227981185024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:00:04.126161 140227981185024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.128579 140227981185024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:00:04.134227 140227981185024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.134486 140227981185024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:00:04.137218 140227981185024 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:00:04.140996 140227981185024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:00:04.141054 140227981185024 attention.py:418] Single window, no scan.
I0123 14:00:04.141089 140227981185024 transformer_layer.py:389] tlayer: self-attention.
I0123 14:00:04.141121 140227981185024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.141182 140227981185024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.141795 140227981185024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.141873 140227981185024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.142239 140227981185024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.143024 140227981185024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.145510 140227981185024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.146147 140227981185024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.146226 140227981185024 transformer_layer.py:468] tlayer: End windows.
I0123 14:00:04.146261 140227981185024 transformer_layer.py:472] tlayer: final FFN.
I0123 14:00:04.146319 140227981185024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.146450 140227981185024 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:00:04.146770 140227981185024 nn_components.py:325] mlp: activation = None
I0123 14:00:04.146812 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:00:04.148724 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.148820 140227981185024 nn_components.py:261] mlp: residual
I0123 14:00:04.151393 140227981185024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.151473 140227981185024 transformer_base.py:443] tbase: final FFN
I0123 14:00:04.151902 140227981185024 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:00:04.154239 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:00:04.156184 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.156278 140227981185024 nn_components.py:261] mlp: residual
I0123 14:00:04.156575 140227981185024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.156656 140227981185024 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 14:00:04.156766 140227981185024 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:00:04.156805 140227981185024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:00:04.156836 140227981185024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:00:04.158705 140227981185024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.161167 140227981185024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:00:04.166846 140227981185024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.167119 140227981185024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:00:04.169811 140227981185024 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:00:04.173628 140227981185024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:00:04.173689 140227981185024 attention.py:418] Single window, no scan.
I0123 14:00:04.173723 140227981185024 transformer_layer.py:389] tlayer: self-attention.
I0123 14:00:04.173754 140227981185024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.173815 140227981185024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.174380 140227981185024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.174455 140227981185024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.174815 140227981185024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.175596 140227981185024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.178108 140227981185024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.178742 140227981185024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.178819 140227981185024 transformer_layer.py:468] tlayer: End windows.
I0123 14:00:04.178854 140227981185024 transformer_layer.py:472] tlayer: final FFN.
I0123 14:00:04.178913 140227981185024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.179045 140227981185024 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:00:04.179365 140227981185024 nn_components.py:325] mlp: activation = None
I0123 14:00:04.179408 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:00:04.181384 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.181475 140227981185024 nn_components.py:261] mlp: residual
I0123 14:00:04.184005 140227981185024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.184090 140227981185024 transformer_base.py:443] tbase: final FFN
I0123 14:00:04.184516 140227981185024 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:00:04.187171 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:00:04.189218 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.189320 140227981185024 nn_components.py:261] mlp: residual
I0123 14:00:04.189616 140227981185024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.189838 140227981185024 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 14:00:04.189949 140227981185024 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:00:04.189989 140227981185024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:00:04.190020 140227981185024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:00:04.326005 140227981185024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.329052 140227981185024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:00:04.334956 140227981185024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.335254 140227981185024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:00:04.337987 140227981185024 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:00:04.341910 140227981185024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:00:04.341969 140227981185024 attention.py:418] Single window, no scan.
I0123 14:00:04.342006 140227981185024 transformer_layer.py:389] tlayer: self-attention.
I0123 14:00:04.342039 140227981185024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.342104 140227981185024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.342716 140227981185024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.342793 140227981185024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.343163 140227981185024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.343951 140227981185024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.346576 140227981185024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.347224 140227981185024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.347302 140227981185024 transformer_layer.py:468] tlayer: End windows.
I0123 14:00:04.347337 140227981185024 transformer_layer.py:472] tlayer: final FFN.
I0123 14:00:04.347398 140227981185024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.347525 140227981185024 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:00:04.347861 140227981185024 nn_components.py:325] mlp: activation = None
I0123 14:00:04.347904 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:00:04.349833 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.349927 140227981185024 nn_components.py:261] mlp: residual
I0123 14:00:04.352519 140227981185024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.352598 140227981185024 transformer_base.py:443] tbase: final FFN
I0123 14:00:04.353054 140227981185024 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:00:04.355387 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:00:04.357325 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.357429 140227981185024 nn_components.py:261] mlp: residual
I0123 14:00:04.357735 140227981185024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.357820 140227981185024 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 14:00:04.357931 140227981185024 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:00:04.357971 140227981185024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:00:04.358002 140227981185024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:00:04.359925 140227981185024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.362340 140227981185024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:00:04.367981 140227981185024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.368249 140227981185024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:00:04.370949 140227981185024 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:00:04.374722 140227981185024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:00:04.374778 140227981185024 attention.py:418] Single window, no scan.
I0123 14:00:04.374814 140227981185024 transformer_layer.py:389] tlayer: self-attention.
I0123 14:00:04.374846 140227981185024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.374907 140227981185024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.375467 140227981185024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.375543 140227981185024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.375907 140227981185024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.376681 140227981185024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.379251 140227981185024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.379867 140227981185024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.379944 140227981185024 transformer_layer.py:468] tlayer: End windows.
I0123 14:00:04.379980 140227981185024 transformer_layer.py:472] tlayer: final FFN.
I0123 14:00:04.380039 140227981185024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.380171 140227981185024 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:00:04.380492 140227981185024 nn_components.py:325] mlp: activation = None
I0123 14:00:04.380535 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:00:04.382452 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.382550 140227981185024 nn_components.py:261] mlp: residual
I0123 14:00:04.385118 140227981185024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.385197 140227981185024 transformer_base.py:443] tbase: final FFN
I0123 14:00:04.385625 140227981185024 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:00:04.387900 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:00:04.389864 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.389959 140227981185024 nn_components.py:261] mlp: residual
I0123 14:00:04.390254 140227981185024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.390342 140227981185024 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 14:00:04.390454 140227981185024 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:00:04.390494 140227981185024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:00:04.390526 140227981185024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:00:04.392354 140227981185024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.394793 140227981185024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:00:04.400669 140227981185024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.400935 140227981185024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:00:04.403956 140227981185024 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:00:04.407706 140227981185024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:00:04.407763 140227981185024 attention.py:418] Single window, no scan.
I0123 14:00:04.407799 140227981185024 transformer_layer.py:389] tlayer: self-attention.
I0123 14:00:04.407830 140227981185024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.407896 140227981185024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.408496 140227981185024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.408573 140227981185024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.408941 140227981185024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.409724 140227981185024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.412228 140227981185024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.412850 140227981185024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.412930 140227981185024 transformer_layer.py:468] tlayer: End windows.
I0123 14:00:04.412966 140227981185024 transformer_layer.py:472] tlayer: final FFN.
I0123 14:00:04.413026 140227981185024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.413154 140227981185024 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:00:04.413475 140227981185024 nn_components.py:325] mlp: activation = None
I0123 14:00:04.413518 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:00:04.415436 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.415530 140227981185024 nn_components.py:261] mlp: residual
I0123 14:00:04.418089 140227981185024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.418174 140227981185024 transformer_base.py:443] tbase: final FFN
I0123 14:00:04.418599 140227981185024 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:00:04.420886 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:00:04.422807 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.422907 140227981185024 nn_components.py:261] mlp: residual
I0123 14:00:04.423207 140227981185024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.423296 140227981185024 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 14:00:04.423410 140227981185024 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:00:04.423449 140227981185024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:00:04.423479 140227981185024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:00:04.425292 140227981185024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.427738 140227981185024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:00:04.433330 140227981185024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.433591 140227981185024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:00:04.436223 140227981185024 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:00:04.439963 140227981185024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:00:04.440018 140227981185024 attention.py:418] Single window, no scan.
I0123 14:00:04.440054 140227981185024 transformer_layer.py:389] tlayer: self-attention.
I0123 14:00:04.440085 140227981185024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.440148 140227981185024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.440715 140227981185024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.440791 140227981185024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.441142 140227981185024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.441911 140227981185024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.444399 140227981185024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.445018 140227981185024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.445095 140227981185024 transformer_layer.py:468] tlayer: End windows.
I0123 14:00:04.445130 140227981185024 transformer_layer.py:472] tlayer: final FFN.
I0123 14:00:04.445188 140227981185024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.445316 140227981185024 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:00:04.445634 140227981185024 nn_components.py:325] mlp: activation = None
I0123 14:00:04.445684 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:00:04.447625 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.447716 140227981185024 nn_components.py:261] mlp: residual
I0123 14:00:04.450492 140227981185024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.450572 140227981185024 transformer_base.py:443] tbase: final FFN
I0123 14:00:04.450997 140227981185024 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:00:04.453303 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:00:04.455208 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.455303 140227981185024 nn_components.py:261] mlp: residual
I0123 14:00:04.455597 140227981185024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.455677 140227981185024 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 14:00:04.455791 140227981185024 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:00:04.455830 140227981185024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:00:04.455862 140227981185024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:00:04.457761 140227981185024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.460140 140227981185024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:00:04.465732 140227981185024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.465989 140227981185024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:00:04.468629 140227981185024 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:00:04.472470 140227981185024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:00:04.472526 140227981185024 attention.py:418] Single window, no scan.
I0123 14:00:04.472562 140227981185024 transformer_layer.py:389] tlayer: self-attention.
I0123 14:00:04.472594 140227981185024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.472656 140227981185024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.473221 140227981185024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.473298 140227981185024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.473670 140227981185024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.474448 140227981185024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.476949 140227981185024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.477945 140227981185024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.478024 140227981185024 transformer_layer.py:468] tlayer: End windows.
I0123 14:00:04.478060 140227981185024 transformer_layer.py:472] tlayer: final FFN.
I0123 14:00:04.478119 140227981185024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.478247 140227981185024 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:00:04.478568 140227981185024 nn_components.py:325] mlp: activation = None
I0123 14:00:04.478612 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:00:04.480534 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.480626 140227981185024 nn_components.py:261] mlp: residual
I0123 14:00:04.483146 140227981185024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.483226 140227981185024 transformer_base.py:443] tbase: final FFN
I0123 14:00:04.483706 140227981185024 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:00:04.485949 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:00:04.487867 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.487962 140227981185024 nn_components.py:261] mlp: residual
I0123 14:00:04.488257 140227981185024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.488543 140227981185024 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 14:00:04.488612 140227981185024 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 14:00:04.488678 140227981185024 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 14:00:04.488734 140227981185024 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 14:00:04.488788 140227981185024 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 14:00:04.488841 140227981185024 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 14:00:04.488893 140227981185024 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 14:00:04.488944 140227981185024 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 14:00:04.488995 140227981185024 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 14:00:04.489044 140227981185024 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 14:00:04.489096 140227981185024 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 14:00:04.489147 140227981185024 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 14:00:04.489184 140227981185024 decoder_stack.py:344] dstack: Final layernorm.
I0123 14:00:04.492769 140227981185024 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:00:04.541079 140227981185024 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.541165 140227981185024 decoder_stack.py:333] dstack: autoregressive generator.
I0123 14:00:04.541218 140227981185024 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 14:00:04.541321 140227981185024 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:00:04.541360 140227981185024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:00:04.541391 140227981185024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:00:04.541454 140227981185024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.543907 140227981185024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:00:04.549403 140227981185024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.549672 140227981185024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:00:04.552405 140227981185024 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:00:04.569137 140227981185024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:00:04.569195 140227981185024 attention.py:418] Single window, no scan.
I0123 14:00:04.569232 140227981185024 transformer_layer.py:389] tlayer: self-attention.
I0123 14:00:04.569265 140227981185024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.569327 140227981185024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.570469 140227981185024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.570548 140227981185024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.571263 140227981185024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.573270 140227981185024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.578007 140227981185024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.579319 140227981185024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.579405 140227981185024 transformer_layer.py:468] tlayer: End windows.
I0123 14:00:04.579442 140227981185024 transformer_layer.py:472] tlayer: final FFN.
I0123 14:00:04.579501 140227981185024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.579635 140227981185024 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:00:04.579746 140227981185024 nn_components.py:325] mlp: activation = None
I0123 14:00:04.579785 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:00:04.581692 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.581787 140227981185024 nn_components.py:261] mlp: residual
I0123 14:00:04.584227 140227981185024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.584306 140227981185024 transformer_base.py:443] tbase: final FFN
I0123 14:00:04.584414 140227981185024 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:00:04.586643 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:00:04.588593 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.588689 140227981185024 nn_components.py:261] mlp: residual
I0123 14:00:04.588982 140227981185024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.589064 140227981185024 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 14:00:04.589173 140227981185024 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:00:04.589212 140227981185024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:00:04.589243 140227981185024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:00:04.589308 140227981185024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.591591 140227981185024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:00:04.597123 140227981185024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.597385 140227981185024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:00:04.600096 140227981185024 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:00:04.613262 140227981185024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:00:04.613319 140227981185024 attention.py:418] Single window, no scan.
I0123 14:00:04.613355 140227981185024 transformer_layer.py:389] tlayer: self-attention.
I0123 14:00:04.613387 140227981185024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.613448 140227981185024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.618339 140227981185024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.618456 140227981185024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.618866 140227981185024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.619606 140227981185024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.622272 140227981185024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.622932 140227981185024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.623012 140227981185024 transformer_layer.py:468] tlayer: End windows.
I0123 14:00:04.623053 140227981185024 transformer_layer.py:472] tlayer: final FFN.
I0123 14:00:04.623130 140227981185024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.623261 140227981185024 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:00:04.623380 140227981185024 nn_components.py:325] mlp: activation = None
I0123 14:00:04.623420 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:00:04.625481 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.625574 140227981185024 nn_components.py:261] mlp: residual
I0123 14:00:04.628136 140227981185024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.628216 140227981185024 transformer_base.py:443] tbase: final FFN
I0123 14:00:04.628326 140227981185024 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:00:04.630603 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:00:04.632564 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.632658 140227981185024 nn_components.py:261] mlp: residual
I0123 14:00:04.632950 140227981185024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.633033 140227981185024 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 14:00:04.633146 140227981185024 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:00:04.633188 140227981185024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:00:04.633221 140227981185024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:00:04.633289 140227981185024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.635607 140227981185024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:00:04.641113 140227981185024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.641376 140227981185024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:00:04.644116 140227981185024 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:00:04.657099 140227981185024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:00:04.657157 140227981185024 attention.py:418] Single window, no scan.
I0123 14:00:04.657193 140227981185024 transformer_layer.py:389] tlayer: self-attention.
I0123 14:00:04.657226 140227981185024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.657289 140227981185024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.657855 140227981185024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.657930 140227981185024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.658288 140227981185024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.658989 140227981185024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.661488 140227981185024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.662127 140227981185024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.662204 140227981185024 transformer_layer.py:468] tlayer: End windows.
I0123 14:00:04.662239 140227981185024 transformer_layer.py:472] tlayer: final FFN.
I0123 14:00:04.662303 140227981185024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.662435 140227981185024 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:00:04.662544 140227981185024 nn_components.py:325] mlp: activation = None
I0123 14:00:04.662582 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:00:04.664540 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.664634 140227981185024 nn_components.py:261] mlp: residual
I0123 14:00:04.667094 140227981185024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.667174 140227981185024 transformer_base.py:443] tbase: final FFN
I0123 14:00:04.667287 140227981185024 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:00:04.669710 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:00:04.671654 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.671748 140227981185024 nn_components.py:261] mlp: residual
I0123 14:00:04.672038 140227981185024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.672119 140227981185024 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 14:00:04.672228 140227981185024 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:00:04.672267 140227981185024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:00:04.672298 140227981185024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:00:04.672361 140227981185024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.674632 140227981185024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:00:04.680148 140227981185024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.680406 140227981185024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:00:04.683116 140227981185024 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:00:04.695933 140227981185024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:00:04.695990 140227981185024 attention.py:418] Single window, no scan.
I0123 14:00:04.696025 140227981185024 transformer_layer.py:389] tlayer: self-attention.
I0123 14:00:04.696057 140227981185024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.696122 140227981185024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.696680 140227981185024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.696755 140227981185024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.697111 140227981185024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.697818 140227981185024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.700307 140227981185024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.700930 140227981185024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.701007 140227981185024 transformer_layer.py:468] tlayer: End windows.
I0123 14:00:04.701042 140227981185024 transformer_layer.py:472] tlayer: final FFN.
I0123 14:00:04.701101 140227981185024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.701242 140227981185024 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:00:04.701352 140227981185024 nn_components.py:325] mlp: activation = None
I0123 14:00:04.701391 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:00:04.703378 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.703473 140227981185024 nn_components.py:261] mlp: residual
I0123 14:00:04.705932 140227981185024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.706012 140227981185024 transformer_base.py:443] tbase: final FFN
I0123 14:00:04.706121 140227981185024 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:00:04.708352 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:00:04.710267 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.710364 140227981185024 nn_components.py:261] mlp: residual
I0123 14:00:04.710654 140227981185024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.710736 140227981185024 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 14:00:04.710845 140227981185024 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:00:04.710883 140227981185024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:00:04.710914 140227981185024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:00:04.710978 140227981185024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.713592 140227981185024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:00:04.719095 140227981185024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.719363 140227981185024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:00:04.722013 140227981185024 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:00:04.734786 140227981185024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:00:04.734841 140227981185024 attention.py:418] Single window, no scan.
I0123 14:00:04.734877 140227981185024 transformer_layer.py:389] tlayer: self-attention.
I0123 14:00:04.734907 140227981185024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.734968 140227981185024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.735529 140227981185024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.735604 140227981185024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.735954 140227981185024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.736650 140227981185024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.739229 140227981185024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.739865 140227981185024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.739943 140227981185024 transformer_layer.py:468] tlayer: End windows.
I0123 14:00:04.739978 140227981185024 transformer_layer.py:472] tlayer: final FFN.
I0123 14:00:04.740036 140227981185024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.740175 140227981185024 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:00:04.740285 140227981185024 nn_components.py:325] mlp: activation = None
I0123 14:00:04.740323 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:00:04.742252 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.742345 140227981185024 nn_components.py:261] mlp: residual
I0123 14:00:04.744789 140227981185024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.744870 140227981185024 transformer_base.py:443] tbase: final FFN
I0123 14:00:04.744979 140227981185024 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:00:04.747284 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:00:04.749162 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.749258 140227981185024 nn_components.py:261] mlp: residual
I0123 14:00:04.749545 140227981185024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.749626 140227981185024 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 14:00:04.749744 140227981185024 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:00:04.749783 140227981185024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:00:04.749814 140227981185024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:00:04.749877 140227981185024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.752146 140227981185024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:00:04.757700 140227981185024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.757962 140227981185024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:00:04.760675 140227981185024 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:00:04.773501 140227981185024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:00:04.773556 140227981185024 attention.py:418] Single window, no scan.
I0123 14:00:04.773592 140227981185024 transformer_layer.py:389] tlayer: self-attention.
I0123 14:00:04.773623 140227981185024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.773692 140227981185024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.774250 140227981185024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.774333 140227981185024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.774695 140227981185024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.775405 140227981185024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.777922 140227981185024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.778543 140227981185024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.778623 140227981185024 transformer_layer.py:468] tlayer: End windows.
I0123 14:00:04.778658 140227981185024 transformer_layer.py:472] tlayer: final FFN.
I0123 14:00:04.778716 140227981185024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.778845 140227981185024 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:00:04.778960 140227981185024 nn_components.py:325] mlp: activation = None
I0123 14:00:04.778999 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:00:04.780952 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.781044 140227981185024 nn_components.py:261] mlp: residual
I0123 14:00:04.783467 140227981185024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.783545 140227981185024 transformer_base.py:443] tbase: final FFN
I0123 14:00:04.783657 140227981185024 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:00:04.785901 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:00:04.787781 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.787874 140227981185024 nn_components.py:261] mlp: residual
I0123 14:00:04.788159 140227981185024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.788238 140227981185024 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 14:00:04.788344 140227981185024 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:00:04.788383 140227981185024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:00:04.788413 140227981185024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:00:04.788476 140227981185024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.790765 140227981185024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:00:04.796462 140227981185024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.796721 140227981185024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:00:04.799354 140227981185024 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:00:04.812186 140227981185024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:00:04.812242 140227981185024 attention.py:418] Single window, no scan.
I0123 14:00:04.812279 140227981185024 transformer_layer.py:389] tlayer: self-attention.
I0123 14:00:04.812310 140227981185024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.812376 140227981185024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.812942 140227981185024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.813020 140227981185024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.813383 140227981185024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.814092 140227981185024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.816617 140227981185024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.817648 140227981185024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.817729 140227981185024 transformer_layer.py:468] tlayer: End windows.
I0123 14:00:04.817764 140227981185024 transformer_layer.py:472] tlayer: final FFN.
I0123 14:00:04.817823 140227981185024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.817957 140227981185024 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:00:04.818068 140227981185024 nn_components.py:325] mlp: activation = None
I0123 14:00:04.818114 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:00:04.820015 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.820110 140227981185024 nn_components.py:261] mlp: residual
I0123 14:00:04.822559 140227981185024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.822638 140227981185024 transformer_base.py:443] tbase: final FFN
I0123 14:00:04.822746 140227981185024 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:00:04.825005 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:00:04.826989 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.827084 140227981185024 nn_components.py:261] mlp: residual
I0123 14:00:04.827376 140227981185024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.827458 140227981185024 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 14:00:04.827569 140227981185024 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:00:04.827608 140227981185024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:00:04.827639 140227981185024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:00:04.827702 140227981185024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.830004 140227981185024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:00:04.835493 140227981185024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.835768 140227981185024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:00:04.838483 140227981185024 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:00:04.851258 140227981185024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:00:04.851314 140227981185024 attention.py:418] Single window, no scan.
I0123 14:00:04.851350 140227981185024 transformer_layer.py:389] tlayer: self-attention.
I0123 14:00:04.851381 140227981185024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.851441 140227981185024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.852043 140227981185024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.852120 140227981185024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.852480 140227981185024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.853179 140227981185024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.855695 140227981185024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.856328 140227981185024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.856404 140227981185024 transformer_layer.py:468] tlayer: End windows.
I0123 14:00:04.856439 140227981185024 transformer_layer.py:472] tlayer: final FFN.
I0123 14:00:04.856498 140227981185024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.856627 140227981185024 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:00:04.856740 140227981185024 nn_components.py:325] mlp: activation = None
I0123 14:00:04.856785 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:00:04.858697 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.858791 140227981185024 nn_components.py:261] mlp: residual
I0123 14:00:04.861298 140227981185024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.861377 140227981185024 transformer_base.py:443] tbase: final FFN
I0123 14:00:04.861484 140227981185024 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:00:04.863864 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:00:04.865807 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.865902 140227981185024 nn_components.py:261] mlp: residual
I0123 14:00:04.866201 140227981185024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.866284 140227981185024 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 14:00:04.866397 140227981185024 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:00:04.866438 140227981185024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:00:04.866469 140227981185024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:00:04.866533 140227981185024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.868843 140227981185024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:00:04.874462 140227981185024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.874732 140227981185024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:00:04.877447 140227981185024 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:00:04.890443 140227981185024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:00:04.890501 140227981185024 attention.py:418] Single window, no scan.
I0123 14:00:04.890538 140227981185024 transformer_layer.py:389] tlayer: self-attention.
I0123 14:00:04.890572 140227981185024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.890637 140227981185024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.891224 140227981185024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.891313 140227981185024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.891683 140227981185024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.892369 140227981185024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.894928 140227981185024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.895647 140227981185024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.895724 140227981185024 transformer_layer.py:468] tlayer: End windows.
I0123 14:00:04.895761 140227981185024 transformer_layer.py:472] tlayer: final FFN.
I0123 14:00:04.895820 140227981185024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.895953 140227981185024 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:00:04.896063 140227981185024 nn_components.py:325] mlp: activation = None
I0123 14:00:04.896102 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:00:04.898005 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.898101 140227981185024 nn_components.py:261] mlp: residual
I0123 14:00:04.900575 140227981185024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.900654 140227981185024 transformer_base.py:443] tbase: final FFN
I0123 14:00:04.900763 140227981185024 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:00:04.903038 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:00:04.905024 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.905118 140227981185024 nn_components.py:261] mlp: residual
I0123 14:00:04.905407 140227981185024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.905488 140227981185024 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 14:00:04.905597 140227981185024 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:00:04.905636 140227981185024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:00:04.905676 140227981185024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:00:04.905740 140227981185024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.908096 140227981185024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:00:04.913631 140227981185024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.913904 140227981185024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:00:04.916682 140227981185024 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:00:04.929605 140227981185024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:00:04.929670 140227981185024 attention.py:418] Single window, no scan.
I0123 14:00:04.929705 140227981185024 transformer_layer.py:389] tlayer: self-attention.
I0123 14:00:04.929737 140227981185024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.929798 140227981185024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.930430 140227981185024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.930505 140227981185024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.930862 140227981185024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.931547 140227981185024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.934030 140227981185024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.934688 140227981185024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.934764 140227981185024 transformer_layer.py:468] tlayer: End windows.
I0123 14:00:04.934799 140227981185024 transformer_layer.py:472] tlayer: final FFN.
I0123 14:00:04.934857 140227981185024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.934987 140227981185024 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:00:04.935094 140227981185024 nn_components.py:325] mlp: activation = None
I0123 14:00:04.935132 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:00:04.937017 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.937115 140227981185024 nn_components.py:261] mlp: residual
I0123 14:00:04.939650 140227981185024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.939730 140227981185024 transformer_base.py:443] tbase: final FFN
I0123 14:00:04.939836 140227981185024 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:00:04.942066 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:00:04.943956 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.944050 140227981185024 nn_components.py:261] mlp: residual
I0123 14:00:04.944338 140227981185024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.944417 140227981185024 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 14:00:04.944524 140227981185024 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:00:04.944562 140227981185024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:00:04.944593 140227981185024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:00:04.944654 140227981185024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.946932 140227981185024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:00:04.952506 140227981185024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.952765 140227981185024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:00:04.955429 140227981185024 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:00:04.968184 140227981185024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:00:04.968240 140227981185024 attention.py:418] Single window, no scan.
I0123 14:00:04.968276 140227981185024 transformer_layer.py:389] tlayer: self-attention.
I0123 14:00:04.968306 140227981185024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.968367 140227981185024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.968923 140227981185024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.968999 140227981185024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.969359 140227981185024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.970061 140227981185024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.972559 140227981185024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.973217 140227981185024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.973294 140227981185024 transformer_layer.py:468] tlayer: End windows.
I0123 14:00:04.973329 140227981185024 transformer_layer.py:472] tlayer: final FFN.
I0123 14:00:04.973387 140227981185024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.973517 140227981185024 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:00:04.973626 140227981185024 nn_components.py:325] mlp: activation = None
I0123 14:00:04.973673 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:00:04.975590 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.975687 140227981185024 nn_components.py:261] mlp: residual
I0123 14:00:04.978110 140227981185024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.978189 140227981185024 transformer_base.py:443] tbase: final FFN
I0123 14:00:04.978301 140227981185024 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:00:04.980508 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:00:04.982457 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.982552 140227981185024 nn_components.py:261] mlp: residual
I0123 14:00:04.982840 140227981185024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.982921 140227981185024 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 14:00:04.983031 140227981185024 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:00:04.983071 140227981185024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:00:04.983101 140227981185024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:00:04.983163 140227981185024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.985412 140227981185024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:00:04.990864 140227981185024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:04.991125 140227981185024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:00:04.993823 140227981185024 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:00:05.006503 140227981185024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:00:05.006559 140227981185024 attention.py:418] Single window, no scan.
I0123 14:00:05.006594 140227981185024 transformer_layer.py:389] tlayer: self-attention.
I0123 14:00:05.006625 140227981185024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:05.006686 140227981185024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:05.007239 140227981185024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:05.007315 140227981185024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:05.007671 140227981185024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:05.008405 140227981185024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:05.010931 140227981185024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:05.011557 140227981185024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:05.011634 140227981185024 transformer_layer.py:468] tlayer: End windows.
I0123 14:00:05.011670 140227981185024 transformer_layer.py:472] tlayer: final FFN.
I0123 14:00:05.011729 140227981185024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:05.011860 140227981185024 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:00:05.011967 140227981185024 nn_components.py:325] mlp: activation = None
I0123 14:00:05.012006 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:00:05.013902 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:05.013995 140227981185024 nn_components.py:261] mlp: residual
I0123 14:00:05.016420 140227981185024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:05.016499 140227981185024 transformer_base.py:443] tbase: final FFN
I0123 14:00:05.016606 140227981185024 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:00:05.018881 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:00:05.020751 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:05.020844 140227981185024 nn_components.py:261] mlp: residual
I0123 14:00:05.021130 140227981185024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:05.021219 140227981185024 decoder_stack.py:344] dstack: Final layernorm.
I0123 14:00:05.024107 140227981185024 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:00:05.080128 140227981185024 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.080214 140227981185024 decoder_stack.py:333] dstack: autoregressive generator.
I0123 14:00:05.080269 140227981185024 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 14:00:05.080371 140227981185024 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:00:05.080409 140227981185024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:00:05.080439 140227981185024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:00:05.080502 140227981185024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.083175 140227981185024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:00:05.088543 140227981185024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.088801 140227981185024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:00:05.091375 140227981185024 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:00:05.103810 140227981185024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:00:05.103867 140227981185024 attention.py:418] Single window, no scan.
I0123 14:00:05.103902 140227981185024 transformer_layer.py:389] tlayer: self-attention.
I0123 14:00:05.103933 140227981185024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.103994 140227981185024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.104547 140227981185024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.104623 140227981185024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.104982 140227981185024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.105668 140227981185024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.108157 140227981185024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.108766 140227981185024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.108842 140227981185024 transformer_layer.py:468] tlayer: End windows.
I0123 14:00:05.108876 140227981185024 transformer_layer.py:472] tlayer: final FFN.
I0123 14:00:05.108935 140227981185024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.109061 140227981185024 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:00:05.109176 140227981185024 nn_components.py:325] mlp: activation = None
I0123 14:00:05.109215 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:00:05.111065 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.111158 140227981185024 nn_components.py:261] mlp: residual
I0123 14:00:05.113549 140227981185024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.113628 140227981185024 transformer_base.py:443] tbase: final FFN
I0123 14:00:05.113743 140227981185024 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:00:05.115982 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:00:05.117944 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.118039 140227981185024 nn_components.py:261] mlp: residual
I0123 14:00:05.118324 140227981185024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.118404 140227981185024 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 14:00:05.118511 140227981185024 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:00:05.118550 140227981185024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:00:05.118580 140227981185024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:00:05.118643 140227981185024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.120859 140227981185024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:00:05.126171 140227981185024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.126430 140227981185024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:00:05.129086 140227981185024 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:00:05.141389 140227981185024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:00:05.141444 140227981185024 attention.py:418] Single window, no scan.
I0123 14:00:05.141479 140227981185024 transformer_layer.py:389] tlayer: self-attention.
I0123 14:00:05.141510 140227981185024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.141571 140227981185024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.142129 140227981185024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.142205 140227981185024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.142566 140227981185024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.143241 140227981185024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.145752 140227981185024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.146363 140227981185024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.146440 140227981185024 transformer_layer.py:468] tlayer: End windows.
I0123 14:00:05.146475 140227981185024 transformer_layer.py:472] tlayer: final FFN.
I0123 14:00:05.146534 140227981185024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.146660 140227981185024 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:00:05.146767 140227981185024 nn_components.py:325] mlp: activation = None
I0123 14:00:05.146811 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:00:05.148651 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.148742 140227981185024 nn_components.py:261] mlp: residual
I0123 14:00:05.151141 140227981185024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.151220 140227981185024 transformer_base.py:443] tbase: final FFN
I0123 14:00:05.151329 140227981185024 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:00:05.153563 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:00:05.155401 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.155497 140227981185024 nn_components.py:261] mlp: residual
I0123 14:00:05.155782 140227981185024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.155862 140227981185024 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 14:00:05.155970 140227981185024 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:00:05.156008 140227981185024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:00:05.156039 140227981185024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:00:05.156101 140227981185024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.158337 140227981185024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:00:05.163689 140227981185024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.163954 140227981185024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:00:05.166629 140227981185024 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:00:05.178824 140227981185024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:00:05.178880 140227981185024 attention.py:418] Single window, no scan.
I0123 14:00:05.178917 140227981185024 transformer_layer.py:389] tlayer: self-attention.
I0123 14:00:05.178949 140227981185024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.179010 140227981185024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.179562 140227981185024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.179638 140227981185024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.179993 140227981185024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.180671 140227981185024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.183189 140227981185024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.183798 140227981185024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.183875 140227981185024 transformer_layer.py:468] tlayer: End windows.
I0123 14:00:05.183909 140227981185024 transformer_layer.py:472] tlayer: final FFN.
I0123 14:00:05.183968 140227981185024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.184091 140227981185024 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:00:05.184198 140227981185024 nn_components.py:325] mlp: activation = None
I0123 14:00:05.184236 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:00:05.186081 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.186173 140227981185024 nn_components.py:261] mlp: residual
I0123 14:00:05.188537 140227981185024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.188616 140227981185024 transformer_base.py:443] tbase: final FFN
I0123 14:00:05.188724 140227981185024 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:00:05.191398 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:00:05.193245 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.193340 140227981185024 nn_components.py:261] mlp: residual
I0123 14:00:05.193627 140227981185024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.193714 140227981185024 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 14:00:05.193821 140227981185024 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:00:05.193860 140227981185024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:00:05.193891 140227981185024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:00:05.193953 140227981185024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.196180 140227981185024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:00:05.201558 140227981185024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.201823 140227981185024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:00:05.204499 140227981185024 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:00:05.216889 140227981185024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:00:05.216944 140227981185024 attention.py:418] Single window, no scan.
I0123 14:00:05.216982 140227981185024 transformer_layer.py:389] tlayer: self-attention.
I0123 14:00:05.217020 140227981185024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.217082 140227981185024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.217802 140227981185024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.217879 140227981185024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.218239 140227981185024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.218919 140227981185024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.221443 140227981185024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.222075 140227981185024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.222152 140227981185024 transformer_layer.py:468] tlayer: End windows.
I0123 14:00:05.222186 140227981185024 transformer_layer.py:472] tlayer: final FFN.
I0123 14:00:05.222244 140227981185024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.222369 140227981185024 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:00:05.222476 140227981185024 nn_components.py:325] mlp: activation = None
I0123 14:00:05.222516 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:00:05.224388 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.224479 140227981185024 nn_components.py:261] mlp: residual
I0123 14:00:05.226881 140227981185024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.226959 140227981185024 transformer_base.py:443] tbase: final FFN
I0123 14:00:05.227065 140227981185024 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:00:05.229351 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:00:05.231244 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.231339 140227981185024 nn_components.py:261] mlp: residual
I0123 14:00:05.231626 140227981185024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.231705 140227981185024 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 14:00:05.231811 140227981185024 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:00:05.231848 140227981185024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:00:05.231878 140227981185024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:00:05.231939 140227981185024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.234198 140227981185024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:00:05.239605 140227981185024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.239865 140227981185024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:00:05.242574 140227981185024 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:00:05.255110 140227981185024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:00:05.255163 140227981185024 attention.py:418] Single window, no scan.
I0123 14:00:05.255197 140227981185024 transformer_layer.py:389] tlayer: self-attention.
I0123 14:00:05.255226 140227981185024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.255286 140227981185024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.255836 140227981185024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.255909 140227981185024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.256266 140227981185024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.256947 140227981185024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.259490 140227981185024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.260107 140227981185024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.260183 140227981185024 transformer_layer.py:468] tlayer: End windows.
I0123 14:00:05.260217 140227981185024 transformer_layer.py:472] tlayer: final FFN.
I0123 14:00:05.260275 140227981185024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.260399 140227981185024 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:00:05.260506 140227981185024 nn_components.py:325] mlp: activation = None
I0123 14:00:05.260543 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:00:05.262423 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.262520 140227981185024 nn_components.py:261] mlp: residual
I0123 14:00:05.264942 140227981185024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.265019 140227981185024 transformer_base.py:443] tbase: final FFN
I0123 14:00:05.265127 140227981185024 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:00:05.267411 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:00:05.269282 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.269373 140227981185024 nn_components.py:261] mlp: residual
I0123 14:00:05.269663 140227981185024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.269744 140227981185024 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 14:00:05.269849 140227981185024 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:00:05.269886 140227981185024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:00:05.269915 140227981185024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:00:05.269976 140227981185024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.272213 140227981185024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:00:05.277590 140227981185024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.277853 140227981185024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:00:05.280527 140227981185024 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:00:05.293025 140227981185024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:00:05.293079 140227981185024 attention.py:418] Single window, no scan.
I0123 14:00:05.293114 140227981185024 transformer_layer.py:389] tlayer: self-attention.
I0123 14:00:05.293143 140227981185024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.293203 140227981185024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.293767 140227981185024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.293843 140227981185024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.294202 140227981185024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.294886 140227981185024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.297460 140227981185024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.298086 140227981185024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.298163 140227981185024 transformer_layer.py:468] tlayer: End windows.
I0123 14:00:05.298196 140227981185024 transformer_layer.py:472] tlayer: final FFN.
I0123 14:00:05.298253 140227981185024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.298376 140227981185024 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:00:05.298480 140227981185024 nn_components.py:325] mlp: activation = None
I0123 14:00:05.298516 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:00:05.300376 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.300473 140227981185024 nn_components.py:261] mlp: residual
I0123 14:00:05.302922 140227981185024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.303001 140227981185024 transformer_base.py:443] tbase: final FFN
I0123 14:00:05.303109 140227981185024 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:00:05.305803 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:00:05.307677 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.307770 140227981185024 nn_components.py:261] mlp: residual
I0123 14:00:05.308054 140227981185024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.308133 140227981185024 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 14:00:05.308239 140227981185024 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:00:05.308275 140227981185024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:00:05.308305 140227981185024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:00:05.308365 140227981185024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.310604 140227981185024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:00:05.316023 140227981185024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.316280 140227981185024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:00:05.318978 140227981185024 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:00:05.331820 140227981185024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:00:05.331874 140227981185024 attention.py:418] Single window, no scan.
I0123 14:00:05.331907 140227981185024 transformer_layer.py:389] tlayer: self-attention.
I0123 14:00:05.331937 140227981185024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.331997 140227981185024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.332551 140227981185024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.332625 140227981185024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.332977 140227981185024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.333667 140227981185024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.336212 140227981185024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.336848 140227981185024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.336924 140227981185024 transformer_layer.py:468] tlayer: End windows.
I0123 14:00:05.336957 140227981185024 transformer_layer.py:472] tlayer: final FFN.
I0123 14:00:05.337015 140227981185024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.337140 140227981185024 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:00:05.337249 140227981185024 nn_components.py:325] mlp: activation = None
I0123 14:00:05.337286 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:00:05.339302 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.339392 140227981185024 nn_components.py:261] mlp: residual
I0123 14:00:05.341824 140227981185024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.341901 140227981185024 transformer_base.py:443] tbase: final FFN
I0123 14:00:05.342012 140227981185024 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:00:05.344293 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:00:05.346169 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.346264 140227981185024 nn_components.py:261] mlp: residual
I0123 14:00:05.346552 140227981185024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.346631 140227981185024 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 14:00:05.346736 140227981185024 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:00:05.346773 140227981185024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:00:05.346803 140227981185024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:00:05.346865 140227981185024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.349114 140227981185024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:00:05.354582 140227981185024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.354842 140227981185024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:00:05.357543 140227981185024 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:00:05.370088 140227981185024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:00:05.370142 140227981185024 attention.py:418] Single window, no scan.
I0123 14:00:05.370177 140227981185024 transformer_layer.py:389] tlayer: self-attention.
I0123 14:00:05.370207 140227981185024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.370266 140227981185024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.370820 140227981185024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.370895 140227981185024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.371253 140227981185024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.371939 140227981185024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.374497 140227981185024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.375130 140227981185024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.375206 140227981185024 transformer_layer.py:468] tlayer: End windows.
I0123 14:00:05.375240 140227981185024 transformer_layer.py:472] tlayer: final FFN.
I0123 14:00:05.375298 140227981185024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.375425 140227981185024 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:00:05.375535 140227981185024 nn_components.py:325] mlp: activation = None
I0123 14:00:05.375572 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:00:05.377435 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.377525 140227981185024 nn_components.py:261] mlp: residual
I0123 14:00:05.379944 140227981185024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.380031 140227981185024 transformer_base.py:443] tbase: final FFN
I0123 14:00:05.380139 140227981185024 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:00:05.382418 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:00:05.384264 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.384358 140227981185024 nn_components.py:261] mlp: residual
I0123 14:00:05.384642 140227981185024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.384721 140227981185024 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 14:00:05.384826 140227981185024 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:00:05.384863 140227981185024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:00:05.384893 140227981185024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:00:05.384954 140227981185024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.387206 140227981185024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:00:05.392648 140227981185024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.392908 140227981185024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:00:05.395617 140227981185024 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:00:05.408079 140227981185024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:00:05.408134 140227981185024 attention.py:418] Single window, no scan.
I0123 14:00:05.408168 140227981185024 transformer_layer.py:389] tlayer: self-attention.
I0123 14:00:05.408199 140227981185024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.408260 140227981185024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.408818 140227981185024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.408893 140227981185024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.409247 140227981185024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.409959 140227981185024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.412533 140227981185024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.413144 140227981185024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.413219 140227981185024 transformer_layer.py:468] tlayer: End windows.
I0123 14:00:05.413253 140227981185024 transformer_layer.py:472] tlayer: final FFN.
I0123 14:00:05.413311 140227981185024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.413441 140227981185024 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:00:05.413547 140227981185024 nn_components.py:325] mlp: activation = None
I0123 14:00:05.413585 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:00:05.415469 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.415560 140227981185024 nn_components.py:261] mlp: residual
I0123 14:00:05.417986 140227981185024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.418068 140227981185024 transformer_base.py:443] tbase: final FFN
I0123 14:00:05.418178 140227981185024 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:00:05.420833 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:00:05.422706 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.422800 140227981185024 nn_components.py:261] mlp: residual
I0123 14:00:05.423086 140227981185024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.423164 140227981185024 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 14:00:05.423270 140227981185024 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:00:05.423307 140227981185024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:00:05.423338 140227981185024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:00:05.423400 140227981185024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.425673 140227981185024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:00:05.431108 140227981185024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.431364 140227981185024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:00:05.434051 140227981185024 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:00:05.446905 140227981185024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:00:05.446960 140227981185024 attention.py:418] Single window, no scan.
I0123 14:00:05.446995 140227981185024 transformer_layer.py:389] tlayer: self-attention.
I0123 14:00:05.447026 140227981185024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.447085 140227981185024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.447640 140227981185024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.447714 140227981185024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.448072 140227981185024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.448765 140227981185024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.451318 140227981185024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.451937 140227981185024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.452015 140227981185024 transformer_layer.py:468] tlayer: End windows.
I0123 14:00:05.452049 140227981185024 transformer_layer.py:472] tlayer: final FFN.
I0123 14:00:05.452104 140227981185024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.452230 140227981185024 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:00:05.452336 140227981185024 nn_components.py:325] mlp: activation = None
I0123 14:00:05.452373 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:00:05.454717 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.454811 140227981185024 nn_components.py:261] mlp: residual
I0123 14:00:05.457194 140227981185024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.457269 140227981185024 transformer_base.py:443] tbase: final FFN
I0123 14:00:05.457381 140227981185024 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:00:05.459624 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:00:05.461462 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.461554 140227981185024 nn_components.py:261] mlp: residual
I0123 14:00:05.461844 140227981185024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.461924 140227981185024 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 14:00:05.462029 140227981185024 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:00:05.462066 140227981185024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:00:05.462095 140227981185024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:00:05.462155 140227981185024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.464386 140227981185024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:00:05.469813 140227981185024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.470067 140227981185024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:00:05.472739 140227981185024 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:00:05.485141 140227981185024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:00:05.485194 140227981185024 attention.py:418] Single window, no scan.
I0123 14:00:05.485228 140227981185024 transformer_layer.py:389] tlayer: self-attention.
I0123 14:00:05.485257 140227981185024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.485316 140227981185024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.485878 140227981185024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.485953 140227981185024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.486305 140227981185024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.486990 140227981185024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.489508 140227981185024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.490136 140227981185024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.490212 140227981185024 transformer_layer.py:468] tlayer: End windows.
I0123 14:00:05.490246 140227981185024 transformer_layer.py:472] tlayer: final FFN.
I0123 14:00:05.490301 140227981185024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.490425 140227981185024 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:00:05.490532 140227981185024 nn_components.py:325] mlp: activation = None
I0123 14:00:05.490568 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:00:05.492455 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.492545 140227981185024 nn_components.py:261] mlp: residual
I0123 14:00:05.494967 140227981185024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.495045 140227981185024 transformer_base.py:443] tbase: final FFN
I0123 14:00:05.495151 140227981185024 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:00:05.497423 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:00:05.499282 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.499375 140227981185024 nn_components.py:261] mlp: residual
I0123 14:00:05.499660 140227981185024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.499738 140227981185024 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 14:00:05.499843 140227981185024 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:00:05.499880 140227981185024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:00:05.499909 140227981185024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:00:05.499969 140227981185024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.502216 140227981185024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:00:05.507641 140227981185024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.507897 140227981185024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:00:05.510589 140227981185024 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:00:05.523006 140227981185024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:00:05.523059 140227981185024 attention.py:418] Single window, no scan.
I0123 14:00:05.523092 140227981185024 transformer_layer.py:389] tlayer: self-attention.
I0123 14:00:05.523122 140227981185024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.523182 140227981185024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.523734 140227981185024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.523811 140227981185024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.524164 140227981185024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.524849 140227981185024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.527401 140227981185024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.528017 140227981185024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.528095 140227981185024 transformer_layer.py:468] tlayer: End windows.
I0123 14:00:05.528129 140227981185024 transformer_layer.py:472] tlayer: final FFN.
I0123 14:00:05.528185 140227981185024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.528313 140227981185024 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:00:05.528420 140227981185024 nn_components.py:325] mlp: activation = None
I0123 14:00:05.528456 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:00:05.530333 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.530424 140227981185024 nn_components.py:261] mlp: residual
I0123 14:00:05.532814 140227981185024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.532890 140227981185024 transformer_base.py:443] tbase: final FFN
I0123 14:00:05.532996 140227981185024 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:00:05.535660 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:00:05.537516 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.537609 140227981185024 nn_components.py:261] mlp: residual
I0123 14:00:05.537899 140227981185024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:05.537983 140227981185024 decoder_stack.py:344] dstack: Final layernorm.
I0123 14:00:05.540998 140227981185024 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:00:09.972010 140227981185024 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
I0123 14:00:10.476254 140227981185024 training_loop.py:409] No working directory specified.
I0123 14:00:10.476373 140227981185024 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0123 14:00:10.477145 140227981185024 checkpoints.py:1062] Restoring legacy Flax checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0123 14:00:13.746817 140227981185024 training_loop.py:447] Only restoring trainable parameters.
I0123 14:00:13.747442 140227981185024 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0123 14:00:13.747525 140227981185024 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0123 14:00:13.747575 140227981185024 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 14:00:13.747620 140227981185024 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 14:00:13.747662 140227981185024 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:00:13.747701 140227981185024 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 14:00:13.747741 140227981185024 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:00:13.747779 140227981185024 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:00:13.747817 140227981185024 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 14:00:13.747854 140227981185024 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 14:00:13.747890 140227981185024 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:00:13.747927 140227981185024 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 14:00:13.747964 140227981185024 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 14:00:13.748000 140227981185024 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 14:00:13.748036 140227981185024 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:00:13.748072 140227981185024 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 14:00:13.748109 140227981185024 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:00:13.748145 140227981185024 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:00:13.748181 140227981185024 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 14:00:13.748216 140227981185024 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 14:00:13.748267 140227981185024 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:00:13.748306 140227981185024 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 14:00:13.748342 140227981185024 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 14:00:13.748377 140227981185024 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 14:00:13.748413 140227981185024 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:00:13.748448 140227981185024 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 14:00:13.748483 140227981185024 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:00:13.748518 140227981185024 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:00:13.748554 140227981185024 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 14:00:13.748591 140227981185024 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 14:00:13.748627 140227981185024 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:00:13.748663 140227981185024 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 14:00:13.748698 140227981185024 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 14:00:13.748733 140227981185024 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 14:00:13.748768 140227981185024 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:00:13.748803 140227981185024 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 14:00:13.748837 140227981185024 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:00:13.748872 140227981185024 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:00:13.748907 140227981185024 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 14:00:13.748942 140227981185024 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 14:00:13.748977 140227981185024 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:00:13.749012 140227981185024 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 14:00:13.749047 140227981185024 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 14:00:13.749082 140227981185024 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 14:00:13.749116 140227981185024 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:00:13.749151 140227981185024 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 14:00:13.749191 140227981185024 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:00:13.749228 140227981185024 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:00:13.749264 140227981185024 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 14:00:13.749299 140227981185024 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 14:00:13.749333 140227981185024 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:00:13.749367 140227981185024 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 14:00:13.749402 140227981185024 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 14:00:13.749436 140227981185024 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 14:00:13.749471 140227981185024 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:00:13.749506 140227981185024 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 14:00:13.749541 140227981185024 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:00:13.749576 140227981185024 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:00:13.749611 140227981185024 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 14:00:13.749655 140227981185024 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 14:00:13.749696 140227981185024 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:00:13.749732 140227981185024 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 14:00:13.749768 140227981185024 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 14:00:13.749802 140227981185024 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 14:00:13.749837 140227981185024 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:00:13.749872 140227981185024 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 14:00:13.749907 140227981185024 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:00:13.749941 140227981185024 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:00:13.749976 140227981185024 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 14:00:13.750010 140227981185024 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 14:00:13.750044 140227981185024 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:00:13.750079 140227981185024 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 14:00:13.750113 140227981185024 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 14:00:13.750154 140227981185024 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 14:00:13.750190 140227981185024 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:00:13.750225 140227981185024 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 14:00:13.750260 140227981185024 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:00:13.750296 140227981185024 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:00:13.750330 140227981185024 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 14:00:13.750364 140227981185024 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 14:00:13.750398 140227981185024 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:00:13.750433 140227981185024 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 14:00:13.750468 140227981185024 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 14:00:13.750502 140227981185024 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 14:00:13.750537 140227981185024 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:00:13.750572 140227981185024 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 14:00:13.750608 140227981185024 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:00:13.750643 140227981185024 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:00:13.750677 140227981185024 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 14:00:13.750711 140227981185024 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 14:00:13.750745 140227981185024 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:00:13.750780 140227981185024 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 14:00:13.750814 140227981185024 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 14:00:13.750848 140227981185024 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 14:00:13.750883 140227981185024 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:00:13.750918 140227981185024 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 14:00:13.750952 140227981185024 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:00:13.750987 140227981185024 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:00:13.751021 140227981185024 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 14:00:13.751056 140227981185024 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 14:00:13.751095 140227981185024 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:00:13.751131 140227981185024 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 14:00:13.751165 140227981185024 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 14:00:13.751199 140227981185024 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 14:00:13.751233 140227981185024 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:00:13.751268 140227981185024 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 14:00:13.751302 140227981185024 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:00:13.751336 140227981185024 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:00:13.751370 140227981185024 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 14:00:13.751405 140227981185024 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 14:00:13.751441 140227981185024 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:00:13.751476 140227981185024 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 14:00:13.751511 140227981185024 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 14:00:13.751546 140227981185024 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 14:00:13.751582 140227981185024 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:00:13.751617 140227981185024 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 14:00:13.751652 140227981185024 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:00:13.751687 140227981185024 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:00:13.751722 140227981185024 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 14:00:13.751756 140227981185024 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 14:00:13.751791 140227981185024 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 14:00:13.751826 140227981185024 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 14:00:13.751854 140227981185024 training_loop.py:725] Total parameters: 152072288
I0123 14:00:13.752080 140227981185024 training_loop.py:739] Total state size: 0
I0123 14:00:13.776445 140227981185024 training_loop.py:492] Training loop: creating task for mode beam_search
I0123 14:00:13.776687 140227981185024 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0123 14:00:13.777080 140227981185024 training_loop.py:652] Compiling mode beam_search with jit.
I0123 14:00:13.777412 140227981185024 training_loop.py:89] registering functions: dict_keys([])
I0123 14:00:13.793772 140227981185024 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = on_circle h d c, on_line h g c; i = midpoint i e h; j = foot j h i d; k = on_line k h j, on_line k b a; l = mirror l h d; m = mirror m g h; n = foot n l k m; o = midpoint o g n ? cong d c d o
I0123 14:00:18.934017 140227981185024 ddar.py:60] Depth 1/1000 time = 5.089924097061157
I0123 14:00:26.017261 140227981185024 ddar.py:60] Depth 2/1000 time = 7.083029508590698
I0123 14:00:34.948497 140227981185024 ddar.py:60] Depth 3/1000 time = 8.930969476699829
I0123 14:00:44.175410 140227981185024 ddar.py:60] Depth 4/1000 time = 9.226697206497192
I0123 14:00:53.382189 140227981185024 ddar.py:60] Depth 5/1000 time = 9.206430673599243
I0123 14:01:02.732265 140227981185024 ddar.py:60] Depth 6/1000 time = 9.349374055862427
I0123 14:01:12.370201 140227981185024 ddar.py:60] Depth 7/1000 time = 9.608467102050781
I0123 14:01:22.351867 140227981185024 ddar.py:60] Depth 8/1000 time = 9.981462955474854
I0123 14:01:34.440773 140227981185024 ddar.py:60] Depth 9/1000 time = 12.088683366775513
I0123 14:01:46.602839 140227981185024 ddar.py:60] Depth 10/1000 time = 12.161868572235107
I0123 14:01:58.363884 140227981185024 ddar.py:60] Depth 11/1000 time = 11.760846614837646
I0123 14:02:10.479704 140227981185024 ddar.py:60] Depth 12/1000 time = 12.060323238372803
I0123 14:02:10.480143 140227981185024 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:02:10.480237 140227981185024 alphageometry.py:540] Depth 0. There are 1 nodes to expand:
I0123 14:02:10.480272 140227981185024 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a b e 02 ^ c a c e c e c b 03 ; f : C a c f 04 ^ b a b f b f b c 05 ; g : C b f g 06 C c e g 07 ; h : C c g h 08 D c d d h 09 ; i : C e h i 10 D e i h i 11 ; j : C d i j 12 T d i h j 13 ; k : C a b k 14 C h j k 15 ; l : C d h l 16 D d h d l 17 ; m : C g h m 18 D g h h m 19 ; n : C k m n 20 T k m l n 21 ; o : C g n o 22 D g o n o 23 ? D d c d o {F1} x00
I0123 14:02:10.480305 140227981185024 alphageometry.py:549] Decoding from {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a b e 02 ^ c a c e c e c b 03 ; f : C a c f 04 ^ b a b f b f b c 05 ; g : C b f g 06 C c e g 07 ; h : C c g h 08 D c d d h 09 ; i : C e h i 10 D e i h i 11 ; j : C d i j 12 T d i h j 13 ; k : C a b k 14 C h j k 15 ; l : C d h l 16 D d h d l 17 ; m : C g h m 18 D g h h m 19 ; n : C k m n 20 T k m l n 21 ; o : C g n o 22 D g o n o 23 ? D d c d o {F1} x00
I0123 14:02:10.623334 140227981185024 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.623510 140227981185024 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 14:02:10.623606 140227981185024 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 14:02:10.623679 140227981185024 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 14:02:10.623749 140227981185024 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 14:02:10.623818 140227981185024 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 14:02:10.623887 140227981185024 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 14:02:10.623953 140227981185024 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 14:02:10.624019 140227981185024 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 14:02:10.624084 140227981185024 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 14:02:10.624149 140227981185024 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 14:02:10.624215 140227981185024 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 14:02:10.624281 140227981185024 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 14:02:10.624357 140227981185024 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 14:02:10.624399 140227981185024 attention.py:418] Single window, no scan.
I0123 14:02:10.624442 140227981185024 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 14:02:10.624550 140227981185024 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:02:10.624588 140227981185024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:02:10.624617 140227981185024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:02:10.626560 140227981185024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.629057 140227981185024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:02:10.634884 140227981185024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.635157 140227981185024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:02:10.637793 140227981185024 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:02:10.641591 140227981185024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:02:10.641652 140227981185024 attention.py:418] Single window, no scan.
I0123 14:02:10.641690 140227981185024 transformer_layer.py:389] tlayer: self-attention.
I0123 14:02:10.641722 140227981185024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.641783 140227981185024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.642446 140227981185024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.642522 140227981185024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.642887 140227981185024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.643662 140227981185024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.646174 140227981185024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.646801 140227981185024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.646877 140227981185024 transformer_layer.py:468] tlayer: End windows.
I0123 14:02:10.646910 140227981185024 transformer_layer.py:472] tlayer: final FFN.
I0123 14:02:10.646969 140227981185024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.647266 140227981185024 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:02:10.647587 140227981185024 nn_components.py:325] mlp: activation = None
I0123 14:02:10.647629 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:02:10.649777 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.649869 140227981185024 nn_components.py:261] mlp: residual
I0123 14:02:10.652331 140227981185024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.652409 140227981185024 transformer_base.py:443] tbase: final FFN
I0123 14:02:10.652834 140227981185024 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:02:10.655133 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:02:10.657058 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.657150 140227981185024 nn_components.py:261] mlp: residual
I0123 14:02:10.657447 140227981185024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.657526 140227981185024 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 14:02:10.657633 140227981185024 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:02:10.657678 140227981185024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:02:10.657709 140227981185024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:02:10.659536 140227981185024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.661837 140227981185024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:02:10.667333 140227981185024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.667585 140227981185024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:02:10.670217 140227981185024 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:02:10.673842 140227981185024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:02:10.673896 140227981185024 attention.py:418] Single window, no scan.
I0123 14:02:10.673930 140227981185024 transformer_layer.py:389] tlayer: self-attention.
I0123 14:02:10.673960 140227981185024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.674021 140227981185024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.674571 140227981185024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.674645 140227981185024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.674995 140227981185024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.675750 140227981185024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.678203 140227981185024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.678872 140227981185024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.678948 140227981185024 transformer_layer.py:468] tlayer: End windows.
I0123 14:02:10.678982 140227981185024 transformer_layer.py:472] tlayer: final FFN.
I0123 14:02:10.679038 140227981185024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.679163 140227981185024 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:02:10.679475 140227981185024 nn_components.py:325] mlp: activation = None
I0123 14:02:10.679517 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:02:10.681408 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.681500 140227981185024 nn_components.py:261] mlp: residual
I0123 14:02:10.683935 140227981185024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.684013 140227981185024 transformer_base.py:443] tbase: final FFN
I0123 14:02:10.684429 140227981185024 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:02:10.686749 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:02:10.688638 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.688731 140227981185024 nn_components.py:261] mlp: residual
I0123 14:02:10.689017 140227981185024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.689102 140227981185024 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 14:02:10.689209 140227981185024 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:02:10.689247 140227981185024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:02:10.689276 140227981185024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:02:10.691055 140227981185024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.693345 140227981185024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:02:10.698993 140227981185024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.699248 140227981185024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:02:10.701806 140227981185024 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:02:10.705415 140227981185024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:02:10.705468 140227981185024 attention.py:418] Single window, no scan.
I0123 14:02:10.705503 140227981185024 transformer_layer.py:389] tlayer: self-attention.
I0123 14:02:10.705534 140227981185024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.705594 140227981185024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.706208 140227981185024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.706283 140227981185024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.706633 140227981185024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.707373 140227981185024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.709804 140227981185024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.710424 140227981185024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.710500 140227981185024 transformer_layer.py:468] tlayer: End windows.
I0123 14:02:10.710534 140227981185024 transformer_layer.py:472] tlayer: final FFN.
I0123 14:02:10.710592 140227981185024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.710718 140227981185024 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:02:10.711032 140227981185024 nn_components.py:325] mlp: activation = None
I0123 14:02:10.711073 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:02:10.713030 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.713122 140227981185024 nn_components.py:261] mlp: residual
I0123 14:02:10.715574 140227981185024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.715652 140227981185024 transformer_base.py:443] tbase: final FFN
I0123 14:02:10.716073 140227981185024 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:02:10.718328 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:02:10.720216 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.720309 140227981185024 nn_components.py:261] mlp: residual
I0123 14:02:10.720597 140227981185024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.720682 140227981185024 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 14:02:10.720789 140227981185024 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:02:10.720827 140227981185024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:02:10.720857 140227981185024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:02:10.723041 140227981185024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.725363 140227981185024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:02:10.730953 140227981185024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.731209 140227981185024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:02:10.733777 140227981185024 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:02:10.737447 140227981185024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:02:10.737500 140227981185024 attention.py:418] Single window, no scan.
I0123 14:02:10.737534 140227981185024 transformer_layer.py:389] tlayer: self-attention.
I0123 14:02:10.737564 140227981185024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.737624 140227981185024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.738187 140227981185024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.738262 140227981185024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.738617 140227981185024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.739371 140227981185024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.741829 140227981185024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.742440 140227981185024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.742517 140227981185024 transformer_layer.py:468] tlayer: End windows.
I0123 14:02:10.742551 140227981185024 transformer_layer.py:472] tlayer: final FFN.
I0123 14:02:10.742609 140227981185024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.742734 140227981185024 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:02:10.743099 140227981185024 nn_components.py:325] mlp: activation = None
I0123 14:02:10.743141 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:02:10.745060 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.745151 140227981185024 nn_components.py:261] mlp: residual
I0123 14:02:10.747587 140227981185024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.747665 140227981185024 transformer_base.py:443] tbase: final FFN
I0123 14:02:10.748086 140227981185024 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:02:10.750550 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:02:10.752541 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.752634 140227981185024 nn_components.py:261] mlp: residual
I0123 14:02:10.752925 140227981185024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.753005 140227981185024 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 14:02:10.753112 140227981185024 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:02:10.753156 140227981185024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:02:10.753188 140227981185024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:02:10.754969 140227981185024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.757277 140227981185024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:02:10.763097 140227981185024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.763350 140227981185024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:02:10.765918 140227981185024 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:02:10.769538 140227981185024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:02:10.769591 140227981185024 attention.py:418] Single window, no scan.
I0123 14:02:10.769625 140227981185024 transformer_layer.py:389] tlayer: self-attention.
I0123 14:02:10.769664 140227981185024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.769726 140227981185024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.770332 140227981185024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.770407 140227981185024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.770764 140227981185024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.771519 140227981185024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.773997 140227981185024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.774608 140227981185024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.774683 140227981185024 transformer_layer.py:468] tlayer: End windows.
I0123 14:02:10.774716 140227981185024 transformer_layer.py:472] tlayer: final FFN.
I0123 14:02:10.774774 140227981185024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.774897 140227981185024 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:02:10.775209 140227981185024 nn_components.py:325] mlp: activation = None
I0123 14:02:10.775251 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:02:10.777210 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.777300 140227981185024 nn_components.py:261] mlp: residual
I0123 14:02:10.779753 140227981185024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.779833 140227981185024 transformer_base.py:443] tbase: final FFN
I0123 14:02:10.780254 140227981185024 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:02:10.782510 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:02:10.784405 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.784496 140227981185024 nn_components.py:261] mlp: residual
I0123 14:02:10.784783 140227981185024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.784862 140227981185024 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 14:02:10.784967 140227981185024 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:02:10.785004 140227981185024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:02:10.785040 140227981185024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:02:10.786893 140227981185024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.789194 140227981185024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:02:10.794735 140227981185024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.794992 140227981185024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:02:10.797542 140227981185024 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:02:10.801219 140227981185024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:02:10.801273 140227981185024 attention.py:418] Single window, no scan.
I0123 14:02:10.801307 140227981185024 transformer_layer.py:389] tlayer: self-attention.
I0123 14:02:10.801338 140227981185024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.801399 140227981185024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.801958 140227981185024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.802033 140227981185024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.802391 140227981185024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.803138 140227981185024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.805561 140227981185024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.806188 140227981185024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.806263 140227981185024 transformer_layer.py:468] tlayer: End windows.
I0123 14:02:10.806297 140227981185024 transformer_layer.py:472] tlayer: final FFN.
I0123 14:02:10.806353 140227981185024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.806505 140227981185024 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:02:10.806869 140227981185024 nn_components.py:325] mlp: activation = None
I0123 14:02:10.806910 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:02:10.808824 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.808915 140227981185024 nn_components.py:261] mlp: residual
I0123 14:02:10.811362 140227981185024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.811440 140227981185024 transformer_base.py:443] tbase: final FFN
I0123 14:02:10.811859 140227981185024 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:02:10.814122 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:02:10.816087 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.816180 140227981185024 nn_components.py:261] mlp: residual
I0123 14:02:10.816468 140227981185024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.816546 140227981185024 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 14:02:10.816653 140227981185024 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:02:10.816690 140227981185024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:02:10.816720 140227981185024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:02:10.818500 140227981185024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.820799 140227981185024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:02:10.826437 140227981185024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.826692 140227981185024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:02:10.829226 140227981185024 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:02:10.832803 140227981185024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:02:10.832857 140227981185024 attention.py:418] Single window, no scan.
I0123 14:02:10.832891 140227981185024 transformer_layer.py:389] tlayer: self-attention.
I0123 14:02:10.832920 140227981185024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.832980 140227981185024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.833922 140227981185024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.833999 140227981185024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.834354 140227981185024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.835111 140227981185024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.837551 140227981185024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.838170 140227981185024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.838246 140227981185024 transformer_layer.py:468] tlayer: End windows.
I0123 14:02:10.838279 140227981185024 transformer_layer.py:472] tlayer: final FFN.
I0123 14:02:10.838335 140227981185024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.838458 140227981185024 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:02:10.838766 140227981185024 nn_components.py:325] mlp: activation = None
I0123 14:02:10.838806 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:02:10.840757 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.840848 140227981185024 nn_components.py:261] mlp: residual
I0123 14:02:10.843295 140227981185024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.843374 140227981185024 transformer_base.py:443] tbase: final FFN
I0123 14:02:10.843796 140227981185024 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:02:10.846013 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:02:10.847895 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.847986 140227981185024 nn_components.py:261] mlp: residual
I0123 14:02:10.848271 140227981185024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.848350 140227981185024 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 14:02:10.848454 140227981185024 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:02:10.848493 140227981185024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:02:10.848522 140227981185024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:02:10.850366 140227981185024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.852690 140227981185024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:02:10.858222 140227981185024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.858481 140227981185024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:02:10.861293 140227981185024 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:02:10.864964 140227981185024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:02:10.865017 140227981185024 attention.py:418] Single window, no scan.
I0123 14:02:10.865050 140227981185024 transformer_layer.py:389] tlayer: self-attention.
I0123 14:02:10.865079 140227981185024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.865139 140227981185024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.865693 140227981185024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.865769 140227981185024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.866123 140227981185024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.866908 140227981185024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.869504 140227981185024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.870184 140227981185024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.870263 140227981185024 transformer_layer.py:468] tlayer: End windows.
I0123 14:02:10.870298 140227981185024 transformer_layer.py:472] tlayer: final FFN.
I0123 14:02:10.870355 140227981185024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.870487 140227981185024 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:02:10.870813 140227981185024 nn_components.py:325] mlp: activation = None
I0123 14:02:10.870856 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:02:10.872804 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.872895 140227981185024 nn_components.py:261] mlp: residual
I0123 14:02:10.875400 140227981185024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.875482 140227981185024 transformer_base.py:443] tbase: final FFN
I0123 14:02:10.875915 140227981185024 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:02:10.878265 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:02:10.880240 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.880334 140227981185024 nn_components.py:261] mlp: residual
I0123 14:02:10.880625 140227981185024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.880704 140227981185024 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 14:02:10.880811 140227981185024 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:02:10.880848 140227981185024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:02:10.880878 140227981185024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:02:10.882670 140227981185024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.885246 140227981185024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:02:10.891276 140227981185024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.891551 140227981185024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:02:10.894229 140227981185024 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:02:10.898059 140227981185024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:02:10.898116 140227981185024 attention.py:418] Single window, no scan.
I0123 14:02:10.898154 140227981185024 transformer_layer.py:389] tlayer: self-attention.
I0123 14:02:10.898188 140227981185024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.898255 140227981185024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.898908 140227981185024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.898989 140227981185024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.899374 140227981185024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.900184 140227981185024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.902763 140227981185024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.903429 140227981185024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.903514 140227981185024 transformer_layer.py:468] tlayer: End windows.
I0123 14:02:10.903551 140227981185024 transformer_layer.py:472] tlayer: final FFN.
I0123 14:02:10.903614 140227981185024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.903753 140227981185024 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:02:10.904077 140227981185024 nn_components.py:325] mlp: activation = None
I0123 14:02:10.904119 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:02:10.906109 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.906209 140227981185024 nn_components.py:261] mlp: residual
I0123 14:02:10.908882 140227981185024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.908963 140227981185024 transformer_base.py:443] tbase: final FFN
I0123 14:02:10.909389 140227981185024 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:02:10.911767 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:02:10.913742 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.913839 140227981185024 nn_components.py:261] mlp: residual
I0123 14:02:10.914139 140227981185024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.914227 140227981185024 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 14:02:10.914342 140227981185024 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:02:10.914383 140227981185024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:02:10.914415 140227981185024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:02:10.916258 140227981185024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.918686 140227981185024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:02:10.924343 140227981185024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.924598 140227981185024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:02:10.927209 140227981185024 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:02:10.930838 140227981185024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:02:10.930894 140227981185024 attention.py:418] Single window, no scan.
I0123 14:02:10.930930 140227981185024 transformer_layer.py:389] tlayer: self-attention.
I0123 14:02:10.930961 140227981185024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.931078 140227981185024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.931654 140227981185024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.931731 140227981185024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.932089 140227981185024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.932842 140227981185024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.935347 140227981185024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.935978 140227981185024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.936053 140227981185024 transformer_layer.py:468] tlayer: End windows.
I0123 14:02:10.936087 140227981185024 transformer_layer.py:472] tlayer: final FFN.
I0123 14:02:10.936144 140227981185024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.936270 140227981185024 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:02:10.936583 140227981185024 nn_components.py:325] mlp: activation = None
I0123 14:02:10.936624 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:02:10.938574 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.938669 140227981185024 nn_components.py:261] mlp: residual
I0123 14:02:10.941212 140227981185024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.941289 140227981185024 transformer_base.py:443] tbase: final FFN
I0123 14:02:10.941716 140227981185024 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:02:10.944027 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:02:10.945935 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.946028 140227981185024 nn_components.py:261] mlp: residual
I0123 14:02:10.946315 140227981185024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.946393 140227981185024 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 14:02:10.946498 140227981185024 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:02:10.946535 140227981185024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:02:10.946565 140227981185024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:02:10.948338 140227981185024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.951125 140227981185024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:02:10.956760 140227981185024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.957022 140227981185024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:02:10.959601 140227981185024 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:02:10.963282 140227981185024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:02:10.963339 140227981185024 attention.py:418] Single window, no scan.
I0123 14:02:10.963374 140227981185024 transformer_layer.py:389] tlayer: self-attention.
I0123 14:02:10.963404 140227981185024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.963523 140227981185024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.964087 140227981185024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.964161 140227981185024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.964514 140227981185024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.965271 140227981185024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.967731 140227981185024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.968342 140227981185024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.968417 140227981185024 transformer_layer.py:468] tlayer: End windows.
I0123 14:02:10.968451 140227981185024 transformer_layer.py:472] tlayer: final FFN.
I0123 14:02:10.968508 140227981185024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.968634 140227981185024 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:02:10.968940 140227981185024 nn_components.py:325] mlp: activation = None
I0123 14:02:10.968981 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:02:10.970916 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.971008 140227981185024 nn_components.py:261] mlp: residual
I0123 14:02:10.973689 140227981185024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.973767 140227981185024 transformer_base.py:443] tbase: final FFN
I0123 14:02:10.974302 140227981185024 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:02:10.976555 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:02:10.978450 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.978544 140227981185024 nn_components.py:261] mlp: residual
I0123 14:02:10.978835 140227981185024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.978915 140227981185024 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 14:02:10.979022 140227981185024 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:02:10.979059 140227981185024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:02:10.979088 140227981185024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:02:10.980841 140227981185024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.983227 140227981185024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:02:10.988791 140227981185024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.989047 140227981185024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:02:10.991608 140227981185024 transformer_layer.py:213] tlayer: windowed attention.
I0123 14:02:10.995202 140227981185024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:02:10.995255 140227981185024 attention.py:418] Single window, no scan.
I0123 14:02:10.995289 140227981185024 transformer_layer.py:389] tlayer: self-attention.
I0123 14:02:10.995319 140227981185024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.995434 140227981185024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.995984 140227981185024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.996058 140227981185024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.996414 140227981185024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.997173 140227981185024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:10.999666 140227981185024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:11.000290 140227981185024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:11.000366 140227981185024 transformer_layer.py:468] tlayer: End windows.
I0123 14:02:11.000400 140227981185024 transformer_layer.py:472] tlayer: final FFN.
I0123 14:02:11.000457 140227981185024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:11.000584 140227981185024 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:02:11.000896 140227981185024 nn_components.py:325] mlp: activation = None
I0123 14:02:11.000936 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:02:11.002857 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:11.002949 140227981185024 nn_components.py:261] mlp: residual
I0123 14:02:11.005433 140227981185024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:11.005510 140227981185024 transformer_base.py:443] tbase: final FFN
I0123 14:02:11.005932 140227981185024 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:02:11.008176 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:02:11.010088 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:11.010181 140227981185024 nn_components.py:261] mlp: residual
I0123 14:02:11.010467 140227981185024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:11.010708 140227981185024 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 14:02:11.010773 140227981185024 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 14:02:11.010827 140227981185024 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 14:02:11.010878 140227981185024 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 14:02:11.010928 140227981185024 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 14:02:11.010977 140227981185024 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 14:02:11.011026 140227981185024 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 14:02:11.011076 140227981185024 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 14:02:11.011132 140227981185024 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 14:02:11.011184 140227981185024 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 14:02:11.011234 140227981185024 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 14:02:11.011283 140227981185024 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 14:02:11.011317 140227981185024 decoder_stack.py:344] dstack: Final layernorm.
I0123 14:02:11.014215 140227981185024 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 14:02:11.059472 140227981185024 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.059555 140227981185024 decoder_stack.py:333] dstack: autoregressive generator.
I0123 14:02:11.059606 140227981185024 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 14:02:11.059710 140227981185024 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:02:11.059746 140227981185024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:02:11.059776 140227981185024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:02:11.059835 140227981185024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.062232 140227981185024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:02:11.067659 140227981185024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.067921 140227981185024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:02:11.070505 140227981185024 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:02:11.083195 140227981185024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:02:11.083251 140227981185024 attention.py:418] Single window, no scan.
I0123 14:02:11.083286 140227981185024 transformer_layer.py:389] tlayer: self-attention.
I0123 14:02:11.083316 140227981185024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.083375 140227981185024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.083929 140227981185024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.084003 140227981185024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.084371 140227981185024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.085060 140227981185024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.087585 140227981185024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.088196 140227981185024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.088271 140227981185024 transformer_layer.py:468] tlayer: End windows.
I0123 14:02:11.088304 140227981185024 transformer_layer.py:472] tlayer: final FFN.
I0123 14:02:11.088361 140227981185024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.088487 140227981185024 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:02:11.088593 140227981185024 nn_components.py:325] mlp: activation = None
I0123 14:02:11.088630 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:02:11.090475 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.090571 140227981185024 nn_components.py:261] mlp: residual
I0123 14:02:11.092942 140227981185024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.093019 140227981185024 transformer_base.py:443] tbase: final FFN
I0123 14:02:11.093126 140227981185024 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:02:11.095349 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:02:11.097170 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.097261 140227981185024 nn_components.py:261] mlp: residual
I0123 14:02:11.097551 140227981185024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.097629 140227981185024 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 14:02:11.097742 140227981185024 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:02:11.097779 140227981185024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:02:11.097807 140227981185024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:02:11.097866 140227981185024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.100067 140227981185024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:02:11.105406 140227981185024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.105673 140227981185024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:02:11.108322 140227981185024 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:02:11.120953 140227981185024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:02:11.121008 140227981185024 attention.py:418] Single window, no scan.
I0123 14:02:11.121043 140227981185024 transformer_layer.py:389] tlayer: self-attention.
I0123 14:02:11.121073 140227981185024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.121134 140227981185024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.121692 140227981185024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.121767 140227981185024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.122125 140227981185024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.122847 140227981185024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.125301 140227981185024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.125924 140227981185024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.126001 140227981185024 transformer_layer.py:468] tlayer: End windows.
I0123 14:02:11.126034 140227981185024 transformer_layer.py:472] tlayer: final FFN.
I0123 14:02:11.126091 140227981185024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.126216 140227981185024 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:02:11.126482 140227981185024 nn_components.py:325] mlp: activation = None
I0123 14:02:11.126519 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:02:11.128365 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.128465 140227981185024 nn_components.py:261] mlp: residual
I0123 14:02:11.130880 140227981185024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.130958 140227981185024 transformer_base.py:443] tbase: final FFN
I0123 14:02:11.131065 140227981185024 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:02:11.133731 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:02:11.135591 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.135683 140227981185024 nn_components.py:261] mlp: residual
I0123 14:02:11.135977 140227981185024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.136056 140227981185024 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 14:02:11.136164 140227981185024 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:02:11.136202 140227981185024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:02:11.136232 140227981185024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:02:11.136294 140227981185024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.138559 140227981185024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:02:11.143933 140227981185024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.144188 140227981185024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:02:11.146850 140227981185024 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:02:11.159192 140227981185024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:02:11.159245 140227981185024 attention.py:418] Single window, no scan.
I0123 14:02:11.159280 140227981185024 transformer_layer.py:389] tlayer: self-attention.
I0123 14:02:11.159309 140227981185024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.159369 140227981185024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.159913 140227981185024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.159986 140227981185024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.160342 140227981185024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.161066 140227981185024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.163519 140227981185024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.164128 140227981185024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.164202 140227981185024 transformer_layer.py:468] tlayer: End windows.
I0123 14:02:11.164236 140227981185024 transformer_layer.py:472] tlayer: final FFN.
I0123 14:02:11.164296 140227981185024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.164424 140227981185024 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:02:11.164530 140227981185024 nn_components.py:325] mlp: activation = None
I0123 14:02:11.164567 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:02:11.166445 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.166537 140227981185024 nn_components.py:261] mlp: residual
I0123 14:02:11.168971 140227981185024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.169049 140227981185024 transformer_base.py:443] tbase: final FFN
I0123 14:02:11.169157 140227981185024 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:02:11.171422 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:02:11.173283 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.173376 140227981185024 nn_components.py:261] mlp: residual
I0123 14:02:11.173674 140227981185024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.173755 140227981185024 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 14:02:11.173863 140227981185024 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:02:11.173901 140227981185024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:02:11.173931 140227981185024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:02:11.173992 140227981185024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.176242 140227981185024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:02:11.181680 140227981185024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.181943 140227981185024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:02:11.184616 140227981185024 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:02:11.197106 140227981185024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:02:11.197162 140227981185024 attention.py:418] Single window, no scan.
I0123 14:02:11.197196 140227981185024 transformer_layer.py:389] tlayer: self-attention.
I0123 14:02:11.197227 140227981185024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.197289 140227981185024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.197861 140227981185024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.197938 140227981185024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.198301 140227981185024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.199034 140227981185024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.201508 140227981185024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.202127 140227981185024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.202202 140227981185024 transformer_layer.py:468] tlayer: End windows.
I0123 14:02:11.202236 140227981185024 transformer_layer.py:472] tlayer: final FFN.
I0123 14:02:11.202293 140227981185024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.202421 140227981185024 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:02:11.202533 140227981185024 nn_components.py:325] mlp: activation = None
I0123 14:02:11.202571 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:02:11.204452 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.204544 140227981185024 nn_components.py:261] mlp: residual
I0123 14:02:11.206990 140227981185024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.207073 140227981185024 transformer_base.py:443] tbase: final FFN
I0123 14:02:11.207182 140227981185024 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:02:11.209425 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:02:11.211341 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.211439 140227981185024 nn_components.py:261] mlp: residual
I0123 14:02:11.211742 140227981185024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.211824 140227981185024 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 14:02:11.211930 140227981185024 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:02:11.211967 140227981185024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:02:11.211996 140227981185024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:02:11.212057 140227981185024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.214290 140227981185024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:02:11.219710 140227981185024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.219970 140227981185024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:02:11.222634 140227981185024 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:02:11.235249 140227981185024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:02:11.235303 140227981185024 attention.py:418] Single window, no scan.
I0123 14:02:11.235337 140227981185024 transformer_layer.py:389] tlayer: self-attention.
I0123 14:02:11.235368 140227981185024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.235427 140227981185024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.235985 140227981185024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.236059 140227981185024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.236420 140227981185024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.237152 140227981185024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.239811 140227981185024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.240420 140227981185024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.240495 140227981185024 transformer_layer.py:468] tlayer: End windows.
I0123 14:02:11.240528 140227981185024 transformer_layer.py:472] tlayer: final FFN.
I0123 14:02:11.240583 140227981185024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.240709 140227981185024 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:02:11.240814 140227981185024 nn_components.py:325] mlp: activation = None
I0123 14:02:11.240851 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:02:11.242728 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.242821 140227981185024 nn_components.py:261] mlp: residual
I0123 14:02:11.245250 140227981185024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.245333 140227981185024 transformer_base.py:443] tbase: final FFN
I0123 14:02:11.245441 140227981185024 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:02:11.248098 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:02:11.249966 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.250060 140227981185024 nn_components.py:261] mlp: residual
I0123 14:02:11.250354 140227981185024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.250434 140227981185024 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 14:02:11.250541 140227981185024 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:02:11.250578 140227981185024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:02:11.250608 140227981185024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:02:11.250671 140227981185024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.252917 140227981185024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:02:11.258315 140227981185024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.258570 140227981185024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:02:11.261214 140227981185024 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:02:11.273513 140227981185024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:02:11.273567 140227981185024 attention.py:418] Single window, no scan.
I0123 14:02:11.273601 140227981185024 transformer_layer.py:389] tlayer: self-attention.
I0123 14:02:11.273632 140227981185024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.273700 140227981185024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.274250 140227981185024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.274324 140227981185024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.274675 140227981185024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.275392 140227981185024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.277998 140227981185024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.278611 140227981185024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.278686 140227981185024 transformer_layer.py:468] tlayer: End windows.
I0123 14:02:11.278720 140227981185024 transformer_layer.py:472] tlayer: final FFN.
I0123 14:02:11.278778 140227981185024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.278905 140227981185024 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:02:11.279011 140227981185024 nn_components.py:325] mlp: activation = None
I0123 14:02:11.279049 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:02:11.281052 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.281142 140227981185024 nn_components.py:261] mlp: residual
I0123 14:02:11.283545 140227981185024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.283622 140227981185024 transformer_base.py:443] tbase: final FFN
I0123 14:02:11.283736 140227981185024 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:02:11.286007 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:02:11.287848 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.287940 140227981185024 nn_components.py:261] mlp: residual
I0123 14:02:11.288231 140227981185024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.288311 140227981185024 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 14:02:11.288418 140227981185024 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:02:11.288455 140227981185024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:02:11.288485 140227981185024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:02:11.288545 140227981185024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.290786 140227981185024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:02:11.296169 140227981185024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.296433 140227981185024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:02:11.299088 140227981185024 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:02:11.311326 140227981185024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:02:11.311381 140227981185024 attention.py:418] Single window, no scan.
I0123 14:02:11.311416 140227981185024 transformer_layer.py:389] tlayer: self-attention.
I0123 14:02:11.311446 140227981185024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.311507 140227981185024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.312055 140227981185024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.312128 140227981185024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.312479 140227981185024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.313207 140227981185024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.315657 140227981185024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.316272 140227981185024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.316347 140227981185024 transformer_layer.py:468] tlayer: End windows.
I0123 14:02:11.316380 140227981185024 transformer_layer.py:472] tlayer: final FFN.
I0123 14:02:11.316436 140227981185024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.316563 140227981185024 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:02:11.316670 140227981185024 nn_components.py:325] mlp: activation = None
I0123 14:02:11.316708 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:02:11.318561 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.318653 140227981185024 nn_components.py:261] mlp: residual
I0123 14:02:11.321060 140227981185024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.321138 140227981185024 transformer_base.py:443] tbase: final FFN
I0123 14:02:11.321247 140227981185024 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:02:11.323511 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:02:11.325361 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.325453 140227981185024 nn_components.py:261] mlp: residual
I0123 14:02:11.325750 140227981185024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.325831 140227981185024 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 14:02:11.325937 140227981185024 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:02:11.325975 140227981185024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:02:11.326004 140227981185024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:02:11.326064 140227981185024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.328291 140227981185024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:02:11.333693 140227981185024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.333960 140227981185024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:02:11.336599 140227981185024 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:02:11.348880 140227981185024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:02:11.348935 140227981185024 attention.py:418] Single window, no scan.
I0123 14:02:11.348970 140227981185024 transformer_layer.py:389] tlayer: self-attention.
I0123 14:02:11.349000 140227981185024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.349062 140227981185024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.349619 140227981185024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.349714 140227981185024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.350076 140227981185024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.350813 140227981185024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.353274 140227981185024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.353906 140227981185024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.353986 140227981185024 transformer_layer.py:468] tlayer: End windows.
I0123 14:02:11.354021 140227981185024 transformer_layer.py:472] tlayer: final FFN.
I0123 14:02:11.354079 140227981185024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.354210 140227981185024 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:02:11.354320 140227981185024 nn_components.py:325] mlp: activation = None
I0123 14:02:11.354359 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:02:11.356317 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.356413 140227981185024 nn_components.py:261] mlp: residual
I0123 14:02:11.358856 140227981185024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.358935 140227981185024 transformer_base.py:443] tbase: final FFN
I0123 14:02:11.359042 140227981185024 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:02:11.361697 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:02:11.363573 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.363668 140227981185024 nn_components.py:261] mlp: residual
I0123 14:02:11.363961 140227981185024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.364040 140227981185024 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 14:02:11.364146 140227981185024 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:02:11.364184 140227981185024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:02:11.364213 140227981185024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:02:11.364274 140227981185024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.366529 140227981185024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:02:11.371939 140227981185024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.372198 140227981185024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:02:11.374876 140227981185024 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:02:11.387474 140227981185024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:02:11.387530 140227981185024 attention.py:418] Single window, no scan.
I0123 14:02:11.387565 140227981185024 transformer_layer.py:389] tlayer: self-attention.
I0123 14:02:11.387595 140227981185024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.387656 140227981185024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.388207 140227981185024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.388281 140227981185024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.388634 140227981185024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.389317 140227981185024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.391855 140227981185024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.392474 140227981185024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.392549 140227981185024 transformer_layer.py:468] tlayer: End windows.
I0123 14:02:11.392583 140227981185024 transformer_layer.py:472] tlayer: final FFN.
I0123 14:02:11.392639 140227981185024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.392766 140227981185024 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:02:11.392875 140227981185024 nn_components.py:325] mlp: activation = None
I0123 14:02:11.392912 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:02:11.394786 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.394879 140227981185024 nn_components.py:261] mlp: residual
I0123 14:02:11.397290 140227981185024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.397371 140227981185024 transformer_base.py:443] tbase: final FFN
I0123 14:02:11.397480 140227981185024 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:02:11.399744 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:02:11.401624 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.401725 140227981185024 nn_components.py:261] mlp: residual
I0123 14:02:11.402017 140227981185024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.402096 140227981185024 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 14:02:11.402202 140227981185024 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:02:11.402239 140227981185024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:02:11.402268 140227981185024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:02:11.402328 140227981185024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.404566 140227981185024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:02:11.409970 140227981185024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.410227 140227981185024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:02:11.412862 140227981185024 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:02:11.425333 140227981185024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:02:11.425387 140227981185024 attention.py:418] Single window, no scan.
I0123 14:02:11.425421 140227981185024 transformer_layer.py:389] tlayer: self-attention.
I0123 14:02:11.425451 140227981185024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.425511 140227981185024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.426070 140227981185024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.426145 140227981185024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.426496 140227981185024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.427172 140227981185024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.429696 140227981185024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.430313 140227981185024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.430388 140227981185024 transformer_layer.py:468] tlayer: End windows.
I0123 14:02:11.430422 140227981185024 transformer_layer.py:472] tlayer: final FFN.
I0123 14:02:11.430478 140227981185024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.430605 140227981185024 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:02:11.430712 140227981185024 nn_components.py:325] mlp: activation = None
I0123 14:02:11.430749 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:02:11.432615 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.432707 140227981185024 nn_components.py:261] mlp: residual
I0123 14:02:11.435117 140227981185024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.435195 140227981185024 transformer_base.py:443] tbase: final FFN
I0123 14:02:11.435302 140227981185024 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:02:11.437555 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:02:11.439414 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.439515 140227981185024 nn_components.py:261] mlp: residual
I0123 14:02:11.439810 140227981185024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.439890 140227981185024 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 14:02:11.439998 140227981185024 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:02:11.440035 140227981185024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:02:11.440065 140227981185024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:02:11.440124 140227981185024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.442364 140227981185024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:02:11.447886 140227981185024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.448142 140227981185024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:02:11.450954 140227981185024 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:02:11.463343 140227981185024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:02:11.463397 140227981185024 attention.py:418] Single window, no scan.
I0123 14:02:11.463432 140227981185024 transformer_layer.py:389] tlayer: self-attention.
I0123 14:02:11.463463 140227981185024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.463523 140227981185024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.464075 140227981185024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.464149 140227981185024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.464696 140227981185024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.465372 140227981185024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.467880 140227981185024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.468493 140227981185024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.468569 140227981185024 transformer_layer.py:468] tlayer: End windows.
I0123 14:02:11.468604 140227981185024 transformer_layer.py:472] tlayer: final FFN.
I0123 14:02:11.468660 140227981185024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.468787 140227981185024 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:02:11.468894 140227981185024 nn_components.py:325] mlp: activation = None
I0123 14:02:11.468931 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:02:11.470773 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.470865 140227981185024 nn_components.py:261] mlp: residual
I0123 14:02:11.473253 140227981185024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.473330 140227981185024 transformer_base.py:443] tbase: final FFN
I0123 14:02:11.473437 140227981185024 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:02:11.476065 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:02:11.477926 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.478026 140227981185024 nn_components.py:261] mlp: residual
I0123 14:02:11.478321 140227981185024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.478400 140227981185024 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 14:02:11.478508 140227981185024 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:02:11.478546 140227981185024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:02:11.478575 140227981185024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:02:11.478636 140227981185024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.480858 140227981185024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:02:11.486222 140227981185024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.486478 140227981185024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:02:11.489131 140227981185024 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:02:11.501367 140227981185024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:02:11.501421 140227981185024 attention.py:418] Single window, no scan.
I0123 14:02:11.501455 140227981185024 transformer_layer.py:389] tlayer: self-attention.
I0123 14:02:11.501486 140227981185024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.501546 140227981185024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.502102 140227981185024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.502177 140227981185024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.502530 140227981185024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.503212 140227981185024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.505713 140227981185024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.506318 140227981185024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.506394 140227981185024 transformer_layer.py:468] tlayer: End windows.
I0123 14:02:11.506428 140227981185024 transformer_layer.py:472] tlayer: final FFN.
I0123 14:02:11.506485 140227981185024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.506610 140227981185024 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:02:11.506715 140227981185024 nn_components.py:325] mlp: activation = None
I0123 14:02:11.506752 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:02:11.508590 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.508681 140227981185024 nn_components.py:261] mlp: residual
I0123 14:02:11.511074 140227981185024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.511152 140227981185024 transformer_base.py:443] tbase: final FFN
I0123 14:02:11.511259 140227981185024 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:02:11.513500 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:02:11.515341 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.515436 140227981185024 nn_components.py:261] mlp: residual
I0123 14:02:11.515734 140227981185024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.515820 140227981185024 decoder_stack.py:344] dstack: Final layernorm.
I0123 14:02:11.518652 140227981185024 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 14:02:11.569517 140227981185024 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.569601 140227981185024 decoder_stack.py:333] dstack: autoregressive generator.
I0123 14:02:11.569659 140227981185024 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 14:02:11.569763 140227981185024 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:02:11.569799 140227981185024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:02:11.569828 140227981185024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:02:11.569889 140227981185024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.572185 140227981185024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:02:11.577681 140227981185024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.577939 140227981185024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:02:11.580524 140227981185024 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:02:11.592979 140227981185024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:02:11.593035 140227981185024 attention.py:418] Single window, no scan.
I0123 14:02:11.593069 140227981185024 transformer_layer.py:389] tlayer: self-attention.
I0123 14:02:11.593100 140227981185024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.593159 140227981185024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.593724 140227981185024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.593800 140227981185024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.594156 140227981185024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.594961 140227981185024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.597607 140227981185024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.598228 140227981185024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.598304 140227981185024 transformer_layer.py:468] tlayer: End windows.
I0123 14:02:11.598337 140227981185024 transformer_layer.py:472] tlayer: final FFN.
I0123 14:02:11.598393 140227981185024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.598519 140227981185024 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:02:11.598625 140227981185024 nn_components.py:325] mlp: activation = None
I0123 14:02:11.598661 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:02:11.600570 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.600661 140227981185024 nn_components.py:261] mlp: residual
I0123 14:02:11.603138 140227981185024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.603217 140227981185024 transformer_base.py:443] tbase: final FFN
I0123 14:02:11.603333 140227981185024 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:02:11.605525 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:02:11.607387 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.607480 140227981185024 nn_components.py:261] mlp: residual
I0123 14:02:11.607773 140227981185024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.607852 140227981185024 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 14:02:11.607957 140227981185024 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:02:11.607994 140227981185024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:02:11.608024 140227981185024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:02:11.608085 140227981185024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.610328 140227981185024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:02:11.615780 140227981185024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.616043 140227981185024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:02:11.618654 140227981185024 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:02:11.631116 140227981185024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:02:11.631170 140227981185024 attention.py:418] Single window, no scan.
I0123 14:02:11.631204 140227981185024 transformer_layer.py:389] tlayer: self-attention.
I0123 14:02:11.631234 140227981185024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.631295 140227981185024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.631846 140227981185024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.631919 140227981185024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.632271 140227981185024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.632945 140227981185024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.635396 140227981185024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.636006 140227981185024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.636080 140227981185024 transformer_layer.py:468] tlayer: End windows.
I0123 14:02:11.636114 140227981185024 transformer_layer.py:472] tlayer: final FFN.
I0123 14:02:11.636172 140227981185024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.636297 140227981185024 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:02:11.636402 140227981185024 nn_components.py:325] mlp: activation = None
I0123 14:02:11.636440 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:02:11.638343 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.638435 140227981185024 nn_components.py:261] mlp: residual
I0123 14:02:11.640824 140227981185024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.640902 140227981185024 transformer_base.py:443] tbase: final FFN
I0123 14:02:11.641008 140227981185024 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:02:11.643220 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:02:11.645067 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.645161 140227981185024 nn_components.py:261] mlp: residual
I0123 14:02:11.645452 140227981185024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.645533 140227981185024 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 14:02:11.645645 140227981185024 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:02:11.645684 140227981185024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:02:11.645714 140227981185024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:02:11.645775 140227981185024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.648006 140227981185024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:02:11.653484 140227981185024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.653749 140227981185024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:02:11.656346 140227981185024 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:02:11.669190 140227981185024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:02:11.669244 140227981185024 attention.py:418] Single window, no scan.
I0123 14:02:11.669277 140227981185024 transformer_layer.py:389] tlayer: self-attention.
I0123 14:02:11.669308 140227981185024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.669367 140227981185024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.669924 140227981185024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.669998 140227981185024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.670351 140227981185024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.671032 140227981185024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.673459 140227981185024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.674073 140227981185024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.674149 140227981185024 transformer_layer.py:468] tlayer: End windows.
I0123 14:02:11.674183 140227981185024 transformer_layer.py:472] tlayer: final FFN.
I0123 14:02:11.674239 140227981185024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.674364 140227981185024 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:02:11.674470 140227981185024 nn_components.py:325] mlp: activation = None
I0123 14:02:11.674508 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:02:11.676408 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.676500 140227981185024 nn_components.py:261] mlp: residual
I0123 14:02:11.678910 140227981185024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.678988 140227981185024 transformer_base.py:443] tbase: final FFN
I0123 14:02:11.679095 140227981185024 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:02:11.681297 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:02:11.683161 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.683255 140227981185024 nn_components.py:261] mlp: residual
I0123 14:02:11.683547 140227981185024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.683627 140227981185024 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 14:02:11.683735 140227981185024 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:02:11.683773 140227981185024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:02:11.683803 140227981185024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:02:11.683864 140227981185024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.686099 140227981185024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:02:11.691543 140227981185024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.691800 140227981185024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:02:11.694371 140227981185024 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:02:11.706826 140227981185024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:02:11.706880 140227981185024 attention.py:418] Single window, no scan.
I0123 14:02:11.706915 140227981185024 transformer_layer.py:389] tlayer: self-attention.
I0123 14:02:11.706946 140227981185024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.707008 140227981185024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.707557 140227981185024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.707631 140227981185024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.707991 140227981185024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.708668 140227981185024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.711134 140227981185024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.711913 140227981185024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.711987 140227981185024 transformer_layer.py:468] tlayer: End windows.
I0123 14:02:11.712020 140227981185024 transformer_layer.py:472] tlayer: final FFN.
I0123 14:02:11.712076 140227981185024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.712199 140227981185024 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:02:11.712304 140227981185024 nn_components.py:325] mlp: activation = None
I0123 14:02:11.712341 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:02:11.714614 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.714706 140227981185024 nn_components.py:261] mlp: residual
I0123 14:02:11.717097 140227981185024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.717175 140227981185024 transformer_base.py:443] tbase: final FFN
I0123 14:02:11.717284 140227981185024 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:02:11.719490 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:02:11.721349 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.721442 140227981185024 nn_components.py:261] mlp: residual
I0123 14:02:11.721741 140227981185024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.721824 140227981185024 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 14:02:11.721931 140227981185024 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:02:11.721968 140227981185024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:02:11.721998 140227981185024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:02:11.722061 140227981185024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.724284 140227981185024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:02:11.730389 140227981185024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.730646 140227981185024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:02:11.733505 140227981185024 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:02:11.745794 140227981185024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:02:11.745849 140227981185024 attention.py:418] Single window, no scan.
I0123 14:02:11.745884 140227981185024 transformer_layer.py:389] tlayer: self-attention.
I0123 14:02:11.745916 140227981185024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.745977 140227981185024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.746527 140227981185024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.746602 140227981185024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.746962 140227981185024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.747639 140227981185024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.750118 140227981185024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.750734 140227981185024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.750810 140227981185024 transformer_layer.py:468] tlayer: End windows.
I0123 14:02:11.750843 140227981185024 transformer_layer.py:472] tlayer: final FFN.
I0123 14:02:11.750900 140227981185024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.751026 140227981185024 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:02:11.751132 140227981185024 nn_components.py:325] mlp: activation = None
I0123 14:02:11.751170 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:02:11.753090 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.753181 140227981185024 nn_components.py:261] mlp: residual
I0123 14:02:11.755597 140227981185024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.755677 140227981185024 transformer_base.py:443] tbase: final FFN
I0123 14:02:11.755787 140227981185024 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:02:11.757966 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:02:11.759825 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.759923 140227981185024 nn_components.py:261] mlp: residual
I0123 14:02:11.760214 140227981185024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.760293 140227981185024 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 14:02:11.760399 140227981185024 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:02:11.760436 140227981185024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:02:11.760466 140227981185024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:02:11.760526 140227981185024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.762771 140227981185024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:02:11.768233 140227981185024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.768488 140227981185024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:02:11.771078 140227981185024 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:02:11.783947 140227981185024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:02:11.784001 140227981185024 attention.py:418] Single window, no scan.
I0123 14:02:11.784035 140227981185024 transformer_layer.py:389] tlayer: self-attention.
I0123 14:02:11.784065 140227981185024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.784124 140227981185024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.784671 140227981185024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.784745 140227981185024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.785098 140227981185024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.785778 140227981185024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.788242 140227981185024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.788857 140227981185024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.788932 140227981185024 transformer_layer.py:468] tlayer: End windows.
I0123 14:02:11.788966 140227981185024 transformer_layer.py:472] tlayer: final FFN.
I0123 14:02:11.789024 140227981185024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.789151 140227981185024 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:02:11.789257 140227981185024 nn_components.py:325] mlp: activation = None
I0123 14:02:11.789295 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:02:11.791208 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.791300 140227981185024 nn_components.py:261] mlp: residual
I0123 14:02:11.793679 140227981185024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.793757 140227981185024 transformer_base.py:443] tbase: final FFN
I0123 14:02:11.793867 140227981185024 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:02:11.796041 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:02:11.797881 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.797981 140227981185024 nn_components.py:261] mlp: residual
I0123 14:02:11.798273 140227981185024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.798354 140227981185024 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 14:02:11.798459 140227981185024 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:02:11.798496 140227981185024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:02:11.798526 140227981185024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:02:11.798586 140227981185024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.800802 140227981185024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:02:11.806251 140227981185024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.806511 140227981185024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:02:11.809093 140227981185024 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:02:11.821400 140227981185024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:02:11.821455 140227981185024 attention.py:418] Single window, no scan.
I0123 14:02:11.821489 140227981185024 transformer_layer.py:389] tlayer: self-attention.
I0123 14:02:11.821519 140227981185024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.821579 140227981185024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.822134 140227981185024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.822209 140227981185024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.822560 140227981185024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.823234 140227981185024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.825696 140227981185024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.826306 140227981185024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.826382 140227981185024 transformer_layer.py:468] tlayer: End windows.
I0123 14:02:11.826415 140227981185024 transformer_layer.py:472] tlayer: final FFN.
I0123 14:02:11.826472 140227981185024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.826598 140227981185024 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:02:11.826706 140227981185024 nn_components.py:325] mlp: activation = None
I0123 14:02:11.826743 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:02:11.828653 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.828745 140227981185024 nn_components.py:261] mlp: residual
I0123 14:02:11.831142 140227981185024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.831219 140227981185024 transformer_base.py:443] tbase: final FFN
I0123 14:02:11.831326 140227981185024 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:02:11.833675 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:02:11.835504 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.835596 140227981185024 nn_components.py:261] mlp: residual
I0123 14:02:11.835892 140227981185024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.835971 140227981185024 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 14:02:11.836078 140227981185024 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:02:11.836115 140227981185024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:02:11.836144 140227981185024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:02:11.836207 140227981185024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.838510 140227981185024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:02:11.843954 140227981185024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.844212 140227981185024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:02:11.846804 140227981185024 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:02:11.859184 140227981185024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:02:11.859238 140227981185024 attention.py:418] Single window, no scan.
I0123 14:02:11.859273 140227981185024 transformer_layer.py:389] tlayer: self-attention.
I0123 14:02:11.859303 140227981185024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.859364 140227981185024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.859914 140227981185024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.859988 140227981185024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.860342 140227981185024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.861017 140227981185024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.863465 140227981185024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.864080 140227981185024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.864154 140227981185024 transformer_layer.py:468] tlayer: End windows.
I0123 14:02:11.864188 140227981185024 transformer_layer.py:472] tlayer: final FFN.
I0123 14:02:11.864244 140227981185024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.864368 140227981185024 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:02:11.864474 140227981185024 nn_components.py:325] mlp: activation = None
I0123 14:02:11.864511 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:02:11.866441 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.866531 140227981185024 nn_components.py:261] mlp: residual
I0123 14:02:11.868929 140227981185024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.869004 140227981185024 transformer_base.py:443] tbase: final FFN
I0123 14:02:11.869112 140227981185024 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:02:11.871347 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:02:11.873199 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.873291 140227981185024 nn_components.py:261] mlp: residual
I0123 14:02:11.873584 140227981185024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.873677 140227981185024 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 14:02:11.873788 140227981185024 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:02:11.873826 140227981185024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:02:11.873855 140227981185024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:02:11.873916 140227981185024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.876147 140227981185024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:02:11.881629 140227981185024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.881895 140227981185024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:02:11.884506 140227981185024 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:02:11.897487 140227981185024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:02:11.897541 140227981185024 attention.py:418] Single window, no scan.
I0123 14:02:11.897576 140227981185024 transformer_layer.py:389] tlayer: self-attention.
I0123 14:02:11.897606 140227981185024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.897676 140227981185024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.898231 140227981185024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.898303 140227981185024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.898660 140227981185024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.899345 140227981185024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.901837 140227981185024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.902452 140227981185024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.902526 140227981185024 transformer_layer.py:468] tlayer: End windows.
I0123 14:02:11.902560 140227981185024 transformer_layer.py:472] tlayer: final FFN.
I0123 14:02:11.902618 140227981185024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.902742 140227981185024 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:02:11.902850 140227981185024 nn_components.py:325] mlp: activation = None
I0123 14:02:11.902887 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:02:11.904815 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.904905 140227981185024 nn_components.py:261] mlp: residual
I0123 14:02:11.907324 140227981185024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.907401 140227981185024 transformer_base.py:443] tbase: final FFN
I0123 14:02:11.907510 140227981185024 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:02:11.909723 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:02:11.911589 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.911682 140227981185024 nn_components.py:261] mlp: residual
I0123 14:02:11.911972 140227981185024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.912056 140227981185024 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 14:02:11.912164 140227981185024 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:02:11.912201 140227981185024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:02:11.912231 140227981185024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:02:11.912293 140227981185024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.914554 140227981185024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:02:11.920063 140227981185024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.920320 140227981185024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:02:11.922931 140227981185024 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:02:11.935405 140227981185024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:02:11.935458 140227981185024 attention.py:418] Single window, no scan.
I0123 14:02:11.935493 140227981185024 transformer_layer.py:389] tlayer: self-attention.
I0123 14:02:11.935523 140227981185024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.935582 140227981185024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.936136 140227981185024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.936210 140227981185024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.936566 140227981185024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.937257 140227981185024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.939713 140227981185024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.940329 140227981185024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.940403 140227981185024 transformer_layer.py:468] tlayer: End windows.
I0123 14:02:11.940437 140227981185024 transformer_layer.py:472] tlayer: final FFN.
I0123 14:02:11.940494 140227981185024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.940619 140227981185024 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:02:11.940725 140227981185024 nn_components.py:325] mlp: activation = None
I0123 14:02:11.940763 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:02:11.942685 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.942775 140227981185024 nn_components.py:261] mlp: residual
I0123 14:02:11.945164 140227981185024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.945240 140227981185024 transformer_base.py:443] tbase: final FFN
I0123 14:02:11.945348 140227981185024 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:02:11.947541 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:02:11.949371 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.949462 140227981185024 nn_components.py:261] mlp: residual
I0123 14:02:11.949757 140227981185024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.949836 140227981185024 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 14:02:11.949950 140227981185024 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:02:11.949988 140227981185024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:02:11.950019 140227981185024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:02:11.950081 140227981185024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.952318 140227981185024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:02:11.957794 140227981185024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.958051 140227981185024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:02:11.960812 140227981185024 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:02:11.973282 140227981185024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:02:11.973335 140227981185024 attention.py:418] Single window, no scan.
I0123 14:02:11.973370 140227981185024 transformer_layer.py:389] tlayer: self-attention.
I0123 14:02:11.973400 140227981185024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.973458 140227981185024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.974013 140227981185024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.974087 140227981185024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.974438 140227981185024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.975108 140227981185024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.977571 140227981185024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.978184 140227981185024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.978259 140227981185024 transformer_layer.py:468] tlayer: End windows.
I0123 14:02:11.978292 140227981185024 transformer_layer.py:472] tlayer: final FFN.
I0123 14:02:11.978348 140227981185024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.978473 140227981185024 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:02:11.978580 140227981185024 nn_components.py:325] mlp: activation = None
I0123 14:02:11.978618 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:02:11.980544 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.980633 140227981185024 nn_components.py:261] mlp: residual
I0123 14:02:11.983041 140227981185024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.983118 140227981185024 transformer_base.py:443] tbase: final FFN
I0123 14:02:11.983224 140227981185024 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:02:11.985596 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:02:11.987455 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.987547 140227981185024 nn_components.py:261] mlp: residual
I0123 14:02:11.987839 140227981185024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.987917 140227981185024 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 14:02:11.988023 140227981185024 transformer_layer.py:154] tlayer: recurrent = False
I0123 14:02:11.988066 140227981185024 transformer_layer.py:155] tlayer: compute_importance = False
I0123 14:02:11.988096 140227981185024 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 14:02:11.988157 140227981185024 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.990414 140227981185024 transformer_base.py:161] kvq: pre_attn dropout.
I0123 14:02:11.995941 140227981185024 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:11.996199 140227981185024 transformer_base.py:194] kvq: normalize keys, queries.
I0123 14:02:11.998823 140227981185024 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 14:02:12.011646 140227981185024 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 14:02:12.011700 140227981185024 attention.py:418] Single window, no scan.
I0123 14:02:12.011734 140227981185024 transformer_layer.py:389] tlayer: self-attention.
I0123 14:02:12.011764 140227981185024 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:12.011824 140227981185024 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:12.012370 140227981185024 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:12.012443 140227981185024 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:12.012792 140227981185024 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:12.013467 140227981185024 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:12.015934 140227981185024 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:12.016544 140227981185024 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:12.016618 140227981185024 transformer_layer.py:468] tlayer: End windows.
I0123 14:02:12.016652 140227981185024 transformer_layer.py:472] tlayer: final FFN.
I0123 14:02:12.016708 140227981185024 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:12.016833 140227981185024 transformer_base.py:410] tbase: post-attention MLP.
I0123 14:02:12.016939 140227981185024 nn_components.py:325] mlp: activation = None
I0123 14:02:12.016976 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:02:12.018924 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:12.019015 140227981185024 nn_components.py:261] mlp: residual
I0123 14:02:12.021423 140227981185024 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:12.021499 140227981185024 transformer_base.py:443] tbase: final FFN
I0123 14:02:12.021606 140227981185024 nn_components.py:320] mlp: hidden 4096, relu
I0123 14:02:12.023818 140227981185024 nn_components.py:329] mlp: final activation = None
I0123 14:02:12.025688 140227981185024 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:12.025781 140227981185024 nn_components.py:261] mlp: residual
I0123 14:02:12.026072 140227981185024 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:12.026154 140227981185024 decoder_stack.py:344] dstack: Final layernorm.
I0123 14:02:12.028970 140227981185024 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 14:02:23.721910 140227981185024 alphageometry.py:566] LM output (score=-0.505237): "p : C e f p 24 D e p f p 25 ;"
I0123 14:02:23.722058 140227981185024 alphageometry.py:567] Translation: "p = on_line p e f, on_bline p f e"

I0123 14:02:23.722100 140227981185024 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = on_circle h d c, on_line h g c; i = midpoint i e h; j = foot j h i d; k = on_line k h j, on_line k b a; l = mirror l h d; m = mirror m g h; n = foot n l k m; o = midpoint o g n; p = on_line p e f, on_bline p f e ? cong d c d o"
I0123 14:02:23.722266 140227981185024 graph.py:498] 
I0123 14:02:23.722321 140227981185024 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = on_circle h d c, on_line h g c; i = midpoint i e h; j = foot j h i d; k = on_line k h j, on_line k b a; l = mirror l h d; m = mirror m g h; n = foot n l k m; o = midpoint o g n; p = on_line p e f, on_bline p f e ? cong d c d o
I0123 14:02:29.484782 140227981185024 ddar.py:60] Depth 1/1000 time = 5.706696271896362
I0123 14:02:38.523845 140227981185024 ddar.py:60] Depth 2/1000 time = 9.03890061378479
I0123 14:02:48.694380 140227981185024 ddar.py:60] Depth 3/1000 time = 10.170348882675171
I0123 14:02:58.986263 140227981185024 ddar.py:60] Depth 4/1000 time = 10.291703701019287
I0123 14:03:09.830843 140227981185024 ddar.py:60] Depth 5/1000 time = 10.844383716583252
I0123 14:03:20.272935 140227981185024 ddar.py:60] Depth 6/1000 time = 10.44124960899353
I0123 14:03:30.829126 140227981185024 ddar.py:60] Depth 7/1000 time = 10.522149801254272
I0123 14:03:42.516699 140227981185024 ddar.py:60] Depth 8/1000 time = 11.68739914894104
I0123 14:03:56.203058 140227981185024 ddar.py:60] Depth 9/1000 time = 13.686189889907837
I0123 14:04:09.973731 140227981185024 ddar.py:60] Depth 10/1000 time = 13.770476818084717
I0123 14:04:23.704225 140227981185024 ddar.py:60] Depth 11/1000 time = 13.730241775512695
I0123 14:04:37.786593 140227981185024 ddar.py:60] Depth 12/1000 time = 14.018315553665161
I0123 14:04:37.787005 140227981185024 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:04:37.787110 140227981185024 alphageometry.py:566] LM output (score=-1.333526): "p : C f n p 24 D f p n p 25 ;"
I0123 14:04:37.787146 140227981185024 alphageometry.py:567] Translation: "p = on_line p f n, on_bline p n f"

I0123 14:04:37.787184 140227981185024 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = on_circle h d c, on_line h g c; i = midpoint i e h; j = foot j h i d; k = on_line k h j, on_line k b a; l = mirror l h d; m = mirror m g h; n = foot n l k m; o = midpoint o g n; p = on_line p f n, on_bline p n f ? cong d c d o"
I0123 14:04:37.787369 140227981185024 graph.py:498] 
I0123 14:04:37.787429 140227981185024 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = on_circle h d c, on_line h g c; i = midpoint i e h; j = foot j h i d; k = on_line k h j, on_line k b a; l = mirror l h d; m = mirror m g h; n = foot n l k m; o = midpoint o g n; p = on_line p f n, on_bline p n f ? cong d c d o
I0123 14:04:43.391793 140227981185024 ddar.py:60] Depth 1/1000 time = 5.546486854553223
I0123 14:04:52.104540 140227981185024 ddar.py:60] Depth 2/1000 time = 8.712583780288696
I0123 14:05:01.766080 140227981185024 ddar.py:60] Depth 3/1000 time = 9.6613609790802
I0123 14:05:12.210306 140227981185024 ddar.py:60] Depth 4/1000 time = 10.444024085998535
I0123 14:05:22.962354 140227981185024 ddar.py:60] Depth 5/1000 time = 10.751829862594604
I0123 14:05:33.340153 140227981185024 ddar.py:60] Depth 6/1000 time = 10.377065181732178
I0123 14:05:44.141649 140227981185024 ddar.py:60] Depth 7/1000 time = 10.765211343765259
I0123 14:05:55.801114 140227981185024 ddar.py:60] Depth 8/1000 time = 11.659271955490112
I0123 14:06:09.884762 140227981185024 ddar.py:60] Depth 9/1000 time = 14.083449125289917
I0123 14:06:23.597885 140227981185024 ddar.py:60] Depth 10/1000 time = 13.712913751602173
I0123 14:06:37.057355 140227981185024 ddar.py:60] Depth 11/1000 time = 13.459175825119019
I0123 14:06:51.067120 140227981185024 ddar.py:60] Depth 12/1000 time = 13.946076393127441
I0123 14:06:51.067351 140227981185024 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:06:51.067447 140227981185024 alphageometry.py:566] LM output (score=-1.473781): "p : C h n p 24 D h p n p 25 ;"
I0123 14:06:51.067483 140227981185024 alphageometry.py:567] Translation: "p = on_line p h n, on_bline p n h"

I0123 14:06:51.067521 140227981185024 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = on_circle h d c, on_line h g c; i = midpoint i e h; j = foot j h i d; k = on_line k h j, on_line k b a; l = mirror l h d; m = mirror m g h; n = foot n l k m; o = midpoint o g n; p = on_line p h n, on_bline p n h ? cong d c d o"
I0123 14:06:51.067706 140227981185024 graph.py:498] 
I0123 14:06:51.067768 140227981185024 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = on_circle h d c, on_line h g c; i = midpoint i e h; j = foot j h i d; k = on_line k h j, on_line k b a; l = mirror l h d; m = mirror m g h; n = foot n l k m; o = midpoint o g n; p = on_line p h n, on_bline p n h ? cong d c d o
I0123 14:06:56.889489 140227981185024 ddar.py:60] Depth 1/1000 time = 5.75655198097229
I0123 14:07:05.973396 140227981185024 ddar.py:60] Depth 2/1000 time = 9.083716630935669
I0123 14:07:17.558010 140227981185024 ddar.py:60] Depth 3/1000 time = 11.58437204360962
I0123 14:07:29.533854 140227981185024 ddar.py:60] Depth 4/1000 time = 11.975525140762329
I0123 14:07:41.866277 140227981185024 ddar.py:60] Depth 5/1000 time = 12.33220624923706
I0123 14:07:54.160951 140227981185024 ddar.py:60] Depth 6/1000 time = 12.293842077255249
I0123 14:08:06.350499 140227981185024 ddar.py:60] Depth 7/1000 time = 12.147719383239746
I0123 14:08:19.685115 140227981185024 ddar.py:60] Depth 8/1000 time = 13.33436107635498
I0123 14:08:35.708529 140227981185024 ddar.py:60] Depth 9/1000 time = 16.023115158081055
I0123 14:08:50.550873 140227981185024 ddar.py:60] Depth 10/1000 time = 14.842108964920044
I0123 14:09:06.123502 140227981185024 ddar.py:60] Depth 11/1000 time = 15.572288990020752
I0123 14:09:21.444747 140227981185024 ddar.py:60] Depth 12/1000 time = 15.231491088867188
I0123 14:09:21.445046 140227981185024 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:09:21.445155 140227981185024 alphageometry.py:566] LM output (score=-1.582086): "p : C d e p 24 D d p e p 25 ;"
I0123 14:09:21.445192 140227981185024 alphageometry.py:567] Translation: "p = on_line p d e, on_bline p e d"

I0123 14:09:21.445237 140227981185024 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = on_circle h d c, on_line h g c; i = midpoint i e h; j = foot j h i d; k = on_line k h j, on_line k b a; l = mirror l h d; m = mirror m g h; n = foot n l k m; o = midpoint o g n; p = on_line p d e, on_bline p e d ? cong d c d o"
I0123 14:09:21.445431 140227981185024 graph.py:498] 
I0123 14:09:21.445493 140227981185024 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = on_circle h d c, on_line h g c; i = midpoint i e h; j = foot j h i d; k = on_line k h j, on_line k b a; l = mirror l h d; m = mirror m g h; n = foot n l k m; o = midpoint o g n; p = on_line p d e, on_bline p e d ? cong d c d o
I0123 14:09:27.609254 140227981185024 ddar.py:60] Depth 1/1000 time = 6.101749420166016
I0123 14:09:36.415074 140227981185024 ddar.py:60] Depth 2/1000 time = 8.80565071105957
I0123 14:09:46.983603 140227981185024 ddar.py:60] Depth 3/1000 time = 10.568278312683105
I0123 14:09:58.174313 140227981185024 ddar.py:60] Depth 4/1000 time = 11.190407037734985
I0123 14:10:09.419217 140227981185024 ddar.py:60] Depth 5/1000 time = 11.244662523269653
I0123 14:10:20.907712 140227981185024 ddar.py:60] Depth 6/1000 time = 11.487611532211304
I0123 14:10:32.577277 140227981185024 ddar.py:60] Depth 7/1000 time = 11.63316011428833
I0123 14:10:44.849928 140227981185024 ddar.py:60] Depth 8/1000 time = 12.272440671920776
I0123 14:10:58.686982 140227981185024 ddar.py:60] Depth 9/1000 time = 13.836856126785278
I0123 14:11:13.096611 140227981185024 ddar.py:60] Depth 10/1000 time = 14.409355878829956
I0123 14:11:27.814718 140227981185024 ddar.py:60] Depth 11/1000 time = 14.717788457870483
I0123 14:11:41.878751 140227981185024 ddar.py:60] Depth 12/1000 time = 13.999831914901733
I0123 14:11:41.879119 140227981185024 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:11:41.879211 140227981185024 alphageometry.py:566] LM output (score=-1.742414): "p : C e n p 24 D e p n p 25 ;"
I0123 14:11:41.879248 140227981185024 alphageometry.py:567] Translation: "p = on_line p e n, on_bline p n e"

I0123 14:11:41.879291 140227981185024 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = on_circle h d c, on_line h g c; i = midpoint i e h; j = foot j h i d; k = on_line k h j, on_line k b a; l = mirror l h d; m = mirror m g h; n = foot n l k m; o = midpoint o g n; p = on_line p e n, on_bline p n e ? cong d c d o"
I0123 14:11:41.879472 140227981185024 graph.py:498] 
I0123 14:11:41.879536 140227981185024 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = on_circle h d c, on_line h g c; i = midpoint i e h; j = foot j h i d; k = on_line k h j, on_line k b a; l = mirror l h d; m = mirror m g h; n = foot n l k m; o = midpoint o g n; p = on_line p e n, on_bline p n e ? cong d c d o
I0123 14:11:47.886262 140227981185024 ddar.py:60] Depth 1/1000 time = 5.947192668914795
I0123 14:11:57.097416 140227981185024 ddar.py:60] Depth 2/1000 time = 9.210973739624023
I0123 14:12:07.910905 140227981185024 ddar.py:60] Depth 3/1000 time = 10.813321590423584
I0123 14:12:19.410612 140227981185024 ddar.py:60] Depth 4/1000 time = 11.499505281448364
I0123 14:12:31.415936 140227981185024 ddar.py:60] Depth 5/1000 time = 12.00510048866272
I0123 14:12:43.148901 140227981185024 ddar.py:60] Depth 6/1000 time = 11.732162475585938
I0123 14:12:55.291900 140227981185024 ddar.py:60] Depth 7/1000 time = 12.10003924369812
I0123 14:13:07.805179 140227981185024 ddar.py:60] Depth 8/1000 time = 12.513073444366455
I0123 14:13:22.314009 140227981185024 ddar.py:60] Depth 9/1000 time = 14.508553504943848
I0123 14:13:37.549042 140227981185024 ddar.py:60] Depth 10/1000 time = 15.234696626663208
I0123 14:13:52.551624 140227981185024 ddar.py:60] Depth 11/1000 time = 15.002341747283936
I0123 14:14:07.544074 140227981185024 ddar.py:60] Depth 12/1000 time = 14.91506052017212
I0123 14:14:07.544366 140227981185024 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:14:07.544466 140227981185024 alphageometry.py:566] LM output (score=-2.010902): "p : C f g p 24 D f p g p 25 ;"
I0123 14:14:07.544504 140227981185024 alphageometry.py:567] Translation: "p = on_line p f g, on_bline p g f"

I0123 14:14:07.544553 140227981185024 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = on_circle h d c, on_line h g c; i = midpoint i e h; j = foot j h i d; k = on_line k h j, on_line k b a; l = mirror l h d; m = mirror m g h; n = foot n l k m; o = midpoint o g n; p = on_line p f g, on_bline p g f ? cong d c d o"
I0123 14:14:07.544754 140227981185024 graph.py:498] 
I0123 14:14:07.544817 140227981185024 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = on_circle h d c, on_line h g c; i = midpoint i e h; j = foot j h i d; k = on_line k h j, on_line k b a; l = mirror l h d; m = mirror m g h; n = foot n l k m; o = midpoint o g n; p = on_line p f g, on_bline p g f ? cong d c d o
I0123 14:14:14.874650 140227981185024 ddar.py:60] Depth 1/1000 time = 7.270574331283569
I0123 14:14:24.857699 140227981185024 ddar.py:60] Depth 2/1000 time = 9.982790231704712
I0123 14:14:36.623251 140227981185024 ddar.py:60] Depth 3/1000 time = 11.76525330543518
I0123 14:14:49.008200 140227981185024 ddar.py:60] Depth 4/1000 time = 12.384740114212036
I0123 14:15:00.976912 140227981185024 ddar.py:60] Depth 5/1000 time = 11.968502044677734
I0123 14:15:12.867312 140227981185024 ddar.py:60] Depth 6/1000 time = 11.889599800109863
I0123 14:15:25.192867 140227981185024 ddar.py:60] Depth 7/1000 time = 12.290850639343262
I0123 14:15:38.493668 140227981185024 ddar.py:60] Depth 8/1000 time = 13.300601720809937
I0123 14:15:54.443979 140227981185024 ddar.py:60] Depth 9/1000 time = 15.950106382369995
I0123 14:16:10.359929 140227981185024 ddar.py:60] Depth 10/1000 time = 15.91565227508545
I0123 14:16:26.383958 140227981185024 ddar.py:60] Depth 11/1000 time = 16.02367353439331
I0123 14:16:42.550989 140227981185024 ddar.py:60] Depth 12/1000 time = 16.096226453781128
I0123 14:16:42.551269 140227981185024 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:16:42.551362 140227981185024 alphageometry.py:566] LM output (score=-2.141605): "p : C f h p 24 D f p h p 25 ;"
I0123 14:16:42.551399 140227981185024 alphageometry.py:567] Translation: "p = on_line p f h, on_bline p h f"

I0123 14:16:42.551444 140227981185024 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = on_circle h d c, on_line h g c; i = midpoint i e h; j = foot j h i d; k = on_line k h j, on_line k b a; l = mirror l h d; m = mirror m g h; n = foot n l k m; o = midpoint o g n; p = on_line p f h, on_bline p h f ? cong d c d o"
I0123 14:16:42.551638 140227981185024 graph.py:498] 
I0123 14:16:42.551703 140227981185024 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = on_circle h d c, on_line h g c; i = midpoint i e h; j = foot j h i d; k = on_line k h j, on_line k b a; l = mirror l h d; m = mirror m g h; n = foot n l k m; o = midpoint o g n; p = on_line p f h, on_bline p h f ? cong d c d o
I0123 14:16:49.074875 140227981185024 ddar.py:60] Depth 1/1000 time = 6.458282947540283
I0123 14:16:58.668158 140227981185024 ddar.py:60] Depth 2/1000 time = 9.59310531616211
I0123 14:17:09.241576 140227981185024 ddar.py:60] Depth 3/1000 time = 10.573217630386353
I0123 14:17:21.489151 140227981185024 ddar.py:60] Depth 4/1000 time = 12.247355699539185
I0123 14:17:32.908396 140227981185024 ddar.py:60] Depth 5/1000 time = 11.419040441513062
I0123 14:17:44.291803 140227981185024 ddar.py:60] Depth 6/1000 time = 11.38260555267334
I0123 14:17:56.412567 140227981185024 ddar.py:60] Depth 7/1000 time = 12.088221073150635
I0123 14:18:09.265425 140227981185024 ddar.py:60] Depth 8/1000 time = 12.852556228637695
I0123 14:18:24.475372 140227981185024 ddar.py:60] Depth 9/1000 time = 15.209694147109985
I0123 14:18:39.330698 140227981185024 ddar.py:60] Depth 10/1000 time = 14.854988098144531
I0123 14:18:54.141716 140227981185024 ddar.py:60] Depth 11/1000 time = 14.810749530792236
I0123 14:19:09.399843 140227981185024 ddar.py:60] Depth 12/1000 time = 15.189757585525513
I0123 14:19:09.400144 140227981185024 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:19:09.400244 140227981185024 alphageometry.py:566] LM output (score=-2.177963): "p : C i n p 24 D i p n p 25 ;"
I0123 14:19:09.400281 140227981185024 alphageometry.py:567] Translation: "p = on_line p i n, on_bline p n i"

I0123 14:19:09.400326 140227981185024 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = on_circle h d c, on_line h g c; i = midpoint i e h; j = foot j h i d; k = on_line k h j, on_line k b a; l = mirror l h d; m = mirror m g h; n = foot n l k m; o = midpoint o g n; p = on_line p i n, on_bline p n i ? cong d c d o"
I0123 14:19:09.400512 140227981185024 graph.py:498] 
I0123 14:19:09.400573 140227981185024 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = on_circle h d c, on_line h g c; i = midpoint i e h; j = foot j h i d; k = on_line k h j, on_line k b a; l = mirror l h d; m = mirror m g h; n = foot n l k m; o = midpoint o g n; p = on_line p i n, on_bline p n i ? cong d c d o
I0123 14:19:15.198206 140227981185024 ddar.py:60] Depth 1/1000 time = 5.737647294998169
I0123 14:19:24.908029 140227981185024 ddar.py:60] Depth 2/1000 time = 9.709651470184326
I0123 14:19:35.931016 140227981185024 ddar.py:60] Depth 3/1000 time = 11.022790670394897
I0123 14:19:47.712463 140227981185024 ddar.py:60] Depth 4/1000 time = 11.78121566772461
I0123 14:19:59.638635 140227981185024 ddar.py:60] Depth 5/1000 time = 11.92587947845459
I0123 14:20:11.353392 140227981185024 ddar.py:60] Depth 6/1000 time = 11.713956594467163
I0123 14:20:22.865845 140227981185024 ddar.py:60] Depth 7/1000 time = 11.471521139144897
I0123 14:20:35.670190 140227981185024 ddar.py:60] Depth 8/1000 time = 12.804152727127075
I0123 14:20:49.789557 140227981185024 ddar.py:60] Depth 9/1000 time = 14.119118452072144
I0123 14:21:03.824188 140227981185024 ddar.py:60] Depth 10/1000 time = 14.034332990646362
I0123 14:21:18.624906 140227981185024 ddar.py:60] Depth 11/1000 time = 14.800493478775024
I0123 14:21:33.581451 140227981185024 ddar.py:60] Depth 12/1000 time = 14.890822649002075
I0123 14:21:33.581974 140227981185024 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:21:33.582103 140227981185024 alphageometry.py:566] LM output (score=-2.192059): "p : C e l p 24 D e p l p 25 ;"
I0123 14:21:33.582140 140227981185024 alphageometry.py:567] Translation: "p = on_line p e l, on_bline p l e"

I0123 14:21:33.582191 140227981185024 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = on_circle h d c, on_line h g c; i = midpoint i e h; j = foot j h i d; k = on_line k h j, on_line k b a; l = mirror l h d; m = mirror m g h; n = foot n l k m; o = midpoint o g n; p = on_line p e l, on_bline p l e ? cong d c d o"
I0123 14:21:33.582395 140227981185024 graph.py:498] 
I0123 14:21:33.582457 140227981185024 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = on_circle h d c, on_line h g c; i = midpoint i e h; j = foot j h i d; k = on_line k h j, on_line k b a; l = mirror l h d; m = mirror m g h; n = foot n l k m; o = midpoint o g n; p = on_line p e l, on_bline p l e ? cong d c d o
I0123 14:21:39.908599 140227981185024 ddar.py:60] Depth 1/1000 time = 6.265724182128906
I0123 14:21:49.606021 140227981185024 ddar.py:60] Depth 2/1000 time = 9.697266817092896
I0123 14:22:03.482498 140227981185024 ddar.py:60] Depth 3/1000 time = 13.87628698348999
I0123 14:22:19.477119 140227981185024 ddar.py:60] Depth 4/1000 time = 15.994371891021729
I0123 14:22:35.442115 140227981185024 ddar.py:60] Depth 5/1000 time = 15.964209079742432
I0123 14:22:50.787349 140227981185024 ddar.py:60] Depth 6/1000 time = 15.30162000656128
I0123 14:23:07.830477 140227981185024 ddar.py:60] Depth 7/1000 time = 17.042932987213135
I0123 14:23:26.665405 140227981185024 ddar.py:60] Depth 8/1000 time = 18.834651470184326
I0123 14:23:45.402316 140227981185024 ddar.py:60] Depth 9/1000 time = 18.736546754837036
I0123 14:24:05.194303 140227981185024 ddar.py:60] Depth 10/1000 time = 19.79160165786743
I0123 14:24:24.326262 140227981185024 ddar.py:60] Depth 11/1000 time = 19.102176427841187
I0123 14:24:44.144172 140227981185024 ddar.py:60] Depth 12/1000 time = 19.773127555847168
I0123 14:24:44.144574 140227981185024 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:24:44.144682 140227981185024 alphageometry.py:566] LM output (score=-2.242517): "p : C b h p 24 D b p h p 25 ;"
I0123 14:24:44.144720 140227981185024 alphageometry.py:567] Translation: "p = on_line p b h, on_bline p h b"

I0123 14:24:44.144762 140227981185024 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = on_circle h d c, on_line h g c; i = midpoint i e h; j = foot j h i d; k = on_line k h j, on_line k b a; l = mirror l h d; m = mirror m g h; n = foot n l k m; o = midpoint o g n; p = on_line p b h, on_bline p h b ? cong d c d o"
I0123 14:24:44.144952 140227981185024 graph.py:498] 
I0123 14:24:44.145014 140227981185024 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = on_circle h d c, on_line h g c; i = midpoint i e h; j = foot j h i d; k = on_line k h j, on_line k b a; l = mirror l h d; m = mirror m g h; n = foot n l k m; o = midpoint o g n; p = on_line p b h, on_bline p h b ? cong d c d o
I0123 14:24:50.496181 140227981185024 ddar.py:60] Depth 1/1000 time = 6.2805259227752686
I0123 14:25:00.062295 140227981185024 ddar.py:60] Depth 2/1000 time = 9.565940856933594
I0123 14:25:12.828724 140227981185024 ddar.py:60] Depth 3/1000 time = 12.766231775283813
I0123 14:25:27.359745 140227981185024 ddar.py:60] Depth 4/1000 time = 14.5308256149292
I0123 14:25:42.205749 140227981185024 ddar.py:60] Depth 5/1000 time = 14.84580111503601
I0123 14:25:57.087984 140227981185024 ddar.py:60] Depth 6/1000 time = 14.881993055343628
I0123 14:26:12.252715 140227981185024 ddar.py:60] Depth 7/1000 time = 15.163675785064697
I0123 14:26:27.988718 140227981185024 ddar.py:60] Depth 8/1000 time = 15.690098762512207
I0123 14:26:44.137166 140227981185024 ddar.py:60] Depth 9/1000 time = 16.148234605789185
I0123 14:27:03.649234 140227981185024 ddar.py:60] Depth 10/1000 time = 19.51177978515625
I0123 14:27:22.662889 140227981185024 ddar.py:60] Depth 11/1000 time = 19.013246297836304
I0123 14:27:42.087705 140227981185024 ddar.py:60] Depth 12/1000 time = 19.42436933517456
I0123 14:28:01.215286 140227981185024 ddar.py:60] Depth 13/1000 time = 19.125944137573242
I0123 14:28:20.526668 140227981185024 ddar.py:60] Depth 14/1000 time = 19.252145767211914
I0123 14:28:39.700848 140227981185024 ddar.py:60] Depth 15/1000 time = 19.163612127304077
I0123 14:29:04.243109 140227981185024 ddar.py:60] Depth 16/1000 time = 24.541946172714233
I0123 14:29:29.180826 140227981185024 ddar.py:60] Depth 17/1000 time = 24.9372456073761
I0123 14:29:54.110071 140227981185024 ddar.py:60] Depth 18/1000 time = 24.928791761398315
I0123 14:30:18.640468 140227981185024 ddar.py:60] Depth 19/1000 time = 24.51801824569702
I0123 14:30:43.542291 140227981185024 ddar.py:60] Depth 20/1000 time = 24.856202125549316
I0123 14:31:08.550087 140227981185024 ddar.py:60] Depth 21/1000 time = 24.987895727157593
I0123 14:31:33.852848 140227981185024 ddar.py:60] Depth 22/1000 time = 25.26577663421631
I0123 14:32:01.557936 140227981185024 ddar.py:60] Depth 23/1000 time = 27.704566955566406
I0123 14:32:32.198913 140227981185024 ddar.py:60] Depth 24/1000 time = 30.640576362609863
I0123 14:32:32.572096 140227981185024 alphageometry.py:191] 
==========================
 * From theorem premises:
A B C D E F G H I J K L M N O : Points
DC = DB [00]
DB = DA [01]
BCE = ECA [02]
A,B,E are collinear [03]
CBF = FBA [04]
C,G,E are collinear [05]
G,B,F are collinear [06]
H,C,G are collinear [07]
DH = DC [08]
(BH-CE) = (BH-CE) [09]
H,I,E are collinear [10]
IE = IH [11]
D,I,J are collinear [12]
HJ  DI [13]
BHJ = BHJ [14]
K,A,B are collinear [15]
K,H,J are collinear [16]
DH = DL [17]
H,D,L are collinear [18]
H,M,G are collinear [19]
HG = HM [20]
K,M,N are collinear [21]
LN  KM [22]
O,N,G are collinear [23]
OG = ON [24]

 * Auxiliary Constructions:
P : Points
H,P,B are collinear [25]
PH = PB [26]

 * Proof steps:
001. H,C,G are collinear [07] & C,G,E are collinear [05] & BCE = ECA [02]   BCH = ECA [27]
002. H,C,G are collinear [07] & C,G,E are collinear [05] & BCE = ECA [02]   BCH = HCA [28]
003. DC = DB [00] & DH = DC [08] & DB = DA [01]   H,A,C,B are concyclic [29]
004. H,A,C,B are concyclic [29]   CHB = CAB [30]
005. CHB = CAB [30] & H,C,G are collinear [07] & C,G,E are collinear [05]   (BH-CE) = BAC [31]
006. H,C,G are collinear [07] & C,G,E are collinear [05] & A,B,E are collinear [03] & (BH-CE) = BAC [31]   BHC = EAC [32]
007. BCH = ECA [27] & BHC = EAC [32] (Similar Triangles)  CB:HC = CE:AC [33]
008. BCH = ECA [27] & BHC = EAC [32] (Similar Triangles)  BC:BH = EC:EA [34]
009. BCE = ECA [02] & CBF = FBA [04] & (CE-BH) = CAB [31] (Angle chase)  (BF-CE) = HBF [35]
010. F,B,G are collinear [06] & H,C,G are collinear [07] & C,G,E are collinear [05] & HBF = (BF-CE) [35]   HBG = BGH [36]
011. HBG = BGH [36]   HB = HG [37]
012. H,A,C,B are concyclic [29] & BCH = HCA [28]   BH = HA [38]
013. HG = HM [20] & HB = HG [37] & BH = HA [38]   A,M,B,G are concyclic [39]
014. A,M,B,G are concyclic [39]   AMG = ABG [40]
015. F,B,G are collinear [06] & H,M,G are collinear [19] & H,G,C are collinear [07] & C,G,E are collinear [05] & AMG = ABG [40] & FBC = ABF [04]   GBC = AMC [41]
016. C,G,E are collinear [05] & H,M,G are collinear [19] & H,G,C are collinear [07] & ECB = ACE [02]   GCB = ACM [42]
017. GBC = AMC [41] & GCB = ACM [42] (Similar Triangles)  BG:CG = AM:AC [43]
018. GBC = AMC [41] & GCB = ACM [42] (Similar Triangles)  BG:CB = AM:MC [44]
019. BCE = ECA [02] & A,B,E are collinear [03]   AE:BE = AC:CB [45]
020. I,H,E are collinear [10] & IE = IH [11]   I is midpoint of HE [46]
021. H,P,B are collinear [25] & PH = PB [26]   P is midpoint of HB [47]
022. I is midpoint of HE [46] & P is midpoint of HB [47]   IP  EB [48]
023. BE  PI [48] & H,P,B are collinear [25] & H,I,E are collinear [10]   HB:HP = BE:PI [49]
024. HB:HP = BE:PI [49] & BH = HA [38] & PH = PB [26]   AH:PB = BE:PI [50]
025. BC:BH = EC:EA [34] & BH = HA [38]   CB:AH = CE:AE [51]
026. DH = DC [08] & DH = DL [17]   D is the circumcenter of \Delta LCH [52]
027. D is the circumcenter of \Delta LCH [52] & H,D,L are collinear [18]   LC  CH [53]
028. D,I,J are collinear [12] & LC  CH [53] & H,C,G are collinear [07] & C,G,E are collinear [05] & HJ  DI [13]   HJI = ECL [54]
029. H,D,L are collinear [18] & DH = DL [17]   D is midpoint of HL [55]
030. I is midpoint of HE [46] & D is midpoint of HL [55]   ID  EL [56]
031. H,I,E are collinear [10] & H,C,G are collinear [07] & C,G,E are collinear [05] & D,I,J are collinear [12] & DI  EL [56]   HIJ = CEL [57]
032. HJI = ECL [54] & HIJ = CEL [57] (Similar Triangles)  HJ:HI = CL:LE [58]
033. H,C,G are collinear [07] & C,G,E are collinear [05] & H,P,B are collinear [25] & CHB = CAB [30] & BCE = ECA [02] & IP  EB [48] & A,B,E are collinear [03]   BCH = HPI [59]
034. H,C,G are collinear [07] & C,G,E are collinear [05] & H,P,B are collinear [25] & H,I,E are collinear [10] & (BH-CE) = (BH-CE) [09]   BHC = PHI [60]
035. BCH = HPI [59] & BHC = PHI [60] (Similar Triangles)  BC:BH = IP:IH [61]
036. BC:BH = IP:IH [61] & BH = HA [38]   CB:AH = PI:HI [62]
037. DC = DB [00] & DH = DC [08] & DH = DL [17]   H,C,B,L are concyclic [63]
038. DC = DB [00] & DH = DC [08] & DH = DL [17]   D is the circumcenter of \Delta LBH [64]
039. H,C,B,L are concyclic [63] & H,A,C,B are concyclic [29]   C,H,A,L are concyclic [65]
040. H,C,B,L are concyclic [63] & H,A,C,B are concyclic [29]   B,H,A,L are concyclic [66]
041. C,H,A,L are concyclic [65]   CHL = CAL [67]
042. C,B,H,L are concyclic [63]   CBL = CHL [68]
043. C,B,H,L are concyclic [63]   CBH = CLH [69]
044. CHL = CAL [67] & H,C,G are collinear [07] & C,G,E are collinear [05] & H,D,L are collinear [18] & BCE = ECA [02] & CBL = CHL [68]   (BL-HD) = (HD-AL) [70]
045. B,H,A,L are concyclic [66]   BHL = BAL [71]
046. B,H,A,L are concyclic [66]   HBL = HAL [72]
047. H,P,B are collinear [25] & K,A,B are collinear [15] & BHL = BAL [71] & H,D,L are collinear [18]   DHP = LAK [73]
048. (BL-HD) = (HD-AL) [70] & DHP = LAK [73]   (BL-HP) = (HD-KA) [74]
049. D is the circumcenter of \Delta LBH [64] & H,D,L are collinear [18]   BL  BH [75]
050. D is midpoint of HL [55] & P is midpoint of HB [47]   DP  LB [76]
051. H,P,B are collinear [25] & D,I,J are collinear [12] & HJ  DI [13] & BL  BH [75] & BL  DP [76]   HPD = HJD [77]
052. HPD = HJD [77]   H,D,P,J are concyclic [78]
053. H,D,P,J are concyclic [78]   HDP = HJP [79]
054. H,D,P,J are concyclic [78]   HDJ = HPJ [80]
055. K,A,B are collinear [15] & (BL-HP) = (HD-KA) [74] & H,P,B are collinear [25] & HDP = HJP [79] & DP  BL [76]   PJH = HBK [81]
056. H,P,B are collinear [25] & K,H,J are collinear [16] & BHJ = BHJ [14]   PHJ = BHK [82]
057. PJH = HBK [81] & PHJ = BHK [82] (Similar Triangles)  PJ:PH = KB:KH [83]
058. PJH = HBK [81] & PHJ = BHK [82] (Similar Triangles)  JP:JH = BK:BH [84]
059. PJ:PH = KB:KH [83] & PH = PB [26]   PJ:PB = KB:KH [85]
060. JP:JH = BK:BH [84] & BH = HA [38]   PJ:HJ = KB:AH [86]
061. AE:BE = AC:CB [45] & AH:PB = BE:PI [50] & CB:AH = CE:AE [51] & CB:HC = CE:AC [33] & HJ:HI = CL:LE [58] & CB:AH = PI:HI [62] & PJ:PB = KB:KH [85] & PJ:HJ = KB:AH [86] (Ratio chase)  CL:LE = HC:KH [87]
062. H,I,E are collinear [10] & H,C,G are collinear [07] & C,G,E are collinear [05] & LC  CH [53]   CI  CL [88]
063. K,H,J are collinear [16] & HJ  DI [13] & DI  EL [56]   KH  LE [89]
064. CI  CL [88] & KH  LE [89]   (CI-KH) = CLE [90]
065. CI  CL [88] & KH  LE [89]   (CI-LE) = (CL-KH) [91]
066. H,C,G are collinear [07] & C,G,E are collinear [05] & K,H,J are collinear [16] & (CI-KH) = CLE [90] & H,I,E are collinear [10]   CHK = CLE [92]
067. CL:LE = HC:KH [87] & CHK = CLE [92] (Similar Triangles)  HC:KC = CL:CE [93]
068. CL:LE = HC:KH [87] & CHK = CLE [92] (Similar Triangles)  HCK = LCE [94]
069. CL:LE = HC:KH [87] & CHK = CLE [92] (Similar Triangles)  CKH = CEL [95]
070. CB:HC = CE:AC [33] & BG:CG = AM:AC [43] & BG:CB = AM:MC [44] & HC:KC = CL:CE [93] (Ratio chase)  CG:CL = KC:MC [96]
071. CI  CL [88] & LN  KM [22]   (CI-NL) = (CL-KM) [97]
072. CI  CL [88] & LN  KM [22]   (CI-KM) = CLN [98]
073. H,M,G are collinear [19] & H,G,C are collinear [07] & C,G,E are collinear [05] & K,M,N are collinear [21] & (CI-NL) = (CL-KM) [97] & H,I,E are collinear [10] & HCK = LCE [94]   KCM = LNK [99]
074. DP  BL [76] & H,P,B are collinear [25] & H,D,L are collinear [18]   HB:HP = BL:PD [100]
075. HB:HP = BL:PD [100] & BH = HA [38] & PH = PB [26]   AH:PB = BL:DP [101]
076. H,P,B are collinear [25] & HBL = HAL [72] & BL  DP [76]   BPD = HAL [102]
077. DC = DB [00] & DH = DC [08]   D is the circumcenter of \Delta CBH [103]
078. D is the circumcenter of \Delta CBH [103] & P is midpoint of HB [47]   BCH = BDP [104]
079. H,D,L are collinear [18] & CHL = CAL [67] & H,C,G are collinear [07] & C,G,E are collinear [05] & BCE = ECA [02] & BCH = BDP [104] & DP  BL [76]   BDP = HLA [105]
080. BPD = HAL [102] & BDP = HLA [105] (Similar Triangles)  PB:DP = AH:AL [106]
081. D,I,J are collinear [12] & A,B,E are collinear [03] & BHL = BAL [71] & H,D,L are collinear [18] & HDJ = HPJ [80] & H,P,B are collinear [25]   PJI = EAL [107]
082. D,I,J are collinear [12] & A,B,E are collinear [03] & IP  EB [48] & DI  EL [56]   PIJ = AEL [108]
083. PJI = EAL [107] & PIJ = AEL [108] (Similar Triangles)  PJ:PI = AL:LE [109]
084. PH = PB [26] & BH = HA [38] & AH:PB = BL:DP [101] & PB:DP = AH:AL [106] & HJ:HI = CL:LE [58] & CB:AH = PI:HI [62] & PJ:PI = AL:LE [109] & PJ:HJ = KB:AH [86] (Ratio chase)  CB:CL = KB:BL [110]
085. H,P,B are collinear [25] & BL  BH [75]   (HP-BL) = (BL-HP) [111]
086. H,P,B are collinear [25] & K,A,B are collinear [15] & (BL-HP) = (HD-KA) [74]   (BL-HD) = (HP-KA) [112]
087. (HP-BL) = (BL-HP) [111] & (BL-HD) = (HP-KA) [112]   PHD = (BL-KA) [113]
088. H,P,B are collinear [25] & K,A,B are collinear [15] & PHD = (BL-KA) [113]   (HP-BL) = (HD-KA) [114]
089. H,P,B are collinear [25] & CBH = CLH [69] & H,D,L are collinear [18]   (HP-CB) = (HD-CL) [115]
090. (HP-BL) = (HD-KA) [114] & (HP-CB) = (HD-CL) [115]   LBC = (KA-CL) [116]
091. K,A,B are collinear [15] & LBC = (KA-CL) [116]   KBL = LCB [117]
092. BK:BL = CB:CL [110] & KBL = LCB [117] (Similar Triangles)  KL  CL [118]
093. H,M,G are collinear [19] & H,G,C are collinear [07] & C,G,E are collinear [05] & (CI-KM) = CLN [98] & H,I,E are collinear [10] & CL  KL [118]   KMC = NLK [119]
094. KCM = LNK [99] & KMC = NLK [119] (Similar Triangles)  KC:MC = KN:NL [120]
095. CG:CL = KC:MC [96] & KC:MC = KN:NL [120]   NK:NL = CG:CL [121]
096. KL  CL [118]   K,C,L are collinear [122]
097. K,H,J are collinear [16] & HJ  DI [13]   KH  DI [123]
098. KH  DI [123] & LN  KM [22]   HKM = (DI-NL) [124]
099. K,M,N are collinear [21] & K,C,L are collinear [122] & C,G,E are collinear [05] & CKH = CEL [95] & K,H,J are collinear [16] & DI  EL [56] & HKM = (DI-NL) [124]   KNL = LCG [125]
100. NK:NL = CG:CL [121] & KNL = LCG [125] (Similar Triangles)  KLN = CLG [126]
101. KLN = CLG [126] & K,C,L are collinear [122]   LN  GL [127]
102. LN  GL [127]   N,G,L are collinear [128]
103. H,I,E are collinear [10] & H,C,G are collinear [07] & C,G,E are collinear [05] & (CI-LE) = (CL-KH) [91] & K,H,J are collinear [16] & DI  EL [56] & CKH = CEL [95]   KCI = LCI [129]
104. KCI = LCI [129]   KC  CL [130]
105. H,M,G are collinear [19] & H,G,C are collinear [07] & C,G,E are collinear [05] & K,C,L are collinear [122] & HCK = LCE [94] & CL  CK [130]   KCM = GCL [131]
106. CG:CL = KC:MC [96] & KCM = GCL [131] (Similar Triangles)  KMC = GLC [132]
107. CG:CL = KC:MC [96] & KCM = GCL [131] (Similar Triangles)  CKM = CGL [133]
108. H,G,M are collinear [19] & HG = HM [20]   H is midpoint of GM [134]
109. O,N,G are collinear [23] & OG = ON [24]   O is midpoint of GN [135]
110. H is midpoint of GM [134] & O is midpoint of GN [135]   HO  MN [136]
111. N,G,L are collinear [128] & O,N,G are collinear [23] & KMC = GLC [132] & H,M,G are collinear [19] & H,G,C are collinear [07] & C,G,E are collinear [05] & K,C,L are collinear [122] & CK  CL [130] & CKM = CGL [133] & HO  MN [136] & K,M,N are collinear [21]   HO  OL [137]
112. HO  OL [137] & D is midpoint of HL [55]   HD = OD [138]
113. HD = OD [138] & DH = DC [08]   DC = DO
==========================

I0123 14:32:32.572197 140227981185024 alphageometry.py:582] Solved.
