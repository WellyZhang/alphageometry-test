I0123 11:19:30.136296 140345384898560 inference_utils.py:69] Parsing gin configuration.
I0123 11:19:30.136392 140345384898560 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0123 11:19:30.136591 140345384898560 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0123 11:19:30.136623 140345384898560 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0123 11:19:30.136651 140345384898560 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0123 11:19:30.136679 140345384898560 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0123 11:19:30.136704 140345384898560 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0123 11:19:30.136729 140345384898560 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0123 11:19:30.136755 140345384898560 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0123 11:19:30.136782 140345384898560 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=32
I0123 11:19:30.136807 140345384898560 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0123 11:19:30.136832 140345384898560 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0123 11:19:30.136878 140345384898560 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0123 11:19:30.137012 140345384898560 resource_reader.py:55] Path not found: base_htrans.gin
I0123 11:19:30.137213 140345384898560 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0123 11:19:30.137310 140345384898560 resource_reader.py:55] Path not found: trainer_configuration.gin
I0123 11:19:30.143559 140345384898560 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0123 11:19:30.143676 140345384898560 resource_reader.py:55] Path not found: size/medium_150M.gin
I0123 11:19:30.144002 140345384898560 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0123 11:19:30.144106 140345384898560 resource_reader.py:55] Path not found: options/positions_t5.gin
I0123 11:19:30.144391 140345384898560 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0123 11:19:30.144491 140345384898560 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0123 11:19:30.144903 140345384898560 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0123 11:19:30.145002 140345384898560 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0123 11:19:30.148654 140345384898560 training_loop.py:334] ==== Training loop: initializing model ====
I0123 11:19:30.242244 140345384898560 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0123 11:19:30.243981 140345384898560 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0123 11:19:30.250566 140345384898560 training_loop.py:335] Process 0 of 1
I0123 11:19:30.250621 140345384898560 training_loop.py:336] Local device count = 1
I0123 11:19:30.250660 140345384898560 training_loop.py:337] Number of replicas = 1
I0123 11:19:30.250695 140345384898560 training_loop.py:339] Using random number seed 42
I0123 11:19:30.718054 140345384898560 training_loop.py:359] Initializing the model.
I0123 11:19:31.089998 140345384898560 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.090284 140345384898560 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 11:19:31.090396 140345384898560 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:19:31.090481 140345384898560 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:19:31.090562 140345384898560 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:19:31.090650 140345384898560 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:19:31.090727 140345384898560 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:19:31.090802 140345384898560 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:19:31.090878 140345384898560 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:19:31.090952 140345384898560 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:19:31.091025 140345384898560 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:19:31.091097 140345384898560 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:19:31.091168 140345384898560 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:19:31.091238 140345384898560 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 11:19:31.091279 140345384898560 attention.py:418] Single window, no scan.
I0123 11:19:31.091326 140345384898560 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 11:19:31.091446 140345384898560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:19:31.091487 140345384898560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:19:31.091519 140345384898560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:19:31.093569 140345384898560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.098940 140345384898560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:19:31.109714 140345384898560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.109999 140345384898560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:19:31.114392 140345384898560 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:19:31.125127 140345384898560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:19:31.125186 140345384898560 attention.py:418] Single window, no scan.
I0123 11:19:31.125225 140345384898560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:19:31.125260 140345384898560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.125324 140345384898560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.126522 140345384898560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.126603 140345384898560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.127321 140345384898560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.129792 140345384898560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.136029 140345384898560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.137357 140345384898560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.137438 140345384898560 transformer_layer.py:468] tlayer: End windows.
I0123 11:19:31.137474 140345384898560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:19:31.137536 140345384898560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.137676 140345384898560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:19:31.138016 140345384898560 nn_components.py:325] mlp: activation = None
I0123 11:19:31.138065 140345384898560 nn_components.py:329] mlp: final activation = None
I0123 11:19:31.139994 140345384898560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.140094 140345384898560 nn_components.py:261] mlp: residual
I0123 11:19:31.143012 140345384898560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.143092 140345384898560 transformer_base.py:443] tbase: final FFN
I0123 11:19:31.143594 140345384898560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:19:31.153910 140345384898560 nn_components.py:329] mlp: final activation = None
I0123 11:19:31.162968 140345384898560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.163069 140345384898560 nn_components.py:261] mlp: residual
I0123 11:19:31.163373 140345384898560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.163456 140345384898560 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 11:19:31.163567 140345384898560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:19:31.163607 140345384898560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:19:31.163640 140345384898560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:19:31.165523 140345384898560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.168077 140345384898560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:19:31.173781 140345384898560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.174048 140345384898560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:19:31.176744 140345384898560 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:19:31.180621 140345384898560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:19:31.180676 140345384898560 attention.py:418] Single window, no scan.
I0123 11:19:31.180713 140345384898560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:19:31.180745 140345384898560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.180808 140345384898560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.181389 140345384898560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.181466 140345384898560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.181842 140345384898560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.182632 140345384898560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.185169 140345384898560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.185812 140345384898560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.185891 140345384898560 transformer_layer.py:468] tlayer: End windows.
I0123 11:19:31.185926 140345384898560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:19:31.185988 140345384898560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.186115 140345384898560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:19:31.186442 140345384898560 nn_components.py:325] mlp: activation = None
I0123 11:19:31.186485 140345384898560 nn_components.py:329] mlp: final activation = None
I0123 11:19:31.188441 140345384898560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.188537 140345384898560 nn_components.py:261] mlp: residual
I0123 11:19:31.191087 140345384898560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.191168 140345384898560 transformer_base.py:443] tbase: final FFN
I0123 11:19:31.191607 140345384898560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:19:31.193946 140345384898560 nn_components.py:329] mlp: final activation = None
I0123 11:19:31.195949 140345384898560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.196048 140345384898560 nn_components.py:261] mlp: residual
I0123 11:19:31.196352 140345384898560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.196432 140345384898560 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 11:19:31.196542 140345384898560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:19:31.196580 140345384898560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:19:31.196612 140345384898560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:19:31.198880 140345384898560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.201270 140345384898560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:19:31.206936 140345384898560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.207203 140345384898560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:19:31.209877 140345384898560 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:19:31.213765 140345384898560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:19:31.213821 140345384898560 attention.py:418] Single window, no scan.
I0123 11:19:31.213858 140345384898560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:19:31.213891 140345384898560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.213953 140345384898560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.214518 140345384898560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.214594 140345384898560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.214956 140345384898560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.215724 140345384898560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.218303 140345384898560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.218977 140345384898560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.219056 140345384898560 transformer_layer.py:468] tlayer: End windows.
I0123 11:19:31.219093 140345384898560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:19:31.219153 140345384898560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.219280 140345384898560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:19:31.219597 140345384898560 nn_components.py:325] mlp: activation = None
I0123 11:19:31.219640 140345384898560 nn_components.py:329] mlp: final activation = None
I0123 11:19:31.221589 140345384898560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.221690 140345384898560 nn_components.py:261] mlp: residual
I0123 11:19:31.224235 140345384898560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.224324 140345384898560 transformer_base.py:443] tbase: final FFN
I0123 11:19:31.224814 140345384898560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:19:31.227133 140345384898560 nn_components.py:329] mlp: final activation = None
I0123 11:19:31.229067 140345384898560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.229161 140345384898560 nn_components.py:261] mlp: residual
I0123 11:19:31.229458 140345384898560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.229539 140345384898560 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 11:19:31.229655 140345384898560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:19:31.229696 140345384898560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:19:31.229727 140345384898560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:19:31.231645 140345384898560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.234095 140345384898560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:19:31.239811 140345384898560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.240076 140345384898560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:19:31.242769 140345384898560 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:19:31.246628 140345384898560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:19:31.246683 140345384898560 attention.py:418] Single window, no scan.
I0123 11:19:31.246720 140345384898560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:19:31.246752 140345384898560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.246814 140345384898560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.247380 140345384898560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.247456 140345384898560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.247825 140345384898560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.248607 140345384898560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.251258 140345384898560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.251935 140345384898560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.252015 140345384898560 transformer_layer.py:468] tlayer: End windows.
I0123 11:19:31.252053 140345384898560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:19:31.252112 140345384898560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.252244 140345384898560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:19:31.252563 140345384898560 nn_components.py:325] mlp: activation = None
I0123 11:19:31.252606 140345384898560 nn_components.py:329] mlp: final activation = None
I0123 11:19:31.254542 140345384898560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.254635 140345384898560 nn_components.py:261] mlp: residual
I0123 11:19:31.257253 140345384898560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.257339 140345384898560 transformer_base.py:443] tbase: final FFN
I0123 11:19:31.257778 140345384898560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:19:31.260069 140345384898560 nn_components.py:329] mlp: final activation = None
I0123 11:19:31.262001 140345384898560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.262096 140345384898560 nn_components.py:261] mlp: residual
I0123 11:19:31.262391 140345384898560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.262472 140345384898560 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 11:19:31.262581 140345384898560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:19:31.262619 140345384898560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:19:31.262651 140345384898560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:19:31.264574 140345384898560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.266996 140345384898560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:19:31.272750 140345384898560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.273015 140345384898560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:19:31.276091 140345384898560 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:19:31.279872 140345384898560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:19:31.279929 140345384898560 attention.py:418] Single window, no scan.
I0123 11:19:31.279966 140345384898560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:19:31.279999 140345384898560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.280061 140345384898560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.280639 140345384898560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.280715 140345384898560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.281078 140345384898560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.281862 140345384898560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.284466 140345384898560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.285100 140345384898560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.285178 140345384898560 transformer_layer.py:468] tlayer: End windows.
I0123 11:19:31.285214 140345384898560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:19:31.285273 140345384898560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.285409 140345384898560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:19:31.285745 140345384898560 nn_components.py:325] mlp: activation = None
I0123 11:19:31.285790 140345384898560 nn_components.py:329] mlp: final activation = None
I0123 11:19:31.287712 140345384898560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.287805 140345384898560 nn_components.py:261] mlp: residual
I0123 11:19:31.290409 140345384898560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.290488 140345384898560 transformer_base.py:443] tbase: final FFN
I0123 11:19:31.290925 140345384898560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:19:31.293228 140345384898560 nn_components.py:329] mlp: final activation = None
I0123 11:19:31.295216 140345384898560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.295311 140345384898560 nn_components.py:261] mlp: residual
I0123 11:19:31.295707 140345384898560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.295789 140345384898560 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 11:19:31.295898 140345384898560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:19:31.295938 140345384898560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:19:31.295969 140345384898560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:19:31.297991 140345384898560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.300423 140345384898560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:19:31.307054 140345384898560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.307378 140345384898560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:19:31.310119 140345384898560 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:19:31.313930 140345384898560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:19:31.313991 140345384898560 attention.py:418] Single window, no scan.
I0123 11:19:31.314028 140345384898560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:19:31.314061 140345384898560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.314124 140345384898560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.314752 140345384898560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.314831 140345384898560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.315200 140345384898560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.315985 140345384898560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.318553 140345384898560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.319181 140345384898560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.319259 140345384898560 transformer_layer.py:468] tlayer: End windows.
I0123 11:19:31.319294 140345384898560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:19:31.319353 140345384898560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.319481 140345384898560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:19:31.319807 140345384898560 nn_components.py:325] mlp: activation = None
I0123 11:19:31.319849 140345384898560 nn_components.py:329] mlp: final activation = None
I0123 11:19:31.321785 140345384898560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.321879 140345384898560 nn_components.py:261] mlp: residual
I0123 11:19:31.324504 140345384898560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.324586 140345384898560 transformer_base.py:443] tbase: final FFN
I0123 11:19:31.325029 140345384898560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:19:31.327407 140345384898560 nn_components.py:329] mlp: final activation = None
I0123 11:19:31.329367 140345384898560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.329461 140345384898560 nn_components.py:261] mlp: residual
I0123 11:19:31.329773 140345384898560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.329856 140345384898560 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 11:19:31.329967 140345384898560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:19:31.330008 140345384898560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:19:31.330039 140345384898560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:19:31.331904 140345384898560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.334392 140345384898560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:19:31.340476 140345384898560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.340745 140345384898560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:19:31.343634 140345384898560 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:19:31.347795 140345384898560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:19:31.347851 140345384898560 attention.py:418] Single window, no scan.
I0123 11:19:31.347888 140345384898560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:19:31.347921 140345384898560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.347983 140345384898560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.348545 140345384898560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.348624 140345384898560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.348985 140345384898560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.349781 140345384898560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.352334 140345384898560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.352967 140345384898560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.353044 140345384898560 transformer_layer.py:468] tlayer: End windows.
I0123 11:19:31.353080 140345384898560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:19:31.353140 140345384898560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.353271 140345384898560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:19:31.353594 140345384898560 nn_components.py:325] mlp: activation = None
I0123 11:19:31.353638 140345384898560 nn_components.py:329] mlp: final activation = None
I0123 11:19:31.355962 140345384898560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.356057 140345384898560 nn_components.py:261] mlp: residual
I0123 11:19:31.358639 140345384898560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.358719 140345384898560 transformer_base.py:443] tbase: final FFN
I0123 11:19:31.359156 140345384898560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:19:31.499992 140345384898560 nn_components.py:329] mlp: final activation = None
I0123 11:19:31.502196 140345384898560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.502344 140345384898560 nn_components.py:261] mlp: residual
I0123 11:19:31.502666 140345384898560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.502758 140345384898560 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 11:19:31.502873 140345384898560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:19:31.502915 140345384898560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:19:31.502948 140345384898560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:19:31.504992 140345384898560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.507521 140345384898560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:19:31.513351 140345384898560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.513635 140345384898560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:19:31.516463 140345384898560 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:19:31.520454 140345384898560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:19:31.520512 140345384898560 attention.py:418] Single window, no scan.
I0123 11:19:31.520550 140345384898560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:19:31.520583 140345384898560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.520645 140345384898560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.521257 140345384898560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.521334 140345384898560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.521713 140345384898560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.522509 140345384898560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.525134 140345384898560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.525790 140345384898560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.525870 140345384898560 transformer_layer.py:468] tlayer: End windows.
I0123 11:19:31.525906 140345384898560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:19:31.525971 140345384898560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.526101 140345384898560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:19:31.526424 140345384898560 nn_components.py:325] mlp: activation = None
I0123 11:19:31.526467 140345384898560 nn_components.py:329] mlp: final activation = None
I0123 11:19:31.528382 140345384898560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.528474 140345384898560 nn_components.py:261] mlp: residual
I0123 11:19:31.531022 140345384898560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.531102 140345384898560 transformer_base.py:443] tbase: final FFN
I0123 11:19:31.531587 140345384898560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:19:31.533886 140345384898560 nn_components.py:329] mlp: final activation = None
I0123 11:19:31.535814 140345384898560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.535913 140345384898560 nn_components.py:261] mlp: residual
I0123 11:19:31.536211 140345384898560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.536293 140345384898560 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 11:19:31.536404 140345384898560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:19:31.536443 140345384898560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:19:31.536475 140345384898560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:19:31.538422 140345384898560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.540972 140345384898560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:19:31.546684 140345384898560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.546951 140345384898560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:19:31.549674 140345384898560 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:19:31.553498 140345384898560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:19:31.553553 140345384898560 attention.py:418] Single window, no scan.
I0123 11:19:31.553590 140345384898560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:19:31.553623 140345384898560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.553697 140345384898560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.554273 140345384898560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.554348 140345384898560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.554719 140345384898560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.555502 140345384898560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.558128 140345384898560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.558752 140345384898560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.558833 140345384898560 transformer_layer.py:468] tlayer: End windows.
I0123 11:19:31.558868 140345384898560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:19:31.558927 140345384898560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.559053 140345384898560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:19:31.559367 140345384898560 nn_components.py:325] mlp: activation = None
I0123 11:19:31.559409 140345384898560 nn_components.py:329] mlp: final activation = None
I0123 11:19:31.561329 140345384898560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.561421 140345384898560 nn_components.py:261] mlp: residual
I0123 11:19:31.563996 140345384898560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.564076 140345384898560 transformer_base.py:443] tbase: final FFN
I0123 11:19:31.564511 140345384898560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:19:31.566807 140345384898560 nn_components.py:329] mlp: final activation = None
I0123 11:19:31.568737 140345384898560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.568832 140345384898560 nn_components.py:261] mlp: residual
I0123 11:19:31.569125 140345384898560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.569211 140345384898560 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 11:19:31.569321 140345384898560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:19:31.569360 140345384898560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:19:31.569391 140345384898560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:19:31.571325 140345384898560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.573734 140345384898560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:19:31.579854 140345384898560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.580114 140345384898560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:19:31.582866 140345384898560 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:19:31.586662 140345384898560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:19:31.586718 140345384898560 attention.py:418] Single window, no scan.
I0123 11:19:31.586755 140345384898560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:19:31.586787 140345384898560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.586849 140345384898560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.587413 140345384898560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.587493 140345384898560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.587858 140345384898560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.588680 140345384898560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.591227 140345384898560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.591857 140345384898560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.591933 140345384898560 transformer_layer.py:468] tlayer: End windows.
I0123 11:19:31.591968 140345384898560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:19:31.592027 140345384898560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.592157 140345384898560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:19:31.592472 140345384898560 nn_components.py:325] mlp: activation = None
I0123 11:19:31.592516 140345384898560 nn_components.py:329] mlp: final activation = None
I0123 11:19:31.594430 140345384898560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.594526 140345384898560 nn_components.py:261] mlp: residual
I0123 11:19:31.597104 140345384898560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.597183 140345384898560 transformer_base.py:443] tbase: final FFN
I0123 11:19:31.597614 140345384898560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:19:31.599909 140345384898560 nn_components.py:329] mlp: final activation = None
I0123 11:19:31.601911 140345384898560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.602007 140345384898560 nn_components.py:261] mlp: residual
I0123 11:19:31.602303 140345384898560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.602389 140345384898560 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 11:19:31.602500 140345384898560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:19:31.602540 140345384898560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:19:31.602572 140345384898560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:19:31.604410 140345384898560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.606857 140345384898560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:19:31.612448 140345384898560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.612709 140345384898560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:19:31.615432 140345384898560 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:19:31.619180 140345384898560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:19:31.619236 140345384898560 attention.py:418] Single window, no scan.
I0123 11:19:31.619277 140345384898560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:19:31.619310 140345384898560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.619415 140345384898560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.619987 140345384898560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.620064 140345384898560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.620429 140345384898560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.621217 140345384898560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.623754 140345384898560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.624392 140345384898560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.624469 140345384898560 transformer_layer.py:468] tlayer: End windows.
I0123 11:19:31.624505 140345384898560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:19:31.624565 140345384898560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.624695 140345384898560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:19:31.625013 140345384898560 nn_components.py:325] mlp: activation = None
I0123 11:19:31.625056 140345384898560 nn_components.py:329] mlp: final activation = None
I0123 11:19:31.627039 140345384898560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.627136 140345384898560 nn_components.py:261] mlp: residual
I0123 11:19:31.629925 140345384898560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.630005 140345384898560 transformer_base.py:443] tbase: final FFN
I0123 11:19:31.630441 140345384898560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:19:31.632783 140345384898560 nn_components.py:329] mlp: final activation = None
I0123 11:19:31.634720 140345384898560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.634818 140345384898560 nn_components.py:261] mlp: residual
I0123 11:19:31.635114 140345384898560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.635195 140345384898560 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 11:19:31.635311 140345384898560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:19:31.635352 140345384898560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:19:31.635384 140345384898560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:19:31.637239 140345384898560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.639723 140345384898560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:19:31.645430 140345384898560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.645708 140345384898560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:19:31.648370 140345384898560 transformer_layer.py:213] tlayer: windowed attention.
I0123 11:19:31.652528 140345384898560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:19:31.652585 140345384898560 attention.py:418] Single window, no scan.
I0123 11:19:31.652621 140345384898560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:19:31.652654 140345384898560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.652715 140345384898560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.653289 140345384898560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.653366 140345384898560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.653739 140345384898560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.654518 140345384898560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.657021 140345384898560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.657661 140345384898560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.657741 140345384898560 transformer_layer.py:468] tlayer: End windows.
I0123 11:19:31.657777 140345384898560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:19:31.657838 140345384898560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.657967 140345384898560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:19:31.658291 140345384898560 nn_components.py:325] mlp: activation = None
I0123 11:19:31.658334 140345384898560 nn_components.py:329] mlp: final activation = None
I0123 11:19:31.660295 140345384898560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.660388 140345384898560 nn_components.py:261] mlp: residual
I0123 11:19:31.662933 140345384898560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.663013 140345384898560 transformer_base.py:443] tbase: final FFN
I0123 11:19:31.663448 140345384898560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:19:31.665775 140345384898560 nn_components.py:329] mlp: final activation = None
I0123 11:19:31.667707 140345384898560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.667802 140345384898560 nn_components.py:261] mlp: residual
I0123 11:19:31.668101 140345384898560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.668385 140345384898560 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:19:31.668456 140345384898560 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:19:31.668524 140345384898560 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:19:31.668582 140345384898560 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:19:31.668638 140345384898560 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:19:31.668692 140345384898560 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:19:31.668746 140345384898560 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:19:31.668800 140345384898560 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:19:31.668853 140345384898560 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:19:31.668905 140345384898560 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:19:31.668958 140345384898560 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:19:31.669013 140345384898560 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 11:19:31.669051 140345384898560 decoder_stack.py:344] dstack: Final layernorm.
I0123 11:19:31.672641 140345384898560 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 11:19:31.721102 140345384898560 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.721187 140345384898560 decoder_stack.py:333] dstack: autoregressive generator.
I0123 11:19:31.721242 140345384898560 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 11:19:31.721347 140345384898560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:19:31.721385 140345384898560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:19:31.721416 140345384898560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:19:31.721479 140345384898560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.723957 140345384898560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:19:31.729533 140345384898560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.729800 140345384898560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:19:31.732476 140345384898560 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:19:31.749233 140345384898560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:19:31.749289 140345384898560 attention.py:418] Single window, no scan.
I0123 11:19:31.749325 140345384898560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:19:31.749359 140345384898560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.749420 140345384898560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.750562 140345384898560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.750645 140345384898560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.751358 140345384898560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.753381 140345384898560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.758239 140345384898560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.759566 140345384898560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.759650 140345384898560 transformer_layer.py:468] tlayer: End windows.
I0123 11:19:31.759687 140345384898560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:19:31.759749 140345384898560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.759880 140345384898560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:19:31.759989 140345384898560 nn_components.py:325] mlp: activation = None
I0123 11:19:31.760028 140345384898560 nn_components.py:329] mlp: final activation = None
I0123 11:19:31.761971 140345384898560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.762068 140345384898560 nn_components.py:261] mlp: residual
I0123 11:19:31.764557 140345384898560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.764635 140345384898560 transformer_base.py:443] tbase: final FFN
I0123 11:19:31.764745 140345384898560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:19:31.767039 140345384898560 nn_components.py:329] mlp: final activation = None
I0123 11:19:31.769024 140345384898560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.769119 140345384898560 nn_components.py:261] mlp: residual
I0123 11:19:31.769417 140345384898560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.769499 140345384898560 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 11:19:31.769610 140345384898560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:19:31.769655 140345384898560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:19:31.769689 140345384898560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:19:31.769755 140345384898560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.772050 140345384898560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:19:31.777845 140345384898560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.778109 140345384898560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:19:31.781016 140345384898560 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:19:31.794365 140345384898560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:19:31.794421 140345384898560 attention.py:418] Single window, no scan.
I0123 11:19:31.794458 140345384898560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:19:31.794489 140345384898560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.794552 140345384898560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.795118 140345384898560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.795194 140345384898560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.795563 140345384898560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.796268 140345384898560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.798819 140345384898560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.799444 140345384898560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.799525 140345384898560 transformer_layer.py:468] tlayer: End windows.
I0123 11:19:31.799570 140345384898560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:19:31.799632 140345384898560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.799766 140345384898560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:19:31.799876 140345384898560 nn_components.py:325] mlp: activation = None
I0123 11:19:31.799915 140345384898560 nn_components.py:329] mlp: final activation = None
I0123 11:19:31.801872 140345384898560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.801967 140345384898560 nn_components.py:261] mlp: residual
I0123 11:19:31.804409 140345384898560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.804488 140345384898560 transformer_base.py:443] tbase: final FFN
I0123 11:19:31.804596 140345384898560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:19:31.806861 140345384898560 nn_components.py:329] mlp: final activation = None
I0123 11:19:31.808806 140345384898560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.808902 140345384898560 nn_components.py:261] mlp: residual
I0123 11:19:31.809195 140345384898560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.809275 140345384898560 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 11:19:31.809383 140345384898560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:19:31.809422 140345384898560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:19:31.809453 140345384898560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:19:31.809517 140345384898560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.811786 140345384898560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:19:31.817311 140345384898560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.817573 140345384898560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:19:31.820296 140345384898560 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:19:31.833176 140345384898560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:19:31.833232 140345384898560 attention.py:418] Single window, no scan.
I0123 11:19:31.833268 140345384898560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:19:31.833300 140345384898560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.833363 140345384898560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.833934 140345384898560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.834014 140345384898560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.834377 140345384898560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.835086 140345384898560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.837604 140345384898560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.838241 140345384898560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.838319 140345384898560 transformer_layer.py:468] tlayer: End windows.
I0123 11:19:31.838355 140345384898560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:19:31.838422 140345384898560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.838553 140345384898560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:19:31.838662 140345384898560 nn_components.py:325] mlp: activation = None
I0123 11:19:31.838701 140345384898560 nn_components.py:329] mlp: final activation = None
I0123 11:19:31.840661 140345384898560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.840755 140345384898560 nn_components.py:261] mlp: residual
I0123 11:19:31.843251 140345384898560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.843331 140345384898560 transformer_base.py:443] tbase: final FFN
I0123 11:19:31.843441 140345384898560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:19:31.845702 140345384898560 nn_components.py:329] mlp: final activation = None
I0123 11:19:31.847670 140345384898560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.847764 140345384898560 nn_components.py:261] mlp: residual
I0123 11:19:31.848058 140345384898560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.848139 140345384898560 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 11:19:31.848248 140345384898560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:19:31.848286 140345384898560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:19:31.848319 140345384898560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:19:31.848383 140345384898560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.850685 140345384898560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:19:31.856281 140345384898560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.856543 140345384898560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:19:31.859290 140345384898560 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:19:31.872385 140345384898560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:19:31.872440 140345384898560 attention.py:418] Single window, no scan.
I0123 11:19:31.872475 140345384898560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:19:31.872506 140345384898560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.872570 140345384898560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.873130 140345384898560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.873207 140345384898560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.873569 140345384898560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.874284 140345384898560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.876825 140345384898560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.877448 140345384898560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.877526 140345384898560 transformer_layer.py:468] tlayer: End windows.
I0123 11:19:31.877560 140345384898560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:19:31.877620 140345384898560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.877768 140345384898560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:19:31.877878 140345384898560 nn_components.py:325] mlp: activation = None
I0123 11:19:31.877916 140345384898560 nn_components.py:329] mlp: final activation = None
I0123 11:19:31.880398 140345384898560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.880493 140345384898560 nn_components.py:261] mlp: residual
I0123 11:19:31.882981 140345384898560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.883061 140345384898560 transformer_base.py:443] tbase: final FFN
I0123 11:19:31.883171 140345384898560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:19:31.885441 140345384898560 nn_components.py:329] mlp: final activation = None
I0123 11:19:31.887505 140345384898560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.887602 140345384898560 nn_components.py:261] mlp: residual
I0123 11:19:31.887897 140345384898560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.887979 140345384898560 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 11:19:31.888088 140345384898560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:19:31.888128 140345384898560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:19:31.888159 140345384898560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:19:31.888222 140345384898560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.890607 140345384898560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:19:31.896233 140345384898560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.896505 140345384898560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:19:31.899216 140345384898560 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:19:31.912200 140345384898560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:19:31.912256 140345384898560 attention.py:418] Single window, no scan.
I0123 11:19:31.912292 140345384898560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:19:31.912323 140345384898560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.912384 140345384898560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.912945 140345384898560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.913021 140345384898560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.913383 140345384898560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.914099 140345384898560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.916698 140345384898560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.917324 140345384898560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.917401 140345384898560 transformer_layer.py:468] tlayer: End windows.
I0123 11:19:31.917436 140345384898560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:19:31.917495 140345384898560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.917647 140345384898560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:19:31.917760 140345384898560 nn_components.py:325] mlp: activation = None
I0123 11:19:31.917799 140345384898560 nn_components.py:329] mlp: final activation = None
I0123 11:19:31.919709 140345384898560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.919803 140345384898560 nn_components.py:261] mlp: residual
I0123 11:19:31.922257 140345384898560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.922337 140345384898560 transformer_base.py:443] tbase: final FFN
I0123 11:19:31.922445 140345384898560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:19:31.924755 140345384898560 nn_components.py:329] mlp: final activation = None
I0123 11:19:31.926644 140345384898560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.926740 140345384898560 nn_components.py:261] mlp: residual
I0123 11:19:31.927031 140345384898560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.927114 140345384898560 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 11:19:31.927223 140345384898560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:19:31.927262 140345384898560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:19:31.927294 140345384898560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:19:31.927358 140345384898560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.929639 140345384898560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:19:31.935175 140345384898560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.935438 140345384898560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:19:31.938184 140345384898560 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:19:31.951030 140345384898560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:19:31.951085 140345384898560 attention.py:418] Single window, no scan.
I0123 11:19:31.951122 140345384898560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:19:31.951154 140345384898560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.951216 140345384898560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.951788 140345384898560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.951867 140345384898560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.952232 140345384898560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.952937 140345384898560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.955515 140345384898560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.956141 140345384898560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.956222 140345384898560 transformer_layer.py:468] tlayer: End windows.
I0123 11:19:31.956258 140345384898560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:19:31.956321 140345384898560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.956451 140345384898560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:19:31.956566 140345384898560 nn_components.py:325] mlp: activation = None
I0123 11:19:31.956606 140345384898560 nn_components.py:329] mlp: final activation = None
I0123 11:19:31.962688 140345384898560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.962828 140345384898560 nn_components.py:261] mlp: residual
I0123 11:19:31.965436 140345384898560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.965515 140345384898560 transformer_base.py:443] tbase: final FFN
I0123 11:19:31.965633 140345384898560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:19:31.967973 140345384898560 nn_components.py:329] mlp: final activation = None
I0123 11:19:31.969897 140345384898560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.969992 140345384898560 nn_components.py:261] mlp: residual
I0123 11:19:31.970282 140345384898560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.970371 140345384898560 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 11:19:31.970482 140345384898560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:19:31.970524 140345384898560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:19:31.970557 140345384898560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:19:31.970627 140345384898560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.972932 140345384898560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:19:31.978590 140345384898560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.978856 140345384898560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:19:31.981526 140345384898560 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:19:31.995178 140345384898560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:19:31.995237 140345384898560 attention.py:418] Single window, no scan.
I0123 11:19:31.995274 140345384898560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:19:31.995306 140345384898560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.995372 140345384898560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.995969 140345384898560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.996045 140345384898560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.996420 140345384898560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.997125 140345384898560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:31.999707 140345384898560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.000393 140345384898560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.000472 140345384898560 transformer_layer.py:468] tlayer: End windows.
I0123 11:19:32.000507 140345384898560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:19:32.000567 140345384898560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.000699 140345384898560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:19:32.000808 140345384898560 nn_components.py:325] mlp: activation = None
I0123 11:19:32.000851 140345384898560 nn_components.py:329] mlp: final activation = None
I0123 11:19:32.002777 140345384898560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.002871 140345384898560 nn_components.py:261] mlp: residual
I0123 11:19:32.005326 140345384898560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.005404 140345384898560 transformer_base.py:443] tbase: final FFN
I0123 11:19:32.005511 140345384898560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:19:32.007765 140345384898560 nn_components.py:329] mlp: final activation = None
I0123 11:19:32.009717 140345384898560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.009813 140345384898560 nn_components.py:261] mlp: residual
I0123 11:19:32.010105 140345384898560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.010185 140345384898560 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 11:19:32.010293 140345384898560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:19:32.010332 140345384898560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:19:32.010364 140345384898560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:19:32.010426 140345384898560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.012691 140345384898560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:19:32.018223 140345384898560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.018496 140345384898560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:19:32.021217 140345384898560 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:19:32.034106 140345384898560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:19:32.034162 140345384898560 attention.py:418] Single window, no scan.
I0123 11:19:32.034202 140345384898560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:19:32.034234 140345384898560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.034296 140345384898560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.034903 140345384898560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.034979 140345384898560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.035343 140345384898560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.036054 140345384898560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.038606 140345384898560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.039235 140345384898560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.039313 140345384898560 transformer_layer.py:468] tlayer: End windows.
I0123 11:19:32.039349 140345384898560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:19:32.039408 140345384898560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.039541 140345384898560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:19:32.039653 140345384898560 nn_components.py:325] mlp: activation = None
I0123 11:19:32.039697 140345384898560 nn_components.py:329] mlp: final activation = None
I0123 11:19:32.041628 140345384898560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.041731 140345384898560 nn_components.py:261] mlp: residual
I0123 11:19:32.044255 140345384898560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.044336 140345384898560 transformer_base.py:443] tbase: final FFN
I0123 11:19:32.044447 140345384898560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:19:32.046738 140345384898560 nn_components.py:329] mlp: final activation = None
I0123 11:19:32.048646 140345384898560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.048740 140345384898560 nn_components.py:261] mlp: residual
I0123 11:19:32.049032 140345384898560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.049113 140345384898560 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 11:19:32.049222 140345384898560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:19:32.049261 140345384898560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:19:32.049294 140345384898560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:19:32.049358 140345384898560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.051649 140345384898560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:19:32.057269 140345384898560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.057534 140345384898560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:19:32.060220 140345384898560 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:19:32.073201 140345384898560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:19:32.073257 140345384898560 attention.py:418] Single window, no scan.
I0123 11:19:32.073294 140345384898560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:19:32.073327 140345384898560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.073389 140345384898560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.073965 140345384898560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.074043 140345384898560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.074409 140345384898560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.075111 140345384898560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.077686 140345384898560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.078370 140345384898560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.078448 140345384898560 transformer_layer.py:468] tlayer: End windows.
I0123 11:19:32.078483 140345384898560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:19:32.078543 140345384898560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.078677 140345384898560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:19:32.078788 140345384898560 nn_components.py:325] mlp: activation = None
I0123 11:19:32.078828 140345384898560 nn_components.py:329] mlp: final activation = None
I0123 11:19:32.080750 140345384898560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.080845 140345384898560 nn_components.py:261] mlp: residual
I0123 11:19:32.083338 140345384898560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.083418 140345384898560 transformer_base.py:443] tbase: final FFN
I0123 11:19:32.083528 140345384898560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:19:32.085810 140345384898560 nn_components.py:329] mlp: final activation = None
I0123 11:19:32.088023 140345384898560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.088120 140345384898560 nn_components.py:261] mlp: residual
I0123 11:19:32.088415 140345384898560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.088496 140345384898560 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 11:19:32.088606 140345384898560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:19:32.088645 140345384898560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:19:32.088677 140345384898560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:19:32.088742 140345384898560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.091045 140345384898560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:19:32.096615 140345384898560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.096874 140345384898560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:19:32.099908 140345384898560 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:19:32.112896 140345384898560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:19:32.112955 140345384898560 attention.py:418] Single window, no scan.
I0123 11:19:32.112992 140345384898560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:19:32.113023 140345384898560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.113085 140345384898560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.113711 140345384898560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.113790 140345384898560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.114152 140345384898560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.114849 140345384898560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.117402 140345384898560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.118061 140345384898560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.118141 140345384898560 transformer_layer.py:468] tlayer: End windows.
I0123 11:19:32.118178 140345384898560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:19:32.118238 140345384898560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.118375 140345384898560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:19:32.118489 140345384898560 nn_components.py:325] mlp: activation = None
I0123 11:19:32.118528 140345384898560 nn_components.py:329] mlp: final activation = None
I0123 11:19:32.120459 140345384898560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.120557 140345384898560 nn_components.py:261] mlp: residual
I0123 11:19:32.123080 140345384898560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.123160 140345384898560 transformer_base.py:443] tbase: final FFN
I0123 11:19:32.123270 140345384898560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:19:32.125553 140345384898560 nn_components.py:329] mlp: final activation = None
I0123 11:19:32.127434 140345384898560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.127528 140345384898560 nn_components.py:261] mlp: residual
I0123 11:19:32.127816 140345384898560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.127897 140345384898560 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 11:19:32.128005 140345384898560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:19:32.128045 140345384898560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:19:32.128077 140345384898560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:19:32.128140 140345384898560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.130432 140345384898560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:19:32.136029 140345384898560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.136290 140345384898560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:19:32.138963 140345384898560 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:19:32.151749 140345384898560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:19:32.151804 140345384898560 attention.py:418] Single window, no scan.
I0123 11:19:32.151840 140345384898560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:19:32.151872 140345384898560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.151935 140345384898560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.152492 140345384898560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.152567 140345384898560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.152933 140345384898560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.153622 140345384898560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.156170 140345384898560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.156842 140345384898560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.156919 140345384898560 transformer_layer.py:468] tlayer: End windows.
I0123 11:19:32.156955 140345384898560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:19:32.157015 140345384898560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.157145 140345384898560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:19:32.157253 140345384898560 nn_components.py:325] mlp: activation = None
I0123 11:19:32.157291 140345384898560 nn_components.py:329] mlp: final activation = None
I0123 11:19:32.159201 140345384898560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.159301 140345384898560 nn_components.py:261] mlp: residual
I0123 11:19:32.161787 140345384898560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.161866 140345384898560 transformer_base.py:443] tbase: final FFN
I0123 11:19:32.161977 140345384898560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:19:32.164232 140345384898560 nn_components.py:329] mlp: final activation = None
I0123 11:19:32.166188 140345384898560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.166284 140345384898560 nn_components.py:261] mlp: residual
I0123 11:19:32.166575 140345384898560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.166655 140345384898560 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 11:19:32.166765 140345384898560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:19:32.166804 140345384898560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:19:32.166835 140345384898560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:19:32.166898 140345384898560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.169163 140345384898560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:19:32.174698 140345384898560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.174960 140345384898560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:19:32.177978 140345384898560 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:19:32.190844 140345384898560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:19:32.190901 140345384898560 attention.py:418] Single window, no scan.
I0123 11:19:32.190937 140345384898560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:19:32.190969 140345384898560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.191031 140345384898560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.191592 140345384898560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.191669 140345384898560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.192037 140345384898560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.192743 140345384898560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.195350 140345384898560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.195976 140345384898560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.196054 140345384898560 transformer_layer.py:468] tlayer: End windows.
I0123 11:19:32.196090 140345384898560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:19:32.196150 140345384898560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.196282 140345384898560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:19:32.196391 140345384898560 nn_components.py:325] mlp: activation = None
I0123 11:19:32.196429 140345384898560 nn_components.py:329] mlp: final activation = None
I0123 11:19:32.198343 140345384898560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.198438 140345384898560 nn_components.py:261] mlp: residual
I0123 11:19:32.200916 140345384898560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.200995 140345384898560 transformer_base.py:443] tbase: final FFN
I0123 11:19:32.201105 140345384898560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:19:32.203755 140345384898560 nn_components.py:329] mlp: final activation = None
I0123 11:19:32.205674 140345384898560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.205770 140345384898560 nn_components.py:261] mlp: residual
I0123 11:19:32.206061 140345384898560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.206148 140345384898560 decoder_stack.py:344] dstack: Final layernorm.
I0123 11:19:32.209059 140345384898560 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 11:19:32.265089 140345384898560 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.265176 140345384898560 decoder_stack.py:333] dstack: autoregressive generator.
I0123 11:19:32.265230 140345384898560 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 11:19:32.265333 140345384898560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:19:32.265372 140345384898560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:19:32.265403 140345384898560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:19:32.265468 140345384898560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.267872 140345384898560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:19:32.273366 140345384898560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.273631 140345384898560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:19:32.276256 140345384898560 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:19:32.289006 140345384898560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:19:32.289063 140345384898560 attention.py:418] Single window, no scan.
I0123 11:19:32.289099 140345384898560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:19:32.289132 140345384898560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.289195 140345384898560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.289770 140345384898560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.289848 140345384898560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.290215 140345384898560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.290899 140345384898560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.293459 140345384898560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.294087 140345384898560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.294165 140345384898560 transformer_layer.py:468] tlayer: End windows.
I0123 11:19:32.294200 140345384898560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:19:32.294260 140345384898560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.294389 140345384898560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:19:32.294504 140345384898560 nn_components.py:325] mlp: activation = None
I0123 11:19:32.294543 140345384898560 nn_components.py:329] mlp: final activation = None
I0123 11:19:32.296414 140345384898560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.296507 140345384898560 nn_components.py:261] mlp: residual
I0123 11:19:32.298957 140345384898560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.299036 140345384898560 transformer_base.py:443] tbase: final FFN
I0123 11:19:32.299145 140345384898560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:19:32.301418 140345384898560 nn_components.py:329] mlp: final activation = None
I0123 11:19:32.303300 140345384898560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.303396 140345384898560 nn_components.py:261] mlp: residual
I0123 11:19:32.303690 140345384898560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.303773 140345384898560 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 11:19:32.303881 140345384898560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:19:32.303921 140345384898560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:19:32.303953 140345384898560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:19:32.304018 140345384898560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.306306 140345384898560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:19:32.311792 140345384898560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.312053 140345384898560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:19:32.314784 140345384898560 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:19:32.327353 140345384898560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:19:32.327409 140345384898560 attention.py:418] Single window, no scan.
I0123 11:19:32.327446 140345384898560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:19:32.327479 140345384898560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.327541 140345384898560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.328100 140345384898560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.328176 140345384898560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.328539 140345384898560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.329227 140345384898560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.331798 140345384898560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.332419 140345384898560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.332496 140345384898560 transformer_layer.py:468] tlayer: End windows.
I0123 11:19:32.332532 140345384898560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:19:32.332592 140345384898560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.332722 140345384898560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:19:32.332831 140345384898560 nn_components.py:325] mlp: activation = None
I0123 11:19:32.332875 140345384898560 nn_components.py:329] mlp: final activation = None
I0123 11:19:32.334762 140345384898560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.334856 140345384898560 nn_components.py:261] mlp: residual
I0123 11:19:32.337298 140345384898560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.337376 140345384898560 transformer_base.py:443] tbase: final FFN
I0123 11:19:32.337484 140345384898560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:19:32.339775 140345384898560 nn_components.py:329] mlp: final activation = None
I0123 11:19:32.341656 140345384898560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.341752 140345384898560 nn_components.py:261] mlp: residual
I0123 11:19:32.342043 140345384898560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.342124 140345384898560 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 11:19:32.342231 140345384898560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:19:32.342271 140345384898560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:19:32.342304 140345384898560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:19:32.342367 140345384898560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.344633 140345384898560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:19:32.350062 140345384898560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.350321 140345384898560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:19:32.353021 140345384898560 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:19:32.365493 140345384898560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:19:32.365549 140345384898560 attention.py:418] Single window, no scan.
I0123 11:19:32.365585 140345384898560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:19:32.365618 140345384898560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.365693 140345384898560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.366261 140345384898560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.366338 140345384898560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.366701 140345384898560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.367387 140345384898560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.370392 140345384898560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.371021 140345384898560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.371101 140345384898560 transformer_layer.py:468] tlayer: End windows.
I0123 11:19:32.371137 140345384898560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:19:32.371198 140345384898560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.371327 140345384898560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:19:32.371436 140345384898560 nn_components.py:325] mlp: activation = None
I0123 11:19:32.371475 140345384898560 nn_components.py:329] mlp: final activation = None
I0123 11:19:32.373391 140345384898560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.373490 140345384898560 nn_components.py:261] mlp: residual
I0123 11:19:32.376060 140345384898560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.376139 140345384898560 transformer_base.py:443] tbase: final FFN
I0123 11:19:32.376248 140345384898560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:19:32.378692 140345384898560 nn_components.py:329] mlp: final activation = None
I0123 11:19:32.380603 140345384898560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.380700 140345384898560 nn_components.py:261] mlp: residual
I0123 11:19:32.380997 140345384898560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.381079 140345384898560 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 11:19:32.381188 140345384898560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:19:32.381227 140345384898560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:19:32.381258 140345384898560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:19:32.381323 140345384898560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.383626 140345384898560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:19:32.389130 140345384898560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.389389 140345384898560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:19:32.392125 140345384898560 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:19:32.404793 140345384898560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:19:32.404852 140345384898560 attention.py:418] Single window, no scan.
I0123 11:19:32.404894 140345384898560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:19:32.404935 140345384898560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.405002 140345384898560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.405565 140345384898560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.405647 140345384898560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.406016 140345384898560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.406734 140345384898560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.409311 140345384898560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.409946 140345384898560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.410025 140345384898560 transformer_layer.py:468] tlayer: End windows.
I0123 11:19:32.410061 140345384898560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:19:32.410124 140345384898560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.410257 140345384898560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:19:32.410371 140345384898560 nn_components.py:325] mlp: activation = None
I0123 11:19:32.410412 140345384898560 nn_components.py:329] mlp: final activation = None
I0123 11:19:32.412339 140345384898560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.412432 140345384898560 nn_components.py:261] mlp: residual
I0123 11:19:32.415024 140345384898560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.415104 140345384898560 transformer_base.py:443] tbase: final FFN
I0123 11:19:32.415215 140345384898560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:19:32.417677 140345384898560 nn_components.py:329] mlp: final activation = None
I0123 11:19:32.419602 140345384898560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.419696 140345384898560 nn_components.py:261] mlp: residual
I0123 11:19:32.419988 140345384898560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.420068 140345384898560 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 11:19:32.420173 140345384898560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:19:32.420210 140345384898560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:19:32.420241 140345384898560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:19:32.420304 140345384898560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.422614 140345384898560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:19:32.428148 140345384898560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.428407 140345384898560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:19:32.431166 140345384898560 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:19:32.444176 140345384898560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:19:32.444231 140345384898560 attention.py:418] Single window, no scan.
I0123 11:19:32.444267 140345384898560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:19:32.444299 140345384898560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.444361 140345384898560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.444930 140345384898560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.445005 140345384898560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.445369 140345384898560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.446079 140345384898560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.448647 140345384898560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.449277 140345384898560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.449354 140345384898560 transformer_layer.py:468] tlayer: End windows.
I0123 11:19:32.449388 140345384898560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:19:32.449449 140345384898560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.449578 140345384898560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:19:32.449692 140345384898560 nn_components.py:325] mlp: activation = None
I0123 11:19:32.449731 140345384898560 nn_components.py:329] mlp: final activation = None
I0123 11:19:32.451633 140345384898560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.451730 140345384898560 nn_components.py:261] mlp: residual
I0123 11:19:32.454179 140345384898560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.454258 140345384898560 transformer_base.py:443] tbase: final FFN
I0123 11:19:32.454365 140345384898560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:19:32.456672 140345384898560 nn_components.py:329] mlp: final activation = None
I0123 11:19:32.458571 140345384898560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.458667 140345384898560 nn_components.py:261] mlp: residual
I0123 11:19:32.458957 140345384898560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.459038 140345384898560 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 11:19:32.459144 140345384898560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:19:32.459181 140345384898560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:19:32.459212 140345384898560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:19:32.459276 140345384898560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.461550 140345384898560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:19:32.467045 140345384898560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.467306 140345384898560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:19:32.470058 140345384898560 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:19:32.482775 140345384898560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:19:32.482830 140345384898560 attention.py:418] Single window, no scan.
I0123 11:19:32.482865 140345384898560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:19:32.482896 140345384898560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.482958 140345384898560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.483519 140345384898560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.483594 140345384898560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.483951 140345384898560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.484650 140345384898560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.487653 140345384898560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.488279 140345384898560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.488355 140345384898560 transformer_layer.py:468] tlayer: End windows.
I0123 11:19:32.488389 140345384898560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:19:32.488447 140345384898560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.488573 140345384898560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:19:32.488684 140345384898560 nn_components.py:325] mlp: activation = None
I0123 11:19:32.488721 140345384898560 nn_components.py:329] mlp: final activation = None
I0123 11:19:32.490613 140345384898560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.490711 140345384898560 nn_components.py:261] mlp: residual
I0123 11:19:32.493146 140345384898560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.493227 140345384898560 transformer_base.py:443] tbase: final FFN
I0123 11:19:32.493333 140345384898560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:19:32.495649 140345384898560 nn_components.py:329] mlp: final activation = None
I0123 11:19:32.497534 140345384898560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.497627 140345384898560 nn_components.py:261] mlp: residual
I0123 11:19:32.497925 140345384898560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.498005 140345384898560 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 11:19:32.498112 140345384898560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:19:32.498150 140345384898560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:19:32.498181 140345384898560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:19:32.498244 140345384898560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.500492 140345384898560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:19:32.505970 140345384898560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.506229 140345384898560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:19:32.508966 140345384898560 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:19:32.521661 140345384898560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:19:32.521718 140345384898560 attention.py:418] Single window, no scan.
I0123 11:19:32.521753 140345384898560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:19:32.521785 140345384898560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.521848 140345384898560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.522416 140345384898560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.522491 140345384898560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.522857 140345384898560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.523548 140345384898560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.526138 140345384898560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.526772 140345384898560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.526847 140345384898560 transformer_layer.py:468] tlayer: End windows.
I0123 11:19:32.526881 140345384898560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:19:32.526940 140345384898560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.527071 140345384898560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:19:32.527181 140345384898560 nn_components.py:325] mlp: activation = None
I0123 11:19:32.527219 140345384898560 nn_components.py:329] mlp: final activation = None
I0123 11:19:32.529111 140345384898560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.529202 140345384898560 nn_components.py:261] mlp: residual
I0123 11:19:32.531666 140345384898560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.531744 140345384898560 transformer_base.py:443] tbase: final FFN
I0123 11:19:32.531854 140345384898560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:19:32.534194 140345384898560 nn_components.py:329] mlp: final activation = None
I0123 11:19:32.536284 140345384898560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.536378 140345384898560 nn_components.py:261] mlp: residual
I0123 11:19:32.536764 140345384898560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.536844 140345384898560 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 11:19:32.536952 140345384898560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:19:32.536990 140345384898560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:19:32.537021 140345384898560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:19:32.537084 140345384898560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.539350 140345384898560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:19:32.544862 140345384898560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.545126 140345384898560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:19:32.547869 140345384898560 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:19:32.560717 140345384898560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:19:32.560772 140345384898560 attention.py:418] Single window, no scan.
I0123 11:19:32.560808 140345384898560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:19:32.560840 140345384898560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.560904 140345384898560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.561476 140345384898560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.561552 140345384898560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.561933 140345384898560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.562643 140345384898560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.565298 140345384898560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.565946 140345384898560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.566026 140345384898560 transformer_layer.py:468] tlayer: End windows.
I0123 11:19:32.566061 140345384898560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:19:32.566121 140345384898560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.566253 140345384898560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:19:32.566365 140345384898560 nn_components.py:325] mlp: activation = None
I0123 11:19:32.566405 140345384898560 nn_components.py:329] mlp: final activation = None
I0123 11:19:32.568325 140345384898560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.568417 140345384898560 nn_components.py:261] mlp: residual
I0123 11:19:32.570890 140345384898560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.570974 140345384898560 transformer_base.py:443] tbase: final FFN
I0123 11:19:32.571085 140345384898560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:19:32.573405 140345384898560 nn_components.py:329] mlp: final activation = None
I0123 11:19:32.575301 140345384898560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.575395 140345384898560 nn_components.py:261] mlp: residual
I0123 11:19:32.575685 140345384898560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.575766 140345384898560 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 11:19:32.575872 140345384898560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:19:32.575909 140345384898560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:19:32.575940 140345384898560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:19:32.576002 140345384898560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.578288 140345384898560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:19:32.583802 140345384898560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.584062 140345384898560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:19:32.586818 140345384898560 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:19:32.599477 140345384898560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:19:32.599530 140345384898560 attention.py:418] Single window, no scan.
I0123 11:19:32.599568 140345384898560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:19:32.599599 140345384898560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.599660 140345384898560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.600218 140345384898560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.600292 140345384898560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.600659 140345384898560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.601353 140345384898560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.604332 140345384898560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.604963 140345384898560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.605041 140345384898560 transformer_layer.py:468] tlayer: End windows.
I0123 11:19:32.605074 140345384898560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:19:32.605132 140345384898560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.605257 140345384898560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:19:32.605364 140345384898560 nn_components.py:325] mlp: activation = None
I0123 11:19:32.605401 140345384898560 nn_components.py:329] mlp: final activation = None
I0123 11:19:32.607319 140345384898560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.607412 140345384898560 nn_components.py:261] mlp: residual
I0123 11:19:32.609859 140345384898560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.609943 140345384898560 transformer_base.py:443] tbase: final FFN
I0123 11:19:32.610053 140345384898560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:19:32.612359 140345384898560 nn_components.py:329] mlp: final activation = None
I0123 11:19:32.614260 140345384898560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.614354 140345384898560 nn_components.py:261] mlp: residual
I0123 11:19:32.614643 140345384898560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.614722 140345384898560 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 11:19:32.614829 140345384898560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:19:32.614866 140345384898560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:19:32.614896 140345384898560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:19:32.614958 140345384898560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.617214 140345384898560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:19:32.622727 140345384898560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.622986 140345384898560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:19:32.625704 140345384898560 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:19:32.638626 140345384898560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:19:32.638680 140345384898560 attention.py:418] Single window, no scan.
I0123 11:19:32.638714 140345384898560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:19:32.638745 140345384898560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.638807 140345384898560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.639373 140345384898560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.639448 140345384898560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.639805 140345384898560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.640501 140345384898560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.643078 140345384898560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.643711 140345384898560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.643787 140345384898560 transformer_layer.py:468] tlayer: End windows.
I0123 11:19:32.643821 140345384898560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:19:32.643879 140345384898560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.644007 140345384898560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:19:32.644114 140345384898560 nn_components.py:325] mlp: activation = None
I0123 11:19:32.644150 140345384898560 nn_components.py:329] mlp: final activation = None
I0123 11:19:32.646609 140345384898560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.646703 140345384898560 nn_components.py:261] mlp: residual
I0123 11:19:32.649139 140345384898560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.649215 140345384898560 transformer_base.py:443] tbase: final FFN
I0123 11:19:32.649327 140345384898560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:19:32.651614 140345384898560 nn_components.py:329] mlp: final activation = None
I0123 11:19:32.653484 140345384898560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.653577 140345384898560 nn_components.py:261] mlp: residual
I0123 11:19:32.653874 140345384898560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.653955 140345384898560 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 11:19:32.654061 140345384898560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:19:32.654099 140345384898560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:19:32.654129 140345384898560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:19:32.654192 140345384898560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.656447 140345384898560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:19:32.661953 140345384898560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.662211 140345384898560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:19:32.664897 140345384898560 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:19:32.677514 140345384898560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:19:32.677567 140345384898560 attention.py:418] Single window, no scan.
I0123 11:19:32.677603 140345384898560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:19:32.677634 140345384898560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.677705 140345384898560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.678262 140345384898560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.678337 140345384898560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.678694 140345384898560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.679387 140345384898560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.681990 140345384898560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.682614 140345384898560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.682691 140345384898560 transformer_layer.py:468] tlayer: End windows.
I0123 11:19:32.682726 140345384898560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:19:32.682784 140345384898560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.682911 140345384898560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:19:32.683017 140345384898560 nn_components.py:325] mlp: activation = None
I0123 11:19:32.683055 140345384898560 nn_components.py:329] mlp: final activation = None
I0123 11:19:32.684959 140345384898560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.685051 140345384898560 nn_components.py:261] mlp: residual
I0123 11:19:32.687478 140345384898560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.687556 140345384898560 transformer_base.py:443] tbase: final FFN
I0123 11:19:32.687663 140345384898560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:19:32.689972 140345384898560 nn_components.py:329] mlp: final activation = None
I0123 11:19:32.691843 140345384898560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.691936 140345384898560 nn_components.py:261] mlp: residual
I0123 11:19:32.692222 140345384898560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.692301 140345384898560 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 11:19:32.692408 140345384898560 transformer_layer.py:154] tlayer: recurrent = False
I0123 11:19:32.692446 140345384898560 transformer_layer.py:155] tlayer: compute_importance = False
I0123 11:19:32.692476 140345384898560 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 11:19:32.692538 140345384898560 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.694797 140345384898560 transformer_base.py:161] kvq: pre_attn dropout.
I0123 11:19:32.700284 140345384898560 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.700545 140345384898560 transformer_base.py:194] kvq: normalize keys, queries.
I0123 11:19:32.703258 140345384898560 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 11:19:32.715847 140345384898560 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 11:19:32.715900 140345384898560 attention.py:418] Single window, no scan.
I0123 11:19:32.715936 140345384898560 transformer_layer.py:389] tlayer: self-attention.
I0123 11:19:32.715967 140345384898560 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.716028 140345384898560 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.716585 140345384898560 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.716662 140345384898560 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.717023 140345384898560 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.717720 140345384898560 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.720652 140345384898560 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.721284 140345384898560 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.721364 140345384898560 transformer_layer.py:468] tlayer: End windows.
I0123 11:19:32.721399 140345384898560 transformer_layer.py:472] tlayer: final FFN.
I0123 11:19:32.721456 140345384898560 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.721582 140345384898560 transformer_base.py:410] tbase: post-attention MLP.
I0123 11:19:32.721696 140345384898560 nn_components.py:325] mlp: activation = None
I0123 11:19:32.721736 140345384898560 nn_components.py:329] mlp: final activation = None
I0123 11:19:32.723632 140345384898560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.723724 140345384898560 nn_components.py:261] mlp: residual
I0123 11:19:32.726180 140345384898560 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.726258 140345384898560 transformer_base.py:443] tbase: final FFN
I0123 11:19:32.726364 140345384898560 nn_components.py:320] mlp: hidden 4096, relu
I0123 11:19:32.728673 140345384898560 nn_components.py:329] mlp: final activation = None
I0123 11:19:32.730588 140345384898560 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.730683 140345384898560 nn_components.py:261] mlp: residual
I0123 11:19:32.730974 140345384898560 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:32.731060 140345384898560 decoder_stack.py:344] dstack: Final layernorm.
I0123 11:19:32.733947 140345384898560 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 11:19:37.154124 140345384898560 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
I0123 11:19:37.661883 140345384898560 training_loop.py:409] No working directory specified.
I0123 11:19:37.661998 140345384898560 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0123 11:19:37.662760 140345384898560 checkpoints.py:1062] Restoring legacy Flax checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0123 11:19:40.563617 140345384898560 training_loop.py:447] Only restoring trainable parameters.
I0123 11:19:40.564322 140345384898560 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0123 11:19:40.564379 140345384898560 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0123 11:19:40.564424 140345384898560 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:19:40.564465 140345384898560 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:19:40.564505 140345384898560 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:19:40.564543 140345384898560 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:19:40.564581 140345384898560 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:19:40.564617 140345384898560 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:19:40.564655 140345384898560 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:19:40.564692 140345384898560 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:19:40.564730 140345384898560 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:19:40.564767 140345384898560 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:19:40.564804 140345384898560 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:19:40.564840 140345384898560 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:19:40.564876 140345384898560 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:19:40.564913 140345384898560 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:19:40.564949 140345384898560 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:19:40.564985 140345384898560 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:19:40.565022 140345384898560 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:19:40.565058 140345384898560 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:19:40.565108 140345384898560 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:19:40.565147 140345384898560 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:19:40.565184 140345384898560 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:19:40.565220 140345384898560 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:19:40.565255 140345384898560 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:19:40.565290 140345384898560 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:19:40.565326 140345384898560 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:19:40.565362 140345384898560 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:19:40.565397 140345384898560 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:19:40.565431 140345384898560 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:19:40.565467 140345384898560 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:19:40.565502 140345384898560 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:19:40.565537 140345384898560 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:19:40.565574 140345384898560 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:19:40.565610 140345384898560 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:19:40.565653 140345384898560 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:19:40.565693 140345384898560 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:19:40.565730 140345384898560 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:19:40.565765 140345384898560 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:19:40.565801 140345384898560 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:19:40.565837 140345384898560 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:19:40.565873 140345384898560 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:19:40.565910 140345384898560 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:19:40.565947 140345384898560 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:19:40.565983 140345384898560 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:19:40.566020 140345384898560 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:19:40.566065 140345384898560 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:19:40.566111 140345384898560 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:19:40.566150 140345384898560 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:19:40.566187 140345384898560 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:19:40.566226 140345384898560 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:19:40.566262 140345384898560 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:19:40.566299 140345384898560 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:19:40.566335 140345384898560 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:19:40.566372 140345384898560 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:19:40.566408 140345384898560 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:19:40.566444 140345384898560 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:19:40.566480 140345384898560 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:19:40.566515 140345384898560 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:19:40.566553 140345384898560 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:19:40.566590 140345384898560 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:19:40.566626 140345384898560 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:19:40.566663 140345384898560 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:19:40.566699 140345384898560 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:19:40.566735 140345384898560 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:19:40.566772 140345384898560 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:19:40.566807 140345384898560 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:19:40.566844 140345384898560 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:19:40.566880 140345384898560 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:19:40.566916 140345384898560 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:19:40.566952 140345384898560 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:19:40.566987 140345384898560 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:19:40.567023 140345384898560 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:19:40.567065 140345384898560 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:19:40.567103 140345384898560 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:19:40.567140 140345384898560 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:19:40.567177 140345384898560 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:19:40.567213 140345384898560 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:19:40.567248 140345384898560 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:19:40.567285 140345384898560 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:19:40.567321 140345384898560 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:19:40.567357 140345384898560 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:19:40.567393 140345384898560 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:19:40.567429 140345384898560 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:19:40.567465 140345384898560 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:19:40.567501 140345384898560 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:19:40.567536 140345384898560 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:19:40.567572 140345384898560 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:19:40.567608 140345384898560 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:19:40.567643 140345384898560 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:19:40.567678 140345384898560 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:19:40.567715 140345384898560 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:19:40.567750 140345384898560 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:19:40.567786 140345384898560 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:19:40.567823 140345384898560 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:19:40.567859 140345384898560 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:19:40.567896 140345384898560 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:19:40.567933 140345384898560 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:19:40.567970 140345384898560 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:19:40.568006 140345384898560 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:19:40.568048 140345384898560 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:19:40.568087 140345384898560 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:19:40.568124 140345384898560 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:19:40.568160 140345384898560 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:19:40.568195 140345384898560 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:19:40.568230 140345384898560 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:19:40.568267 140345384898560 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:19:40.568302 140345384898560 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:19:40.568338 140345384898560 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:19:40.568373 140345384898560 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:19:40.568407 140345384898560 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:19:40.568443 140345384898560 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:19:40.568478 140345384898560 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 11:19:40.568513 140345384898560 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 11:19:40.568548 140345384898560 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:19:40.568583 140345384898560 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 11:19:40.568617 140345384898560 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:19:40.568651 140345384898560 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:19:40.568686 140345384898560 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 11:19:40.568721 140345384898560 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 11:19:40.568756 140345384898560 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 11:19:40.568790 140345384898560 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 11:19:40.568818 140345384898560 training_loop.py:725] Total parameters: 152072288
I0123 11:19:40.569027 140345384898560 training_loop.py:739] Total state size: 0
I0123 11:19:40.588858 140345384898560 training_loop.py:492] Training loop: creating task for mode beam_search
I0123 11:19:40.589116 140345384898560 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0123 11:19:40.589482 140345384898560 training_loop.py:652] Compiling mode beam_search with jit.
I0123 11:19:40.589803 140345384898560 training_loop.py:89] registering functions: dict_keys([])
I0123 11:19:40.606411 140345384898560 graph.py:499] a b c = triangle a b c; d = circle d b a c; e = lc_tangent e c d, on_line e a d; f = on_line f c e, on_line f b a; g = on_circle g f c, on_line g b a; h = on_circle h f c, on_line h b a; i = on_line i c h; j = on_circle j f c, on_line j f i; k = on_circle k f c, on_line k f i; l = on_circle l d c, on_line l k c; m = on_circle m d a, on_line m k a; n = on_circle n d b, on_line n k b ? cong l n l m
I0123 11:19:44.736692 140345384898560 ddar.py:60] Depth 1/1000 time = 4.081902980804443
I0123 11:19:55.166328 140345384898560 ddar.py:60] Depth 2/1000 time = 10.42946457862854
I0123 11:20:05.970063 140345384898560 ddar.py:60] Depth 3/1000 time = 10.803476095199585
I0123 11:20:18.685516 140345384898560 ddar.py:60] Depth 4/1000 time = 12.715088367462158
I0123 11:20:31.651564 140345384898560 ddar.py:60] Depth 5/1000 time = 12.965671300888062
I0123 11:20:31.660121 140345384898560 alphageometry.py:191] 
==========================
 * From theorem premises:
A B C D E F I K L M N : Points
DA = DC [00]
DB = DA [01]
CE  CD [02]
A,F,B are collinear [03]
F,C,E are collinear [04]
F,K,I are collinear [05]
FK = FC [06]
DL = DC [07]
C,K,L are collinear [08]
DM = DA [09]
A,K,M are collinear [10]
DN = DB [11]
N,B,K are collinear [12]

 * Auxiliary Constructions:
: Points


 * Proof steps:
001. DL = DC [07] & DA = DC [00] & DM = DA [09]   D is the circumcenter of \Delta CLM [13]
002. DL = DC [07] & DA = DC [00] & DB = DA [01] & DN = DB [11] & DM = DA [09]   B,N,L,M are concyclic [14]
003. DA = DC [00] & DB = DA [01]   D is the circumcenter of \Delta CAB [15]
004. F,C,E are collinear [04] & CD  CE [02]   DC  CF [16]
005. D is the circumcenter of \Delta CAB [15] & DC  CF [16]   FCA = CBA [17]
006. A,F,B are collinear [03] & F,C,E are collinear [04] & FCA = CBA [17]   FAC = BCF [18]
007. F,C,E are collinear [04] & A,F,B are collinear [03] & FCA = CBA [17]   FCA = CBF [19]
008. FAC = BCF [18] & FCA = CBF [19] (Similar Triangles)  AF:FC = FC:FB [20]
009. FC:FB = AF:FC [20] & FK = FC [06]   FK:FB = FA:FK [21]
010. F,K,I are collinear [05] & A,F,B are collinear [03]   KFB = KFA [22]
011. FK:FB = FA:FK [21] & KFB = KFA [22] (Similar Triangles)  FKB = KAF [23]
012. DL = DC [07] & DA = DC [00] & DM = DA [09] & DB = DA [01]   B,L,M,A are concyclic [24]
013. DL = DC [07] & DA = DC [00] & DM = DA [09] & DB = DA [01]   B,C,L,M are concyclic [25]
014. B,L,M,A are concyclic [24]   BLM = BAM [26]
015. N,B,K are collinear [12] & F,K,I are collinear [05] & FKB = KAF [23] & A,F,B are collinear [03] & BLM = BAM [26] & A,K,M are collinear [10]   BLM = (NB-FK) [27]
016. FK = FC [06]   FCK = CKF [28]
017. D is the circumcenter of \Delta CLM [13] & DC  CF [16]   FCL = CML [29]
018. F,K,I are collinear [05] & FCK = CKF [28] & F,C,E are collinear [04] & FCL = CML [29] & C,K,L are collinear [08]   LMC = FKC [30]
019. B,C,L,M are concyclic [25]   BLC = BMC [31]
020. BLC = BMC [31] & C,K,L are collinear [08]   (BL-CK) = BMC [32]
021. LMC = FKC [30] & (BL-CK) = BMC [32]   LMB = (FK-BL) [33]
022. BLM = (NB-FK) [27] & BML = (BL-FK) [33]   LBM = NBL [34]
023. B,N,L,M are concyclic [14] & NBL = LBM [34]   NL = LM
==========================

