I0123 12:39:30.911885 139872438390784 inference_utils.py:69] Parsing gin configuration.
I0123 12:39:30.911984 139872438390784 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0123 12:39:30.912190 139872438390784 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0123 12:39:30.912224 139872438390784 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0123 12:39:30.912254 139872438390784 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0123 12:39:30.912282 139872438390784 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0123 12:39:30.912308 139872438390784 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0123 12:39:30.912338 139872438390784 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0123 12:39:30.912367 139872438390784 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0123 12:39:30.912395 139872438390784 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=32
I0123 12:39:30.912421 139872438390784 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0123 12:39:30.912447 139872438390784 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0123 12:39:30.912493 139872438390784 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0123 12:39:30.912628 139872438390784 resource_reader.py:55] Path not found: base_htrans.gin
I0123 12:39:30.912840 139872438390784 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0123 12:39:30.912939 139872438390784 resource_reader.py:55] Path not found: trainer_configuration.gin
I0123 12:39:30.919256 139872438390784 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0123 12:39:30.919376 139872438390784 resource_reader.py:55] Path not found: size/medium_150M.gin
I0123 12:39:30.919703 139872438390784 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0123 12:39:30.919808 139872438390784 resource_reader.py:55] Path not found: options/positions_t5.gin
I0123 12:39:30.920088 139872438390784 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0123 12:39:30.920189 139872438390784 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0123 12:39:30.920601 139872438390784 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0123 12:39:30.920702 139872438390784 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0123 12:39:30.924452 139872438390784 training_loop.py:334] ==== Training loop: initializing model ====
I0123 12:39:31.018941 139872438390784 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0123 12:39:31.019676 139872438390784 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0123 12:39:31.026424 139872438390784 training_loop.py:335] Process 0 of 1
I0123 12:39:31.026478 139872438390784 training_loop.py:336] Local device count = 1
I0123 12:39:31.026517 139872438390784 training_loop.py:337] Number of replicas = 1
I0123 12:39:31.026549 139872438390784 training_loop.py:339] Using random number seed 42
I0123 12:39:31.501597 139872438390784 training_loop.py:359] Initializing the model.
I0123 12:39:31.899322 139872438390784 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:31.899565 139872438390784 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 12:39:31.899671 139872438390784 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:39:31.899810 139872438390784 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:39:31.899893 139872438390784 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:39:31.899977 139872438390784 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:39:31.900052 139872438390784 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:39:31.900125 139872438390784 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:39:31.900197 139872438390784 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:39:31.900267 139872438390784 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:39:31.900337 139872438390784 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:39:31.900408 139872438390784 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:39:31.900479 139872438390784 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:39:31.900549 139872438390784 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:39:31.900589 139872438390784 attention.py:418] Single window, no scan.
I0123 12:39:31.900635 139872438390784 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 12:39:31.900943 139872438390784 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:39:31.900984 139872438390784 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:39:31.901015 139872438390784 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:39:31.903214 139872438390784 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:31.908512 139872438390784 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:39:31.919255 139872438390784 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:31.919539 139872438390784 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:39:31.923864 139872438390784 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:39:31.934594 139872438390784 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:39:31.934653 139872438390784 attention.py:418] Single window, no scan.
I0123 12:39:31.934693 139872438390784 transformer_layer.py:389] tlayer: self-attention.
I0123 12:39:31.934728 139872438390784 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:31.934792 139872438390784 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:31.935995 139872438390784 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:31.936074 139872438390784 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:31.936794 139872438390784 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:31.939293 139872438390784 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:31.945573 139872438390784 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:31.946855 139872438390784 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:31.946941 139872438390784 transformer_layer.py:468] tlayer: End windows.
I0123 12:39:31.946978 139872438390784 transformer_layer.py:472] tlayer: final FFN.
I0123 12:39:31.947043 139872438390784 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:31.947177 139872438390784 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:39:31.947523 139872438390784 nn_components.py:325] mlp: activation = None
I0123 12:39:31.947572 139872438390784 nn_components.py:329] mlp: final activation = None
I0123 12:39:31.949535 139872438390784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:31.949636 139872438390784 nn_components.py:261] mlp: residual
I0123 12:39:31.952647 139872438390784 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:31.952731 139872438390784 transformer_base.py:443] tbase: final FFN
I0123 12:39:31.953181 139872438390784 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:39:31.963553 139872438390784 nn_components.py:329] mlp: final activation = None
I0123 12:39:31.972524 139872438390784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:31.972624 139872438390784 nn_components.py:261] mlp: residual
I0123 12:39:31.972932 139872438390784 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:31.973016 139872438390784 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 12:39:31.973132 139872438390784 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:39:31.973172 139872438390784 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:39:31.973206 139872438390784 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:39:31.975224 139872438390784 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:31.977662 139872438390784 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:39:31.983372 139872438390784 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:31.983644 139872438390784 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:39:31.986327 139872438390784 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:39:31.990271 139872438390784 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:39:31.990326 139872438390784 attention.py:418] Single window, no scan.
I0123 12:39:31.990362 139872438390784 transformer_layer.py:389] tlayer: self-attention.
I0123 12:39:31.990395 139872438390784 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:31.990459 139872438390784 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:31.991041 139872438390784 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:31.991118 139872438390784 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:31.991490 139872438390784 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:31.992275 139872438390784 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:31.994814 139872438390784 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:31.995506 139872438390784 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:31.995590 139872438390784 transformer_layer.py:468] tlayer: End windows.
I0123 12:39:31.995627 139872438390784 transformer_layer.py:472] tlayer: final FFN.
I0123 12:39:31.995689 139872438390784 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:31.995821 139872438390784 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:39:31.996151 139872438390784 nn_components.py:325] mlp: activation = None
I0123 12:39:31.996195 139872438390784 nn_components.py:329] mlp: final activation = None
I0123 12:39:31.998156 139872438390784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:31.998251 139872438390784 nn_components.py:261] mlp: residual
I0123 12:39:32.000816 139872438390784 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.000899 139872438390784 transformer_base.py:443] tbase: final FFN
I0123 12:39:32.001394 139872438390784 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:39:32.003839 139872438390784 nn_components.py:329] mlp: final activation = None
I0123 12:39:32.005772 139872438390784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.005868 139872438390784 nn_components.py:261] mlp: residual
I0123 12:39:32.006169 139872438390784 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.006250 139872438390784 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 12:39:32.006363 139872438390784 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:39:32.006404 139872438390784 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:39:32.006436 139872438390784 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:39:32.008368 139872438390784 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.010798 139872438390784 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:39:32.016881 139872438390784 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.017150 139872438390784 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:39:32.019859 139872438390784 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:39:32.023820 139872438390784 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:39:32.023877 139872438390784 attention.py:418] Single window, no scan.
I0123 12:39:32.023914 139872438390784 transformer_layer.py:389] tlayer: self-attention.
I0123 12:39:32.023947 139872438390784 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.024012 139872438390784 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.024588 139872438390784 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.024664 139872438390784 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.025031 139872438390784 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.025825 139872438390784 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.028442 139872438390784 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.029085 139872438390784 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.029164 139872438390784 transformer_layer.py:468] tlayer: End windows.
I0123 12:39:32.029201 139872438390784 transformer_layer.py:472] tlayer: final FFN.
I0123 12:39:32.029263 139872438390784 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.029401 139872438390784 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:39:32.029746 139872438390784 nn_components.py:325] mlp: activation = None
I0123 12:39:32.029792 139872438390784 nn_components.py:329] mlp: final activation = None
I0123 12:39:32.031748 139872438390784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.031842 139872438390784 nn_components.py:261] mlp: residual
I0123 12:39:32.034489 139872438390784 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.034577 139872438390784 transformer_base.py:443] tbase: final FFN
I0123 12:39:32.035032 139872438390784 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:39:32.037385 139872438390784 nn_components.py:329] mlp: final activation = None
I0123 12:39:32.039359 139872438390784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.039457 139872438390784 nn_components.py:261] mlp: residual
I0123 12:39:32.039759 139872438390784 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.039841 139872438390784 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 12:39:32.039954 139872438390784 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:39:32.039995 139872438390784 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:39:32.040027 139872438390784 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:39:32.041985 139872438390784 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.044629 139872438390784 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:39:32.050399 139872438390784 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.050675 139872438390784 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:39:32.053431 139872438390784 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:39:32.057297 139872438390784 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:39:32.057353 139872438390784 attention.py:418] Single window, no scan.
I0123 12:39:32.057400 139872438390784 transformer_layer.py:389] tlayer: self-attention.
I0123 12:39:32.057437 139872438390784 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.057506 139872438390784 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.058088 139872438390784 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.058166 139872438390784 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.058540 139872438390784 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.059342 139872438390784 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.061971 139872438390784 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.062602 139872438390784 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.062678 139872438390784 transformer_layer.py:468] tlayer: End windows.
I0123 12:39:32.062713 139872438390784 transformer_layer.py:472] tlayer: final FFN.
I0123 12:39:32.062774 139872438390784 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.062908 139872438390784 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:39:32.063240 139872438390784 nn_components.py:325] mlp: activation = None
I0123 12:39:32.063284 139872438390784 nn_components.py:329] mlp: final activation = None
I0123 12:39:32.065239 139872438390784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.065333 139872438390784 nn_components.py:261] mlp: residual
I0123 12:39:32.067956 139872438390784 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.068043 139872438390784 transformer_base.py:443] tbase: final FFN
I0123 12:39:32.068493 139872438390784 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:39:32.070828 139872438390784 nn_components.py:329] mlp: final activation = None
I0123 12:39:32.072838 139872438390784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.072938 139872438390784 nn_components.py:261] mlp: residual
I0123 12:39:32.073244 139872438390784 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.073328 139872438390784 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 12:39:32.073440 139872438390784 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:39:32.073479 139872438390784 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:39:32.073512 139872438390784 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:39:32.075404 139872438390784 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.077836 139872438390784 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:39:32.083594 139872438390784 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.083859 139872438390784 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:39:32.086681 139872438390784 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:39:32.090565 139872438390784 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:39:32.090622 139872438390784 attention.py:418] Single window, no scan.
I0123 12:39:32.090660 139872438390784 transformer_layer.py:389] tlayer: self-attention.
I0123 12:39:32.090692 139872438390784 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.090757 139872438390784 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.091699 139872438390784 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.091777 139872438390784 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.092150 139872438390784 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.092951 139872438390784 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.095538 139872438390784 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.096174 139872438390784 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.096254 139872438390784 transformer_layer.py:468] tlayer: End windows.
I0123 12:39:32.096291 139872438390784 transformer_layer.py:472] tlayer: final FFN.
I0123 12:39:32.096354 139872438390784 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.096499 139872438390784 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:39:32.096833 139872438390784 nn_components.py:325] mlp: activation = None
I0123 12:39:32.096878 139872438390784 nn_components.py:329] mlp: final activation = None
I0123 12:39:32.099717 139872438390784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.099886 139872438390784 nn_components.py:261] mlp: residual
I0123 12:39:32.102578 139872438390784 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.102660 139872438390784 transformer_base.py:443] tbase: final FFN
I0123 12:39:32.103115 139872438390784 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:39:32.105520 139872438390784 nn_components.py:329] mlp: final activation = None
I0123 12:39:32.107491 139872438390784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.107595 139872438390784 nn_components.py:261] mlp: residual
I0123 12:39:32.107897 139872438390784 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.107981 139872438390784 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 12:39:32.108096 139872438390784 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:39:32.108137 139872438390784 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:39:32.108170 139872438390784 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:39:32.110092 139872438390784 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.112605 139872438390784 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:39:32.118393 139872438390784 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.118664 139872438390784 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:39:32.121346 139872438390784 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:39:32.125328 139872438390784 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:39:32.125388 139872438390784 attention.py:418] Single window, no scan.
I0123 12:39:32.125425 139872438390784 transformer_layer.py:389] tlayer: self-attention.
I0123 12:39:32.125458 139872438390784 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.125522 139872438390784 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.126118 139872438390784 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.126197 139872438390784 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.126569 139872438390784 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.127388 139872438390784 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.129962 139872438390784 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.130604 139872438390784 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.130685 139872438390784 transformer_layer.py:468] tlayer: End windows.
I0123 12:39:32.130721 139872438390784 transformer_layer.py:472] tlayer: final FFN.
I0123 12:39:32.130781 139872438390784 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.130915 139872438390784 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:39:32.131245 139872438390784 nn_components.py:325] mlp: activation = None
I0123 12:39:32.131288 139872438390784 nn_components.py:329] mlp: final activation = None
I0123 12:39:32.133306 139872438390784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.133401 139872438390784 nn_components.py:261] mlp: residual
I0123 12:39:32.135981 139872438390784 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.136062 139872438390784 transformer_base.py:443] tbase: final FFN
I0123 12:39:32.136504 139872438390784 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:39:32.138882 139872438390784 nn_components.py:329] mlp: final activation = None
I0123 12:39:32.140852 139872438390784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.140949 139872438390784 nn_components.py:261] mlp: residual
I0123 12:39:32.141248 139872438390784 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.141333 139872438390784 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 12:39:32.141446 139872438390784 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:39:32.141487 139872438390784 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:39:32.141519 139872438390784 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:39:32.143616 139872438390784 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.146093 139872438390784 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:39:32.151821 139872438390784 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.152093 139872438390784 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:39:32.154817 139872438390784 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:39:32.158740 139872438390784 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:39:32.158796 139872438390784 attention.py:418] Single window, no scan.
I0123 12:39:32.158832 139872438390784 transformer_layer.py:389] tlayer: self-attention.
I0123 12:39:32.158864 139872438390784 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.158928 139872438390784 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.159505 139872438390784 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.159581 139872438390784 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.159947 139872438390784 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.160755 139872438390784 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.163285 139872438390784 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.163972 139872438390784 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.164051 139872438390784 transformer_layer.py:468] tlayer: End windows.
I0123 12:39:32.164087 139872438390784 transformer_layer.py:472] tlayer: final FFN.
I0123 12:39:32.164147 139872438390784 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.164284 139872438390784 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:39:32.164611 139872438390784 nn_components.py:325] mlp: activation = None
I0123 12:39:32.164654 139872438390784 nn_components.py:329] mlp: final activation = None
I0123 12:39:32.166614 139872438390784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.166709 139872438390784 nn_components.py:261] mlp: residual
I0123 12:39:32.169283 139872438390784 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.169366 139872438390784 transformer_base.py:443] tbase: final FFN
I0123 12:39:32.170177 139872438390784 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:39:32.172508 139872438390784 nn_components.py:329] mlp: final activation = None
I0123 12:39:32.174471 139872438390784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.174574 139872438390784 nn_components.py:261] mlp: residual
I0123 12:39:32.174874 139872438390784 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.174957 139872438390784 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 12:39:32.175071 139872438390784 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:39:32.175111 139872438390784 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:39:32.175143 139872438390784 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:39:32.315720 139872438390784 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.318704 139872438390784 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:39:32.324814 139872438390784 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.325136 139872438390784 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:39:32.328006 139872438390784 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:39:32.332006 139872438390784 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:39:32.332065 139872438390784 attention.py:418] Single window, no scan.
I0123 12:39:32.332103 139872438390784 transformer_layer.py:389] tlayer: self-attention.
I0123 12:39:32.332136 139872438390784 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.332204 139872438390784 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.332837 139872438390784 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.332917 139872438390784 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.333288 139872438390784 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.334113 139872438390784 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.336849 139872438390784 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.337529 139872438390784 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.337610 139872438390784 transformer_layer.py:468] tlayer: End windows.
I0123 12:39:32.337656 139872438390784 transformer_layer.py:472] tlayer: final FFN.
I0123 12:39:32.337723 139872438390784 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.337857 139872438390784 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:39:32.338201 139872438390784 nn_components.py:325] mlp: activation = None
I0123 12:39:32.338245 139872438390784 nn_components.py:329] mlp: final activation = None
I0123 12:39:32.340222 139872438390784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.340317 139872438390784 nn_components.py:261] mlp: residual
I0123 12:39:32.342976 139872438390784 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.343058 139872438390784 transformer_base.py:443] tbase: final FFN
I0123 12:39:32.343513 139872438390784 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:39:32.346364 139872438390784 nn_components.py:329] mlp: final activation = None
I0123 12:39:32.348427 139872438390784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.348535 139872438390784 nn_components.py:261] mlp: residual
I0123 12:39:32.348855 139872438390784 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.348941 139872438390784 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 12:39:32.349058 139872438390784 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:39:32.349099 139872438390784 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:39:32.349132 139872438390784 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:39:32.351045 139872438390784 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.353583 139872438390784 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:39:32.359383 139872438390784 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.359655 139872438390784 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:39:32.362428 139872438390784 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:39:32.366400 139872438390784 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:39:32.366456 139872438390784 attention.py:418] Single window, no scan.
I0123 12:39:32.366494 139872438390784 transformer_layer.py:389] tlayer: self-attention.
I0123 12:39:32.366527 139872438390784 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.366590 139872438390784 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.367221 139872438390784 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.367301 139872438390784 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.367671 139872438390784 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.368479 139872438390784 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.371062 139872438390784 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.371706 139872438390784 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.371785 139872438390784 transformer_layer.py:468] tlayer: End windows.
I0123 12:39:32.371822 139872438390784 transformer_layer.py:472] tlayer: final FFN.
I0123 12:39:32.371887 139872438390784 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.372023 139872438390784 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:39:32.372352 139872438390784 nn_components.py:325] mlp: activation = None
I0123 12:39:32.372397 139872438390784 nn_components.py:329] mlp: final activation = None
I0123 12:39:32.374367 139872438390784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.374466 139872438390784 nn_components.py:261] mlp: residual
I0123 12:39:32.377122 139872438390784 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.377203 139872438390784 transformer_base.py:443] tbase: final FFN
I0123 12:39:32.377645 139872438390784 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:39:32.380071 139872438390784 nn_components.py:329] mlp: final activation = None
I0123 12:39:32.382044 139872438390784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.382142 139872438390784 nn_components.py:261] mlp: residual
I0123 12:39:32.382441 139872438390784 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.382529 139872438390784 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 12:39:32.382646 139872438390784 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:39:32.382686 139872438390784 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:39:32.382719 139872438390784 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:39:32.384617 139872438390784 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.387149 139872438390784 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:39:32.393305 139872438390784 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.393576 139872438390784 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:39:32.396304 139872438390784 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:39:32.400285 139872438390784 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:39:32.400342 139872438390784 attention.py:418] Single window, no scan.
I0123 12:39:32.400380 139872438390784 transformer_layer.py:389] tlayer: self-attention.
I0123 12:39:32.400414 139872438390784 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.400480 139872438390784 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.401059 139872438390784 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.401136 139872438390784 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.401506 139872438390784 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.402320 139872438390784 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.404892 139872438390784 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.405524 139872438390784 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.405606 139872438390784 transformer_layer.py:468] tlayer: End windows.
I0123 12:39:32.405650 139872438390784 transformer_layer.py:472] tlayer: final FFN.
I0123 12:39:32.405715 139872438390784 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.405858 139872438390784 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:39:32.406193 139872438390784 nn_components.py:325] mlp: activation = None
I0123 12:39:32.406237 139872438390784 nn_components.py:329] mlp: final activation = None
I0123 12:39:32.408271 139872438390784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.408371 139872438390784 nn_components.py:261] mlp: residual
I0123 12:39:32.410949 139872438390784 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.411034 139872438390784 transformer_base.py:443] tbase: final FFN
I0123 12:39:32.411475 139872438390784 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:39:32.413913 139872438390784 nn_components.py:329] mlp: final activation = None
I0123 12:39:32.415889 139872438390784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.415995 139872438390784 nn_components.py:261] mlp: residual
I0123 12:39:32.416291 139872438390784 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.416381 139872438390784 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 12:39:32.416498 139872438390784 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:39:32.416539 139872438390784 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:39:32.416573 139872438390784 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:39:32.418561 139872438390784 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.421069 139872438390784 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:39:32.426836 139872438390784 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.427118 139872438390784 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:39:32.429808 139872438390784 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:39:32.433760 139872438390784 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:39:32.433815 139872438390784 attention.py:418] Single window, no scan.
I0123 12:39:32.433852 139872438390784 transformer_layer.py:389] tlayer: self-attention.
I0123 12:39:32.433884 139872438390784 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.433947 139872438390784 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.434526 139872438390784 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.434604 139872438390784 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.434978 139872438390784 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.435781 139872438390784 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.438340 139872438390784 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.439021 139872438390784 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.439101 139872438390784 transformer_layer.py:468] tlayer: End windows.
I0123 12:39:32.439137 139872438390784 transformer_layer.py:472] tlayer: final FFN.
I0123 12:39:32.439204 139872438390784 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.439338 139872438390784 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:39:32.439670 139872438390784 nn_components.py:325] mlp: activation = None
I0123 12:39:32.439715 139872438390784 nn_components.py:329] mlp: final activation = None
I0123 12:39:32.441658 139872438390784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.441754 139872438390784 nn_components.py:261] mlp: residual
I0123 12:39:32.444581 139872438390784 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.444660 139872438390784 transformer_base.py:443] tbase: final FFN
I0123 12:39:32.445158 139872438390784 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:39:32.447562 139872438390784 nn_components.py:329] mlp: final activation = None
I0123 12:39:32.449498 139872438390784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.449593 139872438390784 nn_components.py:261] mlp: residual
I0123 12:39:32.449900 139872438390784 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.449984 139872438390784 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 12:39:32.450106 139872438390784 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:39:32.450147 139872438390784 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:39:32.450181 139872438390784 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:39:32.452128 139872438390784 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.454605 139872438390784 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:39:32.460418 139872438390784 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.460687 139872438390784 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:39:32.463369 139872438390784 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:39:32.467246 139872438390784 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:39:32.467302 139872438390784 attention.py:418] Single window, no scan.
I0123 12:39:32.467338 139872438390784 transformer_layer.py:389] tlayer: self-attention.
I0123 12:39:32.467371 139872438390784 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.467434 139872438390784 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.468008 139872438390784 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.468086 139872438390784 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.468451 139872438390784 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.469253 139872438390784 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.472361 139872438390784 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.473003 139872438390784 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.473083 139872438390784 transformer_layer.py:468] tlayer: End windows.
I0123 12:39:32.473120 139872438390784 transformer_layer.py:472] tlayer: final FFN.
I0123 12:39:32.473183 139872438390784 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.473318 139872438390784 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:39:32.473655 139872438390784 nn_components.py:325] mlp: activation = None
I0123 12:39:32.473701 139872438390784 nn_components.py:329] mlp: final activation = None
I0123 12:39:32.475667 139872438390784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.475761 139872438390784 nn_components.py:261] mlp: residual
I0123 12:39:32.478365 139872438390784 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.478448 139872438390784 transformer_base.py:443] tbase: final FFN
I0123 12:39:32.478893 139872438390784 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:39:32.481185 139872438390784 nn_components.py:329] mlp: final activation = None
I0123 12:39:32.483122 139872438390784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.483219 139872438390784 nn_components.py:261] mlp: residual
I0123 12:39:32.483518 139872438390784 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.483802 139872438390784 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:39:32.483875 139872438390784 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:39:32.483943 139872438390784 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:39:32.484004 139872438390784 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:39:32.484061 139872438390784 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:39:32.484117 139872438390784 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:39:32.484172 139872438390784 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:39:32.484226 139872438390784 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:39:32.484280 139872438390784 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:39:32.484333 139872438390784 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:39:32.484388 139872438390784 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:39:32.484442 139872438390784 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:39:32.484481 139872438390784 decoder_stack.py:344] dstack: Final layernorm.
I0123 12:39:32.488110 139872438390784 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:39:32.537415 139872438390784 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.537502 139872438390784 decoder_stack.py:333] dstack: autoregressive generator.
I0123 12:39:32.537558 139872438390784 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 12:39:32.537672 139872438390784 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:39:32.537713 139872438390784 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:39:32.537746 139872438390784 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:39:32.537812 139872438390784 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.540345 139872438390784 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:39:32.546010 139872438390784 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.546285 139872438390784 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:39:32.549012 139872438390784 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:39:32.566275 139872438390784 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:39:32.566334 139872438390784 attention.py:418] Single window, no scan.
I0123 12:39:32.566372 139872438390784 transformer_layer.py:389] tlayer: self-attention.
I0123 12:39:32.566404 139872438390784 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.566469 139872438390784 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.567654 139872438390784 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.567734 139872438390784 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.568449 139872438390784 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.570563 139872438390784 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.575488 139872438390784 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.576803 139872438390784 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.576889 139872438390784 transformer_layer.py:468] tlayer: End windows.
I0123 12:39:32.576925 139872438390784 transformer_layer.py:472] tlayer: final FFN.
I0123 12:39:32.576985 139872438390784 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.577124 139872438390784 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:39:32.577235 139872438390784 nn_components.py:325] mlp: activation = None
I0123 12:39:32.577275 139872438390784 nn_components.py:329] mlp: final activation = None
I0123 12:39:32.579191 139872438390784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.579288 139872438390784 nn_components.py:261] mlp: residual
I0123 12:39:32.581734 139872438390784 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.581816 139872438390784 transformer_base.py:443] tbase: final FFN
I0123 12:39:32.581925 139872438390784 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:39:32.584169 139872438390784 nn_components.py:329] mlp: final activation = None
I0123 12:39:32.586149 139872438390784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.586247 139872438390784 nn_components.py:261] mlp: residual
I0123 12:39:32.586547 139872438390784 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.586629 139872438390784 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 12:39:32.586741 139872438390784 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:39:32.586780 139872438390784 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:39:32.586812 139872438390784 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:39:32.586876 139872438390784 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.589176 139872438390784 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:39:32.594742 139872438390784 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.595009 139872438390784 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:39:32.597751 139872438390784 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:39:32.611080 139872438390784 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:39:32.611136 139872438390784 attention.py:418] Single window, no scan.
I0123 12:39:32.611171 139872438390784 transformer_layer.py:389] tlayer: self-attention.
I0123 12:39:32.611202 139872438390784 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.611262 139872438390784 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.611818 139872438390784 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.611894 139872438390784 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.612258 139872438390784 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.612964 139872438390784 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.615502 139872438390784 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.616123 139872438390784 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.616201 139872438390784 transformer_layer.py:468] tlayer: End windows.
I0123 12:39:32.616241 139872438390784 transformer_layer.py:472] tlayer: final FFN.
I0123 12:39:32.616302 139872438390784 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.616432 139872438390784 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:39:32.616547 139872438390784 nn_components.py:325] mlp: activation = None
I0123 12:39:32.616587 139872438390784 nn_components.py:329] mlp: final activation = None
I0123 12:39:32.618560 139872438390784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.618655 139872438390784 nn_components.py:261] mlp: residual
I0123 12:39:32.621094 139872438390784 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.621174 139872438390784 transformer_base.py:443] tbase: final FFN
I0123 12:39:32.621285 139872438390784 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:39:32.623526 139872438390784 nn_components.py:329] mlp: final activation = None
I0123 12:39:32.625465 139872438390784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.625561 139872438390784 nn_components.py:261] mlp: residual
I0123 12:39:32.625865 139872438390784 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.625952 139872438390784 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 12:39:32.626066 139872438390784 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:39:32.626107 139872438390784 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:39:32.626139 139872438390784 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:39:32.626207 139872438390784 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.628508 139872438390784 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:39:32.634054 139872438390784 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.634316 139872438390784 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:39:32.637005 139872438390784 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:39:32.649892 139872438390784 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:39:32.649947 139872438390784 attention.py:418] Single window, no scan.
I0123 12:39:32.649983 139872438390784 transformer_layer.py:389] tlayer: self-attention.
I0123 12:39:32.650013 139872438390784 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.650076 139872438390784 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.650638 139872438390784 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.650713 139872438390784 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.651073 139872438390784 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.651767 139872438390784 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.654282 139872438390784 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.654911 139872438390784 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.654989 139872438390784 transformer_layer.py:468] tlayer: End windows.
I0123 12:39:32.655023 139872438390784 transformer_layer.py:472] tlayer: final FFN.
I0123 12:39:32.655087 139872438390784 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.655215 139872438390784 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:39:32.655325 139872438390784 nn_components.py:325] mlp: activation = None
I0123 12:39:32.655365 139872438390784 nn_components.py:329] mlp: final activation = None
I0123 12:39:32.657306 139872438390784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.657400 139872438390784 nn_components.py:261] mlp: residual
I0123 12:39:32.659877 139872438390784 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.659957 139872438390784 transformer_base.py:443] tbase: final FFN
I0123 12:39:32.660070 139872438390784 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:39:32.662332 139872438390784 nn_components.py:329] mlp: final activation = None
I0123 12:39:32.664277 139872438390784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.664373 139872438390784 nn_components.py:261] mlp: residual
I0123 12:39:32.664665 139872438390784 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.664747 139872438390784 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 12:39:32.664857 139872438390784 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:39:32.664897 139872438390784 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:39:32.664929 139872438390784 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:39:32.664993 139872438390784 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.667269 139872438390784 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:39:32.672884 139872438390784 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.673148 139872438390784 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:39:32.676029 139872438390784 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:39:32.689039 139872438390784 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:39:32.689095 139872438390784 attention.py:418] Single window, no scan.
I0123 12:39:32.689131 139872438390784 transformer_layer.py:389] tlayer: self-attention.
I0123 12:39:32.689162 139872438390784 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.689225 139872438390784 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.689800 139872438390784 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.689878 139872438390784 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.690239 139872438390784 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.690942 139872438390784 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.693481 139872438390784 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.694127 139872438390784 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.694207 139872438390784 transformer_layer.py:468] tlayer: End windows.
I0123 12:39:32.694241 139872438390784 transformer_layer.py:472] tlayer: final FFN.
I0123 12:39:32.694301 139872438390784 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.694439 139872438390784 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:39:32.694551 139872438390784 nn_components.py:325] mlp: activation = None
I0123 12:39:32.694591 139872438390784 nn_components.py:329] mlp: final activation = None
I0123 12:39:32.696578 139872438390784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.696673 139872438390784 nn_components.py:261] mlp: residual
I0123 12:39:32.699141 139872438390784 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.699223 139872438390784 transformer_base.py:443] tbase: final FFN
I0123 12:39:32.699335 139872438390784 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:39:32.701605 139872438390784 nn_components.py:329] mlp: final activation = None
I0123 12:39:32.703524 139872438390784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.703621 139872438390784 nn_components.py:261] mlp: residual
I0123 12:39:32.703915 139872438390784 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.703999 139872438390784 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 12:39:32.704112 139872438390784 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:39:32.704152 139872438390784 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:39:32.704185 139872438390784 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:39:32.704250 139872438390784 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.706884 139872438390784 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:39:32.712479 139872438390784 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.712749 139872438390784 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:39:32.715422 139872438390784 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:39:32.728407 139872438390784 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:39:32.728463 139872438390784 attention.py:418] Single window, no scan.
I0123 12:39:32.728499 139872438390784 transformer_layer.py:389] tlayer: self-attention.
I0123 12:39:32.728530 139872438390784 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.728593 139872438390784 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.729157 139872438390784 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.729234 139872438390784 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.729595 139872438390784 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.730320 139872438390784 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.732907 139872438390784 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.733541 139872438390784 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.733620 139872438390784 transformer_layer.py:468] tlayer: End windows.
I0123 12:39:32.733663 139872438390784 transformer_layer.py:472] tlayer: final FFN.
I0123 12:39:32.733725 139872438390784 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.733865 139872438390784 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:39:32.733979 139872438390784 nn_components.py:325] mlp: activation = None
I0123 12:39:32.734019 139872438390784 nn_components.py:329] mlp: final activation = None
I0123 12:39:32.735927 139872438390784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.736023 139872438390784 nn_components.py:261] mlp: residual
I0123 12:39:32.738483 139872438390784 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.738566 139872438390784 transformer_base.py:443] tbase: final FFN
I0123 12:39:32.738678 139872438390784 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:39:32.740998 139872438390784 nn_components.py:329] mlp: final activation = None
I0123 12:39:32.742888 139872438390784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.742986 139872438390784 nn_components.py:261] mlp: residual
I0123 12:39:32.743278 139872438390784 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.743360 139872438390784 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 12:39:32.743472 139872438390784 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:39:32.743513 139872438390784 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:39:32.743545 139872438390784 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:39:32.743611 139872438390784 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.745913 139872438390784 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:39:32.751450 139872438390784 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.751718 139872438390784 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:39:32.754457 139872438390784 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:39:32.772805 139872438390784 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:39:32.772888 139872438390784 attention.py:418] Single window, no scan.
I0123 12:39:32.772926 139872438390784 transformer_layer.py:389] tlayer: self-attention.
I0123 12:39:32.772959 139872438390784 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.773036 139872438390784 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.773664 139872438390784 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.773914 139872438390784 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.774294 139872438390784 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.775022 139872438390784 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.777602 139872438390784 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.778253 139872438390784 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.778333 139872438390784 transformer_layer.py:468] tlayer: End windows.
I0123 12:39:32.778368 139872438390784 transformer_layer.py:472] tlayer: final FFN.
I0123 12:39:32.778432 139872438390784 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.778568 139872438390784 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:39:32.778691 139872438390784 nn_components.py:325] mlp: activation = None
I0123 12:39:32.778731 139872438390784 nn_components.py:329] mlp: final activation = None
I0123 12:39:32.780809 139872438390784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.780906 139872438390784 nn_components.py:261] mlp: residual
I0123 12:39:32.783625 139872438390784 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.783707 139872438390784 transformer_base.py:443] tbase: final FFN
I0123 12:39:32.783821 139872438390784 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:39:32.786148 139872438390784 nn_components.py:329] mlp: final activation = None
I0123 12:39:32.788068 139872438390784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.788164 139872438390784 nn_components.py:261] mlp: residual
I0123 12:39:32.788457 139872438390784 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.788543 139872438390784 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 12:39:32.788659 139872438390784 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:39:32.788702 139872438390784 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:39:32.788735 139872438390784 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:39:32.788803 139872438390784 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.791139 139872438390784 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:39:32.796820 139872438390784 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.797085 139872438390784 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:39:32.799786 139872438390784 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:39:32.812773 139872438390784 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:39:32.812827 139872438390784 attention.py:418] Single window, no scan.
I0123 12:39:32.812863 139872438390784 transformer_layer.py:389] tlayer: self-attention.
I0123 12:39:32.812894 139872438390784 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.812956 139872438390784 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.813524 139872438390784 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.813602 139872438390784 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.813973 139872438390784 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.814669 139872438390784 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.817182 139872438390784 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.818191 139872438390784 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.818270 139872438390784 transformer_layer.py:468] tlayer: End windows.
I0123 12:39:32.818305 139872438390784 transformer_layer.py:472] tlayer: final FFN.
I0123 12:39:32.818364 139872438390784 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.818500 139872438390784 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:39:32.818613 139872438390784 nn_components.py:325] mlp: activation = None
I0123 12:39:32.818657 139872438390784 nn_components.py:329] mlp: final activation = None
I0123 12:39:32.820593 139872438390784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.820687 139872438390784 nn_components.py:261] mlp: residual
I0123 12:39:32.823120 139872438390784 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.823200 139872438390784 transformer_base.py:443] tbase: final FFN
I0123 12:39:32.823315 139872438390784 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:39:32.825574 139872438390784 nn_components.py:329] mlp: final activation = None
I0123 12:39:32.827531 139872438390784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.827630 139872438390784 nn_components.py:261] mlp: residual
I0123 12:39:32.827925 139872438390784 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.828008 139872438390784 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 12:39:32.828121 139872438390784 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:39:32.828161 139872438390784 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:39:32.828193 139872438390784 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:39:32.828258 139872438390784 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.830534 139872438390784 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:39:32.836062 139872438390784 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.836343 139872438390784 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:39:32.839076 139872438390784 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:39:32.852056 139872438390784 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:39:32.852110 139872438390784 attention.py:418] Single window, no scan.
I0123 12:39:32.852146 139872438390784 transformer_layer.py:389] tlayer: self-attention.
I0123 12:39:32.852177 139872438390784 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.852238 139872438390784 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.852855 139872438390784 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.852931 139872438390784 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.853291 139872438390784 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.854006 139872438390784 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.856524 139872438390784 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.857164 139872438390784 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.857241 139872438390784 transformer_layer.py:468] tlayer: End windows.
I0123 12:39:32.857275 139872438390784 transformer_layer.py:472] tlayer: final FFN.
I0123 12:39:32.857334 139872438390784 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.857466 139872438390784 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:39:32.857576 139872438390784 nn_components.py:325] mlp: activation = None
I0123 12:39:32.857620 139872438390784 nn_components.py:329] mlp: final activation = None
I0123 12:39:32.859548 139872438390784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.859645 139872438390784 nn_components.py:261] mlp: residual
I0123 12:39:32.862147 139872438390784 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.862232 139872438390784 transformer_base.py:443] tbase: final FFN
I0123 12:39:32.862344 139872438390784 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:39:32.864599 139872438390784 nn_components.py:329] mlp: final activation = None
I0123 12:39:32.866507 139872438390784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.866605 139872438390784 nn_components.py:261] mlp: residual
I0123 12:39:32.866893 139872438390784 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.866975 139872438390784 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 12:39:32.867086 139872438390784 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:39:32.867126 139872438390784 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:39:32.867157 139872438390784 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:39:32.867223 139872438390784 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.869549 139872438390784 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:39:32.875211 139872438390784 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.875478 139872438390784 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:39:32.878137 139872438390784 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:39:32.891560 139872438390784 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:39:32.891618 139872438390784 attention.py:418] Single window, no scan.
I0123 12:39:32.891654 139872438390784 transformer_layer.py:389] tlayer: self-attention.
I0123 12:39:32.891684 139872438390784 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.891747 139872438390784 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.892318 139872438390784 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.892395 139872438390784 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.892763 139872438390784 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.893469 139872438390784 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.895995 139872438390784 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.896674 139872438390784 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.896752 139872438390784 transformer_layer.py:468] tlayer: End windows.
I0123 12:39:32.896787 139872438390784 transformer_layer.py:472] tlayer: final FFN.
I0123 12:39:32.896846 139872438390784 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.896981 139872438390784 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:39:32.897093 139872438390784 nn_components.py:325] mlp: activation = None
I0123 12:39:32.897132 139872438390784 nn_components.py:329] mlp: final activation = None
I0123 12:39:32.899045 139872438390784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.899142 139872438390784 nn_components.py:261] mlp: residual
I0123 12:39:32.901579 139872438390784 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.901665 139872438390784 transformer_base.py:443] tbase: final FFN
I0123 12:39:32.901781 139872438390784 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:39:32.904023 139872438390784 nn_components.py:329] mlp: final activation = None
I0123 12:39:32.906019 139872438390784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.906116 139872438390784 nn_components.py:261] mlp: residual
I0123 12:39:32.906410 139872438390784 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.906493 139872438390784 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 12:39:32.906606 139872438390784 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:39:32.906645 139872438390784 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:39:32.906677 139872438390784 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:39:32.906741 139872438390784 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.908996 139872438390784 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:39:32.914536 139872438390784 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.914798 139872438390784 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:39:32.917492 139872438390784 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:39:32.930659 139872438390784 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:39:32.930715 139872438390784 attention.py:418] Single window, no scan.
I0123 12:39:32.930751 139872438390784 transformer_layer.py:389] tlayer: self-attention.
I0123 12:39:32.930780 139872438390784 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.930841 139872438390784 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.931451 139872438390784 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.931527 139872438390784 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.931892 139872438390784 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.932593 139872438390784 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.935124 139872438390784 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.935766 139872438390784 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.935843 139872438390784 transformer_layer.py:468] tlayer: End windows.
I0123 12:39:32.935880 139872438390784 transformer_layer.py:472] tlayer: final FFN.
I0123 12:39:32.935941 139872438390784 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.936075 139872438390784 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:39:32.936191 139872438390784 nn_components.py:325] mlp: activation = None
I0123 12:39:32.936230 139872438390784 nn_components.py:329] mlp: final activation = None
I0123 12:39:32.938171 139872438390784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.938274 139872438390784 nn_components.py:261] mlp: residual
I0123 12:39:32.940819 139872438390784 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.940899 139872438390784 transformer_base.py:443] tbase: final FFN
I0123 12:39:32.941012 139872438390784 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:39:32.943312 139872438390784 nn_components.py:329] mlp: final activation = None
I0123 12:39:32.945229 139872438390784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.945326 139872438390784 nn_components.py:261] mlp: residual
I0123 12:39:32.945621 139872438390784 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.945714 139872438390784 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 12:39:32.945827 139872438390784 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:39:32.945868 139872438390784 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:39:32.945901 139872438390784 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:39:32.945966 139872438390784 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.948266 139872438390784 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:39:32.953933 139872438390784 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.954199 139872438390784 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:39:32.956885 139872438390784 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:39:32.970011 139872438390784 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:39:32.970068 139872438390784 attention.py:418] Single window, no scan.
I0123 12:39:32.970106 139872438390784 transformer_layer.py:389] tlayer: self-attention.
I0123 12:39:32.970138 139872438390784 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.970201 139872438390784 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.970771 139872438390784 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.970848 139872438390784 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.971214 139872438390784 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.971929 139872438390784 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.974473 139872438390784 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.975163 139872438390784 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.975243 139872438390784 transformer_layer.py:468] tlayer: End windows.
I0123 12:39:32.975278 139872438390784 transformer_layer.py:472] tlayer: final FFN.
I0123 12:39:32.975339 139872438390784 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.975474 139872438390784 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:39:32.975595 139872438390784 nn_components.py:325] mlp: activation = None
I0123 12:39:32.975636 139872438390784 nn_components.py:329] mlp: final activation = None
I0123 12:39:32.977557 139872438390784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.977665 139872438390784 nn_components.py:261] mlp: residual
I0123 12:39:32.980146 139872438390784 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.980226 139872438390784 transformer_base.py:443] tbase: final FFN
I0123 12:39:32.980337 139872438390784 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:39:32.982602 139872438390784 nn_components.py:329] mlp: final activation = None
I0123 12:39:32.984586 139872438390784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.984683 139872438390784 nn_components.py:261] mlp: residual
I0123 12:39:32.984974 139872438390784 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.985057 139872438390784 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 12:39:32.985168 139872438390784 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:39:32.985208 139872438390784 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:39:32.985242 139872438390784 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:39:32.985307 139872438390784 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.987641 139872438390784 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:39:32.993258 139872438390784 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:32.993527 139872438390784 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:39:32.996278 139872438390784 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:39:33.009366 139872438390784 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:39:33.009422 139872438390784 attention.py:418] Single window, no scan.
I0123 12:39:33.009459 139872438390784 transformer_layer.py:389] tlayer: self-attention.
I0123 12:39:33.009491 139872438390784 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:33.009554 139872438390784 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:33.010125 139872438390784 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:33.010202 139872438390784 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:33.010571 139872438390784 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:33.011332 139872438390784 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:33.013896 139872438390784 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:33.014538 139872438390784 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:33.014617 139872438390784 transformer_layer.py:468] tlayer: End windows.
I0123 12:39:33.014653 139872438390784 transformer_layer.py:472] tlayer: final FFN.
I0123 12:39:33.014715 139872438390784 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:33.014851 139872438390784 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:39:33.014963 139872438390784 nn_components.py:325] mlp: activation = None
I0123 12:39:33.015002 139872438390784 nn_components.py:329] mlp: final activation = None
I0123 12:39:33.016942 139872438390784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:33.017039 139872438390784 nn_components.py:261] mlp: residual
I0123 12:39:33.019540 139872438390784 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:33.019623 139872438390784 transformer_base.py:443] tbase: final FFN
I0123 12:39:33.019736 139872438390784 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:39:33.022105 139872438390784 nn_components.py:329] mlp: final activation = None
I0123 12:39:33.024025 139872438390784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:33.024121 139872438390784 nn_components.py:261] mlp: residual
I0123 12:39:33.024417 139872438390784 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:33.024509 139872438390784 decoder_stack.py:344] dstack: Final layernorm.
I0123 12:39:33.027493 139872438390784 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:39:33.083956 139872438390784 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.084042 139872438390784 decoder_stack.py:333] dstack: autoregressive generator.
I0123 12:39:33.084098 139872438390784 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 12:39:33.084208 139872438390784 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:39:33.084248 139872438390784 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:39:33.084280 139872438390784 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:39:33.084345 139872438390784 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.087258 139872438390784 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:39:33.092764 139872438390784 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.093031 139872438390784 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:39:33.095675 139872438390784 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:39:33.108456 139872438390784 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:39:33.108513 139872438390784 attention.py:418] Single window, no scan.
I0123 12:39:33.108550 139872438390784 transformer_layer.py:389] tlayer: self-attention.
I0123 12:39:33.108583 139872438390784 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.108646 139872438390784 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.109214 139872438390784 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.109291 139872438390784 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.109661 139872438390784 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.110358 139872438390784 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.112914 139872438390784 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.113539 139872438390784 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.113618 139872438390784 transformer_layer.py:468] tlayer: End windows.
I0123 12:39:33.113663 139872438390784 transformer_layer.py:472] tlayer: final FFN.
I0123 12:39:33.113728 139872438390784 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.113863 139872438390784 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:39:33.113983 139872438390784 nn_components.py:325] mlp: activation = None
I0123 12:39:33.114024 139872438390784 nn_components.py:329] mlp: final activation = None
I0123 12:39:33.115908 139872438390784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.116003 139872438390784 nn_components.py:261] mlp: residual
I0123 12:39:33.118468 139872438390784 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.118549 139872438390784 transformer_base.py:443] tbase: final FFN
I0123 12:39:33.118662 139872438390784 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:39:33.120956 139872438390784 nn_components.py:329] mlp: final activation = None
I0123 12:39:33.122843 139872438390784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.122940 139872438390784 nn_components.py:261] mlp: residual
I0123 12:39:33.123233 139872438390784 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.123316 139872438390784 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 12:39:33.123425 139872438390784 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:39:33.123465 139872438390784 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:39:33.123497 139872438390784 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:39:33.123563 139872438390784 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.125853 139872438390784 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:39:33.131319 139872438390784 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.131584 139872438390784 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:39:33.134288 139872438390784 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:39:33.146967 139872438390784 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:39:33.147023 139872438390784 attention.py:418] Single window, no scan.
I0123 12:39:33.147060 139872438390784 transformer_layer.py:389] tlayer: self-attention.
I0123 12:39:33.147094 139872438390784 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.147156 139872438390784 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.147718 139872438390784 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.147796 139872438390784 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.148159 139872438390784 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.148844 139872438390784 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.151386 139872438390784 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.152014 139872438390784 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.152093 139872438390784 transformer_layer.py:468] tlayer: End windows.
I0123 12:39:33.152128 139872438390784 transformer_layer.py:472] tlayer: final FFN.
I0123 12:39:33.152189 139872438390784 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.152321 139872438390784 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:39:33.152432 139872438390784 nn_components.py:325] mlp: activation = None
I0123 12:39:33.152477 139872438390784 nn_components.py:329] mlp: final activation = None
I0123 12:39:33.154376 139872438390784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.154472 139872438390784 nn_components.py:261] mlp: residual
I0123 12:39:33.156912 139872438390784 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.156993 139872438390784 transformer_base.py:443] tbase: final FFN
I0123 12:39:33.157108 139872438390784 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:39:33.159441 139872438390784 nn_components.py:329] mlp: final activation = None
I0123 12:39:33.161339 139872438390784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.161437 139872438390784 nn_components.py:261] mlp: residual
I0123 12:39:33.161736 139872438390784 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.161821 139872438390784 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 12:39:33.161933 139872438390784 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:39:33.161973 139872438390784 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:39:33.162005 139872438390784 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:39:33.162072 139872438390784 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.164350 139872438390784 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:39:33.169864 139872438390784 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.170144 139872438390784 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:39:33.172924 139872438390784 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:39:33.185739 139872438390784 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:39:33.185796 139872438390784 attention.py:418] Single window, no scan.
I0123 12:39:33.185832 139872438390784 transformer_layer.py:389] tlayer: self-attention.
I0123 12:39:33.185864 139872438390784 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.185926 139872438390784 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.186487 139872438390784 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.186565 139872438390784 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.186938 139872438390784 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.187642 139872438390784 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.190200 139872438390784 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.190824 139872438390784 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.190903 139872438390784 transformer_layer.py:468] tlayer: End windows.
I0123 12:39:33.190940 139872438390784 transformer_layer.py:472] tlayer: final FFN.
I0123 12:39:33.191001 139872438390784 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.191131 139872438390784 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:39:33.191243 139872438390784 nn_components.py:325] mlp: activation = None
I0123 12:39:33.191283 139872438390784 nn_components.py:329] mlp: final activation = None
I0123 12:39:33.193159 139872438390784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.193254 139872438390784 nn_components.py:261] mlp: residual
I0123 12:39:33.195696 139872438390784 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.195777 139872438390784 transformer_base.py:443] tbase: final FFN
I0123 12:39:33.195890 139872438390784 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:39:33.198636 139872438390784 nn_components.py:329] mlp: final activation = None
I0123 12:39:33.200511 139872438390784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.200610 139872438390784 nn_components.py:261] mlp: residual
I0123 12:39:33.200902 139872438390784 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.200986 139872438390784 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 12:39:33.201099 139872438390784 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:39:33.201139 139872438390784 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:39:33.201172 139872438390784 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:39:33.201238 139872438390784 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.203525 139872438390784 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:39:33.209079 139872438390784 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.209347 139872438390784 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:39:33.212059 139872438390784 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:39:33.224825 139872438390784 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:39:33.224881 139872438390784 attention.py:418] Single window, no scan.
I0123 12:39:33.224920 139872438390784 transformer_layer.py:389] tlayer: self-attention.
I0123 12:39:33.224960 139872438390784 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.225024 139872438390784 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.225605 139872438390784 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.225688 139872438390784 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.226063 139872438390784 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.226771 139872438390784 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.229338 139872438390784 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.229970 139872438390784 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.230047 139872438390784 transformer_layer.py:468] tlayer: End windows.
I0123 12:39:33.230081 139872438390784 transformer_layer.py:472] tlayer: final FFN.
I0123 12:39:33.230142 139872438390784 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.230271 139872438390784 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:39:33.230381 139872438390784 nn_components.py:325] mlp: activation = None
I0123 12:39:33.230422 139872438390784 nn_components.py:329] mlp: final activation = None
I0123 12:39:33.232329 139872438390784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.232422 139872438390784 nn_components.py:261] mlp: residual
I0123 12:39:33.234896 139872438390784 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.234977 139872438390784 transformer_base.py:443] tbase: final FFN
I0123 12:39:33.235087 139872438390784 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:39:33.237450 139872438390784 nn_components.py:329] mlp: final activation = None
I0123 12:39:33.239414 139872438390784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.239511 139872438390784 nn_components.py:261] mlp: residual
I0123 12:39:33.239800 139872438390784 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.239882 139872438390784 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 12:39:33.239992 139872438390784 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:39:33.240030 139872438390784 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:39:33.240062 139872438390784 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:39:33.240126 139872438390784 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.242433 139872438390784 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:39:33.247970 139872438390784 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.248237 139872438390784 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:39:33.250990 139872438390784 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:39:33.264101 139872438390784 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:39:33.264155 139872438390784 attention.py:418] Single window, no scan.
I0123 12:39:33.264189 139872438390784 transformer_layer.py:389] tlayer: self-attention.
I0123 12:39:33.264220 139872438390784 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.264284 139872438390784 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.264852 139872438390784 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.264926 139872438390784 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.265287 139872438390784 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.265990 139872438390784 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.268737 139872438390784 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.269363 139872438390784 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.269439 139872438390784 transformer_layer.py:468] tlayer: End windows.
I0123 12:39:33.269474 139872438390784 transformer_layer.py:472] tlayer: final FFN.
I0123 12:39:33.269532 139872438390784 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.269668 139872438390784 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:39:33.269779 139872438390784 nn_components.py:325] mlp: activation = None
I0123 12:39:33.269818 139872438390784 nn_components.py:329] mlp: final activation = None
I0123 12:39:33.271723 139872438390784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.271824 139872438390784 nn_components.py:261] mlp: residual
I0123 12:39:33.274287 139872438390784 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.274366 139872438390784 transformer_base.py:443] tbase: final FFN
I0123 12:39:33.274475 139872438390784 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:39:33.276823 139872438390784 nn_components.py:329] mlp: final activation = None
I0123 12:39:33.278730 139872438390784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.278826 139872438390784 nn_components.py:261] mlp: residual
I0123 12:39:33.279115 139872438390784 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.279196 139872438390784 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 12:39:33.279306 139872438390784 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:39:33.279345 139872438390784 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:39:33.279377 139872438390784 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:39:33.279441 139872438390784 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.281747 139872438390784 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:39:33.287281 139872438390784 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.287549 139872438390784 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:39:33.290333 139872438390784 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:39:33.303283 139872438390784 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:39:33.303337 139872438390784 attention.py:418] Single window, no scan.
I0123 12:39:33.303371 139872438390784 transformer_layer.py:389] tlayer: self-attention.
I0123 12:39:33.303401 139872438390784 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.303462 139872438390784 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.304025 139872438390784 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.304101 139872438390784 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.304471 139872438390784 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.305176 139872438390784 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.307815 139872438390784 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.308439 139872438390784 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.308515 139872438390784 transformer_layer.py:468] tlayer: End windows.
I0123 12:39:33.308549 139872438390784 transformer_layer.py:472] tlayer: final FFN.
I0123 12:39:33.308608 139872438390784 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.308736 139872438390784 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:39:33.308845 139872438390784 nn_components.py:325] mlp: activation = None
I0123 12:39:33.308883 139872438390784 nn_components.py:329] mlp: final activation = None
I0123 12:39:33.310811 139872438390784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.310910 139872438390784 nn_components.py:261] mlp: residual
I0123 12:39:33.313353 139872438390784 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.313431 139872438390784 transformer_base.py:443] tbase: final FFN
I0123 12:39:33.313541 139872438390784 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:39:33.316291 139872438390784 nn_components.py:329] mlp: final activation = None
I0123 12:39:33.318198 139872438390784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.318294 139872438390784 nn_components.py:261] mlp: residual
I0123 12:39:33.318587 139872438390784 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.318668 139872438390784 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 12:39:33.318778 139872438390784 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:39:33.318817 139872438390784 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:39:33.318848 139872438390784 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:39:33.318912 139872438390784 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.321203 139872438390784 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:39:33.326764 139872438390784 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.327033 139872438390784 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:39:33.329782 139872438390784 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:39:33.342771 139872438390784 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:39:33.342825 139872438390784 attention.py:418] Single window, no scan.
I0123 12:39:33.342860 139872438390784 transformer_layer.py:389] tlayer: self-attention.
I0123 12:39:33.342890 139872438390784 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.342952 139872438390784 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.343518 139872438390784 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.343594 139872438390784 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.343957 139872438390784 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.344665 139872438390784 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.347281 139872438390784 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.347925 139872438390784 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.348002 139872438390784 transformer_layer.py:468] tlayer: End windows.
I0123 12:39:33.348037 139872438390784 transformer_layer.py:472] tlayer: final FFN.
I0123 12:39:33.348097 139872438390784 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.348226 139872438390784 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:39:33.348336 139872438390784 nn_components.py:325] mlp: activation = None
I0123 12:39:33.348373 139872438390784 nn_components.py:329] mlp: final activation = None
I0123 12:39:33.350299 139872438390784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.350394 139872438390784 nn_components.py:261] mlp: residual
I0123 12:39:33.352851 139872438390784 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.352930 139872438390784 transformer_base.py:443] tbase: final FFN
I0123 12:39:33.353043 139872438390784 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:39:33.355396 139872438390784 nn_components.py:329] mlp: final activation = None
I0123 12:39:33.357315 139872438390784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.357410 139872438390784 nn_components.py:261] mlp: residual
I0123 12:39:33.357710 139872438390784 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.357791 139872438390784 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 12:39:33.357902 139872438390784 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:39:33.357941 139872438390784 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:39:33.357972 139872438390784 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:39:33.358036 139872438390784 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.360374 139872438390784 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:39:33.365961 139872438390784 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.366230 139872438390784 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:39:33.369065 139872438390784 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:39:33.382063 139872438390784 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:39:33.382117 139872438390784 attention.py:418] Single window, no scan.
I0123 12:39:33.382152 139872438390784 transformer_layer.py:389] tlayer: self-attention.
I0123 12:39:33.382182 139872438390784 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.382243 139872438390784 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.382809 139872438390784 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.382884 139872438390784 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.383245 139872438390784 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.383951 139872438390784 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.386548 139872438390784 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.387191 139872438390784 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.387267 139872438390784 transformer_layer.py:468] tlayer: End windows.
I0123 12:39:33.387302 139872438390784 transformer_layer.py:472] tlayer: final FFN.
I0123 12:39:33.387362 139872438390784 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.387493 139872438390784 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:39:33.387603 139872438390784 nn_components.py:325] mlp: activation = None
I0123 12:39:33.387641 139872438390784 nn_components.py:329] mlp: final activation = None
I0123 12:39:33.389561 139872438390784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.389661 139872438390784 nn_components.py:261] mlp: residual
I0123 12:39:33.392117 139872438390784 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.392201 139872438390784 transformer_base.py:443] tbase: final FFN
I0123 12:39:33.392318 139872438390784 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:39:33.394665 139872438390784 nn_components.py:329] mlp: final activation = None
I0123 12:39:33.396599 139872438390784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.396695 139872438390784 nn_components.py:261] mlp: residual
I0123 12:39:33.396987 139872438390784 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.397068 139872438390784 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 12:39:33.397177 139872438390784 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:39:33.397216 139872438390784 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:39:33.397247 139872438390784 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:39:33.397311 139872438390784 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.399612 139872438390784 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:39:33.405172 139872438390784 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.405434 139872438390784 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:39:33.408220 139872438390784 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:39:33.421177 139872438390784 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:39:33.421233 139872438390784 attention.py:418] Single window, no scan.
I0123 12:39:33.421269 139872438390784 transformer_layer.py:389] tlayer: self-attention.
I0123 12:39:33.421301 139872438390784 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.421365 139872438390784 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.421967 139872438390784 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.422046 139872438390784 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.422421 139872438390784 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.423135 139872438390784 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.425753 139872438390784 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.426383 139872438390784 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.426459 139872438390784 transformer_layer.py:468] tlayer: End windows.
I0123 12:39:33.426493 139872438390784 transformer_layer.py:472] tlayer: final FFN.
I0123 12:39:33.426552 139872438390784 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.426681 139872438390784 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:39:33.426790 139872438390784 nn_components.py:325] mlp: activation = None
I0123 12:39:33.426828 139872438390784 nn_components.py:329] mlp: final activation = None
I0123 12:39:33.428752 139872438390784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.428845 139872438390784 nn_components.py:261] mlp: residual
I0123 12:39:33.431341 139872438390784 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.431425 139872438390784 transformer_base.py:443] tbase: final FFN
I0123 12:39:33.431539 139872438390784 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:39:33.434273 139872438390784 nn_components.py:329] mlp: final activation = None
I0123 12:39:33.436188 139872438390784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.436284 139872438390784 nn_components.py:261] mlp: residual
I0123 12:39:33.436577 139872438390784 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.436660 139872438390784 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 12:39:33.436770 139872438390784 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:39:33.436809 139872438390784 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:39:33.436840 139872438390784 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:39:33.436904 139872438390784 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.439220 139872438390784 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:39:33.444792 139872438390784 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.445057 139872438390784 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:39:33.447820 139872438390784 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:39:33.460796 139872438390784 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:39:33.460850 139872438390784 attention.py:418] Single window, no scan.
I0123 12:39:33.460885 139872438390784 transformer_layer.py:389] tlayer: self-attention.
I0123 12:39:33.460916 139872438390784 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.460983 139872438390784 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.461555 139872438390784 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.461630 139872438390784 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.462002 139872438390784 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.462698 139872438390784 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.465286 139872438390784 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.465934 139872438390784 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.466011 139872438390784 transformer_layer.py:468] tlayer: End windows.
I0123 12:39:33.466045 139872438390784 transformer_layer.py:472] tlayer: final FFN.
I0123 12:39:33.466104 139872438390784 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.466235 139872438390784 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:39:33.466346 139872438390784 nn_components.py:325] mlp: activation = None
I0123 12:39:33.466384 139872438390784 nn_components.py:329] mlp: final activation = None
I0123 12:39:33.468919 139872438390784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.469013 139872438390784 nn_components.py:261] mlp: residual
I0123 12:39:33.471638 139872438390784 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.471718 139872438390784 transformer_base.py:443] tbase: final FFN
I0123 12:39:33.471834 139872438390784 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:39:33.474139 139872438390784 nn_components.py:329] mlp: final activation = None
I0123 12:39:33.476032 139872438390784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.476128 139872438390784 nn_components.py:261] mlp: residual
I0123 12:39:33.476421 139872438390784 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.476502 139872438390784 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 12:39:33.476614 139872438390784 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:39:33.476653 139872438390784 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:39:33.476685 139872438390784 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:39:33.476749 139872438390784 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.479042 139872438390784 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:39:33.484587 139872438390784 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.484852 139872438390784 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:39:33.487607 139872438390784 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:39:33.500554 139872438390784 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:39:33.500609 139872438390784 attention.py:418] Single window, no scan.
I0123 12:39:33.500644 139872438390784 transformer_layer.py:389] tlayer: self-attention.
I0123 12:39:33.500675 139872438390784 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.500740 139872438390784 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.501311 139872438390784 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.501387 139872438390784 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.501754 139872438390784 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.502464 139872438390784 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.505041 139872438390784 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.505684 139872438390784 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.505761 139872438390784 transformer_layer.py:468] tlayer: End windows.
I0123 12:39:33.505796 139872438390784 transformer_layer.py:472] tlayer: final FFN.
I0123 12:39:33.505855 139872438390784 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.505985 139872438390784 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:39:33.506096 139872438390784 nn_components.py:325] mlp: activation = None
I0123 12:39:33.506134 139872438390784 nn_components.py:329] mlp: final activation = None
I0123 12:39:33.508044 139872438390784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.508136 139872438390784 nn_components.py:261] mlp: residual
I0123 12:39:33.510575 139872438390784 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.510655 139872438390784 transformer_base.py:443] tbase: final FFN
I0123 12:39:33.510766 139872438390784 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:39:33.513087 139872438390784 nn_components.py:329] mlp: final activation = None
I0123 12:39:33.515011 139872438390784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.515108 139872438390784 nn_components.py:261] mlp: residual
I0123 12:39:33.515401 139872438390784 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.515482 139872438390784 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 12:39:33.515593 139872438390784 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:39:33.515631 139872438390784 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:39:33.515661 139872438390784 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:39:33.515725 139872438390784 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.518011 139872438390784 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:39:33.523528 139872438390784 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.523796 139872438390784 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:39:33.526545 139872438390784 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:39:33.539491 139872438390784 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:39:33.539545 139872438390784 attention.py:418] Single window, no scan.
I0123 12:39:33.539582 139872438390784 transformer_layer.py:389] tlayer: self-attention.
I0123 12:39:33.539612 139872438390784 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.539674 139872438390784 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.540238 139872438390784 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.540317 139872438390784 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.540683 139872438390784 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.541383 139872438390784 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.543977 139872438390784 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.544609 139872438390784 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.544689 139872438390784 transformer_layer.py:468] tlayer: End windows.
I0123 12:39:33.544723 139872438390784 transformer_layer.py:472] tlayer: final FFN.
I0123 12:39:33.544782 139872438390784 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.544912 139872438390784 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:39:33.545026 139872438390784 nn_components.py:325] mlp: activation = None
I0123 12:39:33.545065 139872438390784 nn_components.py:329] mlp: final activation = None
I0123 12:39:33.546998 139872438390784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.547092 139872438390784 nn_components.py:261] mlp: residual
I0123 12:39:33.549530 139872438390784 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.549609 139872438390784 transformer_base.py:443] tbase: final FFN
I0123 12:39:33.549726 139872438390784 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:39:33.552452 139872438390784 nn_components.py:329] mlp: final activation = None
I0123 12:39:33.554393 139872438390784 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.554489 139872438390784 nn_components.py:261] mlp: residual
I0123 12:39:33.554780 139872438390784 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:33.554865 139872438390784 decoder_stack.py:344] dstack: Final layernorm.
I0123 12:39:33.557762 139872438390784 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:39:38.013061 139872438390784 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
I0123 12:39:38.540642 139872438390784 training_loop.py:409] No working directory specified.
I0123 12:39:38.540760 139872438390784 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0123 12:39:38.541539 139872438390784 checkpoints.py:1062] Restoring legacy Flax checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0123 12:39:41.365023 139872438390784 training_loop.py:447] Only restoring trainable parameters.
I0123 12:39:41.365716 139872438390784 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0123 12:39:41.365775 139872438390784 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0123 12:39:41.365821 139872438390784 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:39:41.365862 139872438390784 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:39:41.365902 139872438390784 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:39:41.365940 139872438390784 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:39:41.365983 139872438390784 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:39:41.366019 139872438390784 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:39:41.366056 139872438390784 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:39:41.366091 139872438390784 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:39:41.366127 139872438390784 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:39:41.366162 139872438390784 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:39:41.366198 139872438390784 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:39:41.366233 139872438390784 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:39:41.366268 139872438390784 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:39:41.366303 139872438390784 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:39:41.366338 139872438390784 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:39:41.366373 139872438390784 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:39:41.366408 139872438390784 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:39:41.366444 139872438390784 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:39:41.366493 139872438390784 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:39:41.366531 139872438390784 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:39:41.366566 139872438390784 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:39:41.366603 139872438390784 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:39:41.366639 139872438390784 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:39:41.366674 139872438390784 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:39:41.366709 139872438390784 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:39:41.366744 139872438390784 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:39:41.366778 139872438390784 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:39:41.366813 139872438390784 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:39:41.366846 139872438390784 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:39:41.366881 139872438390784 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:39:41.366914 139872438390784 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:39:41.366948 139872438390784 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:39:41.366982 139872438390784 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:39:41.367017 139872438390784 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:39:41.367050 139872438390784 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:39:41.367085 139872438390784 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:39:41.367120 139872438390784 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:39:41.367154 139872438390784 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:39:41.367189 139872438390784 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:39:41.367224 139872438390784 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:39:41.367259 139872438390784 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:39:41.367294 139872438390784 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:39:41.367328 139872438390784 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:39:41.367362 139872438390784 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:39:41.367404 139872438390784 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:39:41.367439 139872438390784 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:39:41.367474 139872438390784 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:39:41.367508 139872438390784 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:39:41.367543 139872438390784 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:39:41.367578 139872438390784 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:39:41.367612 139872438390784 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:39:41.367647 139872438390784 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:39:41.367681 139872438390784 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:39:41.367716 139872438390784 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:39:41.367750 139872438390784 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:39:41.367784 139872438390784 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:39:41.367818 139872438390784 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:39:41.367852 139872438390784 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:39:41.367886 139872438390784 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:39:41.367920 139872438390784 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:39:41.367954 139872438390784 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:39:41.367988 139872438390784 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:39:41.368024 139872438390784 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:39:41.368060 139872438390784 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:39:41.368095 139872438390784 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:39:41.368129 139872438390784 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:39:41.368163 139872438390784 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:39:41.368197 139872438390784 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:39:41.368231 139872438390784 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:39:41.368266 139872438390784 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:39:41.368300 139872438390784 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:39:41.368339 139872438390784 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:39:41.368375 139872438390784 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:39:41.368409 139872438390784 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:39:41.368442 139872438390784 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:39:41.368476 139872438390784 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:39:41.368510 139872438390784 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:39:41.368544 139872438390784 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:39:41.368578 139872438390784 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:39:41.368615 139872438390784 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:39:41.368649 139872438390784 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:39:41.368683 139872438390784 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:39:41.368717 139872438390784 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:39:41.368751 139872438390784 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:39:41.368786 139872438390784 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:39:41.368821 139872438390784 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:39:41.368854 139872438390784 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:39:41.368889 139872438390784 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:39:41.368923 139872438390784 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:39:41.368958 139872438390784 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:39:41.368992 139872438390784 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:39:41.369026 139872438390784 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:39:41.369060 139872438390784 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:39:41.369094 139872438390784 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:39:41.369128 139872438390784 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:39:41.369162 139872438390784 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:39:41.369196 139872438390784 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:39:41.369230 139872438390784 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:39:41.369270 139872438390784 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:39:41.369305 139872438390784 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:39:41.369339 139872438390784 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:39:41.369374 139872438390784 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:39:41.369407 139872438390784 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:39:41.369441 139872438390784 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:39:41.369476 139872438390784 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:39:41.369511 139872438390784 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:39:41.369546 139872438390784 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:39:41.369582 139872438390784 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:39:41.369617 139872438390784 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:39:41.369661 139872438390784 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:39:41.369699 139872438390784 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:39:41.369734 139872438390784 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:39:41.369768 139872438390784 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:39:41.369804 139872438390784 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:39:41.369839 139872438390784 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:39:41.369874 139872438390784 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:39:41.369910 139872438390784 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:39:41.369944 139872438390784 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:39:41.369978 139872438390784 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:39:41.370012 139872438390784 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:39:41.370040 139872438390784 training_loop.py:725] Total parameters: 152072288
I0123 12:39:41.370247 139872438390784 training_loop.py:739] Total state size: 0
I0123 12:39:41.389822 139872438390784 training_loop.py:492] Training loop: creating task for mode beam_search
I0123 12:39:41.390080 139872438390784 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0123 12:39:41.390568 139872438390784 training_loop.py:652] Compiling mode beam_search with jit.
I0123 12:39:41.390882 139872438390784 training_loop.py:89] registering functions: dict_keys([])
I0123 12:39:41.406884 139872438390784 graph.py:499] a b c = triangle a b c; d = on_line d b a; e = lc_tangent e b d, on_line e c d; f = on_circle f b d, on_line f b e; g = on_circle g b d, on_line g b e; h = foot h b g d; i = mirror i b h; j = foot j a d f; k = mirror k a j; l = foot l d h j; m = mirror m d l ? coll k b m
I0123 12:39:42.776763 139872438390784 ddar.py:60] Depth 1/1000 time = 1.3263447284698486
I0123 12:39:53.198760 139872438390784 ddar.py:60] Depth 2/1000 time = 10.421813488006592
I0123 12:40:09.230776 139872438390784 ddar.py:60] Depth 3/1000 time = 16.031728267669678
I0123 12:40:42.146267 139872438390784 ddar.py:60] Depth 4/1000 time = 32.91508412361145
I0123 12:40:42.260388 139872438390784 alphageometry.py:191] 
==========================
 * From theorem premises:
A B D E F G H J K L M : Points
B,A,D are collinear [00]
BE ⟂ BD [01]
BF = BD [02]
B,F,E are collinear [03]
BG = BD [04]
B,G,E are collinear [05]
G,H,D are collinear [06]
BH ⟂ DG [07]
AJ ⟂ DF [08]
F,J,D are collinear [09]
K,J,A are collinear [10]
JA = JK [11]
J,L,H are collinear [12]
DL ⟂ HJ [13]
M,L,D are collinear [14]
LD = LM [15]

 * Auxiliary Constructions:
I : Points
HB = HI [16]
B,I,H are collinear [17]

 * Proof steps:
001. D,G,H are collinear [06] & B,I,H are collinear [17] & BH ⟂ DG [07] ⇒  ∠BHD = ∠DHI [18]
002. HB = HI [16] & ∠BHD = ∠DHI [18] (SAS)⇒  ∠BDH = ∠HDI [19]
003. BG = BD [04] & BF = BD [02] ⇒  B is the circumcenter of \Delta GDF [20]
004. BG = BD [04] & BF = BD [02] ⇒  GB = FB [21]
005. B,F,E are collinear [03] & B,G,E are collinear [05] ⇒  B,G,F are collinear [22]
006. B is the circumcenter of \Delta GDF [20] & B,G,F are collinear [22] ⇒  DF ⟂ GD [23]
007. F,J,D are collinear [09] & K,J,A are collinear [10] & AJ ⟂ DF [08] ⇒  ∠AJD = ∠DJK [24]
008. JA = JK [11] & ∠AJD = ∠DJK [24] (SAS)⇒  ∠JAD = ∠DKJ [25]
009. JA = JK [11] & ∠AJD = ∠DJK [24] (SAS)⇒  ∠ADJ = ∠JDK [26]
010. J,K,A are collinear [10] & ∠BDH = ∠HDI [19] & B,A,D are collinear [00] & G,H,D are collinear [06] & DF ⟂ GD [23] & AJ ⟂ DF [08] & ∠JAD = ∠DKJ [25] ⇒  ∠DKJ = ∠(ID-KJ) [27]
011. ∠DKJ = ∠(ID-KJ) [27] ⇒  KD ∥ ID [28]
012. DK ∥ DI [28] ⇒  K,I,D are collinear [29]
013. D,G,H are collinear [06] & B,A,D are collinear [00] & B,G,E are collinear [05] & BE ⟂ BD [01] & BH ⟂ DG [07] ⇒  ∠BHD = ∠DBG [30]
014. B,A,D are collinear [00] & G,H,D are collinear [06] ⇒  ∠BDH = ∠BDG [31]
015. ∠BHD = ∠DBG [30] & ∠BDH = ∠BDG [31] (Similar Triangles)⇒  BG:BD = HB:HD [32]
016. M,L,D are collinear [14] & LD = LM [15] ⇒  L is midpoint of MD [33]
017. J,L,H are collinear [12] & M,L,D are collinear [14] & HJ ⟂ DL [13] ⇒  HL ⟂ MD [34]
018. L is midpoint of MD [33] & HL ⟂ MD [34] ⇒  HM = HD [35]
019. BG:BD = HB:HD [32] & BG = BD [04] & HM = HD [35] ⇒  H is the circumcenter of \Delta DBM [36]
020. D,G,H are collinear [06] & DG ⟂ DF [23] ⇒  HD ⟂ DF [37]
021. H is the circumcenter of \Delta DBM [36] & HD ⟂ DF [37] ⇒  ∠FDB = ∠DMB [38]
022. F,J,D are collinear [09] & J,K,A are collinear [10] & DF ⟂ AJ [08] ⇒  FJ ⟂ KJ [39]
023. D,G,H are collinear [06] & B,I,H are collinear [17] & DG ⟂ BH [07] ⇒  GH ⟂ BI [40]
024. FJ ⟂ KJ [39] & GH ⟂ BI [40] ⇒  ∠(FJ-GH) = ∠(KJ-BI) [41]
025. D,G,H are collinear [06] & F,J,D are collinear [09] & ∠(FJ-GH) = ∠(KJ-BI) [41] & K,J,A are collinear [10] & B,I,H are collinear [17] ⇒  ∠BHG = ∠AJD [42]
026. B,F,E are collinear [03] & BE ⟂ BD [01] & B,A,D are collinear [00] ⇒  BA ⟂ BF [43]
027. GH ⟂ BI [40] & BA ⟂ BF [43] ⇒  ∠(GH-BF) = ∠IBA [44]
028. B,G,E are collinear [05] & D,G,H are collinear [06] & B,A,D are collinear [00] & F,J,D are collinear [09] & ∠(GH-BF) = ∠IBA [44] & B,F,E are collinear [03] & B,I,H are collinear [17] & DG ⟂ FD [23] & BH ⟂ DG [07] ⇒  ∠BGH = ∠ADJ [45]
029. ∠BHG = ∠AJD [42] & ∠BGH = ∠ADJ [45] (Similar Triangles)⇒  BH:AJ = BG:AD [46]
030. B,I,H are collinear [17] & HB = HI [16] ⇒  H is midpoint of BI [47]
031. H is midpoint of BI [47] & GH ⟂ BI [40] ⇒  GB = GI [48]
032. D,G,H are collinear [06] & B,G,E are collinear [05] & B,A,D are collinear [00] & BE ⟂ BD [01] & BH ⟂ DG [07] ⇒  ∠BHG = ∠GBD [49]
033. B,G,E are collinear [05] & D,G,H are collinear [06] ⇒  ∠BGH = ∠BGD [50]
034. ∠BHG = ∠GBD [49] & ∠BGH = ∠BGD [50] (Similar Triangles)⇒  BD:BG = HB:HG [51]
035. BD:BG = HB:HG [51] & BG = BD [04] ⇒  HB = HG [52]
036. D,G,H are collinear [06] & B,I,H are collinear [17] & BH ⟂ DG [07] ⇒  ∠BHG = ∠GHI [53]
037. HB = HI [16] & ∠BHG = ∠GHI [53] (SAS)⇒  ∠BGH = ∠HGI [54]
038. HB = HI [16] & ∠BHG = ∠GHI [53] (SAS)⇒  ∠HBG = ∠GIH [55]
039. B,G,E are collinear [05] & B,A,D are collinear [00] & B,F,E are collinear [03] & BE ⟂ BD [01] ⇒  ∠GBD = ∠DBF [56]
040. GB = FB [21] & ∠GBD = ∠DBF [56] (SAS)⇒  ∠BGD = ∠DFB [57]
041. GB = FB [21] & ∠GBD = ∠DBF [56] (SAS)⇒  ∠GDB = ∠BDF [58]
042. B,G,E are collinear [05] & D,G,H are collinear [06] & ∠BGH = ∠HGI [54] & DF ⟂ GD [23] & AJ ⟂ DF [08] & ∠BGD = ∠DFB [57] & B,F,E are collinear [03] & BH ⟂ DG [07] ⇒  ∠GBH = ∠IGH [59]
043. GB = GI [48] & HB = HG [52] & ∠GBH = ∠IGH [59] (SAS)⇒  HG = HI [60]
044. F,J,D are collinear [09] & J,K,A are collinear [10] & ∠JAD = ∠DKJ [25] & B,A,D are collinear [00] & DF ⟂ GD [23] & AJ ⟂ DF [08] & ∠GDB = ∠BDF [58] & ∠ADJ = ∠JDK [26] ⇒  ∠JDK = ∠DKJ [61]
045. ∠JDK = ∠DKJ [61] ⇒  JD = JK [62]
046. M,L,D are collinear [14] & J,L,H are collinear [12] & DL ⟂ HJ [13] ⇒  ML ⟂ JL [63]
047. ML ⟂ JL [63] & FJ ⟂ KJ [39] ⇒  ∠(ML-KJ) = ∠LJF [64]
048. ML ⟂ JL [63] & FJ ⟂ KJ [39] ⇒  ∠(ML-FJ) = ∠LJK [65]
049. F,J,D are collinear [09] & G,H,D are collinear [06] & J,L,H are collinear [12] & M,L,D are collinear [14] & ∠(ML-KJ) = ∠LJF [64] & K,J,A are collinear [10] & DF ⟂ GD [23] & AJ ⟂ DF [08] ⇒  ∠JDH = ∠JLM [66]
050. J,L,H are collinear [12] & M,L,D are collinear [14] & DL ⟂ HJ [13] ⇒  ∠DLJ = ∠JLM [67]
051. LD = LM [15] & ∠DLJ = ∠JLM [67] (SAS)⇒  ∠LDJ = ∠JML [68]
052. D,G,H are collinear [06] & M,L,D are collinear [14] & ∠LDJ = ∠JML [68] & F,J,D are collinear [09] & ∠(ML-FJ) = ∠LJK [65] & J,L,H are collinear [12] & K,J,A are collinear [10] & DF ⟂ GD [23] & AJ ⟂ DF [08] ⇒  ∠JHD = ∠JML [69]
053. ∠JDH = ∠JLM [66] & ∠JHD = ∠JML [69] (Similar Triangles)⇒  DJ:DH = LJ:LM [70]
054. GB = GI [48] & BG = BD [04] ⇒  DB = GI [71]
055. FJ ⟂ KJ [39] & BA ⟂ BF [43] ⇒  ∠(FJ-BA) = ∠(KJ-BF) [72]
056. D,G,H are collinear [06] & B,A,D are collinear [00] & ∠(FJ-BA) = ∠(KJ-BF) [72] & F,J,D are collinear [09] & K,J,A are collinear [10] & B,F,E are collinear [03] & ∠BGH = ∠HGI [54] & B,G,E are collinear [05] & DF ⟂ GD [23] & AJ ⟂ DF [08] & BH ⟂ DG [07] ⇒  ∠IGH = ∠HBD [73]
057. DB = GI [71] & HB = HG [52] & ∠IGH = ∠HBD [73] (SAS)⇒  HI = HD [74]
058. G,H,D are collinear [06] & B,I,H are collinear [17] & DG ⟂ BH [07] ⇒  DH ⟂ BI [75]
059. H is midpoint of BI [47] & DH ⟂ BI [75] ⇒  DB = DI [76]
060. BH:AJ = BG:AD [46] & HB = HI [16] & HG = HI [60] & JA = JK [11] & JD = JK [62] & GB = GI [48] & DJ:DH = LJ:LM [70] & HI = HD [74] & LD = ML [15] & DB = GI [71] & DB = DI [76] ⇒  LD:LJ = DI:DA [77]
061. ML ⟂ JL [63] & BA ⟂ BF [43] ⇒  ∠(ML-BA) = ∠(JL-BF) [78]
062. BG = BD [04] (SSS)⇒  ∠BGD = ∠GDB [79]
063. B,F,E are collinear [03] & J,K,A are collinear [10] & ∠BDH = ∠HDI [19] & B,A,D are collinear [00] & G,H,D are collinear [06] & DF ⟂ GD [23] & AJ ⟂ DF [08] & ∠BGD = ∠GDB [79] & B,G,E are collinear [05] ⇒  ∠(BF-KJ) = ∠(ID-KJ) [80]
064. ∠(BF-KJ) = ∠(ID-KJ) [80] ⇒  BF ∥ ID [81]
065. J,L,H are collinear [12] & B,A,D are collinear [00] & ∠(ML-BA) = ∠(JL-BF) [78] & M,L,D are collinear [14] & B,F,E are collinear [03] & BF ∥ ID [81] ⇒  ∠DLJ = ∠ADI [82]
066. LD:LJ = DI:DA [77] & ∠DLJ = ∠ADI [82] (Similar Triangles)⇒  ∠LDJ = ∠AID [83]
067. BF = BD [02] ⇒  ∠BDF = ∠DFB [84]
068. B,I,H are collinear [17] & ∠HBG = ∠GIH [55] & B,G,E are collinear [05] & DG ⟂ FD [23] & BH ⟂ DG [07] & ∠BDF = ∠DFB [84] & B,A,D are collinear [00] & B,F,E are collinear [03] ⇒  ∠ABI = ∠GIB [85]
069. ∠ABI = ∠GIB [85] ⇒  BA ∥ GI [86]
070. K,I,D are collinear [29] & ∠FDB = ∠DMB [38] & B,A,D are collinear [00] & M,L,D are collinear [14] & ∠LDJ = ∠AID [83] & F,J,D are collinear [09] & BF ∥ ID [81] & B,F,E are collinear [03] & AB ∥ GI [86] ⇒  ∠KIA = ∠(GI-MB) [87]
071. K,I,D are collinear [29] & BE ⟂ BD [01] & B,A,D are collinear [00] & AB ∥ GI [86] & BF ∥ ID [81] & B,F,E are collinear [03] ⇒  GI ⟂ KI [88]
072. ∠KIA = ∠(GI-MB) [87] & GI ⟂ KI [88] ⇒  AI ⟂ MB [89]
073. K,J,A are collinear [10] & JA = JK [11] ⇒  J is midpoint of KA [90]
074. F,J,D are collinear [09] & J,K,A are collinear [10] & DF ⟂ AJ [08] ⇒  DJ ⟂ KA [91]
075. J is midpoint of KA [90] & DJ ⟂ KA [91] ⇒  DK = DA [92]
076. B,F,E are collinear [03] & J,K,A are collinear [10] & ∠JAD = ∠DKJ [25] & B,A,D are collinear [00] & DF ⟂ GD [23] & AJ ⟂ DF [08] & ∠BGD = ∠GDB [79] & B,G,E are collinear [05] ⇒  ∠(BF-KJ) = ∠DKJ [93]
077. ∠(BF-KJ) = ∠DKJ [93] ⇒  BF ∥ KD [94]
078. B,A,D are collinear [00] & BE ⟂ BD [01] & BF ∥ KD [94] & B,F,E are collinear [03] & BF ∥ ID [81] ⇒  ∠BDK = ∠IDA [95]
079. DB = DI [76] & DK = DA [92] & ∠BDK = ∠IDA [95] (SAS)⇒  ∠DBK = ∠DIA [96]
080. BF = BD [02] & BG = BD [04] & GB = GI [48] ⇒  GI = FB [97]
081. B,G,F are collinear [22] & BG = BD [04] & BF = BD [02] ⇒  B is midpoint of FG [98]
082. B,G,E are collinear [05] & B,G,F are collinear [22] & BE ⟂ BD [01] & B,A,D are collinear [00] ⇒  AB ⟂ FG [99]
083. B is midpoint of FG [98] & AB ⟂ FG [99] ⇒  AF = AG [100]
084. F,J,D are collinear [09] & J,K,A are collinear [10] & DF ⟂ AJ [08] ⇒  FJ ⟂ KA [101]
085. J is midpoint of KA [90] & FJ ⟂ KA [101] ⇒  FK = FA [102]
086. AF = AG [100] & FK = FA [102] ⇒  FK = GA [103]
087. B,A,D are collinear [00] & B,G,E are collinear [05] & B,G,F are collinear [22] & BE ⟂ BD [01] ⇒  DB ⟂ FG [104]
088. B is midpoint of FG [98] & DB ⟂ FG [104] ⇒  DF = DG [105]
089. FK = GA [103] & DK = DA [92] & DF = DG [105] (SSS)⇒  ∠FKD = ∠GAD [106]
090. F,E,B are collinear [03] & ∠FKD = ∠GAD [106] & B,A,D are collinear [00] & BF ∥ KD [94] & AB ∥ GI [86] ⇒  ∠IGA = ∠BFK [107]
091. GI = FB [97] & FK = GA [103] & ∠IGA = ∠BFK [107] (SAS)⇒  ∠GIA = ∠FBK [108]
092. AI ⟂ MB [89] & ∠DBK = ∠DIA [96] & B,A,D are collinear [00] & BF ∥ ID [81] & B,F,E are collinear [03] & ∠GIA = ∠FBK [108] & AB ∥ GI [86] ⇒  M,B,K are collinear
==========================

