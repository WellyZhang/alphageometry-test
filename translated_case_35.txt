I0123 12:50:46.758884 139995469987840 inference_utils.py:69] Parsing gin configuration.
I0123 12:50:46.759010 139995469987840 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0123 12:50:46.759215 139995469987840 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0123 12:50:46.759248 139995469987840 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0123 12:50:46.759277 139995469987840 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0123 12:50:46.759304 139995469987840 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0123 12:50:46.759329 139995469987840 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0123 12:50:46.759354 139995469987840 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0123 12:50:46.759379 139995469987840 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0123 12:50:46.759405 139995469987840 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=32
I0123 12:50:46.759430 139995469987840 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0123 12:50:46.759455 139995469987840 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0123 12:50:46.759502 139995469987840 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0123 12:50:46.759648 139995469987840 resource_reader.py:55] Path not found: base_htrans.gin
I0123 12:50:46.759893 139995469987840 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0123 12:50:46.759999 139995469987840 resource_reader.py:55] Path not found: trainer_configuration.gin
I0123 12:50:46.766396 139995469987840 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0123 12:50:46.766525 139995469987840 resource_reader.py:55] Path not found: size/medium_150M.gin
I0123 12:50:46.766840 139995469987840 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0123 12:50:46.766943 139995469987840 resource_reader.py:55] Path not found: options/positions_t5.gin
I0123 12:50:46.767215 139995469987840 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0123 12:50:46.767313 139995469987840 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0123 12:50:46.767712 139995469987840 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0123 12:50:46.767809 139995469987840 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0123 12:50:46.771424 139995469987840 training_loop.py:334] ==== Training loop: initializing model ====
I0123 12:50:46.873116 139995469987840 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0123 12:50:46.873925 139995469987840 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0123 12:50:46.880388 139995469987840 training_loop.py:335] Process 0 of 1
I0123 12:50:46.880441 139995469987840 training_loop.py:336] Local device count = 1
I0123 12:50:46.880480 139995469987840 training_loop.py:337] Number of replicas = 1
I0123 12:50:46.880511 139995469987840 training_loop.py:339] Using random number seed 42
I0123 12:50:47.394088 139995469987840 training_loop.py:359] Initializing the model.
I0123 12:50:47.800668 139995469987840 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.800957 139995469987840 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 12:50:47.801060 139995469987840 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:50:47.801137 139995469987840 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:50:47.801213 139995469987840 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:50:47.801296 139995469987840 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:50:47.801367 139995469987840 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:50:47.801436 139995469987840 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:50:47.801504 139995469987840 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:50:47.801571 139995469987840 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:50:47.801638 139995469987840 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:50:47.801832 139995469987840 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:50:47.801900 139995469987840 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:50:47.801966 139995469987840 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 12:50:47.802005 139995469987840 attention.py:418] Single window, no scan.
I0123 12:50:47.802051 139995469987840 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 12:50:47.802164 139995469987840 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:50:47.802202 139995469987840 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:50:47.802232 139995469987840 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:50:47.804201 139995469987840 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.809371 139995469987840 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:50:47.819766 139995469987840 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.820036 139995469987840 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:50:47.824617 139995469987840 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:50:47.835146 139995469987840 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:50:47.835205 139995469987840 attention.py:418] Single window, no scan.
I0123 12:50:47.835242 139995469987840 transformer_layer.py:389] tlayer: self-attention.
I0123 12:50:47.835274 139995469987840 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.835336 139995469987840 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.836515 139995469987840 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.836593 139995469987840 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.837288 139995469987840 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.839704 139995469987840 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.845778 139995469987840 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.847086 139995469987840 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.847168 139995469987840 transformer_layer.py:468] tlayer: End windows.
I0123 12:50:47.847204 139995469987840 transformer_layer.py:472] tlayer: final FFN.
I0123 12:50:47.847265 139995469987840 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.847392 139995469987840 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:50:47.847722 139995469987840 nn_components.py:325] mlp: activation = None
I0123 12:50:47.847769 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:50:47.849662 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.849763 139995469987840 nn_components.py:261] mlp: residual
I0123 12:50:47.852578 139995469987840 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.852660 139995469987840 transformer_base.py:443] tbase: final FFN
I0123 12:50:47.853155 139995469987840 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:50:47.863147 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:50:47.871719 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.871817 139995469987840 nn_components.py:261] mlp: residual
I0123 12:50:47.872108 139995469987840 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.872189 139995469987840 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 12:50:47.872298 139995469987840 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:50:47.872336 139995469987840 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:50:47.872367 139995469987840 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:50:47.874215 139995469987840 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.876647 139995469987840 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:50:47.882178 139995469987840 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.882438 139995469987840 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:50:47.885013 139995469987840 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:50:47.888818 139995469987840 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:50:47.888874 139995469987840 attention.py:418] Single window, no scan.
I0123 12:50:47.888909 139995469987840 transformer_layer.py:389] tlayer: self-attention.
I0123 12:50:47.888939 139995469987840 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.889000 139995469987840 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.889558 139995469987840 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.889638 139995469987840 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.890004 139995469987840 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.890756 139995469987840 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.893175 139995469987840 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.893814 139995469987840 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.893893 139995469987840 transformer_layer.py:468] tlayer: End windows.
I0123 12:50:47.893927 139995469987840 transformer_layer.py:472] tlayer: final FFN.
I0123 12:50:47.893985 139995469987840 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.894112 139995469987840 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:50:47.894429 139995469987840 nn_components.py:325] mlp: activation = None
I0123 12:50:47.894471 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:50:47.896390 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.896482 139995469987840 nn_components.py:261] mlp: residual
I0123 12:50:47.898924 139995469987840 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.899008 139995469987840 transformer_base.py:443] tbase: final FFN
I0123 12:50:47.899435 139995469987840 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:50:47.901711 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:50:47.903589 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.903683 139995469987840 nn_components.py:261] mlp: residual
I0123 12:50:47.903965 139995469987840 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.904044 139995469987840 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 12:50:47.904153 139995469987840 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:50:47.904191 139995469987840 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:50:47.904222 139995469987840 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:50:47.906461 139995469987840 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.908805 139995469987840 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:50:47.914301 139995469987840 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.914563 139995469987840 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:50:47.917164 139995469987840 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:50:47.920953 139995469987840 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:50:47.921008 139995469987840 attention.py:418] Single window, no scan.
I0123 12:50:47.921043 139995469987840 transformer_layer.py:389] tlayer: self-attention.
I0123 12:50:47.921073 139995469987840 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.921133 139995469987840 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.921686 139995469987840 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.921765 139995469987840 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.922108 139995469987840 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.922849 139995469987840 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.925261 139995469987840 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.925939 139995469987840 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.926016 139995469987840 transformer_layer.py:468] tlayer: End windows.
I0123 12:50:47.926050 139995469987840 transformer_layer.py:472] tlayer: final FFN.
I0123 12:50:47.926107 139995469987840 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.926229 139995469987840 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:50:47.926545 139995469987840 nn_components.py:325] mlp: activation = None
I0123 12:50:47.926588 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:50:47.928462 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.928555 139995469987840 nn_components.py:261] mlp: residual
I0123 12:50:47.931027 139995469987840 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.931112 139995469987840 transformer_base.py:443] tbase: final FFN
I0123 12:50:47.931589 139995469987840 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:50:47.933834 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:50:47.935708 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.935802 139995469987840 nn_components.py:261] mlp: residual
I0123 12:50:47.936086 139995469987840 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.936165 139995469987840 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 12:50:47.936275 139995469987840 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:50:47.936314 139995469987840 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:50:47.936343 139995469987840 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:50:47.938232 139995469987840 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.940575 139995469987840 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:50:47.946108 139995469987840 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.946373 139995469987840 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:50:47.948924 139995469987840 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:50:47.952663 139995469987840 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:50:47.952718 139995469987840 attention.py:418] Single window, no scan.
I0123 12:50:47.952752 139995469987840 transformer_layer.py:389] tlayer: self-attention.
I0123 12:50:47.952782 139995469987840 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.952843 139995469987840 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.953394 139995469987840 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.953469 139995469987840 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.953821 139995469987840 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.954575 139995469987840 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.957031 139995469987840 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.957660 139995469987840 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.957737 139995469987840 transformer_layer.py:468] tlayer: End windows.
I0123 12:50:47.957771 139995469987840 transformer_layer.py:472] tlayer: final FFN.
I0123 12:50:47.957828 139995469987840 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.957953 139995469987840 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:50:47.958269 139995469987840 nn_components.py:325] mlp: activation = None
I0123 12:50:47.958310 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:50:47.960173 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.960264 139995469987840 nn_components.py:261] mlp: residual
I0123 12:50:47.962772 139995469987840 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.962859 139995469987840 transformer_base.py:443] tbase: final FFN
I0123 12:50:47.963287 139995469987840 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:50:47.965497 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:50:47.967378 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.967474 139995469987840 nn_components.py:261] mlp: residual
I0123 12:50:47.967761 139995469987840 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.967841 139995469987840 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 12:50:47.967950 139995469987840 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:50:47.967988 139995469987840 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:50:47.968019 139995469987840 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:50:47.969899 139995469987840 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.972248 139995469987840 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:50:47.977801 139995469987840 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.978062 139995469987840 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:50:47.981185 139995469987840 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:50:47.985039 139995469987840 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:50:47.985094 139995469987840 attention.py:418] Single window, no scan.
I0123 12:50:47.985129 139995469987840 transformer_layer.py:389] tlayer: self-attention.
I0123 12:50:47.985158 139995469987840 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.985218 139995469987840 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.985789 139995469987840 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.985867 139995469987840 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.986215 139995469987840 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.986964 139995469987840 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.989407 139995469987840 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.990032 139995469987840 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.990109 139995469987840 transformer_layer.py:468] tlayer: End windows.
I0123 12:50:47.990143 139995469987840 transformer_layer.py:472] tlayer: final FFN.
I0123 12:50:47.990200 139995469987840 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.990331 139995469987840 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:50:47.990654 139995469987840 nn_components.py:325] mlp: activation = None
I0123 12:50:47.990697 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:50:47.992542 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.992634 139995469987840 nn_components.py:261] mlp: residual
I0123 12:50:47.995246 139995469987840 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.995326 139995469987840 transformer_base.py:443] tbase: final FFN
I0123 12:50:47.995749 139995469987840 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:50:47.997976 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:50:47.999890 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:47.999983 139995469987840 nn_components.py:261] mlp: residual
I0123 12:50:48.000271 139995469987840 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.000351 139995469987840 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 12:50:48.000457 139995469987840 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:50:48.000495 139995469987840 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:50:48.000525 139995469987840 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:50:48.002350 139995469987840 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.004673 139995469987840 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:50:48.010175 139995469987840 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.010442 139995469987840 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:50:48.013059 139995469987840 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:50:48.016766 139995469987840 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:50:48.016822 139995469987840 attention.py:418] Single window, no scan.
I0123 12:50:48.016856 139995469987840 transformer_layer.py:389] tlayer: self-attention.
I0123 12:50:48.016886 139995469987840 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.016946 139995469987840 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.017542 139995469987840 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.017618 139995469987840 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.017982 139995469987840 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.018742 139995469987840 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.021173 139995469987840 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.021790 139995469987840 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.021867 139995469987840 transformer_layer.py:468] tlayer: End windows.
I0123 12:50:48.021901 139995469987840 transformer_layer.py:472] tlayer: final FFN.
I0123 12:50:48.021957 139995469987840 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.022084 139995469987840 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:50:48.022405 139995469987840 nn_components.py:325] mlp: activation = None
I0123 12:50:48.022448 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:50:48.024318 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.024410 139995469987840 nn_components.py:261] mlp: residual
I0123 12:50:48.026914 139995469987840 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.026993 139995469987840 transformer_base.py:443] tbase: final FFN
I0123 12:50:48.027413 139995469987840 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:50:48.029682 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:50:48.031551 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.031643 139995469987840 nn_components.py:261] mlp: residual
I0123 12:50:48.031933 139995469987840 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.032013 139995469987840 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 12:50:48.032121 139995469987840 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:50:48.032160 139995469987840 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:50:48.032190 139995469987840 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:50:48.034010 139995469987840 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.036366 139995469987840 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:50:48.041855 139995469987840 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.042116 139995469987840 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:50:48.044677 139995469987840 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:50:48.048384 139995469987840 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:50:48.048438 139995469987840 attention.py:418] Single window, no scan.
I0123 12:50:48.048476 139995469987840 transformer_layer.py:389] tlayer: self-attention.
I0123 12:50:48.048506 139995469987840 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.048566 139995469987840 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.049111 139995469987840 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.049185 139995469987840 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.049526 139995469987840 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.050290 139995469987840 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.052662 139995469987840 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.053273 139995469987840 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.053350 139995469987840 transformer_layer.py:468] tlayer: End windows.
I0123 12:50:48.053383 139995469987840 transformer_layer.py:472] tlayer: final FFN.
I0123 12:50:48.053441 139995469987840 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.053570 139995469987840 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:50:48.053890 139995469987840 nn_components.py:325] mlp: activation = None
I0123 12:50:48.053932 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:50:48.056162 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.056257 139995469987840 nn_components.py:261] mlp: residual
I0123 12:50:48.058691 139995469987840 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.058771 139995469987840 transformer_base.py:443] tbase: final FFN
I0123 12:50:48.059191 139995469987840 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:50:48.200685 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:50:48.202955 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.203125 139995469987840 nn_components.py:261] mlp: residual
I0123 12:50:48.203440 139995469987840 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.203531 139995469987840 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 12:50:48.203646 139995469987840 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:50:48.203687 139995469987840 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:50:48.203718 139995469987840 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:50:48.205774 139995469987840 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.208272 139995469987840 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:50:48.213970 139995469987840 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.214240 139995469987840 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:50:48.216847 139995469987840 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:50:48.220746 139995469987840 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:50:48.220802 139995469987840 attention.py:418] Single window, no scan.
I0123 12:50:48.220838 139995469987840 transformer_layer.py:389] tlayer: self-attention.
I0123 12:50:48.220869 139995469987840 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.220931 139995469987840 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.221541 139995469987840 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.221619 139995469987840 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.221988 139995469987840 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.222756 139995469987840 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.225271 139995469987840 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.225908 139995469987840 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.225985 139995469987840 transformer_layer.py:468] tlayer: End windows.
I0123 12:50:48.226020 139995469987840 transformer_layer.py:472] tlayer: final FFN.
I0123 12:50:48.226078 139995469987840 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.226207 139995469987840 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:50:48.226523 139995469987840 nn_components.py:325] mlp: activation = None
I0123 12:50:48.226565 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:50:48.228431 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.228526 139995469987840 nn_components.py:261] mlp: residual
I0123 12:50:48.230982 139995469987840 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.231061 139995469987840 transformer_base.py:443] tbase: final FFN
I0123 12:50:48.231543 139995469987840 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:50:48.233777 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:50:48.235941 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.236042 139995469987840 nn_components.py:261] mlp: residual
I0123 12:50:48.236329 139995469987840 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.236409 139995469987840 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 12:50:48.236519 139995469987840 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:50:48.236557 139995469987840 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:50:48.236587 139995469987840 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:50:48.238461 139995469987840 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.240776 139995469987840 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:50:48.246301 139995469987840 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.246562 139995469987840 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:50:48.249154 139995469987840 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:50:48.252866 139995469987840 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:50:48.252921 139995469987840 attention.py:418] Single window, no scan.
I0123 12:50:48.252956 139995469987840 transformer_layer.py:389] tlayer: self-attention.
I0123 12:50:48.252985 139995469987840 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.253046 139995469987840 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.253607 139995469987840 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.253689 139995469987840 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.254039 139995469987840 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.254781 139995469987840 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.257246 139995469987840 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.257872 139995469987840 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.257950 139995469987840 transformer_layer.py:468] tlayer: End windows.
I0123 12:50:48.257983 139995469987840 transformer_layer.py:472] tlayer: final FFN.
I0123 12:50:48.258041 139995469987840 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.258164 139995469987840 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:50:48.258476 139995469987840 nn_components.py:325] mlp: activation = None
I0123 12:50:48.258517 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:50:48.260363 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.260458 139995469987840 nn_components.py:261] mlp: residual
I0123 12:50:48.262953 139995469987840 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.263033 139995469987840 transformer_base.py:443] tbase: final FFN
I0123 12:50:48.263446 139995469987840 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:50:48.265716 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:50:48.267588 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.267683 139995469987840 nn_components.py:261] mlp: residual
I0123 12:50:48.267960 139995469987840 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.268046 139995469987840 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 12:50:48.268156 139995469987840 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:50:48.268194 139995469987840 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:50:48.268224 139995469987840 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:50:48.270087 139995469987840 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.272399 139995469987840 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:50:48.278244 139995469987840 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.278508 139995469987840 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:50:48.281138 139995469987840 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:50:48.284853 139995469987840 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:50:48.284908 139995469987840 attention.py:418] Single window, no scan.
I0123 12:50:48.284943 139995469987840 transformer_layer.py:389] tlayer: self-attention.
I0123 12:50:48.284973 139995469987840 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.285036 139995469987840 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.285592 139995469987840 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.285674 139995469987840 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.286018 139995469987840 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.286823 139995469987840 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.289225 139995469987840 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.289847 139995469987840 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.289924 139995469987840 transformer_layer.py:468] tlayer: End windows.
I0123 12:50:48.289958 139995469987840 transformer_layer.py:472] tlayer: final FFN.
I0123 12:50:48.290015 139995469987840 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.290140 139995469987840 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:50:48.290455 139995469987840 nn_components.py:325] mlp: activation = None
I0123 12:50:48.290498 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:50:48.292369 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.292461 139995469987840 nn_components.py:261] mlp: residual
I0123 12:50:48.294949 139995469987840 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.295028 139995469987840 transformer_base.py:443] tbase: final FFN
I0123 12:50:48.295458 139995469987840 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:50:48.297663 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:50:48.299588 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.299684 139995469987840 nn_components.py:261] mlp: residual
I0123 12:50:48.299968 139995469987840 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.300055 139995469987840 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 12:50:48.300166 139995469987840 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:50:48.300204 139995469987840 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:50:48.300235 139995469987840 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:50:48.302049 139995469987840 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.304434 139995469987840 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:50:48.310462 139995469987840 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.310799 139995469987840 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:50:48.313488 139995469987840 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:50:48.317232 139995469987840 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:50:48.317290 139995469987840 attention.py:418] Single window, no scan.
I0123 12:50:48.317325 139995469987840 transformer_layer.py:389] tlayer: self-attention.
I0123 12:50:48.317355 139995469987840 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.317458 139995469987840 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.318039 139995469987840 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.318118 139995469987840 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.318464 139995469987840 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.319212 139995469987840 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.321602 139995469987840 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.322218 139995469987840 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.322294 139995469987840 transformer_layer.py:468] tlayer: End windows.
I0123 12:50:48.322328 139995469987840 transformer_layer.py:472] tlayer: final FFN.
I0123 12:50:48.322388 139995469987840 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.322515 139995469987840 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:50:48.322830 139995469987840 nn_components.py:325] mlp: activation = None
I0123 12:50:48.322873 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:50:48.324784 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.324879 139995469987840 nn_components.py:261] mlp: residual
I0123 12:50:48.327575 139995469987840 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.327656 139995469987840 transformer_base.py:443] tbase: final FFN
I0123 12:50:48.328081 139995469987840 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:50:48.330343 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:50:48.332197 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.332292 139995469987840 nn_components.py:261] mlp: residual
I0123 12:50:48.332569 139995469987840 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.332648 139995469987840 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 12:50:48.332762 139995469987840 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:50:48.332802 139995469987840 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:50:48.332832 139995469987840 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:50:48.334641 139995469987840 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.337013 139995469987840 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:50:48.342444 139995469987840 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.342694 139995469987840 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:50:48.345230 139995469987840 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:50:48.349281 139995469987840 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:50:48.349336 139995469987840 attention.py:418] Single window, no scan.
I0123 12:50:48.349371 139995469987840 transformer_layer.py:389] tlayer: self-attention.
I0123 12:50:48.349400 139995469987840 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.349462 139995469987840 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.350026 139995469987840 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.350104 139995469987840 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.350455 139995469987840 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.351196 139995469987840 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.353976 139995469987840 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.354589 139995469987840 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.354665 139995469987840 transformer_layer.py:468] tlayer: End windows.
I0123 12:50:48.354700 139995469987840 transformer_layer.py:472] tlayer: final FFN.
I0123 12:50:48.354759 139995469987840 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.354885 139995469987840 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:50:48.355192 139995469987840 nn_components.py:325] mlp: activation = None
I0123 12:50:48.355234 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:50:48.357122 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.357215 139995469987840 nn_components.py:261] mlp: residual
I0123 12:50:48.359635 139995469987840 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.359715 139995469987840 transformer_base.py:443] tbase: final FFN
I0123 12:50:48.360136 139995469987840 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:50:48.362398 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:50:48.364240 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.364339 139995469987840 nn_components.py:261] mlp: residual
I0123 12:50:48.364615 139995469987840 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.364896 139995469987840 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:50:48.364967 139995469987840 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:50:48.365033 139995469987840 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:50:48.365089 139995469987840 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:50:48.365143 139995469987840 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:50:48.365195 139995469987840 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:50:48.365247 139995469987840 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:50:48.365298 139995469987840 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:50:48.365349 139995469987840 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:50:48.365400 139995469987840 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:50:48.365452 139995469987840 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:50:48.365503 139995469987840 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 12:50:48.365540 139995469987840 decoder_stack.py:344] dstack: Final layernorm.
I0123 12:50:48.368980 139995469987840 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:50:48.416091 139995469987840 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.416177 139995469987840 decoder_stack.py:333] dstack: autoregressive generator.
I0123 12:50:48.416231 139995469987840 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 12:50:48.416334 139995469987840 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:50:48.416372 139995469987840 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:50:48.416402 139995469987840 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:50:48.416464 139995469987840 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.418859 139995469987840 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:50:48.424253 139995469987840 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.424511 139995469987840 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:50:48.427081 139995469987840 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:50:48.443412 139995469987840 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:50:48.443467 139995469987840 attention.py:418] Single window, no scan.
I0123 12:50:48.443506 139995469987840 transformer_layer.py:389] tlayer: self-attention.
I0123 12:50:48.443538 139995469987840 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.443598 139995469987840 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.444721 139995469987840 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.444799 139995469987840 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.445489 139995469987840 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.447465 139995469987840 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.452109 139995469987840 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.453408 139995469987840 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.453494 139995469987840 transformer_layer.py:468] tlayer: End windows.
I0123 12:50:48.453528 139995469987840 transformer_layer.py:472] tlayer: final FFN.
I0123 12:50:48.453586 139995469987840 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.453723 139995469987840 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:50:48.453832 139995469987840 nn_components.py:325] mlp: activation = None
I0123 12:50:48.453870 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:50:48.455739 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.455832 139995469987840 nn_components.py:261] mlp: residual
I0123 12:50:48.458199 139995469987840 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.458279 139995469987840 transformer_base.py:443] tbase: final FFN
I0123 12:50:48.458385 139995469987840 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:50:48.460571 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:50:48.462483 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.462580 139995469987840 nn_components.py:261] mlp: residual
I0123 12:50:48.462862 139995469987840 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.462944 139995469987840 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 12:50:48.463053 139995469987840 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:50:48.463092 139995469987840 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:50:48.463122 139995469987840 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:50:48.463185 139995469987840 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.465359 139995469987840 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:50:48.470741 139995469987840 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.471000 139995469987840 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:50:48.473600 139995469987840 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:50:48.486582 139995469987840 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:50:48.486637 139995469987840 attention.py:418] Single window, no scan.
I0123 12:50:48.486673 139995469987840 transformer_layer.py:389] tlayer: self-attention.
I0123 12:50:48.486702 139995469987840 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.486762 139995469987840 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.487317 139995469987840 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.487394 139995469987840 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.487745 139995469987840 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.488415 139995469987840 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.490836 139995469987840 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.491443 139995469987840 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.491519 139995469987840 transformer_layer.py:468] tlayer: End windows.
I0123 12:50:48.491558 139995469987840 transformer_layer.py:472] tlayer: final FFN.
I0123 12:50:48.491615 139995469987840 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.491741 139995469987840 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:50:48.491847 139995469987840 nn_components.py:325] mlp: activation = None
I0123 12:50:48.491884 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:50:48.493784 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.493879 139995469987840 nn_components.py:261] mlp: residual
I0123 12:50:48.496214 139995469987840 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.496298 139995469987840 transformer_base.py:443] tbase: final FFN
I0123 12:50:48.496406 139995469987840 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:50:48.498754 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:50:48.500645 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.500740 139995469987840 nn_components.py:261] mlp: residual
I0123 12:50:48.501020 139995469987840 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.501101 139995469987840 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 12:50:48.501209 139995469987840 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:50:48.501248 139995469987840 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:50:48.501279 139995469987840 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:50:48.501343 139995469987840 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.503536 139995469987840 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:50:48.508889 139995469987840 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.509143 139995469987840 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:50:48.511755 139995469987840 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:50:48.524280 139995469987840 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:50:48.524336 139995469987840 attention.py:418] Single window, no scan.
I0123 12:50:48.524370 139995469987840 transformer_layer.py:389] tlayer: self-attention.
I0123 12:50:48.524400 139995469987840 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.524466 139995469987840 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.525027 139995469987840 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.525106 139995469987840 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.525463 139995469987840 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.526144 139995469987840 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.528540 139995469987840 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.529155 139995469987840 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.529231 139995469987840 transformer_layer.py:468] tlayer: End windows.
I0123 12:50:48.529264 139995469987840 transformer_layer.py:472] tlayer: final FFN.
I0123 12:50:48.529325 139995469987840 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.529451 139995469987840 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:50:48.529560 139995469987840 nn_components.py:325] mlp: activation = None
I0123 12:50:48.529598 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:50:48.531495 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.531589 139995469987840 nn_components.py:261] mlp: residual
I0123 12:50:48.533982 139995469987840 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.534060 139995469987840 transformer_base.py:443] tbase: final FFN
I0123 12:50:48.534167 139995469987840 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:50:48.536335 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:50:48.538236 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.538335 139995469987840 nn_components.py:261] mlp: residual
I0123 12:50:48.538613 139995469987840 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.538692 139995469987840 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 12:50:48.538800 139995469987840 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:50:48.538838 139995469987840 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:50:48.538868 139995469987840 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:50:48.538928 139995469987840 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.541115 139995469987840 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:50:48.546425 139995469987840 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.546679 139995469987840 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:50:48.549278 139995469987840 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:50:48.561824 139995469987840 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:50:48.561880 139995469987840 attention.py:418] Single window, no scan.
I0123 12:50:48.561915 139995469987840 transformer_layer.py:389] tlayer: self-attention.
I0123 12:50:48.561944 139995469987840 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.562006 139995469987840 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.562554 139995469987840 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.562630 139995469987840 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.562979 139995469987840 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.563657 139995469987840 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.566075 139995469987840 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.566688 139995469987840 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.566766 139995469987840 transformer_layer.py:468] tlayer: End windows.
I0123 12:50:48.566799 139995469987840 transformer_layer.py:472] tlayer: final FFN.
I0123 12:50:48.566856 139995469987840 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.566987 139995469987840 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:50:48.567095 139995469987840 nn_components.py:325] mlp: activation = None
I0123 12:50:48.567133 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:50:48.569325 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.569420 139995469987840 nn_components.py:261] mlp: residual
I0123 12:50:48.571768 139995469987840 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.571847 139995469987840 transformer_base.py:443] tbase: final FFN
I0123 12:50:48.571952 139995469987840 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:50:48.574155 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:50:48.575993 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.576087 139995469987840 nn_components.py:261] mlp: residual
I0123 12:50:48.576367 139995469987840 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.576448 139995469987840 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 12:50:48.576555 139995469987840 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:50:48.576594 139995469987840 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:50:48.576624 139995469987840 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:50:48.576685 139995469987840 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.578959 139995469987840 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:50:48.584305 139995469987840 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.584568 139995469987840 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:50:48.587115 139995469987840 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:50:48.599657 139995469987840 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:50:48.599714 139995469987840 attention.py:418] Single window, no scan.
I0123 12:50:48.599749 139995469987840 transformer_layer.py:389] tlayer: self-attention.
I0123 12:50:48.599778 139995469987840 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.599838 139995469987840 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.600401 139995469987840 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.600476 139995469987840 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.600823 139995469987840 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.601500 139995469987840 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.603975 139995469987840 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.604593 139995469987840 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.604669 139995469987840 transformer_layer.py:468] tlayer: End windows.
I0123 12:50:48.604703 139995469987840 transformer_layer.py:472] tlayer: final FFN.
I0123 12:50:48.604760 139995469987840 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.604891 139995469987840 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:50:48.604998 139995469987840 nn_components.py:325] mlp: activation = None
I0123 12:50:48.605036 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:50:48.606886 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.606984 139995469987840 nn_components.py:261] mlp: residual
I0123 12:50:48.609318 139995469987840 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.609396 139995469987840 transformer_base.py:443] tbase: final FFN
I0123 12:50:48.609504 139995469987840 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:50:48.611754 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:50:48.613590 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.613692 139995469987840 nn_components.py:261] mlp: residual
I0123 12:50:48.613974 139995469987840 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.614055 139995469987840 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 12:50:48.614163 139995469987840 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:50:48.614201 139995469987840 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:50:48.614231 139995469987840 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:50:48.614294 139995469987840 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.616474 139995469987840 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:50:48.621815 139995469987840 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.622071 139995469987840 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:50:48.624673 139995469987840 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:50:48.637238 139995469987840 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:50:48.637293 139995469987840 attention.py:418] Single window, no scan.
I0123 12:50:48.637328 139995469987840 transformer_layer.py:389] tlayer: self-attention.
I0123 12:50:48.637359 139995469987840 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.637420 139995469987840 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.637991 139995469987840 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.638071 139995469987840 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.638420 139995469987840 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.639101 139995469987840 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.641510 139995469987840 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.642129 139995469987840 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.642209 139995469987840 transformer_layer.py:468] tlayer: End windows.
I0123 12:50:48.642243 139995469987840 transformer_layer.py:472] tlayer: final FFN.
I0123 12:50:48.642300 139995469987840 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.642427 139995469987840 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:50:48.642540 139995469987840 nn_components.py:325] mlp: activation = None
I0123 12:50:48.642579 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:50:48.644483 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.644577 139995469987840 nn_components.py:261] mlp: residual
I0123 12:50:48.646964 139995469987840 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.647047 139995469987840 transformer_base.py:443] tbase: final FFN
I0123 12:50:48.647153 139995469987840 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:50:48.649321 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:50:48.651162 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.651258 139995469987840 nn_components.py:261] mlp: residual
I0123 12:50:48.651537 139995469987840 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.651617 139995469987840 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 12:50:48.651724 139995469987840 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:50:48.651762 139995469987840 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:50:48.651792 139995469987840 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:50:48.651854 139995469987840 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.654051 139995469987840 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:50:48.659465 139995469987840 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.659719 139995469987840 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:50:48.662255 139995469987840 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:50:48.675168 139995469987840 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:50:48.675224 139995469987840 attention.py:418] Single window, no scan.
I0123 12:50:48.675260 139995469987840 transformer_layer.py:389] tlayer: self-attention.
I0123 12:50:48.675290 139995469987840 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.675356 139995469987840 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.675910 139995469987840 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.675986 139995469987840 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.676331 139995469987840 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.677004 139995469987840 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.679417 139995469987840 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.680074 139995469987840 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.680152 139995469987840 transformer_layer.py:468] tlayer: End windows.
I0123 12:50:48.680187 139995469987840 transformer_layer.py:472] tlayer: final FFN.
I0123 12:50:48.680245 139995469987840 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.680375 139995469987840 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:50:48.680484 139995469987840 nn_components.py:325] mlp: activation = None
I0123 12:50:48.680526 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:50:48.682402 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.682495 139995469987840 nn_components.py:261] mlp: residual
I0123 12:50:48.684841 139995469987840 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.684921 139995469987840 transformer_base.py:443] tbase: final FFN
I0123 12:50:48.685028 139995469987840 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:50:48.687205 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:50:48.689090 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.689186 139995469987840 nn_components.py:261] mlp: residual
I0123 12:50:48.689465 139995469987840 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.689547 139995469987840 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 12:50:48.689662 139995469987840 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:50:48.689702 139995469987840 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:50:48.689733 139995469987840 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:50:48.689795 139995469987840 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.691981 139995469987840 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:50:48.697344 139995469987840 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.697609 139995469987840 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:50:48.700217 139995469987840 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:50:48.712846 139995469987840 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:50:48.712901 139995469987840 attention.py:418] Single window, no scan.
I0123 12:50:48.712936 139995469987840 transformer_layer.py:389] tlayer: self-attention.
I0123 12:50:48.712966 139995469987840 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.713028 139995469987840 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.713629 139995469987840 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.713718 139995469987840 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.714071 139995469987840 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.714747 139995469987840 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.717272 139995469987840 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.717904 139995469987840 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.717983 139995469987840 transformer_layer.py:468] tlayer: End windows.
I0123 12:50:48.718017 139995469987840 transformer_layer.py:472] tlayer: final FFN.
I0123 12:50:48.718074 139995469987840 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.718200 139995469987840 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:50:48.718306 139995469987840 nn_components.py:325] mlp: activation = None
I0123 12:50:48.718350 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:50:48.720213 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.720306 139995469987840 nn_components.py:261] mlp: residual
I0123 12:50:48.722709 139995469987840 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.722788 139995469987840 transformer_base.py:443] tbase: final FFN
I0123 12:50:48.722892 139995469987840 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:50:48.725051 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:50:48.726884 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.726980 139995469987840 nn_components.py:261] mlp: residual
I0123 12:50:48.727256 139995469987840 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.727337 139995469987840 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 12:50:48.727444 139995469987840 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:50:48.727483 139995469987840 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:50:48.727513 139995469987840 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:50:48.727573 139995469987840 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.729764 139995469987840 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:50:48.735134 139995469987840 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.735389 139995469987840 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:50:48.737963 139995469987840 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:50:48.750784 139995469987840 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:50:48.750840 139995469987840 attention.py:418] Single window, no scan.
I0123 12:50:48.750876 139995469987840 transformer_layer.py:389] tlayer: self-attention.
I0123 12:50:48.750905 139995469987840 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.750965 139995469987840 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.751520 139995469987840 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.751597 139995469987840 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.751940 139995469987840 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.752607 139995469987840 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.755012 139995469987840 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.755671 139995469987840 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.755749 139995469987840 transformer_layer.py:468] tlayer: End windows.
I0123 12:50:48.755782 139995469987840 transformer_layer.py:472] tlayer: final FFN.
I0123 12:50:48.755840 139995469987840 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.755966 139995469987840 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:50:48.756072 139995469987840 nn_components.py:325] mlp: activation = None
I0123 12:50:48.756110 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:50:48.757956 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.758049 139995469987840 nn_components.py:261] mlp: residual
I0123 12:50:48.760365 139995469987840 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.760445 139995469987840 transformer_base.py:443] tbase: final FFN
I0123 12:50:48.760553 139995469987840 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:50:48.762707 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:50:48.764576 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.764672 139995469987840 nn_components.py:261] mlp: residual
I0123 12:50:48.764956 139995469987840 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.765038 139995469987840 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 12:50:48.765149 139995469987840 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:50:48.765189 139995469987840 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:50:48.765220 139995469987840 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:50:48.765284 139995469987840 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.767476 139995469987840 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:50:48.772766 139995469987840 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.773024 139995469987840 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:50:48.775949 139995469987840 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:50:48.788441 139995469987840 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:50:48.788497 139995469987840 attention.py:418] Single window, no scan.
I0123 12:50:48.788532 139995469987840 transformer_layer.py:389] tlayer: self-attention.
I0123 12:50:48.788563 139995469987840 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.788624 139995469987840 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.789228 139995469987840 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.789304 139995469987840 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.789666 139995469987840 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.790339 139995469987840 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.792738 139995469987840 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.793356 139995469987840 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.793433 139995469987840 transformer_layer.py:468] tlayer: End windows.
I0123 12:50:48.793466 139995469987840 transformer_layer.py:472] tlayer: final FFN.
I0123 12:50:48.793522 139995469987840 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.793659 139995469987840 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:50:48.793769 139995469987840 nn_components.py:325] mlp: activation = None
I0123 12:50:48.793807 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:50:48.795662 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.795763 139995469987840 nn_components.py:261] mlp: residual
I0123 12:50:48.798170 139995469987840 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.798250 139995469987840 transformer_base.py:443] tbase: final FFN
I0123 12:50:48.798360 139995469987840 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:50:48.800548 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:50:48.802392 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.802487 139995469987840 nn_components.py:261] mlp: residual
I0123 12:50:48.802764 139995469987840 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.802844 139995469987840 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 12:50:48.802952 139995469987840 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:50:48.802990 139995469987840 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:50:48.803020 139995469987840 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:50:48.803082 139995469987840 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.805271 139995469987840 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:50:48.810684 139995469987840 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.810944 139995469987840 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:50:48.813503 139995469987840 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:50:48.826119 139995469987840 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:50:48.826174 139995469987840 attention.py:418] Single window, no scan.
I0123 12:50:48.826210 139995469987840 transformer_layer.py:389] tlayer: self-attention.
I0123 12:50:48.826240 139995469987840 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.826307 139995469987840 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.826859 139995469987840 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.826936 139995469987840 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.827285 139995469987840 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.827954 139995469987840 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.830364 139995469987840 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.831025 139995469987840 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.831101 139995469987840 transformer_layer.py:468] tlayer: End windows.
I0123 12:50:48.831135 139995469987840 transformer_layer.py:472] tlayer: final FFN.
I0123 12:50:48.831192 139995469987840 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.831317 139995469987840 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:50:48.831425 139995469987840 nn_components.py:325] mlp: activation = None
I0123 12:50:48.831464 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:50:48.833326 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.833425 139995469987840 nn_components.py:261] mlp: residual
I0123 12:50:48.835826 139995469987840 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.835910 139995469987840 transformer_base.py:443] tbase: final FFN
I0123 12:50:48.836016 139995469987840 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:50:48.838193 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:50:48.840085 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.840181 139995469987840 nn_components.py:261] mlp: residual
I0123 12:50:48.840458 139995469987840 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.840539 139995469987840 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 12:50:48.840648 139995469987840 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:50:48.840687 139995469987840 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:50:48.840717 139995469987840 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:50:48.840778 139995469987840 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.842970 139995469987840 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:50:48.848319 139995469987840 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.848581 139995469987840 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:50:48.851142 139995469987840 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:50:48.863752 139995469987840 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:50:48.863808 139995469987840 attention.py:418] Single window, no scan.
I0123 12:50:48.863844 139995469987840 transformer_layer.py:389] tlayer: self-attention.
I0123 12:50:48.863875 139995469987840 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.863937 139995469987840 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.864489 139995469987840 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.864567 139995469987840 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.864916 139995469987840 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.865590 139995469987840 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.868064 139995469987840 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.868682 139995469987840 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.868759 139995469987840 transformer_layer.py:468] tlayer: End windows.
I0123 12:50:48.868793 139995469987840 transformer_layer.py:472] tlayer: final FFN.
I0123 12:50:48.868850 139995469987840 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.868981 139995469987840 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:50:48.869089 139995469987840 nn_components.py:325] mlp: activation = None
I0123 12:50:48.869127 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:50:48.870994 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.871088 139995469987840 nn_components.py:261] mlp: residual
I0123 12:50:48.873464 139995469987840 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.873543 139995469987840 transformer_base.py:443] tbase: final FFN
I0123 12:50:48.873658 139995469987840 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:50:48.876251 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:50:48.878107 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.878203 139995469987840 nn_components.py:261] mlp: residual
I0123 12:50:48.878486 139995469987840 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.878572 139995469987840 decoder_stack.py:344] dstack: Final layernorm.
I0123 12:50:48.881366 139995469987840 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:50:48.936882 139995469987840 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:48.936969 139995469987840 decoder_stack.py:333] dstack: autoregressive generator.
I0123 12:50:48.937022 139995469987840 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 12:50:48.937123 139995469987840 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:50:48.937160 139995469987840 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:50:48.937189 139995469987840 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:50:48.937250 139995469987840 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:48.939539 139995469987840 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:50:48.944823 139995469987840 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:48.945079 139995469987840 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:50:48.947594 139995469987840 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:50:48.959815 139995469987840 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:50:48.959871 139995469987840 attention.py:418] Single window, no scan.
I0123 12:50:48.959907 139995469987840 transformer_layer.py:389] tlayer: self-attention.
I0123 12:50:48.959937 139995469987840 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:48.959999 139995469987840 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:48.960550 139995469987840 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:48.960626 139995469987840 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:48.960968 139995469987840 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:48.961625 139995469987840 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:48.964048 139995469987840 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:48.964649 139995469987840 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:48.964725 139995469987840 transformer_layer.py:468] tlayer: End windows.
I0123 12:50:48.964759 139995469987840 transformer_layer.py:472] tlayer: final FFN.
I0123 12:50:48.964816 139995469987840 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:48.964940 139995469987840 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:50:48.965056 139995469987840 nn_components.py:325] mlp: activation = None
I0123 12:50:48.965094 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:50:48.971657 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:48.971802 139995469987840 nn_components.py:261] mlp: residual
I0123 12:50:48.974314 139995469987840 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:48.974395 139995469987840 transformer_base.py:443] tbase: final FFN
I0123 12:50:48.974510 139995469987840 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:50:48.976823 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:50:48.978686 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:48.978784 139995469987840 nn_components.py:261] mlp: residual
I0123 12:50:48.979065 139995469987840 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:48.979152 139995469987840 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 12:50:48.979263 139995469987840 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:50:48.979305 139995469987840 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:50:48.979336 139995469987840 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:50:48.979400 139995469987840 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:48.981661 139995469987840 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:50:48.987020 139995469987840 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:48.987285 139995469987840 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:50:48.989981 139995469987840 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:50:49.002543 139995469987840 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:50:49.002599 139995469987840 attention.py:418] Single window, no scan.
I0123 12:50:49.002635 139995469987840 transformer_layer.py:389] tlayer: self-attention.
I0123 12:50:49.002667 139995469987840 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.002728 139995469987840 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.003309 139995469987840 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.003385 139995469987840 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.003737 139995469987840 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.004407 139995469987840 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.007054 139995469987840 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.007679 139995469987840 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.007756 139995469987840 transformer_layer.py:468] tlayer: End windows.
I0123 12:50:49.007791 139995469987840 transformer_layer.py:472] tlayer: final FFN.
I0123 12:50:49.007848 139995469987840 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.007974 139995469987840 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:50:49.008082 139995469987840 nn_components.py:325] mlp: activation = None
I0123 12:50:49.008126 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:50:49.009969 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.010065 139995469987840 nn_components.py:261] mlp: residual
I0123 12:50:49.012441 139995469987840 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.012520 139995469987840 transformer_base.py:443] tbase: final FFN
I0123 12:50:49.012626 139995469987840 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:50:49.014879 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:50:49.016699 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.016795 139995469987840 nn_components.py:261] mlp: residual
I0123 12:50:49.017075 139995469987840 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.017156 139995469987840 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 12:50:49.017262 139995469987840 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:50:49.017302 139995469987840 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:50:49.017332 139995469987840 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:50:49.017393 139995469987840 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.019581 139995469987840 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:50:49.024862 139995469987840 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.025116 139995469987840 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:50:49.027714 139995469987840 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:50:49.040024 139995469987840 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:50:49.040079 139995469987840 attention.py:418] Single window, no scan.
I0123 12:50:49.040114 139995469987840 transformer_layer.py:389] tlayer: self-attention.
I0123 12:50:49.040145 139995469987840 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.040206 139995469987840 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.040757 139995469987840 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.040832 139995469987840 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.041177 139995469987840 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.041855 139995469987840 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.044737 139995469987840 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.045346 139995469987840 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.045424 139995469987840 transformer_layer.py:468] tlayer: End windows.
I0123 12:50:49.045458 139995469987840 transformer_layer.py:472] tlayer: final FFN.
I0123 12:50:49.045517 139995469987840 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.045652 139995469987840 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:50:49.045761 139995469987840 nn_components.py:325] mlp: activation = None
I0123 12:50:49.045799 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:50:49.047631 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.047725 139995469987840 nn_components.py:261] mlp: residual
I0123 12:50:49.050067 139995469987840 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.050147 139995469987840 transformer_base.py:443] tbase: final FFN
I0123 12:50:49.050254 139995469987840 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:50:49.052480 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:50:49.054314 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.054410 139995469987840 nn_components.py:261] mlp: residual
I0123 12:50:49.054686 139995469987840 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.054766 139995469987840 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 12:50:49.054872 139995469987840 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:50:49.054909 139995469987840 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:50:49.054939 139995469987840 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:50:49.055000 139995469987840 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.057180 139995469987840 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:50:49.062504 139995469987840 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.062757 139995469987840 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:50:49.065359 139995469987840 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:50:49.077734 139995469987840 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:50:49.077792 139995469987840 attention.py:418] Single window, no scan.
I0123 12:50:49.077829 139995469987840 transformer_layer.py:389] tlayer: self-attention.
I0123 12:50:49.077866 139995469987840 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.077929 139995469987840 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.078482 139995469987840 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.078558 139995469987840 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.078910 139995469987840 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.079579 139995469987840 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.082058 139995469987840 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.082671 139995469987840 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.082746 139995469987840 transformer_layer.py:468] tlayer: End windows.
I0123 12:50:49.082779 139995469987840 transformer_layer.py:472] tlayer: final FFN.
I0123 12:50:49.082836 139995469987840 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.082958 139995469987840 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:50:49.083063 139995469987840 nn_components.py:325] mlp: activation = None
I0123 12:50:49.083101 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:50:49.084932 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.085024 139995469987840 nn_components.py:261] mlp: residual
I0123 12:50:49.087358 139995469987840 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.087435 139995469987840 transformer_base.py:443] tbase: final FFN
I0123 12:50:49.087538 139995469987840 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:50:49.089790 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:50:49.091610 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.091703 139995469987840 nn_components.py:261] mlp: residual
I0123 12:50:49.091980 139995469987840 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.092059 139995469987840 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 12:50:49.092163 139995469987840 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:50:49.092200 139995469987840 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:50:49.092228 139995469987840 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:50:49.092288 139995469987840 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.094458 139995469987840 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:50:49.099698 139995469987840 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.099948 139995469987840 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:50:49.102560 139995469987840 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:50:49.114958 139995469987840 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:50:49.115013 139995469987840 attention.py:418] Single window, no scan.
I0123 12:50:49.115046 139995469987840 transformer_layer.py:389] tlayer: self-attention.
I0123 12:50:49.115075 139995469987840 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.115134 139995469987840 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.115777 139995469987840 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.115852 139995469987840 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.116206 139995469987840 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.116873 139995469987840 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.119462 139995469987840 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.120073 139995469987840 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.120146 139995469987840 transformer_layer.py:468] tlayer: End windows.
I0123 12:50:49.120178 139995469987840 transformer_layer.py:472] tlayer: final FFN.
I0123 12:50:49.120234 139995469987840 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.120359 139995469987840 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:50:49.120465 139995469987840 nn_components.py:325] mlp: activation = None
I0123 12:50:49.120501 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:50:49.122342 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.122438 139995469987840 nn_components.py:261] mlp: residual
I0123 12:50:49.124780 139995469987840 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.124857 139995469987840 transformer_base.py:443] tbase: final FFN
I0123 12:50:49.124963 139995469987840 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:50:49.127200 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:50:49.129015 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.129108 139995469987840 nn_components.py:261] mlp: residual
I0123 12:50:49.129379 139995469987840 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.129457 139995469987840 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 12:50:49.129562 139995469987840 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:50:49.129599 139995469987840 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:50:49.129628 139995469987840 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:50:49.129698 139995469987840 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.131880 139995469987840 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:50:49.137188 139995469987840 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.137442 139995469987840 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:50:49.140062 139995469987840 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:50:49.152389 139995469987840 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:50:49.152443 139995469987840 attention.py:418] Single window, no scan.
I0123 12:50:49.152476 139995469987840 transformer_layer.py:389] tlayer: self-attention.
I0123 12:50:49.152504 139995469987840 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.152563 139995469987840 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.153112 139995469987840 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.153186 139995469987840 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.153531 139995469987840 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.154200 139995469987840 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.157053 139995469987840 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.157669 139995469987840 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.157744 139995469987840 transformer_layer.py:468] tlayer: End windows.
I0123 12:50:49.157777 139995469987840 transformer_layer.py:472] tlayer: final FFN.
I0123 12:50:49.157832 139995469987840 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.157955 139995469987840 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:50:49.158064 139995469987840 nn_components.py:325] mlp: activation = None
I0123 12:50:49.158100 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:50:49.159921 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.160017 139995469987840 nn_components.py:261] mlp: residual
I0123 12:50:49.162354 139995469987840 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.162431 139995469987840 transformer_base.py:443] tbase: final FFN
I0123 12:50:49.162535 139995469987840 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:50:49.164746 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:50:49.166588 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.166682 139995469987840 nn_components.py:261] mlp: residual
I0123 12:50:49.166955 139995469987840 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.167033 139995469987840 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 12:50:49.167139 139995469987840 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:50:49.167176 139995469987840 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:50:49.167205 139995469987840 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:50:49.167265 139995469987840 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.169424 139995469987840 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:50:49.174721 139995469987840 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.174973 139995469987840 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:50:49.177571 139995469987840 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:50:49.189923 139995469987840 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:50:49.189976 139995469987840 attention.py:418] Single window, no scan.
I0123 12:50:49.190010 139995469987840 transformer_layer.py:389] tlayer: self-attention.
I0123 12:50:49.190039 139995469987840 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.190098 139995469987840 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.190650 139995469987840 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.190725 139995469987840 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.191071 139995469987840 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.191743 139995469987840 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.194200 139995469987840 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.194816 139995469987840 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.194892 139995469987840 transformer_layer.py:468] tlayer: End windows.
I0123 12:50:49.194926 139995469987840 transformer_layer.py:472] tlayer: final FFN.
I0123 12:50:49.194983 139995469987840 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.195109 139995469987840 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:50:49.195215 139995469987840 nn_components.py:325] mlp: activation = None
I0123 12:50:49.195252 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:50:49.197088 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.197179 139995469987840 nn_components.py:261] mlp: residual
I0123 12:50:49.199571 139995469987840 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.199649 139995469987840 transformer_base.py:443] tbase: final FFN
I0123 12:50:49.199754 139995469987840 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:50:49.201999 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:50:49.203830 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.203924 139995469987840 nn_components.py:261] mlp: residual
I0123 12:50:49.204202 139995469987840 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.204281 139995469987840 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 12:50:49.204388 139995469987840 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:50:49.204424 139995469987840 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:50:49.204452 139995469987840 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:50:49.204511 139995469987840 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.206689 139995469987840 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:50:49.212002 139995469987840 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.212257 139995469987840 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:50:49.214869 139995469987840 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:50:49.227591 139995469987840 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:50:49.227644 139995469987840 attention.py:418] Single window, no scan.
I0123 12:50:49.227678 139995469987840 transformer_layer.py:389] tlayer: self-attention.
I0123 12:50:49.227707 139995469987840 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.227767 139995469987840 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.228319 139995469987840 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.228394 139995469987840 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.228734 139995469987840 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.229400 139995469987840 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.231848 139995469987840 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.232467 139995469987840 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.232542 139995469987840 transformer_layer.py:468] tlayer: End windows.
I0123 12:50:49.232574 139995469987840 transformer_layer.py:472] tlayer: final FFN.
I0123 12:50:49.232632 139995469987840 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.232755 139995469987840 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:50:49.232860 139995469987840 nn_components.py:325] mlp: activation = None
I0123 12:50:49.232897 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:50:49.234737 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.234829 139995469987840 nn_components.py:261] mlp: residual
I0123 12:50:49.237174 139995469987840 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.237258 139995469987840 transformer_base.py:443] tbase: final FFN
I0123 12:50:49.237365 139995469987840 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:50:49.239582 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:50:49.241401 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.241495 139995469987840 nn_components.py:261] mlp: residual
I0123 12:50:49.241780 139995469987840 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.241860 139995469987840 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 12:50:49.241965 139995469987840 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:50:49.242002 139995469987840 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:50:49.242030 139995469987840 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:50:49.242090 139995469987840 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.244263 139995469987840 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:50:49.249537 139995469987840 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.249799 139995469987840 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:50:49.252382 139995469987840 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:50:49.264734 139995469987840 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:50:49.264789 139995469987840 attention.py:418] Single window, no scan.
I0123 12:50:49.264823 139995469987840 transformer_layer.py:389] tlayer: self-attention.
I0123 12:50:49.264853 139995469987840 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.264914 139995469987840 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.265467 139995469987840 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.265542 139995469987840 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.265894 139995469987840 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.266565 139995469987840 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.269425 139995469987840 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.270044 139995469987840 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.270122 139995469987840 transformer_layer.py:468] tlayer: End windows.
I0123 12:50:49.270155 139995469987840 transformer_layer.py:472] tlayer: final FFN.
I0123 12:50:49.270212 139995469987840 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.270338 139995469987840 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:50:49.270442 139995469987840 nn_components.py:325] mlp: activation = None
I0123 12:50:49.270479 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:50:49.272304 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.272395 139995469987840 nn_components.py:261] mlp: residual
I0123 12:50:49.274722 139995469987840 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.274806 139995469987840 transformer_base.py:443] tbase: final FFN
I0123 12:50:49.274911 139995469987840 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:50:49.277141 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:50:49.278973 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.279067 139995469987840 nn_components.py:261] mlp: residual
I0123 12:50:49.279342 139995469987840 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.279421 139995469987840 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 12:50:49.279525 139995469987840 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:50:49.279562 139995469987840 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:50:49.279591 139995469987840 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:50:49.279652 139995469987840 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.281840 139995469987840 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:50:49.287126 139995469987840 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.287379 139995469987840 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:50:49.289962 139995469987840 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:50:49.302277 139995469987840 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:50:49.302331 139995469987840 attention.py:418] Single window, no scan.
I0123 12:50:49.302364 139995469987840 transformer_layer.py:389] tlayer: self-attention.
I0123 12:50:49.302393 139995469987840 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.302454 139995469987840 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.303009 139995469987840 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.303083 139995469987840 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.303426 139995469987840 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.304090 139995469987840 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.306537 139995469987840 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.307146 139995469987840 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.307220 139995469987840 transformer_layer.py:468] tlayer: End windows.
I0123 12:50:49.307252 139995469987840 transformer_layer.py:472] tlayer: final FFN.
I0123 12:50:49.307307 139995469987840 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.307430 139995469987840 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:50:49.307538 139995469987840 nn_components.py:325] mlp: activation = None
I0123 12:50:49.307575 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:50:49.310133 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.310227 139995469987840 nn_components.py:261] mlp: residual
I0123 12:50:49.312542 139995469987840 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.312623 139995469987840 transformer_base.py:443] tbase: final FFN
I0123 12:50:49.312733 139995469987840 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:50:49.314935 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:50:49.316725 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.316817 139995469987840 nn_components.py:261] mlp: residual
I0123 12:50:49.317093 139995469987840 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.317173 139995469987840 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 12:50:49.317277 139995469987840 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:50:49.317316 139995469987840 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:50:49.317344 139995469987840 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:50:49.317402 139995469987840 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.319727 139995469987840 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:50:49.325071 139995469987840 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.325324 139995469987840 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:50:49.327920 139995469987840 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:50:49.340181 139995469987840 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:50:49.340236 139995469987840 attention.py:418] Single window, no scan.
I0123 12:50:49.340270 139995469987840 transformer_layer.py:389] tlayer: self-attention.
I0123 12:50:49.340299 139995469987840 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.340358 139995469987840 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.340907 139995469987840 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.340981 139995469987840 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.341323 139995469987840 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.341991 139995469987840 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.344445 139995469987840 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.345057 139995469987840 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.345133 139995469987840 transformer_layer.py:468] tlayer: End windows.
I0123 12:50:49.345165 139995469987840 transformer_layer.py:472] tlayer: final FFN.
I0123 12:50:49.345220 139995469987840 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.345344 139995469987840 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:50:49.345448 139995469987840 nn_components.py:325] mlp: activation = None
I0123 12:50:49.345484 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:50:49.347316 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.347408 139995469987840 nn_components.py:261] mlp: residual
I0123 12:50:49.349741 139995469987840 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.349819 139995469987840 transformer_base.py:443] tbase: final FFN
I0123 12:50:49.349923 139995469987840 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:50:49.352148 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:50:49.353954 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.354048 139995469987840 nn_components.py:261] mlp: residual
I0123 12:50:49.354323 139995469987840 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.354400 139995469987840 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 12:50:49.354503 139995469987840 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:50:49.354540 139995469987840 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:50:49.354568 139995469987840 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:50:49.354628 139995469987840 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.356786 139995469987840 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:50:49.362044 139995469987840 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.362298 139995469987840 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:50:49.364895 139995469987840 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:50:49.377170 139995469987840 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:50:49.377224 139995469987840 attention.py:418] Single window, no scan.
I0123 12:50:49.377257 139995469987840 transformer_layer.py:389] tlayer: self-attention.
I0123 12:50:49.377286 139995469987840 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.377345 139995469987840 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.377904 139995469987840 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.377981 139995469987840 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.378331 139995469987840 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.378989 139995469987840 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.381787 139995469987840 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.382401 139995469987840 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.382483 139995469987840 transformer_layer.py:468] tlayer: End windows.
I0123 12:50:49.382515 139995469987840 transformer_layer.py:472] tlayer: final FFN.
I0123 12:50:49.382570 139995469987840 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.382693 139995469987840 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:50:49.382798 139995469987840 nn_components.py:325] mlp: activation = None
I0123 12:50:49.382834 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:50:49.384638 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.384729 139995469987840 nn_components.py:261] mlp: residual
I0123 12:50:49.387056 139995469987840 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.387135 139995469987840 transformer_base.py:443] tbase: final FFN
I0123 12:50:49.387239 139995469987840 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:50:49.389451 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:50:49.391265 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.391359 139995469987840 nn_components.py:261] mlp: residual
I0123 12:50:49.391631 139995469987840 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:49.391716 139995469987840 decoder_stack.py:344] dstack: Final layernorm.
I0123 12:50:49.394454 139995469987840 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:50:53.819357 139995469987840 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
I0123 12:50:54.363021 139995469987840 training_loop.py:409] No working directory specified.
I0123 12:50:54.363164 139995469987840 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0123 12:50:54.364004 139995469987840 checkpoints.py:1062] Restoring legacy Flax checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0123 12:50:57.692543 139995469987840 training_loop.py:447] Only restoring trainable parameters.
I0123 12:50:57.693358 139995469987840 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0123 12:50:57.693418 139995469987840 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0123 12:50:57.693464 139995469987840 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:50:57.693505 139995469987840 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:50:57.693545 139995469987840 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:57.693583 139995469987840 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:50:57.693619 139995469987840 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:57.693664 139995469987840 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:57.693702 139995469987840 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:50:57.693738 139995469987840 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:50:57.693778 139995469987840 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:57.693815 139995469987840 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:50:57.693852 139995469987840 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:50:57.693889 139995469987840 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:50:57.693924 139995469987840 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:57.693960 139995469987840 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:50:57.693996 139995469987840 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:57.694032 139995469987840 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:57.694068 139995469987840 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:50:57.694103 139995469987840 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:50:57.694164 139995469987840 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:57.694202 139995469987840 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:50:57.694238 139995469987840 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:50:57.694273 139995469987840 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:50:57.694308 139995469987840 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:57.694343 139995469987840 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:50:57.694378 139995469987840 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:57.694413 139995469987840 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:57.694448 139995469987840 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:50:57.694483 139995469987840 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:50:57.694517 139995469987840 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:57.694551 139995469987840 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:50:57.694585 139995469987840 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:50:57.694621 139995469987840 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:50:57.694655 139995469987840 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:57.694690 139995469987840 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:50:57.694725 139995469987840 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:57.694760 139995469987840 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:57.694794 139995469987840 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:50:57.694829 139995469987840 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:50:57.694864 139995469987840 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:57.694898 139995469987840 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:50:57.694933 139995469987840 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:50:57.694967 139995469987840 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:50:57.695002 139995469987840 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:57.695037 139995469987840 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:50:57.695078 139995469987840 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:57.695114 139995469987840 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:57.695149 139995469987840 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:50:57.695184 139995469987840 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:50:57.695219 139995469987840 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:57.695254 139995469987840 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:50:57.695288 139995469987840 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:50:57.695323 139995469987840 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:50:57.695357 139995469987840 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:57.695391 139995469987840 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:50:57.695425 139995469987840 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:57.695460 139995469987840 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:57.695495 139995469987840 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:50:57.695528 139995469987840 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:50:57.695562 139995469987840 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:57.695596 139995469987840 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:50:57.695630 139995469987840 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:50:57.695663 139995469987840 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:50:57.695697 139995469987840 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:57.695730 139995469987840 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:50:57.695765 139995469987840 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:57.695799 139995469987840 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:57.695834 139995469987840 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:50:57.695868 139995469987840 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:50:57.695902 139995469987840 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:57.695936 139995469987840 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:50:57.695970 139995469987840 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:50:57.696009 139995469987840 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:50:57.696044 139995469987840 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:57.696079 139995469987840 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:50:57.696114 139995469987840 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:57.696149 139995469987840 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:57.696183 139995469987840 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:50:57.696217 139995469987840 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:50:57.696252 139995469987840 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:57.696286 139995469987840 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:50:57.696320 139995469987840 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:50:57.696354 139995469987840 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:50:57.696389 139995469987840 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:57.696423 139995469987840 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:50:57.696458 139995469987840 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:57.696491 139995469987840 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:57.696526 139995469987840 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:50:57.696559 139995469987840 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:50:57.696594 139995469987840 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:57.696628 139995469987840 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:50:57.696662 139995469987840 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:50:57.696696 139995469987840 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:50:57.696730 139995469987840 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:57.696765 139995469987840 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:50:57.696800 139995469987840 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:57.696834 139995469987840 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:57.696868 139995469987840 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:50:57.696902 139995469987840 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:50:57.696942 139995469987840 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:57.696978 139995469987840 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:50:57.697013 139995469987840 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:50:57.697047 139995469987840 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:50:57.697082 139995469987840 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:57.697116 139995469987840 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:50:57.697150 139995469987840 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:57.697184 139995469987840 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:57.697219 139995469987840 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:50:57.697252 139995469987840 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:50:57.697286 139995469987840 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:57.697320 139995469987840 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:50:57.697354 139995469987840 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 12:50:57.697388 139995469987840 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 12:50:57.697422 139995469987840 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:57.697456 139995469987840 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 12:50:57.697491 139995469987840 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:57.697525 139995469987840 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:57.697558 139995469987840 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 12:50:57.697592 139995469987840 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 12:50:57.697627 139995469987840 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 12:50:57.697668 139995469987840 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 12:50:57.697696 139995469987840 training_loop.py:725] Total parameters: 152072288
I0123 12:50:57.697925 139995469987840 training_loop.py:739] Total state size: 0
I0123 12:50:57.720123 139995469987840 training_loop.py:492] Training loop: creating task for mode beam_search
I0123 12:50:57.720474 139995469987840 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0123 12:50:57.720847 139995469987840 training_loop.py:652] Compiling mode beam_search with jit.
I0123 12:50:57.721212 139995469987840 training_loop.py:89] registering functions: dict_keys([])
I0123 12:50:57.742695 139995469987840 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = lc_tangent h g c, on_line h a c; i = lc_tangent i g c, on_line i b c; j = on_circle j d c, on_bline j c b; k = on_circle k d a, on_bline k a c; l = on_line l j i, on_line l k h; m = on_line m c l, on_line m i h ? eqangle m b m i m h m a
I0123 12:51:00.039811 139995469987840 ddar.py:60] Depth 1/1000 time = 2.2195613384246826
I0123 12:51:03.528741 139995469987840 ddar.py:60] Depth 2/1000 time = 3.4887099266052246
I0123 12:51:06.964506 139995469987840 ddar.py:60] Depth 3/1000 time = 3.435516119003296
I0123 12:51:11.684059 139995469987840 ddar.py:60] Depth 4/1000 time = 4.717082977294922
I0123 12:51:17.668097 139995469987840 ddar.py:60] Depth 5/1000 time = 5.983734130859375
I0123 12:51:23.956944 139995469987840 ddar.py:60] Depth 6/1000 time = 6.288551092147827
I0123 12:51:35.580226 139995469987840 ddar.py:60] Depth 7/1000 time = 11.622869729995728
I0123 12:51:45.977744 139995469987840 ddar.py:60] Depth 8/1000 time = 10.397135019302368
I0123 12:52:01.940226 139995469987840 ddar.py:60] Depth 9/1000 time = 15.962116479873657
I0123 12:52:17.733834 139995469987840 ddar.py:60] Depth 10/1000 time = 15.793161869049072
I0123 12:52:34.242811 139995469987840 ddar.py:60] Depth 11/1000 time = 16.50849986076355
I0123 12:52:51.020033 139995469987840 ddar.py:60] Depth 12/1000 time = 16.737099170684814
I0123 12:53:07.849499 139995469987840 ddar.py:60] Depth 13/1000 time = 16.810099124908447
I0123 12:53:24.926118 139995469987840 ddar.py:60] Depth 14/1000 time = 16.952428102493286
I0123 12:53:41.736698 139995469987840 ddar.py:60] Depth 15/1000 time = 16.797332286834717
I0123 12:53:41.737377 139995469987840 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:53:41.737497 139995469987840 alphageometry.py:540] Depth 0. There are 1 nodes to expand:
I0123 12:53:41.737534 139995469987840 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a b e 02 ^ c a c e c e c b 03 ; f : C a c f 04 ^ b a b f b f b c 05 ; g : C b f g 06 C c e g 07 ; h : C a c h 08 T c g g h 09 ; i : C b c i 10 T c g g i 11 ; j : D b j c j 12 D c d d j 13 ^ c b c j b j b c 14 ; k : D a d d k 15 D a k c k 16 ^ c a c k a k a c 17 ; l : C h k l 18 C i j l 19 ; m : C c l m 20 C h i m 21 ? ^ m b m i m h m a {F1} x00
I0123 12:53:41.737565 139995469987840 alphageometry.py:549] Decoding from {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C a b e 02 ^ c a c e c e c b 03 ; f : C a c f 04 ^ b a b f b f b c 05 ; g : C b f g 06 C c e g 07 ; h : C a c h 08 T c g g h 09 ; i : C b c i 10 T c g g i 11 ; j : D b j c j 12 D c d d j 13 ^ c b c j b j b c 14 ; k : D a d d k 15 D a k c k 16 ^ c a c k a k a c 17 ; l : C h k l 18 C i j l 19 ; m : C c l m 20 C h i m 21 ? ^ m b m i m h m a {F1} x00
I0123 12:53:41.893327 139995469987840 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:41.893542 139995469987840 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 12:53:41.893658 139995469987840 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 12:53:41.893744 139995469987840 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 12:53:41.893818 139995469987840 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 12:53:41.893885 139995469987840 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 12:53:41.893950 139995469987840 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 12:53:41.894015 139995469987840 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 12:53:41.894081 139995469987840 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 12:53:41.894165 139995469987840 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 12:53:41.894231 139995469987840 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 12:53:41.894297 139995469987840 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 12:53:41.894364 139995469987840 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 12:53:41.894429 139995469987840 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 12:53:41.894466 139995469987840 attention.py:418] Single window, no scan.
I0123 12:53:41.894507 139995469987840 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 12:53:41.894615 139995469987840 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:53:41.894652 139995469987840 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:53:41.894682 139995469987840 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:53:41.896585 139995469987840 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:41.899014 139995469987840 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:53:41.904647 139995469987840 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:41.904924 139995469987840 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:53:41.907476 139995469987840 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:53:41.911277 139995469987840 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:53:41.911330 139995469987840 attention.py:418] Single window, no scan.
I0123 12:53:41.911365 139995469987840 transformer_layer.py:389] tlayer: self-attention.
I0123 12:53:41.911399 139995469987840 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:41.911461 139995469987840 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:41.912114 139995469987840 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:41.912190 139995469987840 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:41.912534 139995469987840 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:41.913276 139995469987840 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:41.915664 139995469987840 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:41.916268 139995469987840 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:41.916344 139995469987840 transformer_layer.py:468] tlayer: End windows.
I0123 12:53:41.916377 139995469987840 transformer_layer.py:472] tlayer: final FFN.
I0123 12:53:41.916432 139995469987840 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:41.916554 139995469987840 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:53:41.916867 139995469987840 nn_components.py:325] mlp: activation = None
I0123 12:53:41.916908 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:53:41.918809 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:41.918900 139995469987840 nn_components.py:261] mlp: residual
I0123 12:53:41.921257 139995469987840 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:41.921333 139995469987840 transformer_base.py:443] tbase: final FFN
I0123 12:53:41.921768 139995469987840 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:53:41.923965 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:53:41.925806 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:41.925900 139995469987840 nn_components.py:261] mlp: residual
I0123 12:53:41.926175 139995469987840 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:41.926252 139995469987840 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 12:53:41.926354 139995469987840 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:53:41.926391 139995469987840 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:53:41.926420 139995469987840 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:53:41.928214 139995469987840 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:41.930441 139995469987840 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:53:41.935779 139995469987840 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:41.936031 139995469987840 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:53:41.938548 139995469987840 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:53:41.942111 139995469987840 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:53:41.942163 139995469987840 attention.py:418] Single window, no scan.
I0123 12:53:41.942196 139995469987840 transformer_layer.py:389] tlayer: self-attention.
I0123 12:53:41.942224 139995469987840 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:41.942283 139995469987840 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:41.942816 139995469987840 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:41.942889 139995469987840 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:41.943221 139995469987840 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:41.943939 139995469987840 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:41.946266 139995469987840 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:41.946912 139995469987840 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:41.946985 139995469987840 transformer_layer.py:468] tlayer: End windows.
I0123 12:53:41.947018 139995469987840 transformer_layer.py:472] tlayer: final FFN.
I0123 12:53:41.947072 139995469987840 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:41.947193 139995469987840 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:53:41.947496 139995469987840 nn_components.py:325] mlp: activation = None
I0123 12:53:41.947535 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:53:41.949365 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:41.949455 139995469987840 nn_components.py:261] mlp: residual
I0123 12:53:41.951819 139995469987840 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:41.951896 139995469987840 transformer_base.py:443] tbase: final FFN
I0123 12:53:41.952308 139995469987840 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:53:41.954971 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:53:41.957149 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:41.957242 139995469987840 nn_components.py:261] mlp: residual
I0123 12:53:41.957528 139995469987840 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:41.957607 139995469987840 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 12:53:41.957718 139995469987840 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:53:41.957756 139995469987840 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:53:41.957786 139995469987840 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:53:41.959518 139995469987840 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:41.961757 139995469987840 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:53:41.967238 139995469987840 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:41.967491 139995469987840 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:53:41.969984 139995469987840 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:53:41.973515 139995469987840 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:53:41.973566 139995469987840 attention.py:418] Single window, no scan.
I0123 12:53:41.973600 139995469987840 transformer_layer.py:389] tlayer: self-attention.
I0123 12:53:41.973629 139995469987840 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:41.973698 139995469987840 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:41.974293 139995469987840 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:41.974367 139995469987840 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:41.974709 139995469987840 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:41.975440 139995469987840 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:41.977793 139995469987840 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:41.978386 139995469987840 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:41.978460 139995469987840 transformer_layer.py:468] tlayer: End windows.
I0123 12:53:41.978492 139995469987840 transformer_layer.py:472] tlayer: final FFN.
I0123 12:53:41.978546 139995469987840 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:41.978667 139995469987840 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:53:41.978973 139995469987840 nn_components.py:325] mlp: activation = None
I0123 12:53:41.979012 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:53:41.980904 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:41.980993 139995469987840 nn_components.py:261] mlp: residual
I0123 12:53:41.983361 139995469987840 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:41.983439 139995469987840 transformer_base.py:443] tbase: final FFN
I0123 12:53:41.983847 139995469987840 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:53:41.986017 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:53:41.987853 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:41.987951 139995469987840 nn_components.py:261] mlp: residual
I0123 12:53:41.988228 139995469987840 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:41.988305 139995469987840 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 12:53:41.988409 139995469987840 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:53:41.988446 139995469987840 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:53:41.988475 139995469987840 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:53:41.990286 139995469987840 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:41.992489 139995469987840 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:53:41.997836 139995469987840 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:41.998084 139995469987840 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:53:42.000515 139995469987840 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:53:42.004103 139995469987840 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:53:42.004156 139995469987840 attention.py:418] Single window, no scan.
I0123 12:53:42.004190 139995469987840 transformer_layer.py:389] tlayer: self-attention.
I0123 12:53:42.004219 139995469987840 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.004279 139995469987840 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.004822 139995469987840 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.004895 139995469987840 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.005234 139995469987840 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.005985 139995469987840 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.008321 139995469987840 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.008925 139995469987840 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.009000 139995469987840 transformer_layer.py:468] tlayer: End windows.
I0123 12:53:42.009033 139995469987840 transformer_layer.py:472] tlayer: final FFN.
I0123 12:53:42.009088 139995469987840 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.009212 139995469987840 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:53:42.009570 139995469987840 nn_components.py:325] mlp: activation = None
I0123 12:53:42.009611 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:53:42.011448 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.011538 139995469987840 nn_components.py:261] mlp: residual
I0123 12:53:42.013887 139995469987840 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.013964 139995469987840 transformer_base.py:443] tbase: final FFN
I0123 12:53:42.014373 139995469987840 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:53:42.016535 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:53:42.018471 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.018569 139995469987840 nn_components.py:261] mlp: residual
I0123 12:53:42.018846 139995469987840 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.018925 139995469987840 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 12:53:42.019029 139995469987840 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:53:42.019066 139995469987840 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:53:42.019095 139995469987840 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:53:42.020838 139995469987840 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.023074 139995469987840 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:53:42.028496 139995469987840 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.028751 139995469987840 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:53:42.031191 139995469987840 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:53:42.034709 139995469987840 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:53:42.034762 139995469987840 attention.py:418] Single window, no scan.
I0123 12:53:42.034796 139995469987840 transformer_layer.py:389] tlayer: self-attention.
I0123 12:53:42.034825 139995469987840 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.034885 139995469987840 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.035476 139995469987840 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.035551 139995469987840 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.035889 139995469987840 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.036622 139995469987840 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.038964 139995469987840 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.039561 139995469987840 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.039636 139995469987840 transformer_layer.py:468] tlayer: End windows.
I0123 12:53:42.039668 139995469987840 transformer_layer.py:472] tlayer: final FFN.
I0123 12:53:42.039721 139995469987840 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.039842 139995469987840 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:53:42.040146 139995469987840 nn_components.py:325] mlp: activation = None
I0123 12:53:42.040185 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:53:42.042079 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.042171 139995469987840 nn_components.py:261] mlp: residual
I0123 12:53:42.044507 139995469987840 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.044582 139995469987840 transformer_base.py:443] tbase: final FFN
I0123 12:53:42.044987 139995469987840 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:53:42.047159 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:53:42.048994 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.049084 139995469987840 nn_components.py:261] mlp: residual
I0123 12:53:42.049368 139995469987840 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.049446 139995469987840 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 12:53:42.049550 139995469987840 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:53:42.049586 139995469987840 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:53:42.049614 139995469987840 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:53:42.051420 139995469987840 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.053660 139995469987840 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:53:42.059029 139995469987840 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.059280 139995469987840 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:53:42.061750 139995469987840 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:53:42.065713 139995469987840 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:53:42.065767 139995469987840 attention.py:418] Single window, no scan.
I0123 12:53:42.065801 139995469987840 transformer_layer.py:389] tlayer: self-attention.
I0123 12:53:42.065830 139995469987840 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.065889 139995469987840 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.066427 139995469987840 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.066501 139995469987840 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.066843 139995469987840 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.067576 139995469987840 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.069896 139995469987840 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.070498 139995469987840 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.070572 139995469987840 transformer_layer.py:468] tlayer: End windows.
I0123 12:53:42.070603 139995469987840 transformer_layer.py:472] tlayer: final FFN.
I0123 12:53:42.070658 139995469987840 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.070799 139995469987840 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:53:42.071154 139995469987840 nn_components.py:325] mlp: activation = None
I0123 12:53:42.071195 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:53:42.073021 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.073110 139995469987840 nn_components.py:261] mlp: residual
I0123 12:53:42.075457 139995469987840 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.075533 139995469987840 transformer_base.py:443] tbase: final FFN
I0123 12:53:42.075940 139995469987840 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:53:42.078116 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:53:42.080024 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.080115 139995469987840 nn_components.py:261] mlp: residual
I0123 12:53:42.080389 139995469987840 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.080474 139995469987840 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 12:53:42.080579 139995469987840 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:53:42.080615 139995469987840 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:53:42.080644 139995469987840 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:53:42.082361 139995469987840 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.084573 139995469987840 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:53:42.090331 139995469987840 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.090589 139995469987840 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:53:42.093131 139995469987840 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:53:42.096786 139995469987840 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:53:42.096840 139995469987840 attention.py:418] Single window, no scan.
I0123 12:53:42.096875 139995469987840 transformer_layer.py:389] tlayer: self-attention.
I0123 12:53:42.096906 139995469987840 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.096970 139995469987840 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.097580 139995469987840 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.097669 139995469987840 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.098027 139995469987840 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.098784 139995469987840 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.101190 139995469987840 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.101829 139995469987840 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.101908 139995469987840 transformer_layer.py:468] tlayer: End windows.
I0123 12:53:42.101942 139995469987840 transformer_layer.py:472] tlayer: final FFN.
I0123 12:53:42.101998 139995469987840 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.102125 139995469987840 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:53:42.102438 139995469987840 nn_components.py:325] mlp: activation = None
I0123 12:53:42.102479 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:53:42.104425 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.104520 139995469987840 nn_components.py:261] mlp: residual
I0123 12:53:42.106945 139995469987840 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.107025 139995469987840 transformer_base.py:443] tbase: final FFN
I0123 12:53:42.107442 139995469987840 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:53:42.109585 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:53:42.111449 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.111540 139995469987840 nn_components.py:261] mlp: residual
I0123 12:53:42.111813 139995469987840 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.111889 139995469987840 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 12:53:42.112000 139995469987840 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:53:42.112037 139995469987840 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:53:42.112066 139995469987840 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:53:42.113853 139995469987840 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.116044 139995469987840 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:53:42.121366 139995469987840 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.121614 139995469987840 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:53:42.124050 139995469987840 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:53:42.127608 139995469987840 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:53:42.127662 139995469987840 attention.py:418] Single window, no scan.
I0123 12:53:42.127695 139995469987840 transformer_layer.py:389] tlayer: self-attention.
I0123 12:53:42.127723 139995469987840 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.127782 139995469987840 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.128312 139995469987840 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.128385 139995469987840 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.128720 139995469987840 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.129436 139995469987840 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.131741 139995469987840 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.132333 139995469987840 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.132406 139995469987840 transformer_layer.py:468] tlayer: End windows.
I0123 12:53:42.132439 139995469987840 transformer_layer.py:472] tlayer: final FFN.
I0123 12:53:42.132494 139995469987840 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.132616 139995469987840 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:53:42.132969 139995469987840 nn_components.py:325] mlp: activation = None
I0123 12:53:42.133009 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:53:42.134850 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.134939 139995469987840 nn_components.py:261] mlp: residual
I0123 12:53:42.137277 139995469987840 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.137354 139995469987840 transformer_base.py:443] tbase: final FFN
I0123 12:53:42.137770 139995469987840 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:53:42.139932 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:53:42.141847 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.141939 139995469987840 nn_components.py:261] mlp: residual
I0123 12:53:42.142216 139995469987840 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.142293 139995469987840 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 12:53:42.142396 139995469987840 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:53:42.142438 139995469987840 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:53:42.142467 139995469987840 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:53:42.144183 139995469987840 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.146404 139995469987840 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:53:42.151819 139995469987840 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.152067 139995469987840 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:53:42.154508 139995469987840 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:53:42.158013 139995469987840 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:53:42.158065 139995469987840 attention.py:418] Single window, no scan.
I0123 12:53:42.158098 139995469987840 transformer_layer.py:389] tlayer: self-attention.
I0123 12:53:42.158127 139995469987840 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.158187 139995469987840 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.158777 139995469987840 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.158849 139995469987840 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.159187 139995469987840 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.159915 139995469987840 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.162252 139995469987840 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.162841 139995469987840 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.162914 139995469987840 transformer_layer.py:468] tlayer: End windows.
I0123 12:53:42.162947 139995469987840 transformer_layer.py:472] tlayer: final FFN.
I0123 12:53:42.163001 139995469987840 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.163123 139995469987840 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:53:42.163424 139995469987840 nn_components.py:325] mlp: activation = None
I0123 12:53:42.163464 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:53:42.165284 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.165373 139995469987840 nn_components.py:261] mlp: residual
I0123 12:53:42.168099 139995469987840 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.168177 139995469987840 transformer_base.py:443] tbase: final FFN
I0123 12:53:42.168583 139995469987840 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:53:42.170753 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:53:42.172568 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.172660 139995469987840 nn_components.py:261] mlp: residual
I0123 12:53:42.172932 139995469987840 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.173010 139995469987840 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 12:53:42.173113 139995469987840 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:53:42.173150 139995469987840 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:53:42.173188 139995469987840 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:53:42.174916 139995469987840 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.177210 139995469987840 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:53:42.182528 139995469987840 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.182773 139995469987840 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:53:42.185202 139995469987840 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:53:42.188722 139995469987840 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:53:42.188776 139995469987840 attention.py:418] Single window, no scan.
I0123 12:53:42.188809 139995469987840 transformer_layer.py:389] tlayer: self-attention.
I0123 12:53:42.188838 139995469987840 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.188897 139995469987840 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.189484 139995469987840 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.189558 139995469987840 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.189902 139995469987840 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.190634 139995469987840 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.192970 139995469987840 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.193561 139995469987840 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.193634 139995469987840 transformer_layer.py:468] tlayer: End windows.
I0123 12:53:42.193674 139995469987840 transformer_layer.py:472] tlayer: final FFN.
I0123 12:53:42.193730 139995469987840 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.193853 139995469987840 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:53:42.194160 139995469987840 nn_components.py:325] mlp: activation = None
I0123 12:53:42.194199 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:53:42.196027 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.196116 139995469987840 nn_components.py:261] mlp: residual
I0123 12:53:42.198533 139995469987840 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.198610 139995469987840 transformer_base.py:443] tbase: final FFN
I0123 12:53:42.199014 139995469987840 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:53:42.201207 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:53:42.203059 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.203151 139995469987840 nn_components.py:261] mlp: residual
I0123 12:53:42.203428 139995469987840 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.203506 139995469987840 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 12:53:42.203609 139995469987840 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:53:42.203645 139995469987840 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:53:42.203673 139995469987840 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:53:42.205402 139995469987840 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.207689 139995469987840 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:53:42.213051 139995469987840 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.213302 139995469987840 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:53:42.215761 139995469987840 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:53:42.219293 139995469987840 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:53:42.219345 139995469987840 attention.py:418] Single window, no scan.
I0123 12:53:42.219378 139995469987840 transformer_layer.py:389] tlayer: self-attention.
I0123 12:53:42.219406 139995469987840 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.219465 139995469987840 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.220051 139995469987840 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.220124 139995469987840 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.220462 139995469987840 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.221192 139995469987840 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.223532 139995469987840 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.224129 139995469987840 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.224203 139995469987840 transformer_layer.py:468] tlayer: End windows.
I0123 12:53:42.224235 139995469987840 transformer_layer.py:472] tlayer: final FFN.
I0123 12:53:42.224288 139995469987840 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.224410 139995469987840 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:53:42.224712 139995469987840 nn_components.py:325] mlp: activation = None
I0123 12:53:42.224751 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:53:42.226579 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.226671 139995469987840 nn_components.py:261] mlp: residual
I0123 12:53:42.229053 139995469987840 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.229129 139995469987840 transformer_base.py:443] tbase: final FFN
I0123 12:53:42.229537 139995469987840 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:53:42.231695 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:53:42.233531 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.233621 139995469987840 nn_components.py:261] mlp: residual
I0123 12:53:42.233904 139995469987840 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.233983 139995469987840 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 12:53:42.234087 139995469987840 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:53:42.234123 139995469987840 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:53:42.234152 139995469987840 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:53:42.235874 139995469987840 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.238175 139995469987840 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:53:42.243474 139995469987840 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.243724 139995469987840 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:53:42.246167 139995469987840 transformer_layer.py:213] tlayer: windowed attention.
I0123 12:53:42.249681 139995469987840 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:53:42.249733 139995469987840 attention.py:418] Single window, no scan.
I0123 12:53:42.249766 139995469987840 transformer_layer.py:389] tlayer: self-attention.
I0123 12:53:42.249794 139995469987840 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.249853 139995469987840 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.250442 139995469987840 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.250516 139995469987840 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.250857 139995469987840 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.251580 139995469987840 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.253917 139995469987840 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.254514 139995469987840 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.254589 139995469987840 transformer_layer.py:468] tlayer: End windows.
I0123 12:53:42.254621 139995469987840 transformer_layer.py:472] tlayer: final FFN.
I0123 12:53:42.254676 139995469987840 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.254796 139995469987840 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:53:42.255101 139995469987840 nn_components.py:325] mlp: activation = None
I0123 12:53:42.255142 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:53:42.256947 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.257036 139995469987840 nn_components.py:261] mlp: residual
I0123 12:53:42.259413 139995469987840 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.259490 139995469987840 transformer_base.py:443] tbase: final FFN
I0123 12:53:42.259895 139995469987840 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:53:42.262043 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:53:42.263858 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.263948 139995469987840 nn_components.py:261] mlp: residual
I0123 12:53:42.264222 139995469987840 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.264460 139995469987840 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 12:53:42.264523 139995469987840 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 12:53:42.264578 139995469987840 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 12:53:42.264628 139995469987840 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 12:53:42.264678 139995469987840 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 12:53:42.264735 139995469987840 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 12:53:42.264787 139995469987840 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 12:53:42.264837 139995469987840 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 12:53:42.264887 139995469987840 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 12:53:42.264936 139995469987840 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 12:53:42.264986 139995469987840 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 12:53:42.265034 139995469987840 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 12:53:42.265068 139995469987840 decoder_stack.py:344] dstack: Final layernorm.
I0123 12:53:42.267833 139995469987840 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 12:53:42.311219 139995469987840 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.311300 139995469987840 decoder_stack.py:333] dstack: autoregressive generator.
I0123 12:53:42.311351 139995469987840 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 12:53:42.311450 139995469987840 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:53:42.311486 139995469987840 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:53:42.311513 139995469987840 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:53:42.311571 139995469987840 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.313821 139995469987840 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:53:42.319020 139995469987840 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.319269 139995469987840 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:53:42.321844 139995469987840 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:53:42.334193 139995469987840 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:53:42.334247 139995469987840 attention.py:418] Single window, no scan.
I0123 12:53:42.334281 139995469987840 transformer_layer.py:389] tlayer: self-attention.
I0123 12:53:42.334310 139995469987840 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.334370 139995469987840 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.334911 139995469987840 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.334985 139995469987840 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.335330 139995469987840 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.335988 139995469987840 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.338402 139995469987840 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.339000 139995469987840 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.339074 139995469987840 transformer_layer.py:468] tlayer: End windows.
I0123 12:53:42.339106 139995469987840 transformer_layer.py:472] tlayer: final FFN.
I0123 12:53:42.339161 139995469987840 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.339291 139995469987840 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:53:42.339397 139995469987840 nn_components.py:325] mlp: activation = None
I0123 12:53:42.339433 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:53:42.341203 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.341292 139995469987840 nn_components.py:261] mlp: residual
I0123 12:53:42.343583 139995469987840 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.343659 139995469987840 transformer_base.py:443] tbase: final FFN
I0123 12:53:42.343762 139995469987840 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:53:42.346256 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:53:42.348044 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.348135 139995469987840 nn_components.py:261] mlp: residual
I0123 12:53:42.348415 139995469987840 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.348492 139995469987840 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 12:53:42.348595 139995469987840 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:53:42.348632 139995469987840 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:53:42.348660 139995469987840 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:53:42.348720 139995469987840 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.350859 139995469987840 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:53:42.356059 139995469987840 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.356313 139995469987840 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:53:42.358878 139995469987840 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:53:42.370930 139995469987840 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:53:42.370984 139995469987840 attention.py:418] Single window, no scan.
I0123 12:53:42.371017 139995469987840 transformer_layer.py:389] tlayer: self-attention.
I0123 12:53:42.371046 139995469987840 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.371104 139995469987840 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.371639 139995469987840 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.371714 139995469987840 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.372054 139995469987840 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.372760 139995469987840 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.375103 139995469987840 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.375696 139995469987840 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.375770 139995469987840 transformer_layer.py:468] tlayer: End windows.
I0123 12:53:42.375802 139995469987840 transformer_layer.py:472] tlayer: final FFN.
I0123 12:53:42.375856 139995469987840 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.375978 139995469987840 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:53:42.376088 139995469987840 nn_components.py:325] mlp: activation = None
I0123 12:53:42.376124 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:53:42.377921 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.378012 139995469987840 nn_components.py:261] mlp: residual
I0123 12:53:42.380311 139995469987840 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.380387 139995469987840 transformer_base.py:443] tbase: final FFN
I0123 12:53:42.380490 139995469987840 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:53:42.382678 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:53:42.384469 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.384560 139995469987840 nn_components.py:261] mlp: residual
I0123 12:53:42.384839 139995469987840 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.384917 139995469987840 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 12:53:42.385021 139995469987840 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:53:42.385058 139995469987840 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:53:42.385086 139995469987840 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:53:42.385145 139995469987840 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.387299 139995469987840 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:53:42.392442 139995469987840 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.392705 139995469987840 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:53:42.395269 139995469987840 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:53:42.407264 139995469987840 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:53:42.407318 139995469987840 attention.py:418] Single window, no scan.
I0123 12:53:42.407351 139995469987840 transformer_layer.py:389] tlayer: self-attention.
I0123 12:53:42.407379 139995469987840 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.407439 139995469987840 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.407975 139995469987840 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.408049 139995469987840 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.408388 139995469987840 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.409095 139995469987840 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.411436 139995469987840 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.412030 139995469987840 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.412105 139995469987840 transformer_layer.py:468] tlayer: End windows.
I0123 12:53:42.412137 139995469987840 transformer_layer.py:472] tlayer: final FFN.
I0123 12:53:42.412194 139995469987840 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.412318 139995469987840 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:53:42.412423 139995469987840 nn_components.py:325] mlp: activation = None
I0123 12:53:42.412465 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:53:42.414264 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.414355 139995469987840 nn_components.py:261] mlp: residual
I0123 12:53:42.416675 139995469987840 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.416750 139995469987840 transformer_base.py:443] tbase: final FFN
I0123 12:53:42.416854 139995469987840 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:53:42.419028 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:53:42.420791 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.420882 139995469987840 nn_components.py:261] mlp: residual
I0123 12:53:42.421159 139995469987840 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.421236 139995469987840 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 12:53:42.421340 139995469987840 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:53:42.421377 139995469987840 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:53:42.421406 139995469987840 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:53:42.421466 139995469987840 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.423614 139995469987840 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:53:42.428754 139995469987840 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.429005 139995469987840 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:53:42.431540 139995469987840 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:53:42.443587 139995469987840 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:53:42.443639 139995469987840 attention.py:418] Single window, no scan.
I0123 12:53:42.443672 139995469987840 transformer_layer.py:389] tlayer: self-attention.
I0123 12:53:42.443701 139995469987840 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.443759 139995469987840 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.444293 139995469987840 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.444367 139995469987840 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.444705 139995469987840 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.445402 139995469987840 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.447760 139995469987840 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.448355 139995469987840 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.448430 139995469987840 transformer_layer.py:468] tlayer: End windows.
I0123 12:53:42.448462 139995469987840 transformer_layer.py:472] tlayer: final FFN.
I0123 12:53:42.448518 139995469987840 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.448641 139995469987840 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:53:42.448745 139995469987840 nn_components.py:325] mlp: activation = None
I0123 12:53:42.448788 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:53:42.450582 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.450672 139995469987840 nn_components.py:261] mlp: residual
I0123 12:53:42.452949 139995469987840 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.453025 139995469987840 transformer_base.py:443] tbase: final FFN
I0123 12:53:42.453129 139995469987840 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:53:42.455729 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:53:42.457503 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.457595 139995469987840 nn_components.py:261] mlp: residual
I0123 12:53:42.457881 139995469987840 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.457960 139995469987840 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 12:53:42.458065 139995469987840 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:53:42.458101 139995469987840 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:53:42.458129 139995469987840 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:53:42.458187 139995469987840 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.460331 139995469987840 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:53:42.465554 139995469987840 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.465815 139995469987840 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:53:42.468351 139995469987840 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:53:42.480419 139995469987840 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:53:42.480473 139995469987840 attention.py:418] Single window, no scan.
I0123 12:53:42.480507 139995469987840 transformer_layer.py:389] tlayer: self-attention.
I0123 12:53:42.480536 139995469987840 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.480596 139995469987840 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.481135 139995469987840 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.481210 139995469987840 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.481548 139995469987840 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.482305 139995469987840 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.484640 139995469987840 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.485239 139995469987840 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.485315 139995469987840 transformer_layer.py:468] tlayer: End windows.
I0123 12:53:42.485347 139995469987840 transformer_layer.py:472] tlayer: final FFN.
I0123 12:53:42.485402 139995469987840 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.485526 139995469987840 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:53:42.485630 139995469987840 nn_components.py:325] mlp: activation = None
I0123 12:53:42.485676 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:53:42.487469 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.487559 139995469987840 nn_components.py:261] mlp: residual
I0123 12:53:42.489869 139995469987840 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.489947 139995469987840 transformer_base.py:443] tbase: final FFN
I0123 12:53:42.490051 139995469987840 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:53:42.492225 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:53:42.494004 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.494096 139995469987840 nn_components.py:261] mlp: residual
I0123 12:53:42.494374 139995469987840 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.494451 139995469987840 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 12:53:42.494553 139995469987840 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:53:42.494590 139995469987840 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:53:42.494618 139995469987840 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:53:42.494677 139995469987840 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.496817 139995469987840 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:53:42.501991 139995469987840 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.502239 139995469987840 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:53:42.504770 139995469987840 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:53:42.516874 139995469987840 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:53:42.516928 139995469987840 attention.py:418] Single window, no scan.
I0123 12:53:42.516961 139995469987840 transformer_layer.py:389] tlayer: self-attention.
I0123 12:53:42.516989 139995469987840 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.517049 139995469987840 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.517582 139995469987840 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.517662 139995469987840 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.518003 139995469987840 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.518712 139995469987840 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.521060 139995469987840 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.521658 139995469987840 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.521734 139995469987840 transformer_layer.py:468] tlayer: End windows.
I0123 12:53:42.521767 139995469987840 transformer_layer.py:472] tlayer: final FFN.
I0123 12:53:42.521823 139995469987840 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.521947 139995469987840 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:53:42.522052 139995469987840 nn_components.py:325] mlp: activation = None
I0123 12:53:42.522088 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:53:42.523867 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.523963 139995469987840 nn_components.py:261] mlp: residual
I0123 12:53:42.526274 139995469987840 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.526351 139995469987840 transformer_base.py:443] tbase: final FFN
I0123 12:53:42.526455 139995469987840 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:53:42.528611 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:53:42.530385 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.530477 139995469987840 nn_components.py:261] mlp: residual
I0123 12:53:42.530751 139995469987840 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.530828 139995469987840 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 12:53:42.530932 139995469987840 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:53:42.530969 139995469987840 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:53:42.530997 139995469987840 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:53:42.531055 139995469987840 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.533179 139995469987840 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:53:42.538393 139995469987840 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.538643 139995469987840 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:53:42.541167 139995469987840 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:53:42.553121 139995469987840 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:53:42.553175 139995469987840 attention.py:418] Single window, no scan.
I0123 12:53:42.553209 139995469987840 transformer_layer.py:389] tlayer: self-attention.
I0123 12:53:42.553238 139995469987840 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.553298 139995469987840 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.553845 139995469987840 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.553920 139995469987840 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.554258 139995469987840 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.554959 139995469987840 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.557283 139995469987840 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.557887 139995469987840 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.557962 139995469987840 transformer_layer.py:468] tlayer: End windows.
I0123 12:53:42.557994 139995469987840 transformer_layer.py:472] tlayer: final FFN.
I0123 12:53:42.558048 139995469987840 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.558171 139995469987840 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:53:42.558274 139995469987840 nn_components.py:325] mlp: activation = None
I0123 12:53:42.558310 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:53:42.560083 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.560177 139995469987840 nn_components.py:261] mlp: residual
I0123 12:53:42.562495 139995469987840 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.562572 139995469987840 transformer_base.py:443] tbase: final FFN
I0123 12:53:42.562676 139995469987840 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:53:42.565250 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:53:42.567049 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.567141 139995469987840 nn_components.py:261] mlp: residual
I0123 12:53:42.567416 139995469987840 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.567493 139995469987840 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 12:53:42.567595 139995469987840 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:53:42.567631 139995469987840 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:53:42.567660 139995469987840 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:53:42.567719 139995469987840 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.569858 139995469987840 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:53:42.575021 139995469987840 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.575273 139995469987840 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:53:42.577827 139995469987840 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:53:42.589872 139995469987840 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:53:42.589925 139995469987840 attention.py:418] Single window, no scan.
I0123 12:53:42.589958 139995469987840 transformer_layer.py:389] tlayer: self-attention.
I0123 12:53:42.589986 139995469987840 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.590047 139995469987840 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.590586 139995469987840 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.590659 139995469987840 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.590999 139995469987840 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.591706 139995469987840 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.594068 139995469987840 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.594669 139995469987840 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.594743 139995469987840 transformer_layer.py:468] tlayer: End windows.
I0123 12:53:42.594775 139995469987840 transformer_layer.py:472] tlayer: final FFN.
I0123 12:53:42.594830 139995469987840 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.594954 139995469987840 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:53:42.595057 139995469987840 nn_components.py:325] mlp: activation = None
I0123 12:53:42.595094 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:53:42.596884 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.596975 139995469987840 nn_components.py:261] mlp: residual
I0123 12:53:42.599290 139995469987840 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.599368 139995469987840 transformer_base.py:443] tbase: final FFN
I0123 12:53:42.599472 139995469987840 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:53:42.601648 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:53:42.603416 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.603507 139995469987840 nn_components.py:261] mlp: residual
I0123 12:53:42.603786 139995469987840 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.603865 139995469987840 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 12:53:42.603970 139995469987840 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:53:42.604006 139995469987840 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:53:42.604033 139995469987840 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:53:42.604092 139995469987840 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.606245 139995469987840 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:53:42.611431 139995469987840 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.611679 139995469987840 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:53:42.614223 139995469987840 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:53:42.626218 139995469987840 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:53:42.626271 139995469987840 attention.py:418] Single window, no scan.
I0123 12:53:42.626305 139995469987840 transformer_layer.py:389] tlayer: self-attention.
I0123 12:53:42.626334 139995469987840 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.626393 139995469987840 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.626929 139995469987840 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.627003 139995469987840 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.627341 139995469987840 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.627991 139995469987840 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.630370 139995469987840 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.630963 139995469987840 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.631036 139995469987840 transformer_layer.py:468] tlayer: End windows.
I0123 12:53:42.631069 139995469987840 transformer_layer.py:472] tlayer: final FFN.
I0123 12:53:42.631124 139995469987840 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.631245 139995469987840 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:53:42.631347 139995469987840 nn_components.py:325] mlp: activation = None
I0123 12:53:42.631383 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:53:42.633172 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.633260 139995469987840 nn_components.py:261] mlp: residual
I0123 12:53:42.635566 139995469987840 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.635650 139995469987840 transformer_base.py:443] tbase: final FFN
I0123 12:53:42.635754 139995469987840 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:53:42.637926 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:53:42.639705 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.639794 139995469987840 nn_components.py:261] mlp: residual
I0123 12:53:42.640070 139995469987840 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.640147 139995469987840 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 12:53:42.640252 139995469987840 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:53:42.640289 139995469987840 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:53:42.640316 139995469987840 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:53:42.640375 139995469987840 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.642520 139995469987840 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:53:42.647704 139995469987840 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.647959 139995469987840 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:53:42.650507 139995469987840 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:53:42.662522 139995469987840 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:53:42.662576 139995469987840 attention.py:418] Single window, no scan.
I0123 12:53:42.662609 139995469987840 transformer_layer.py:389] tlayer: self-attention.
I0123 12:53:42.662637 139995469987840 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.662696 139995469987840 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.663234 139995469987840 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.663308 139995469987840 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.663650 139995469987840 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.664308 139995469987840 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.666724 139995469987840 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.667326 139995469987840 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.667401 139995469987840 transformer_layer.py:468] tlayer: End windows.
I0123 12:53:42.667433 139995469987840 transformer_layer.py:472] tlayer: final FFN.
I0123 12:53:42.667489 139995469987840 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.667612 139995469987840 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:53:42.667715 139995469987840 nn_components.py:325] mlp: activation = None
I0123 12:53:42.667751 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:53:42.669537 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.669627 139995469987840 nn_components.py:261] mlp: residual
I0123 12:53:42.671941 139995469987840 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.672024 139995469987840 transformer_base.py:443] tbase: final FFN
I0123 12:53:42.672129 139995469987840 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:53:42.674750 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:53:42.676543 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.676634 139995469987840 nn_components.py:261] mlp: residual
I0123 12:53:42.676915 139995469987840 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.676993 139995469987840 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 12:53:42.677098 139995469987840 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:53:42.677133 139995469987840 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:53:42.677161 139995469987840 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:53:42.677220 139995469987840 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.679363 139995469987840 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:53:42.684547 139995469987840 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.684796 139995469987840 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:53:42.687343 139995469987840 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:53:42.699415 139995469987840 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:53:42.699469 139995469987840 attention.py:418] Single window, no scan.
I0123 12:53:42.699503 139995469987840 transformer_layer.py:389] tlayer: self-attention.
I0123 12:53:42.699532 139995469987840 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.699591 139995469987840 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.700133 139995469987840 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.700206 139995469987840 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.700550 139995469987840 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.701214 139995469987840 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.703638 139995469987840 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.704235 139995469987840 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.704311 139995469987840 transformer_layer.py:468] tlayer: End windows.
I0123 12:53:42.704343 139995469987840 transformer_layer.py:472] tlayer: final FFN.
I0123 12:53:42.704398 139995469987840 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.704520 139995469987840 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:53:42.704622 139995469987840 nn_components.py:325] mlp: activation = None
I0123 12:53:42.704658 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:53:42.706452 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.706554 139995469987840 nn_components.py:261] mlp: residual
I0123 12:53:42.708868 139995469987840 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.708944 139995469987840 transformer_base.py:443] tbase: final FFN
I0123 12:53:42.709054 139995469987840 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:53:42.711247 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:53:42.713018 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.713110 139995469987840 nn_components.py:261] mlp: residual
I0123 12:53:42.713390 139995469987840 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.713469 139995469987840 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 12:53:42.713573 139995469987840 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:53:42.713609 139995469987840 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:53:42.713637 139995469987840 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:53:42.713706 139995469987840 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.715845 139995469987840 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:53:42.720962 139995469987840 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.721212 139995469987840 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:53:42.723763 139995469987840 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:53:42.735694 139995469987840 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:53:42.735748 139995469987840 attention.py:418] Single window, no scan.
I0123 12:53:42.735781 139995469987840 transformer_layer.py:389] tlayer: self-attention.
I0123 12:53:42.735809 139995469987840 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.735869 139995469987840 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.736405 139995469987840 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.736478 139995469987840 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.736816 139995469987840 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.737472 139995469987840 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.739861 139995469987840 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.740455 139995469987840 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.740530 139995469987840 transformer_layer.py:468] tlayer: End windows.
I0123 12:53:42.740562 139995469987840 transformer_layer.py:472] tlayer: final FFN.
I0123 12:53:42.740619 139995469987840 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.740742 139995469987840 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:53:42.740847 139995469987840 nn_components.py:325] mlp: activation = None
I0123 12:53:42.740884 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:53:42.742654 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.742744 139995469987840 nn_components.py:261] mlp: residual
I0123 12:53:42.745028 139995469987840 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.745104 139995469987840 transformer_base.py:443] tbase: final FFN
I0123 12:53:42.745207 139995469987840 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:53:42.747385 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:53:42.749158 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.749248 139995469987840 nn_components.py:261] mlp: residual
I0123 12:53:42.749528 139995469987840 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.749612 139995469987840 decoder_stack.py:344] dstack: Final layernorm.
I0123 12:53:42.752341 139995469987840 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 12:53:42.802447 139995469987840 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.802531 139995469987840 decoder_stack.py:333] dstack: autoregressive generator.
I0123 12:53:42.802583 139995469987840 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 12:53:42.802682 139995469987840 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:53:42.802718 139995469987840 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:53:42.802745 139995469987840 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:53:42.802804 139995469987840 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.805000 139995469987840 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:53:42.810269 139995469987840 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.810522 139995469987840 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:53:42.813014 139995469987840 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:53:42.825143 139995469987840 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:53:42.825198 139995469987840 attention.py:418] Single window, no scan.
I0123 12:53:42.825231 139995469987840 transformer_layer.py:389] tlayer: self-attention.
I0123 12:53:42.825260 139995469987840 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.825319 139995469987840 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.825869 139995469987840 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.825944 139995469987840 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.826290 139995469987840 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.826941 139995469987840 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.829263 139995469987840 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.829864 139995469987840 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.829939 139995469987840 transformer_layer.py:468] tlayer: End windows.
I0123 12:53:42.829971 139995469987840 transformer_layer.py:472] tlayer: final FFN.
I0123 12:53:42.830027 139995469987840 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.830150 139995469987840 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:53:42.830255 139995469987840 nn_components.py:325] mlp: activation = None
I0123 12:53:42.830291 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:53:42.832132 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.832227 139995469987840 nn_components.py:261] mlp: residual
I0123 12:53:42.834514 139995469987840 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.834591 139995469987840 transformer_base.py:443] tbase: final FFN
I0123 12:53:42.834696 139995469987840 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:53:42.836797 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:53:42.838579 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.838671 139995469987840 nn_components.py:261] mlp: residual
I0123 12:53:42.838947 139995469987840 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.839024 139995469987840 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 12:53:42.839126 139995469987840 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:53:42.839162 139995469987840 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:53:42.839190 139995469987840 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:53:42.839249 139995469987840 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.841388 139995469987840 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:53:42.846626 139995469987840 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.846880 139995469987840 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:53:42.849339 139995469987840 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:53:42.862289 139995469987840 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:53:42.862345 139995469987840 attention.py:418] Single window, no scan.
I0123 12:53:42.862379 139995469987840 transformer_layer.py:389] tlayer: self-attention.
I0123 12:53:42.862408 139995469987840 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.862479 139995469987840 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.863043 139995469987840 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.863120 139995469987840 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.863476 139995469987840 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.864177 139995469987840 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.866650 139995469987840 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.867265 139995469987840 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.867343 139995469987840 transformer_layer.py:468] tlayer: End windows.
I0123 12:53:42.867376 139995469987840 transformer_layer.py:472] tlayer: final FFN.
I0123 12:53:42.867434 139995469987840 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.867561 139995469987840 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:53:42.867671 139995469987840 nn_components.py:325] mlp: activation = None
I0123 12:53:42.867710 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:53:42.869623 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.869736 139995469987840 nn_components.py:261] mlp: residual
I0123 12:53:42.872131 139995469987840 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.872210 139995469987840 transformer_base.py:443] tbase: final FFN
I0123 12:53:42.872321 139995469987840 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:53:42.874514 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:53:42.876369 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.876464 139995469987840 nn_components.py:261] mlp: residual
I0123 12:53:42.876754 139995469987840 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.876836 139995469987840 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 12:53:42.876944 139995469987840 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:53:42.876983 139995469987840 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:53:42.877013 139995469987840 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:53:42.877075 139995469987840 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.879300 139995469987840 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:53:42.884854 139995469987840 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.885116 139995469987840 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:53:42.887742 139995469987840 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:53:42.900584 139995469987840 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:53:42.900640 139995469987840 attention.py:418] Single window, no scan.
I0123 12:53:42.900675 139995469987840 transformer_layer.py:389] tlayer: self-attention.
I0123 12:53:42.900707 139995469987840 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.900770 139995469987840 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.901333 139995469987840 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.901409 139995469987840 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.901777 139995469987840 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.902460 139995469987840 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.904906 139995469987840 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.905524 139995469987840 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.905602 139995469987840 transformer_layer.py:468] tlayer: End windows.
I0123 12:53:42.905636 139995469987840 transformer_layer.py:472] tlayer: final FFN.
I0123 12:53:42.905710 139995469987840 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.905844 139995469987840 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:53:42.905954 139995469987840 nn_components.py:325] mlp: activation = None
I0123 12:53:42.905992 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:53:42.907933 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.908026 139995469987840 nn_components.py:261] mlp: residual
I0123 12:53:42.910447 139995469987840 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.910527 139995469987840 transformer_base.py:443] tbase: final FFN
I0123 12:53:42.910636 139995469987840 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:53:42.912987 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:53:42.914855 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.914950 139995469987840 nn_components.py:261] mlp: residual
I0123 12:53:42.915242 139995469987840 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.915323 139995469987840 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 12:53:42.915432 139995469987840 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:53:42.915471 139995469987840 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:53:42.915501 139995469987840 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:53:42.915563 139995469987840 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.917811 139995469987840 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:53:42.923357 139995469987840 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.923614 139995469987840 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:53:42.926234 139995469987840 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:53:42.938848 139995469987840 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:53:42.938903 139995469987840 attention.py:418] Single window, no scan.
I0123 12:53:42.938938 139995469987840 transformer_layer.py:389] tlayer: self-attention.
I0123 12:53:42.938970 139995469987840 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.939031 139995469987840 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.939585 139995469987840 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.939661 139995469987840 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.940013 139995469987840 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.940697 139995469987840 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.943146 139995469987840 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.943950 139995469987840 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.944027 139995469987840 transformer_layer.py:468] tlayer: End windows.
I0123 12:53:42.944062 139995469987840 transformer_layer.py:472] tlayer: final FFN.
I0123 12:53:42.944121 139995469987840 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.944247 139995469987840 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:53:42.944355 139995469987840 nn_components.py:325] mlp: activation = None
I0123 12:53:42.944392 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:53:42.946516 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.946610 139995469987840 nn_components.py:261] mlp: residual
I0123 12:53:42.949034 139995469987840 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.949119 139995469987840 transformer_base.py:443] tbase: final FFN
I0123 12:53:42.949230 139995469987840 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:53:42.951457 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:53:42.953301 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.953395 139995469987840 nn_components.py:261] mlp: residual
I0123 12:53:42.953692 139995469987840 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.953778 139995469987840 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 12:53:42.953887 139995469987840 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:53:42.953926 139995469987840 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:53:42.953957 139995469987840 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:53:42.954020 139995469987840 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.956241 139995469987840 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:53:42.961737 139995469987840 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.962000 139995469987840 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:53:42.964559 139995469987840 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:53:42.977539 139995469987840 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:53:42.977595 139995469987840 attention.py:418] Single window, no scan.
I0123 12:53:42.977631 139995469987840 transformer_layer.py:389] tlayer: self-attention.
I0123 12:53:42.977676 139995469987840 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.977741 139995469987840 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.978302 139995469987840 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.978378 139995469987840 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.978730 139995469987840 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.979399 139995469987840 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.981810 139995469987840 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.982433 139995469987840 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.982511 139995469987840 transformer_layer.py:468] tlayer: End windows.
I0123 12:53:42.982547 139995469987840 transformer_layer.py:472] tlayer: final FFN.
I0123 12:53:42.982605 139995469987840 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.982733 139995469987840 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:53:42.982842 139995469987840 nn_components.py:325] mlp: activation = None
I0123 12:53:42.982881 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:53:42.984808 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.984901 139995469987840 nn_components.py:261] mlp: residual
I0123 12:53:42.987310 139995469987840 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.987395 139995469987840 transformer_base.py:443] tbase: final FFN
I0123 12:53:42.987507 139995469987840 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:53:42.989704 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:53:42.991546 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.991639 139995469987840 nn_components.py:261] mlp: residual
I0123 12:53:42.991928 139995469987840 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.992008 139995469987840 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 12:53:42.992116 139995469987840 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:53:42.992154 139995469987840 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:53:42.992185 139995469987840 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:53:42.992247 139995469987840 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:42.994474 139995469987840 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:53:42.999947 139995469987840 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.000204 139995469987840 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:53:43.002801 139995469987840 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:53:43.015215 139995469987840 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:53:43.015270 139995469987840 attention.py:418] Single window, no scan.
I0123 12:53:43.015305 139995469987840 transformer_layer.py:389] tlayer: self-attention.
I0123 12:53:43.015336 139995469987840 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.015399 139995469987840 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.015951 139995469987840 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.016028 139995469987840 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.016381 139995469987840 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.017062 139995469987840 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.019501 139995469987840 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.020121 139995469987840 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.020199 139995469987840 transformer_layer.py:468] tlayer: End windows.
I0123 12:53:43.020233 139995469987840 transformer_layer.py:472] tlayer: final FFN.
I0123 12:53:43.020291 139995469987840 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.020416 139995469987840 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:53:43.020524 139995469987840 nn_components.py:325] mlp: activation = None
I0123 12:53:43.020562 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:53:43.022501 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.022594 139995469987840 nn_components.py:261] mlp: residual
I0123 12:53:43.024954 139995469987840 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.025032 139995469987840 transformer_base.py:443] tbase: final FFN
I0123 12:53:43.025147 139995469987840 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:53:43.027351 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:53:43.029203 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.029297 139995469987840 nn_components.py:261] mlp: residual
I0123 12:53:43.029584 139995469987840 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.029675 139995469987840 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 12:53:43.029785 139995469987840 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:53:43.029825 139995469987840 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:53:43.029855 139995469987840 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:53:43.029918 139995469987840 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.032112 139995469987840 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:53:43.037538 139995469987840 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.037804 139995469987840 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:53:43.040358 139995469987840 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:53:43.052724 139995469987840 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:53:43.052780 139995469987840 attention.py:418] Single window, no scan.
I0123 12:53:43.052816 139995469987840 transformer_layer.py:389] tlayer: self-attention.
I0123 12:53:43.052848 139995469987840 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.052912 139995469987840 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.053468 139995469987840 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.053545 139995469987840 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.053910 139995469987840 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.054596 139995469987840 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.057006 139995469987840 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.057623 139995469987840 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.057728 139995469987840 transformer_layer.py:468] tlayer: End windows.
I0123 12:53:43.057764 139995469987840 transformer_layer.py:472] tlayer: final FFN.
I0123 12:53:43.057824 139995469987840 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.057954 139995469987840 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:53:43.058063 139995469987840 nn_components.py:325] mlp: activation = None
I0123 12:53:43.058102 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:53:43.060014 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.060107 139995469987840 nn_components.py:261] mlp: residual
I0123 12:53:43.062489 139995469987840 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.062568 139995469987840 transformer_base.py:443] tbase: final FFN
I0123 12:53:43.062676 139995469987840 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:53:43.064881 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:53:43.066756 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.066852 139995469987840 nn_components.py:261] mlp: residual
I0123 12:53:43.067140 139995469987840 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.067221 139995469987840 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 12:53:43.067328 139995469987840 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:53:43.067367 139995469987840 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:53:43.067398 139995469987840 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:53:43.067461 139995469987840 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.069694 139995469987840 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:53:43.075186 139995469987840 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.075446 139995469987840 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:53:43.078027 139995469987840 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:53:43.091177 139995469987840 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:53:43.091232 139995469987840 attention.py:418] Single window, no scan.
I0123 12:53:43.091268 139995469987840 transformer_layer.py:389] tlayer: self-attention.
I0123 12:53:43.091300 139995469987840 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.091363 139995469987840 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.091916 139995469987840 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.091993 139995469987840 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.092355 139995469987840 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.093047 139995469987840 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.095493 139995469987840 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.096109 139995469987840 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.096186 139995469987840 transformer_layer.py:468] tlayer: End windows.
I0123 12:53:43.096221 139995469987840 transformer_layer.py:472] tlayer: final FFN.
I0123 12:53:43.096279 139995469987840 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.096405 139995469987840 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:53:43.096513 139995469987840 nn_components.py:325] mlp: activation = None
I0123 12:53:43.096551 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:53:43.098496 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.098588 139995469987840 nn_components.py:261] mlp: residual
I0123 12:53:43.100984 139995469987840 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.101061 139995469987840 transformer_base.py:443] tbase: final FFN
I0123 12:53:43.101170 139995469987840 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:53:43.103379 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:53:43.105217 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.105310 139995469987840 nn_components.py:261] mlp: residual
I0123 12:53:43.105597 139995469987840 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.105688 139995469987840 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 12:53:43.105799 139995469987840 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:53:43.105839 139995469987840 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:53:43.105870 139995469987840 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:53:43.105932 139995469987840 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.108125 139995469987840 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:53:43.113609 139995469987840 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.113878 139995469987840 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:53:43.116435 139995469987840 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:53:43.128958 139995469987840 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:53:43.129015 139995469987840 attention.py:418] Single window, no scan.
I0123 12:53:43.129050 139995469987840 transformer_layer.py:389] tlayer: self-attention.
I0123 12:53:43.129081 139995469987840 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.129142 139995469987840 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.129708 139995469987840 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.129786 139995469987840 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.130145 139995469987840 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.130823 139995469987840 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.133273 139995469987840 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.133905 139995469987840 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.133986 139995469987840 transformer_layer.py:468] tlayer: End windows.
I0123 12:53:43.134029 139995469987840 transformer_layer.py:472] tlayer: final FFN.
I0123 12:53:43.134092 139995469987840 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.134224 139995469987840 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:53:43.134335 139995469987840 nn_components.py:325] mlp: activation = None
I0123 12:53:43.134373 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:53:43.136298 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.136389 139995469987840 nn_components.py:261] mlp: residual
I0123 12:53:43.138794 139995469987840 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.138873 139995469987840 transformer_base.py:443] tbase: final FFN
I0123 12:53:43.138980 139995469987840 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:53:43.141175 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:53:43.143051 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.143150 139995469987840 nn_components.py:261] mlp: residual
I0123 12:53:43.143443 139995469987840 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.143523 139995469987840 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 12:53:43.143632 139995469987840 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:53:43.143671 139995469987840 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:53:43.143701 139995469987840 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:53:43.143764 139995469987840 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.145997 139995469987840 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:53:43.151477 139995469987840 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.151738 139995469987840 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:53:43.154348 139995469987840 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:53:43.166876 139995469987840 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:53:43.166931 139995469987840 attention.py:418] Single window, no scan.
I0123 12:53:43.166966 139995469987840 transformer_layer.py:389] tlayer: self-attention.
I0123 12:53:43.166996 139995469987840 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.167057 139995469987840 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.167610 139995469987840 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.167685 139995469987840 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.168034 139995469987840 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.168715 139995469987840 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.171184 139995469987840 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.171797 139995469987840 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.171873 139995469987840 transformer_layer.py:468] tlayer: End windows.
I0123 12:53:43.171908 139995469987840 transformer_layer.py:472] tlayer: final FFN.
I0123 12:53:43.171967 139995469987840 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.172093 139995469987840 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:53:43.172201 139995469987840 nn_components.py:325] mlp: activation = None
I0123 12:53:43.172240 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:53:43.174182 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.174273 139995469987840 nn_components.py:261] mlp: residual
I0123 12:53:43.176656 139995469987840 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.176733 139995469987840 transformer_base.py:443] tbase: final FFN
I0123 12:53:43.176840 139995469987840 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:53:43.179037 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:53:43.180884 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.180984 139995469987840 nn_components.py:261] mlp: residual
I0123 12:53:43.181277 139995469987840 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.181358 139995469987840 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 12:53:43.181467 139995469987840 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:53:43.181506 139995469987840 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:53:43.181537 139995469987840 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:53:43.181600 139995469987840 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.183823 139995469987840 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:53:43.189290 139995469987840 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.189551 139995469987840 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:53:43.192135 139995469987840 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:53:43.204636 139995469987840 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:53:43.204691 139995469987840 attention.py:418] Single window, no scan.
I0123 12:53:43.204724 139995469987840 transformer_layer.py:389] tlayer: self-attention.
I0123 12:53:43.204753 139995469987840 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.204812 139995469987840 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.205349 139995469987840 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.205422 139995469987840 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.205771 139995469987840 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.206434 139995469987840 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.208783 139995469987840 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.209376 139995469987840 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.209448 139995469987840 transformer_layer.py:468] tlayer: End windows.
I0123 12:53:43.209481 139995469987840 transformer_layer.py:472] tlayer: final FFN.
I0123 12:53:43.209535 139995469987840 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.209664 139995469987840 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:53:43.209769 139995469987840 nn_components.py:325] mlp: activation = None
I0123 12:53:43.209806 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:53:43.211641 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.211728 139995469987840 nn_components.py:261] mlp: residual
I0123 12:53:43.214053 139995469987840 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.214135 139995469987840 transformer_base.py:443] tbase: final FFN
I0123 12:53:43.214245 139995469987840 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:53:43.216349 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:53:43.218127 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.218218 139995469987840 nn_components.py:261] mlp: residual
I0123 12:53:43.218502 139995469987840 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.218581 139995469987840 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 12:53:43.218685 139995469987840 transformer_layer.py:154] tlayer: recurrent = False
I0123 12:53:43.218723 139995469987840 transformer_layer.py:155] tlayer: compute_importance = False
I0123 12:53:43.218751 139995469987840 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 12:53:43.218810 139995469987840 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.220920 139995469987840 transformer_base.py:161] kvq: pre_attn dropout.
I0123 12:53:43.226187 139995469987840 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.226435 139995469987840 transformer_base.py:194] kvq: normalize keys, queries.
I0123 12:53:43.228908 139995469987840 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 12:53:43.240896 139995469987840 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 12:53:43.240949 139995469987840 attention.py:418] Single window, no scan.
I0123 12:53:43.240981 139995469987840 transformer_layer.py:389] tlayer: self-attention.
I0123 12:53:43.241009 139995469987840 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.241066 139995469987840 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.241608 139995469987840 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.241688 139995469987840 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.242029 139995469987840 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.242679 139995469987840 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.244990 139995469987840 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.245586 139995469987840 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.245666 139995469987840 transformer_layer.py:468] tlayer: End windows.
I0123 12:53:43.245699 139995469987840 transformer_layer.py:472] tlayer: final FFN.
I0123 12:53:43.245755 139995469987840 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.245880 139995469987840 transformer_base.py:410] tbase: post-attention MLP.
I0123 12:53:43.245986 139995469987840 nn_components.py:325] mlp: activation = None
I0123 12:53:43.246022 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:53:43.247863 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.247951 139995469987840 nn_components.py:261] mlp: residual
I0123 12:53:43.250238 139995469987840 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.250314 139995469987840 transformer_base.py:443] tbase: final FFN
I0123 12:53:43.250418 139995469987840 nn_components.py:320] mlp: hidden 4096, relu
I0123 12:53:43.252513 139995469987840 nn_components.py:329] mlp: final activation = None
I0123 12:53:43.254288 139995469987840 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.254380 139995469987840 nn_components.py:261] mlp: residual
I0123 12:53:43.254669 139995469987840 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:43.254751 139995469987840 decoder_stack.py:344] dstack: Final layernorm.
I0123 12:53:43.257469 139995469987840 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 12:53:57.454862 139995469987840 alphageometry.py:566] LM output (score=-2.012646): "n : C e h n 22 D e n h n 23 ;"
I0123 12:53:57.455157 139995469987840 alphageometry.py:567] Translation: "n = on_line n e h, on_bline n h e"

I0123 12:53:57.455208 139995469987840 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = lc_tangent h g c, on_line h a c; i = lc_tangent i g c, on_line i b c; j = on_circle j d c, on_bline j c b; k = on_circle k d a, on_bline k a c; l = on_line l j i, on_line l k h; m = on_line m c l, on_line m i h; n = on_line n e h, on_bline n h e ? eqangle m b m i m h m a"
I0123 12:53:57.455464 139995469987840 graph.py:498] 
I0123 12:53:57.455527 139995469987840 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = lc_tangent h g c, on_line h a c; i = lc_tangent i g c, on_line i b c; j = on_circle j d c, on_bline j c b; k = on_circle k d a, on_bline k a c; l = on_line l j i, on_line l k h; m = on_line m c l, on_line m i h; n = on_line n e h, on_bline n h e ? eqangle m b m i m h m a
I0123 12:54:00.235439 139995469987840 ddar.py:60] Depth 1/1000 time = 2.67694354057312
I0123 12:54:04.202259 139995469987840 ddar.py:60] Depth 2/1000 time = 3.9665937423706055
I0123 12:54:08.586118 139995469987840 ddar.py:60] Depth 3/1000 time = 4.38361668586731
I0123 12:54:12.752560 139995469987840 ddar.py:60] Depth 4/1000 time = 4.1661906242370605
I0123 12:54:17.152240 139995469987840 ddar.py:60] Depth 5/1000 time = 4.399194240570068
I0123 12:54:22.356260 139995469987840 ddar.py:60] Depth 6/1000 time = 5.2017340660095215
I0123 12:54:29.469028 139995469987840 ddar.py:60] Depth 7/1000 time = 7.112541913986206
I0123 12:54:36.740101 139995469987840 ddar.py:60] Depth 8/1000 time = 7.2708306312561035
I0123 12:54:49.374732 139995469987840 ddar.py:60] Depth 9/1000 time = 12.634365797042847
I0123 12:55:00.853243 139995469987840 ddar.py:60] Depth 10/1000 time = 11.478148460388184
I0123 12:55:17.676432 139995469987840 ddar.py:60] Depth 11/1000 time = 16.82276940345764
I0123 12:55:34.681682 139995469987840 ddar.py:60] Depth 12/1000 time = 17.004812479019165
I0123 12:55:52.061420 139995469987840 ddar.py:60] Depth 13/1000 time = 17.379358530044556
I0123 12:56:09.789195 139995469987840 ddar.py:60] Depth 14/1000 time = 17.684807538986206
I0123 12:56:27.718870 139995469987840 ddar.py:60] Depth 15/1000 time = 17.90703845024109
I0123 12:56:45.548606 139995469987840 ddar.py:60] Depth 16/1000 time = 17.70345973968506
I0123 12:57:03.662039 139995469987840 ddar.py:60] Depth 17/1000 time = 18.10049057006836
I0123 12:57:03.662652 139995469987840 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:57:03.662860 139995469987840 alphageometry.py:566] LM output (score=-2.038247): "n : T f h f n 22 ;"
I0123 12:57:03.662901 139995469987840 alphageometry.py:567] Translation: "n = on_tline n f f h"

I0123 12:57:03.662961 139995469987840 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = lc_tangent h g c, on_line h a c; i = lc_tangent i g c, on_line i b c; j = on_circle j d c, on_bline j c b; k = on_circle k d a, on_bline k a c; l = on_line l j i, on_line l k h; m = on_line m c l, on_line m i h; n = on_tline n f f h ? eqangle m b m i m h m a"
I0123 12:57:03.663187 139995469987840 graph.py:498] 
I0123 12:57:03.663253 139995469987840 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = lc_tangent h g c, on_line h a c; i = lc_tangent i g c, on_line i b c; j = on_circle j d c, on_bline j c b; k = on_circle k d a, on_bline k a c; l = on_line l j i, on_line l k h; m = on_line m c l, on_line m i h; n = on_tline n f f h ? eqangle m b m i m h m a
I0123 12:57:06.168441 139995469987840 ddar.py:60] Depth 1/1000 time = 2.4518532752990723
I0123 12:57:09.481709 139995469987840 ddar.py:60] Depth 2/1000 time = 3.313047170639038
I0123 12:57:13.661379 139995469987840 ddar.py:60] Depth 3/1000 time = 4.179460763931274
I0123 12:57:18.642721 139995469987840 ddar.py:60] Depth 4/1000 time = 4.9766435623168945
I0123 12:57:25.402173 139995469987840 ddar.py:60] Depth 5/1000 time = 6.759192705154419
I0123 12:57:32.074965 139995469987840 ddar.py:60] Depth 6/1000 time = 6.672501087188721
I0123 12:57:44.584315 139995469987840 ddar.py:60] Depth 7/1000 time = 12.509057521820068
I0123 12:57:55.456654 139995469987840 ddar.py:60] Depth 8/1000 time = 10.871949434280396
I0123 12:58:12.076955 139995469987840 ddar.py:60] Depth 9/1000 time = 16.619912147521973
I0123 12:58:28.767880 139995469987840 ddar.py:60] Depth 10/1000 time = 16.69033932685852
I0123 12:58:46.121023 139995469987840 ddar.py:60] Depth 11/1000 time = 17.352686882019043
I0123 12:59:03.378437 139995469987840 ddar.py:60] Depth 12/1000 time = 17.195422649383545
I0123 12:59:20.880073 139995469987840 ddar.py:60] Depth 13/1000 time = 17.379568099975586
I0123 12:59:38.677895 139995469987840 ddar.py:60] Depth 14/1000 time = 17.784742832183838
I0123 12:59:38.678475 139995469987840 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 12:59:38.678634 139995469987840 alphageometry.py:566] LM output (score=-2.062001): "n : T d j j n 22 ;"
I0123 12:59:38.678675 139995469987840 alphageometry.py:567] Translation: "n = on_tline n j d j"

I0123 12:59:38.678722 139995469987840 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = lc_tangent h g c, on_line h a c; i = lc_tangent i g c, on_line i b c; j = on_circle j d c, on_bline j c b; k = on_circle k d a, on_bline k a c; l = on_line l j i, on_line l k h; m = on_line m c l, on_line m i h; n = on_tline n j d j ? eqangle m b m i m h m a"
I0123 12:59:38.678932 139995469987840 graph.py:498] 
I0123 12:59:38.678997 139995469987840 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = lc_tangent h g c, on_line h a c; i = lc_tangent i g c, on_line i b c; j = on_circle j d c, on_bline j c b; k = on_circle k d a, on_bline k a c; l = on_line l j i, on_line l k h; m = on_line m c l, on_line m i h; n = on_tline n j d j ? eqangle m b m i m h m a
I0123 12:59:41.344121 139995469987840 ddar.py:60] Depth 1/1000 time = 2.6232104301452637
I0123 12:59:45.392346 139995469987840 ddar.py:60] Depth 2/1000 time = 4.048004627227783
I0123 12:59:49.472499 139995469987840 ddar.py:60] Depth 3/1000 time = 4.0799291133880615
I0123 12:59:54.695378 139995469987840 ddar.py:60] Depth 4/1000 time = 5.220054388046265
I0123 13:00:01.940192 139995469987840 ddar.py:60] Depth 5/1000 time = 7.244547128677368
I0123 13:00:08.954523 139995469987840 ddar.py:60] Depth 6/1000 time = 7.014076471328735
I0123 13:00:22.497362 139995469987840 ddar.py:60] Depth 7/1000 time = 13.542509317398071
I0123 13:00:34.789462 139995469987840 ddar.py:60] Depth 8/1000 time = 12.29165005683899
I0123 13:00:52.030160 139995469987840 ddar.py:60] Depth 9/1000 time = 17.240224361419678
I0123 13:01:09.068625 139995469987840 ddar.py:60] Depth 10/1000 time = 17.038076639175415
I0123 13:01:26.862565 139995469987840 ddar.py:60] Depth 11/1000 time = 17.793561220169067
I0123 13:01:45.128189 139995469987840 ddar.py:60] Depth 12/1000 time = 18.202826261520386
I0123 13:02:03.253247 139995469987840 ddar.py:60] Depth 13/1000 time = 17.988664865493774
I0123 13:02:21.106537 139995469987840 ddar.py:60] Depth 14/1000 time = 17.83970856666565
I0123 13:02:21.107351 139995469987840 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:02:21.107514 139995469987840 alphageometry.py:566] LM output (score=-2.181114): "n : C e f n 22 D e n f n 23 ;"
I0123 13:02:21.107553 139995469987840 alphageometry.py:567] Translation: "n = on_line n e f, on_bline n f e"

I0123 13:02:21.107611 139995469987840 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = lc_tangent h g c, on_line h a c; i = lc_tangent i g c, on_line i b c; j = on_circle j d c, on_bline j c b; k = on_circle k d a, on_bline k a c; l = on_line l j i, on_line l k h; m = on_line m c l, on_line m i h; n = on_line n e f, on_bline n f e ? eqangle m b m i m h m a"
I0123 13:02:21.107843 139995469987840 graph.py:498] 
I0123 13:02:21.107906 139995469987840 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = lc_tangent h g c, on_line h a c; i = lc_tangent i g c, on_line i b c; j = on_circle j d c, on_bline j c b; k = on_circle k d a, on_bline k a c; l = on_line l j i, on_line l k h; m = on_line m c l, on_line m i h; n = on_line n e f, on_bline n f e ? eqangle m b m i m h m a
I0123 13:02:24.030631 139995469987840 ddar.py:60] Depth 1/1000 time = 2.8620691299438477
I0123 13:02:27.466911 139995469987840 ddar.py:60] Depth 2/1000 time = 3.4360523223876953
I0123 13:02:31.666965 139995469987840 ddar.py:60] Depth 3/1000 time = 4.199822902679443
I0123 13:02:35.901160 139995469987840 ddar.py:60] Depth 4/1000 time = 4.233686685562134
I0123 13:02:41.178949 139995469987840 ddar.py:60] Depth 5/1000 time = 5.2751994132995605
I0123 13:02:48.074244 139995469987840 ddar.py:60] Depth 6/1000 time = 6.8949294090271
I0123 13:02:55.308486 139995469987840 ddar.py:60] Depth 7/1000 time = 7.233991861343384
I0123 13:03:08.481683 139995469987840 ddar.py:60] Depth 8/1000 time = 13.172818422317505
I0123 13:03:20.101601 139995469987840 ddar.py:60] Depth 9/1000 time = 11.619372844696045
I0123 13:03:37.188915 139995469987840 ddar.py:60] Depth 10/1000 time = 17.086888313293457
I0123 13:03:54.186770 139995469987840 ddar.py:60] Depth 11/1000 time = 16.997433185577393
I0123 13:04:12.240163 139995469987840 ddar.py:60] Depth 12/1000 time = 18.052937269210815
I0123 13:04:30.132449 139995469987840 ddar.py:60] Depth 13/1000 time = 17.85127830505371
I0123 13:04:47.983684 139995469987840 ddar.py:60] Depth 14/1000 time = 17.830427169799805
I0123 13:05:06.299132 139995469987840 ddar.py:60] Depth 15/1000 time = 18.182308673858643
I0123 13:05:24.462836 139995469987840 ddar.py:60] Depth 16/1000 time = 18.150423526763916
I0123 13:05:24.463521 139995469987840 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:05:24.463649 139995469987840 alphageometry.py:566] LM output (score=-2.241732): "n : C e j n 22 D e n j n 23 ;"
I0123 13:05:24.463687 139995469987840 alphageometry.py:567] Translation: "n = on_line n e j, on_bline n j e"

I0123 13:05:24.463731 139995469987840 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = lc_tangent h g c, on_line h a c; i = lc_tangent i g c, on_line i b c; j = on_circle j d c, on_bline j c b; k = on_circle k d a, on_bline k a c; l = on_line l j i, on_line l k h; m = on_line m c l, on_line m i h; n = on_line n e j, on_bline n j e ? eqangle m b m i m h m a"
I0123 13:05:24.463944 139995469987840 graph.py:498] 
I0123 13:05:24.464010 139995469987840 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = lc_tangent h g c, on_line h a c; i = lc_tangent i g c, on_line i b c; j = on_circle j d c, on_bline j c b; k = on_circle k d a, on_bline k a c; l = on_line l j i, on_line l k h; m = on_line m c l, on_line m i h; n = on_line n e j, on_bline n j e ? eqangle m b m i m h m a
I0123 13:05:27.132350 139995469987840 ddar.py:60] Depth 1/1000 time = 2.6204006671905518
I0123 13:05:31.037855 139995469987840 ddar.py:60] Depth 2/1000 time = 3.905276298522949
I0123 13:05:35.155185 139995469987840 ddar.py:60] Depth 3/1000 time = 4.117070913314819
I0123 13:05:39.191134 139995469987840 ddar.py:60] Depth 4/1000 time = 4.035446405410767
I0123 13:05:44.538365 139995469987840 ddar.py:60] Depth 5/1000 time = 5.344618320465088
I0123 13:05:51.510028 139995469987840 ddar.py:60] Depth 6/1000 time = 6.971417188644409
I0123 13:05:58.568305 139995469987840 ddar.py:60] Depth 7/1000 time = 7.058014631271362
I0123 13:06:10.698351 139995469987840 ddar.py:60] Depth 8/1000 time = 12.129720211029053
I0123 13:06:21.999338 139995469987840 ddar.py:60] Depth 9/1000 time = 11.300556182861328
I0123 13:06:39.495697 139995469987840 ddar.py:60] Depth 10/1000 time = 17.495834350585938
I0123 13:06:56.558532 139995469987840 ddar.py:60] Depth 11/1000 time = 17.06227397918701
I0123 13:07:14.398741 139995469987840 ddar.py:60] Depth 12/1000 time = 17.839784860610962
I0123 13:07:32.578606 139995469987840 ddar.py:60] Depth 13/1000 time = 18.138384103775024
I0123 13:07:50.830178 139995469987840 ddar.py:60] Depth 14/1000 time = 18.23085641860962
I0123 13:08:09.244082 139995469987840 ddar.py:60] Depth 15/1000 time = 18.288009881973267
I0123 13:08:27.719452 139995469987840 ddar.py:60] Depth 16/1000 time = 18.461930513381958
I0123 13:08:27.720158 139995469987840 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:08:27.720323 139995469987840 alphageometry.py:566] LM output (score=-2.315029): "n : T a b a n 22 ;"
I0123 13:08:27.720365 139995469987840 alphageometry.py:567] Translation: "n = on_tline n a a b"

I0123 13:08:27.720407 139995469987840 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = lc_tangent h g c, on_line h a c; i = lc_tangent i g c, on_line i b c; j = on_circle j d c, on_bline j c b; k = on_circle k d a, on_bline k a c; l = on_line l j i, on_line l k h; m = on_line m c l, on_line m i h; n = on_tline n a a b ? eqangle m b m i m h m a"
I0123 13:08:27.720617 139995469987840 graph.py:498] 
I0123 13:08:27.720686 139995469987840 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = lc_tangent h g c, on_line h a c; i = lc_tangent i g c, on_line i b c; j = on_circle j d c, on_bline j c b; k = on_circle k d a, on_bline k a c; l = on_line l j i, on_line l k h; m = on_line m c l, on_line m i h; n = on_tline n a a b ? eqangle m b m i m h m a
I0123 13:08:30.319800 139995469987840 ddar.py:60] Depth 1/1000 time = 2.56547474861145
I0123 13:08:33.803445 139995469987840 ddar.py:60] Depth 2/1000 time = 3.4834251403808594
I0123 13:08:37.883355 139995469987840 ddar.py:60] Depth 3/1000 time = 4.079678058624268
I0123 13:08:42.108800 139995469987840 ddar.py:60] Depth 4/1000 time = 4.225175380706787
I0123 13:08:47.881160 139995469987840 ddar.py:60] Depth 5/1000 time = 5.7696545124053955
I0123 13:08:54.883967 139995469987840 ddar.py:60] Depth 6/1000 time = 7.002558708190918
I0123 13:09:02.167144 139995469987840 ddar.py:60] Depth 7/1000 time = 7.282934188842773
I0123 13:09:16.340779 139995469987840 ddar.py:60] Depth 8/1000 time = 14.173288345336914
I0123 13:09:28.104558 139995469987840 ddar.py:60] Depth 9/1000 time = 11.763294458389282
I0123 13:09:45.629811 139995469987840 ddar.py:60] Depth 10/1000 time = 17.524729013442993
I0123 13:10:03.132239 139995469987840 ddar.py:60] Depth 11/1000 time = 17.501988172531128
I0123 13:10:21.415062 139995469987840 ddar.py:60] Depth 12/1000 time = 18.282355070114136
I0123 13:10:39.888635 139995469987840 ddar.py:60] Depth 13/1000 time = 18.422568798065186
I0123 13:10:58.172129 139995469987840 ddar.py:60] Depth 14/1000 time = 18.25898241996765
I0123 13:11:16.717534 139995469987840 ddar.py:60] Depth 15/1000 time = 18.40947985649109
I0123 13:11:35.674179 139995469987840 ddar.py:60] Depth 16/1000 time = 18.94196629524231
I0123 13:11:35.674988 139995469987840 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:11:35.675153 139995469987840 alphageometry.py:566] LM output (score=-2.341157): "n : C e i n 22 D e n i n 23 ;"
I0123 13:11:35.675192 139995469987840 alphageometry.py:567] Translation: "n = on_line n e i, on_bline n i e"

I0123 13:11:35.675255 139995469987840 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = lc_tangent h g c, on_line h a c; i = lc_tangent i g c, on_line i b c; j = on_circle j d c, on_bline j c b; k = on_circle k d a, on_bline k a c; l = on_line l j i, on_line l k h; m = on_line m c l, on_line m i h; n = on_line n e i, on_bline n i e ? eqangle m b m i m h m a"
I0123 13:11:35.675494 139995469987840 graph.py:498] 
I0123 13:11:35.675560 139995469987840 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = lc_tangent h g c, on_line h a c; i = lc_tangent i g c, on_line i b c; j = on_circle j d c, on_bline j c b; k = on_circle k d a, on_bline k a c; l = on_line l j i, on_line l k h; m = on_line m c l, on_line m i h; n = on_line n e i, on_bline n i e ? eqangle m b m i m h m a
I0123 13:11:38.536313 139995469987840 ddar.py:60] Depth 1/1000 time = 2.782299757003784
I0123 13:11:42.376906 139995469987840 ddar.py:60] Depth 2/1000 time = 3.8403687477111816
I0123 13:11:46.786741 139995469987840 ddar.py:60] Depth 3/1000 time = 4.409616231918335
I0123 13:11:51.537542 139995469987840 ddar.py:60] Depth 4/1000 time = 4.750559329986572
I0123 13:11:55.988836 139995469987840 ddar.py:60] Depth 5/1000 time = 4.450803279876709
I0123 13:12:01.485155 139995469987840 ddar.py:60] Depth 6/1000 time = 5.493894338607788
I0123 13:12:08.984745 139995469987840 ddar.py:60] Depth 7/1000 time = 7.499329328536987
I0123 13:12:16.516423 139995469987840 ddar.py:60] Depth 8/1000 time = 7.531403064727783
I0123 13:12:28.528551 139995469987840 ddar.py:60] Depth 9/1000 time = 12.011858224868774
I0123 13:12:40.244263 139995469987840 ddar.py:60] Depth 10/1000 time = 11.715306758880615
I0123 13:12:58.292122 139995469987840 ddar.py:60] Depth 11/1000 time = 18.047312021255493
I0123 13:13:15.936419 139995469987840 ddar.py:60] Depth 12/1000 time = 17.643861055374146
I0123 13:13:33.924867 139995469987840 ddar.py:60] Depth 13/1000 time = 17.98799991607666
I0123 13:13:51.942709 139995469987840 ddar.py:60] Depth 14/1000 time = 17.975529670715332
I0123 13:14:09.569400 139995469987840 ddar.py:60] Depth 15/1000 time = 17.603950262069702
I0123 13:14:27.743370 139995469987840 ddar.py:60] Depth 16/1000 time = 18.043460607528687
I0123 13:14:46.124365 139995469987840 ddar.py:60] Depth 17/1000 time = 18.367916107177734
I0123 13:14:46.125088 139995469987840 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:14:46.125262 139995469987840 alphageometry.py:566] LM output (score=-2.400265): "n : C d j n 22 D d j d n 23 ;"
I0123 13:14:46.125300 139995469987840 alphageometry.py:567] Translation: "n = on_line n d j, on_circle n d j"

I0123 13:14:46.125361 139995469987840 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = lc_tangent h g c, on_line h a c; i = lc_tangent i g c, on_line i b c; j = on_circle j d c, on_bline j c b; k = on_circle k d a, on_bline k a c; l = on_line l j i, on_line l k h; m = on_line m c l, on_line m i h; n = on_line n d j, on_circle n d j ? eqangle m b m i m h m a"
I0123 13:14:46.125601 139995469987840 graph.py:498] 
I0123 13:14:46.125697 139995469987840 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = lc_tangent h g c, on_line h a c; i = lc_tangent i g c, on_line i b c; j = on_circle j d c, on_bline j c b; k = on_circle k d a, on_bline k a c; l = on_line l j i, on_line l k h; m = on_line m c l, on_line m i h; n = on_line n d j, on_circle n d j ? eqangle m b m i m h m a
I0123 13:14:49.360449 139995469987840 ddar.py:60] Depth 1/1000 time = 3.0896034240722656
I0123 13:14:58.116076 139995469987840 ddar.py:60] Depth 2/1000 time = 8.755361080169678
I0123 13:15:06.475683 139995469987840 ddar.py:60] Depth 3/1000 time = 8.359357118606567
I0123 13:15:17.792621 139995469987840 ddar.py:60] Depth 4/1000 time = 11.31663179397583
I0123 13:15:28.572399 139995469987840 ddar.py:60] Depth 5/1000 time = 10.779407978057861
I0123 13:15:39.784459 139995469987840 ddar.py:60] Depth 6/1000 time = 11.21165156364441
I0123 13:15:51.169577 139995469987840 ddar.py:60] Depth 7/1000 time = 11.384041786193848
I0123 13:16:03.475588 139995469987840 ddar.py:60] Depth 8/1000 time = 12.30477523803711
I0123 13:16:17.432124 139995469987840 ddar.py:60] Depth 9/1000 time = 13.95603895187378
I0123 13:16:31.812294 139995469987840 ddar.py:60] Depth 10/1000 time = 14.37959337234497
I0123 13:16:51.244819 139995469987840 ddar.py:60] Depth 11/1000 time = 19.43206763267517
I0123 13:17:10.784130 139995469987840 ddar.py:60] Depth 12/1000 time = 19.538862466812134
I0123 13:17:39.737936 139995469987840 ddar.py:60] Depth 13/1000 time = 28.95328974723816
I0123 13:18:12.180227 139995469987840 ddar.py:60] Depth 14/1000 time = 32.44167184829712
I0123 13:18:48.062022 139995469987840 ddar.py:60] Depth 15/1000 time = 35.881266593933105
I0123 13:19:21.718921 139995469987840 ddar.py:60] Depth 16/1000 time = 33.656296491622925
I0123 13:19:56.145759 139995469987840 ddar.py:60] Depth 17/1000 time = 34.29896521568298
I0123 13:20:30.499799 139995469987840 ddar.py:60] Depth 18/1000 time = 34.10479402542114
I0123 13:21:05.038646 139995469987840 ddar.py:60] Depth 19/1000 time = 34.505911111831665
I0123 13:21:05.039306 139995469987840 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:21:05.039439 139995469987840 alphageometry.py:566] LM output (score=-2.414559): "n : C e i n 22 D e i e n 23 ;"
I0123 13:21:05.039478 139995469987840 alphageometry.py:567] Translation: "n = on_line n e i, on_circle n e i"

I0123 13:21:05.039529 139995469987840 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = lc_tangent h g c, on_line h a c; i = lc_tangent i g c, on_line i b c; j = on_circle j d c, on_bline j c b; k = on_circle k d a, on_bline k a c; l = on_line l j i, on_line l k h; m = on_line m c l, on_line m i h; n = on_line n e i, on_circle n e i ? eqangle m b m i m h m a"
I0123 13:21:05.039740 139995469987840 graph.py:498] 
I0123 13:21:05.039806 139995469987840 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = lc_tangent h g c, on_line h a c; i = lc_tangent i g c, on_line i b c; j = on_circle j d c, on_bline j c b; k = on_circle k d a, on_bline k a c; l = on_line l j i, on_line l k h; m = on_line m c l, on_line m i h; n = on_line n e i, on_circle n e i ? eqangle m b m i m h m a
I0123 13:21:07.883061 139995469987840 ddar.py:60] Depth 1/1000 time = 2.7986433506011963
I0123 13:21:12.059780 139995469987840 ddar.py:60] Depth 2/1000 time = 4.1764678955078125
I0123 13:21:16.578125 139995469987840 ddar.py:60] Depth 3/1000 time = 4.518071174621582
I0123 13:21:21.108822 139995469987840 ddar.py:60] Depth 4/1000 time = 4.530450105667114
I0123 13:21:26.043711 139995469987840 ddar.py:60] Depth 5/1000 time = 4.934361219406128
I0123 13:21:31.668474 139995469987840 ddar.py:60] Depth 6/1000 time = 5.622138023376465
I0123 13:21:39.183972 139995469987840 ddar.py:60] Depth 7/1000 time = 7.515107154846191
I0123 13:21:46.544200 139995469987840 ddar.py:60] Depth 8/1000 time = 7.359976530075073
I0123 13:21:58.558494 139995469987840 ddar.py:60] Depth 9/1000 time = 12.01395845413208
I0123 13:22:10.058637 139995469987840 ddar.py:60] Depth 10/1000 time = 11.499737024307251
I0123 13:22:28.463365 139995469987840 ddar.py:60] Depth 11/1000 time = 18.40436315536499
I0123 13:22:46.293313 139995469987840 ddar.py:60] Depth 12/1000 time = 17.829490661621094
I0123 13:23:04.383088 139995469987840 ddar.py:60] Depth 13/1000 time = 18.08931016921997
I0123 13:23:22.633534 139995469987840 ddar.py:60] Depth 14/1000 time = 18.20898985862732
I0123 13:23:41.526151 139995469987840 ddar.py:60] Depth 15/1000 time = 18.867344617843628
I0123 13:23:59.869024 139995469987840 ddar.py:60] Depth 16/1000 time = 18.201364040374756
I0123 13:24:18.356096 139995469987840 ddar.py:60] Depth 17/1000 time = 18.473524570465088
I0123 13:24:18.356484 139995469987840 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:24:18.356632 139995469987840 alphageometry.py:566] LM output (score=-2.427884): "n : T b c c n 22 ;"
I0123 13:24:18.356673 139995469987840 alphageometry.py:567] Translation: "n = on_tline n c b c"

I0123 13:24:18.356718 139995469987840 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = lc_tangent h g c, on_line h a c; i = lc_tangent i g c, on_line i b c; j = on_circle j d c, on_bline j c b; k = on_circle k d a, on_bline k a c; l = on_line l j i, on_line l k h; m = on_line m c l, on_line m i h; n = on_tline n c b c ? eqangle m b m i m h m a"
I0123 13:24:18.356932 139995469987840 graph.py:498] 
I0123 13:24:18.356998 139995469987840 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = lc_tangent h g c, on_line h a c; i = lc_tangent i g c, on_line i b c; j = on_circle j d c, on_bline j c b; k = on_circle k d a, on_bline k a c; l = on_line l j i, on_line l k h; m = on_line m c l, on_line m i h; n = on_tline n c b c ? eqangle m b m i m h m a
I0123 13:24:21.146764 139995469987840 ddar.py:60] Depth 1/1000 time = 2.7523584365844727
I0123 13:24:25.344847 139995469987840 ddar.py:60] Depth 2/1000 time = 4.197734117507935
I0123 13:24:29.600234 139995469987840 ddar.py:60] Depth 3/1000 time = 4.255131006240845
I0123 13:24:34.831139 139995469987840 ddar.py:60] Depth 4/1000 time = 5.228373050689697
I0123 13:24:41.695387 139995469987840 ddar.py:60] Depth 5/1000 time = 6.863962650299072
I0123 13:24:48.878140 139995469987840 ddar.py:60] Depth 6/1000 time = 7.1823930740356445
I0123 13:25:01.239663 139995469987840 ddar.py:60] Depth 7/1000 time = 12.361222267150879
I0123 13:25:12.843581 139995469987840 ddar.py:60] Depth 8/1000 time = 11.603497743606567
I0123 13:25:30.568698 139995469987840 ddar.py:60] Depth 9/1000 time = 17.72467350959778
I0123 13:25:48.211327 139995469987840 ddar.py:60] Depth 10/1000 time = 17.642091989517212
I0123 13:26:06.040271 139995469987840 ddar.py:60] Depth 11/1000 time = 17.828324794769287
I0123 13:26:24.505777 139995469987840 ddar.py:60] Depth 12/1000 time = 18.40254783630371
I0123 13:26:42.671897 139995469987840 ddar.py:60] Depth 13/1000 time = 18.045352458953857
I0123 13:27:00.906064 139995469987840 ddar.py:60] Depth 14/1000 time = 18.221192121505737
I0123 13:27:00.906509 139995469987840 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:27:00.906639 139995469987840 alphageometry.py:566] LM output (score=-2.461435): "n : T b i b n 22 ;"
I0123 13:27:00.906680 139995469987840 alphageometry.py:567] Translation: "n = on_tline n b b i"

I0123 13:27:00.906729 139995469987840 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = lc_tangent h g c, on_line h a c; i = lc_tangent i g c, on_line i b c; j = on_circle j d c, on_bline j c b; k = on_circle k d a, on_bline k a c; l = on_line l j i, on_line l k h; m = on_line m c l, on_line m i h; n = on_tline n b b i ? eqangle m b m i m h m a"
I0123 13:27:00.906982 139995469987840 graph.py:498] 
I0123 13:27:00.907054 139995469987840 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = lc_tangent h g c, on_line h a c; i = lc_tangent i g c, on_line i b c; j = on_circle j d c, on_bline j c b; k = on_circle k d a, on_bline k a c; l = on_line l j i, on_line l k h; m = on_line m c l, on_line m i h; n = on_tline n b b i ? eqangle m b m i m h m a
I0123 13:27:03.870903 139995469987840 ddar.py:60] Depth 1/1000 time = 2.9242193698883057
I0123 13:27:07.711880 139995469987840 ddar.py:60] Depth 2/1000 time = 3.840472459793091
I0123 13:27:12.192100 139995469987840 ddar.py:60] Depth 3/1000 time = 4.479866027832031
I0123 13:27:17.617445 139995469987840 ddar.py:60] Depth 4/1000 time = 5.422660827636719
I0123 13:27:24.627660 139995469987840 ddar.py:60] Depth 5/1000 time = 7.009739875793457
I0123 13:27:32.401179 139995469987840 ddar.py:60] Depth 6/1000 time = 7.773000240325928
I0123 13:27:45.449823 139995469987840 ddar.py:60] Depth 7/1000 time = 13.047891616821289
I0123 13:27:57.318399 139995469987840 ddar.py:60] Depth 8/1000 time = 11.867974281311035
I0123 13:28:15.249887 139995469987840 ddar.py:60] Depth 9/1000 time = 17.931001663208008
I0123 13:28:32.225610 139995469987840 ddar.py:60] Depth 10/1000 time = 16.97521471977234
I0123 13:28:50.577051 139995469987840 ddar.py:60] Depth 11/1000 time = 18.350602626800537
I0123 13:29:09.065772 139995469987840 ddar.py:60] Depth 12/1000 time = 18.425718307495117
I0123 13:29:27.193980 139995469987840 ddar.py:60] Depth 13/1000 time = 18.000889539718628
I0123 13:29:45.264760 139995469987840 ddar.py:60] Depth 14/1000 time = 18.05794405937195
I0123 13:29:45.265206 139995469987840 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:29:45.265332 139995469987840 alphageometry.py:566] LM output (score=-2.535295): "n : T d n e g 22 ;"
I0123 13:29:45.265370 139995469987840 alphageometry.py:567] Translation: "n = on_tline n d e g"

I0123 13:29:45.265417 139995469987840 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = lc_tangent h g c, on_line h a c; i = lc_tangent i g c, on_line i b c; j = on_circle j d c, on_bline j c b; k = on_circle k d a, on_bline k a c; l = on_line l j i, on_line l k h; m = on_line m c l, on_line m i h; n = on_tline n d e g ? eqangle m b m i m h m a"
I0123 13:29:45.265629 139995469987840 graph.py:498] 
I0123 13:29:45.265707 139995469987840 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = lc_tangent h g c, on_line h a c; i = lc_tangent i g c, on_line i b c; j = on_circle j d c, on_bline j c b; k = on_circle k d a, on_bline k a c; l = on_line l j i, on_line l k h; m = on_line m c l, on_line m i h; n = on_tline n d e g ? eqangle m b m i m h m a
I0123 13:29:48.162373 139995469987840 ddar.py:60] Depth 1/1000 time = 2.855222225189209
I0123 13:29:51.632985 139995469987840 ddar.py:60] Depth 2/1000 time = 3.4703433513641357
I0123 13:29:55.616730 139995469987840 ddar.py:60] Depth 3/1000 time = 3.9833710193634033
I0123 13:30:01.592097 139995469987840 ddar.py:60] Depth 4/1000 time = 5.972670793533325
I0123 13:30:08.712975 139995469987840 ddar.py:60] Depth 5/1000 time = 7.120638132095337
I0123 13:30:16.123061 139995469987840 ddar.py:60] Depth 6/1000 time = 7.4098310470581055
I0123 13:30:29.939739 139995469987840 ddar.py:60] Depth 7/1000 time = 13.816356420516968
I0123 13:30:41.802588 139995469987840 ddar.py:60] Depth 8/1000 time = 11.862446784973145
I0123 13:31:00.166659 139995469987840 ddar.py:60] Depth 9/1000 time = 18.363659620285034
I0123 13:31:17.535017 139995469987840 ddar.py:60] Depth 10/1000 time = 17.367924213409424
I0123 13:31:36.188105 139995469987840 ddar.py:60] Depth 11/1000 time = 18.652771949768066
I0123 13:31:54.294061 139995469987840 ddar.py:60] Depth 12/1000 time = 18.06483244895935
I0123 13:32:12.924138 139995469987840 ddar.py:60] Depth 13/1000 time = 18.609622716903687
I0123 13:32:31.782352 139995469987840 ddar.py:60] Depth 14/1000 time = 18.730077981948853
I0123 13:32:49.984108 139995469987840 ddar.py:60] Depth 15/1000 time = 18.188947439193726
I0123 13:32:49.984774 139995469987840 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:32:49.984960 139995469987840 alphageometry.py:566] LM output (score=-2.637620): "n : T b c b n 22 ;"
I0123 13:32:49.985000 139995469987840 alphageometry.py:567] Translation: "n = on_tline n b b c"

I0123 13:32:49.985060 139995469987840 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = lc_tangent h g c, on_line h a c; i = lc_tangent i g c, on_line i b c; j = on_circle j d c, on_bline j c b; k = on_circle k d a, on_bline k a c; l = on_line l j i, on_line l k h; m = on_line m c l, on_line m i h; n = on_tline n b b c ? eqangle m b m i m h m a"
I0123 13:32:49.985288 139995469987840 graph.py:498] 
I0123 13:32:49.985355 139995469987840 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = lc_tangent h g c, on_line h a c; i = lc_tangent i g c, on_line i b c; j = on_circle j d c, on_bline j c b; k = on_circle k d a, on_bline k a c; l = on_line l j i, on_line l k h; m = on_line m c l, on_line m i h; n = on_tline n b b c ? eqangle m b m i m h m a
I0123 13:32:52.947946 139995469987840 ddar.py:60] Depth 1/1000 time = 2.910884141921997
I0123 13:32:56.488946 139995469987840 ddar.py:60] Depth 2/1000 time = 3.5407674312591553
I0123 13:33:00.444056 139995469987840 ddar.py:60] Depth 3/1000 time = 3.9548561573028564
I0123 13:33:06.416379 139995469987840 ddar.py:60] Depth 4/1000 time = 5.9697425365448
I0123 13:33:12.909844 139995469987840 ddar.py:60] Depth 5/1000 time = 6.493175029754639
I0123 13:33:20.683025 139995469987840 ddar.py:60] Depth 6/1000 time = 7.7729363441467285
I0123 13:33:32.630916 139995469987840 ddar.py:60] Depth 7/1000 time = 11.94759202003479
I0123 13:33:43.908396 139995469987840 ddar.py:60] Depth 8/1000 time = 11.277127504348755
I0123 13:34:02.115609 139995469987840 ddar.py:60] Depth 9/1000 time = 18.206804752349854
I0123 13:34:19.441132 139995469987840 ddar.py:60] Depth 10/1000 time = 17.325071573257446
I0123 13:34:37.170239 139995469987840 ddar.py:60] Depth 11/1000 time = 17.728671073913574
I0123 13:34:55.120157 139995469987840 ddar.py:60] Depth 12/1000 time = 17.88691282272339
I0123 13:35:13.105523 139995469987840 ddar.py:60] Depth 13/1000 time = 17.858071327209473
I0123 13:35:31.224289 139995469987840 ddar.py:60] Depth 14/1000 time = 18.105495929718018
I0123 13:35:31.224712 139995469987840 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:35:31.224854 139995469987840 alphageometry.py:566] LM output (score=-2.638842): "n : T b j j n 22 ;"
I0123 13:35:31.224892 139995469987840 alphageometry.py:567] Translation: "n = on_tline n j b j"

I0123 13:35:31.224940 139995469987840 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = lc_tangent h g c, on_line h a c; i = lc_tangent i g c, on_line i b c; j = on_circle j d c, on_bline j c b; k = on_circle k d a, on_bline k a c; l = on_line l j i, on_line l k h; m = on_line m c l, on_line m i h; n = on_tline n j b j ? eqangle m b m i m h m a"
I0123 13:35:31.225157 139995469987840 graph.py:498] 
I0123 13:35:31.225221 139995469987840 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = lc_tangent h g c, on_line h a c; i = lc_tangent i g c, on_line i b c; j = on_circle j d c, on_bline j c b; k = on_circle k d a, on_bline k a c; l = on_line l j i, on_line l k h; m = on_line m c l, on_line m i h; n = on_tline n j b j ? eqangle m b m i m h m a
I0123 13:35:33.737267 139995469987840 ddar.py:60] Depth 1/1000 time = 2.451315402984619
I0123 13:35:37.256596 139995469987840 ddar.py:60] Depth 2/1000 time = 3.5190699100494385
I0123 13:35:41.414196 139995469987840 ddar.py:60] Depth 3/1000 time = 4.157378911972046
I0123 13:35:45.511885 139995469987840 ddar.py:60] Depth 4/1000 time = 4.097415208816528
I0123 13:35:51.169558 139995469987840 ddar.py:60] Depth 5/1000 time = 5.654704809188843
I0123 13:35:58.614251 139995469987840 ddar.py:60] Depth 6/1000 time = 7.444414377212524
I0123 13:36:06.329192 139995469987840 ddar.py:60] Depth 7/1000 time = 7.714676141738892
I0123 13:36:19.957348 139995469987840 ddar.py:60] Depth 8/1000 time = 13.627842426300049
I0123 13:36:32.016481 139995469987840 ddar.py:60] Depth 9/1000 time = 12.05875825881958
I0123 13:36:49.699683 139995469987840 ddar.py:60] Depth 10/1000 time = 17.682847499847412
I0123 13:37:07.416472 139995469987840 ddar.py:60] Depth 11/1000 time = 17.71631121635437
I0123 13:37:25.949828 139995469987840 ddar.py:60] Depth 12/1000 time = 18.532817602157593
I0123 13:37:45.223160 139995469987840 ddar.py:60] Depth 13/1000 time = 19.20478105545044
I0123 13:38:04.085274 139995469987840 ddar.py:60] Depth 14/1000 time = 18.728339195251465
I0123 13:38:23.111402 139995469987840 ddar.py:60] Depth 15/1000 time = 19.011675357818604
I0123 13:38:23.112269 139995469987840 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:38:23.112446 139995469987840 alphageometry.py:566] LM output (score=-2.656693): "n : C d j n 22 D d n j n 23 ;"
I0123 13:38:23.112489 139995469987840 alphageometry.py:567] Translation: "n = on_line n d j, on_bline n j d"

I0123 13:38:23.112554 139995469987840 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = lc_tangent h g c, on_line h a c; i = lc_tangent i g c, on_line i b c; j = on_circle j d c, on_bline j c b; k = on_circle k d a, on_bline k a c; l = on_line l j i, on_line l k h; m = on_line m c l, on_line m i h; n = on_line n d j, on_bline n j d ? eqangle m b m i m h m a"
I0123 13:38:23.112796 139995469987840 graph.py:498] 
I0123 13:38:23.112867 139995469987840 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = lc_tangent h g c, on_line h a c; i = lc_tangent i g c, on_line i b c; j = on_circle j d c, on_bline j c b; k = on_circle k d a, on_bline k a c; l = on_line l j i, on_line l k h; m = on_line m c l, on_line m i h; n = on_line n d j, on_bline n j d ? eqangle m b m i m h m a
I0123 13:38:25.732967 139995469987840 ddar.py:60] Depth 1/1000 time = 2.580266237258911
I0123 13:38:29.417919 139995469987840 ddar.py:60] Depth 2/1000 time = 3.684701919555664
I0123 13:38:34.386609 139995469987840 ddar.py:60] Depth 3/1000 time = 4.968434572219849
I0123 13:38:39.400574 139995469987840 ddar.py:60] Depth 4/1000 time = 5.013657093048096
I0123 13:38:43.943217 139995469987840 ddar.py:60] Depth 5/1000 time = 4.542024612426758
I0123 13:38:50.606400 139995469987840 ddar.py:60] Depth 6/1000 time = 6.660272598266602
I0123 13:38:57.745179 139995469987840 ddar.py:60] Depth 7/1000 time = 7.138511657714844
I0123 13:39:06.446048 139995469987840 ddar.py:60] Depth 8/1000 time = 8.700539588928223
I0123 13:39:20.921774 139995469987840 ddar.py:60] Depth 9/1000 time = 14.475294589996338
I0123 13:39:33.851599 139995469987840 ddar.py:60] Depth 10/1000 time = 12.929489374160767
I0123 13:39:53.222972 139995469987840 ddar.py:60] Depth 11/1000 time = 19.370962619781494
I0123 13:40:11.809997 139995469987840 ddar.py:60] Depth 12/1000 time = 18.58642864227295
I0123 13:40:31.733591 139995469987840 ddar.py:60] Depth 13/1000 time = 19.92292356491089
I0123 13:40:51.627011 139995469987840 ddar.py:60] Depth 14/1000 time = 19.849868535995483
I0123 13:41:11.119125 139995469987840 ddar.py:60] Depth 15/1000 time = 19.467872142791748
I0123 13:41:31.232772 139995469987840 ddar.py:60] Depth 16/1000 time = 19.981359720230103
I0123 13:41:50.902046 139995469987840 ddar.py:60] Depth 17/1000 time = 19.655319690704346
I0123 13:41:50.902678 139995469987840 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:41:50.902841 139995469987840 alphageometry.py:566] LM output (score=-2.657944): "n : T b g f n 22 ;"
I0123 13:41:50.902886 139995469987840 alphageometry.py:567] Translation: "n = on_tline n f b g"

I0123 13:41:50.902931 139995469987840 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = lc_tangent h g c, on_line h a c; i = lc_tangent i g c, on_line i b c; j = on_circle j d c, on_bline j c b; k = on_circle k d a, on_bline k a c; l = on_line l j i, on_line l k h; m = on_line m c l, on_line m i h; n = on_tline n f b g ? eqangle m b m i m h m a"
I0123 13:41:50.903148 139995469987840 graph.py:498] 
I0123 13:41:50.903222 139995469987840 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = lc_tangent h g c, on_line h a c; i = lc_tangent i g c, on_line i b c; j = on_circle j d c, on_bline j c b; k = on_circle k d a, on_bline k a c; l = on_line l j i, on_line l k h; m = on_line m c l, on_line m i h; n = on_tline n f b g ? eqangle m b m i m h m a
I0123 13:41:53.429635 139995469987840 ddar.py:60] Depth 1/1000 time = 2.448000907897949
I0123 13:41:56.994221 139995469987840 ddar.py:60] Depth 2/1000 time = 3.564298152923584
I0123 13:42:01.661753 139995469987840 ddar.py:60] Depth 3/1000 time = 4.6672375202178955
I0123 13:42:07.977240 139995469987840 ddar.py:60] Depth 4/1000 time = 6.308861255645752
I0123 13:42:16.128881 139995469987840 ddar.py:60] Depth 5/1000 time = 8.151295185089111
I0123 13:42:24.318188 139995469987840 ddar.py:60] Depth 6/1000 time = 8.18885588645935
I0123 13:42:38.792435 139995469987840 ddar.py:60] Depth 7/1000 time = 14.473903894424438
I0123 13:42:51.211175 139995469987840 ddar.py:60] Depth 8/1000 time = 12.418204069137573
I0123 13:43:09.603004 139995469987840 ddar.py:60] Depth 9/1000 time = 18.391360998153687
I0123 13:43:27.964236 139995469987840 ddar.py:60] Depth 10/1000 time = 18.360660552978516
I0123 13:43:47.165334 139995469987840 ddar.py:60] Depth 11/1000 time = 19.200449466705322
I0123 13:44:06.592045 139995469987840 ddar.py:60] Depth 12/1000 time = 19.35587430000305
I0123 13:44:26.163015 139995469987840 ddar.py:60] Depth 13/1000 time = 19.432669639587402
I0123 13:44:45.211491 139995469987840 ddar.py:60] Depth 14/1000 time = 19.033583164215088
I0123 13:44:45.212330 139995469987840 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:44:45.212504 139995469987840 alphageometry.py:566] LM output (score=-2.722857): "n : T d n f h 22 ;"
I0123 13:44:45.212542 139995469987840 alphageometry.py:567] Translation: "n = on_tline n d f h"

I0123 13:44:45.212606 139995469987840 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = lc_tangent h g c, on_line h a c; i = lc_tangent i g c, on_line i b c; j = on_circle j d c, on_bline j c b; k = on_circle k d a, on_bline k a c; l = on_line l j i, on_line l k h; m = on_line m c l, on_line m i h; n = on_tline n d f h ? eqangle m b m i m h m a"
I0123 13:44:45.212846 139995469987840 graph.py:498] 
I0123 13:44:45.212911 139995469987840 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = lc_tangent h g c, on_line h a c; i = lc_tangent i g c, on_line i b c; j = on_circle j d c, on_bline j c b; k = on_circle k d a, on_bline k a c; l = on_line l j i, on_line l k h; m = on_line m c l, on_line m i h; n = on_tline n d f h ? eqangle m b m i m h m a
I0123 13:44:48.409936 139995469987840 ddar.py:60] Depth 1/1000 time = 3.1093649864196777
I0123 13:44:52.318402 139995469987840 ddar.py:60] Depth 2/1000 time = 3.9082417488098145
I0123 13:44:57.458367 139995469987840 ddar.py:60] Depth 3/1000 time = 5.139718532562256
I0123 13:45:02.076292 139995469987840 ddar.py:60] Depth 4/1000 time = 4.617650985717773
I0123 13:45:08.292667 139995469987840 ddar.py:60] Depth 5/1000 time = 6.213802337646484
I0123 13:45:16.278470 139995469987840 ddar.py:60] Depth 6/1000 time = 7.985508680343628
I0123 13:45:24.550751 139995469987840 ddar.py:60] Depth 7/1000 time = 8.27196478843689
I0123 13:45:39.344161 139995469987840 ddar.py:60] Depth 8/1000 time = 14.79296088218689
I0123 13:45:52.193881 139995469987840 ddar.py:60] Depth 9/1000 time = 12.84931492805481
I0123 13:46:10.578361 139995469987840 ddar.py:60] Depth 10/1000 time = 18.383983850479126
I0123 13:46:29.263862 139995469987840 ddar.py:60] Depth 11/1000 time = 18.684889554977417
I0123 13:46:49.178274 139995469987840 ddar.py:60] Depth 12/1000 time = 19.9137544631958
I0123 13:47:08.769519 139995469987840 ddar.py:60] Depth 13/1000 time = 19.524601936340332
I0123 13:47:28.661572 139995469987840 ddar.py:60] Depth 14/1000 time = 19.765212774276733
I0123 13:47:48.423686 139995469987840 ddar.py:60] Depth 15/1000 time = 19.74821424484253
I0123 13:47:48.424398 139995469987840 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:47:48.424533 139995469987840 alphageometry.py:566] LM output (score=-2.826774): "n : T b f e n 22 ;"
I0123 13:47:48.424573 139995469987840 alphageometry.py:567] Translation: "n = on_tline n e b f"

I0123 13:47:48.424618 139995469987840 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = lc_tangent h g c, on_line h a c; i = lc_tangent i g c, on_line i b c; j = on_circle j d c, on_bline j c b; k = on_circle k d a, on_bline k a c; l = on_line l j i, on_line l k h; m = on_line m c l, on_line m i h; n = on_tline n e b f ? eqangle m b m i m h m a"
I0123 13:47:48.424833 139995469987840 graph.py:498] 
I0123 13:47:48.424898 139995469987840 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = lc_tangent h g c, on_line h a c; i = lc_tangent i g c, on_line i b c; j = on_circle j d c, on_bline j c b; k = on_circle k d a, on_bline k a c; l = on_line l j i, on_line l k h; m = on_line m c l, on_line m i h; n = on_tline n e b f ? eqangle m b m i m h m a
I0123 13:47:51.024400 139995469987840 ddar.py:60] Depth 1/1000 time = 2.547802686691284
I0123 13:47:55.140268 139995469987840 ddar.py:60] Depth 2/1000 time = 4.115509510040283
I0123 13:47:59.315395 139995469987840 ddar.py:60] Depth 3/1000 time = 4.1748738288879395
I0123 13:48:05.717763 139995469987840 ddar.py:60] Depth 4/1000 time = 6.396849632263184
I0123 13:48:13.816670 139995469987840 ddar.py:60] Depth 5/1000 time = 8.098571300506592
I0123 13:48:21.540874 139995469987840 ddar.py:60] Depth 6/1000 time = 7.723743915557861
I0123 13:48:36.007122 139995469987840 ddar.py:60] Depth 7/1000 time = 14.465868949890137
I0123 13:48:49.062262 139995469987840 ddar.py:60] Depth 8/1000 time = 13.054674625396729
I0123 13:49:07.509721 139995469987840 ddar.py:60] Depth 9/1000 time = 18.447096347808838
I0123 13:49:25.461253 139995469987840 ddar.py:60] Depth 10/1000 time = 17.951074600219727
I0123 13:49:44.850940 139995469987840 ddar.py:60] Depth 11/1000 time = 19.38919687271118
I0123 13:50:03.762183 139995469987840 ddar.py:60] Depth 12/1000 time = 18.839650869369507
I0123 13:50:23.434898 139995469987840 ddar.py:60] Depth 13/1000 time = 19.538301944732666
I0123 13:50:43.316789 139995469987840 ddar.py:60] Depth 14/1000 time = 19.867120504379272
I0123 13:50:43.317592 139995469987840 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:50:43.317777 139995469987840 alphageometry.py:566] LM output (score=-2.835320): "n : T b f f n 22 ;"
I0123 13:50:43.317817 139995469987840 alphageometry.py:567] Translation: "n = on_tline n f b f"

I0123 13:50:43.317882 139995469987840 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = lc_tangent h g c, on_line h a c; i = lc_tangent i g c, on_line i b c; j = on_circle j d c, on_bline j c b; k = on_circle k d a, on_bline k a c; l = on_line l j i, on_line l k h; m = on_line m c l, on_line m i h; n = on_tline n f b f ? eqangle m b m i m h m a"
I0123 13:50:43.318123 139995469987840 graph.py:498] 
I0123 13:50:43.318189 139995469987840 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = lc_tangent h g c, on_line h a c; i = lc_tangent i g c, on_line i b c; j = on_circle j d c, on_bline j c b; k = on_circle k d a, on_bline k a c; l = on_line l j i, on_line l k h; m = on_line m c l, on_line m i h; n = on_tline n f b f ? eqangle m b m i m h m a
I0123 13:50:45.910929 139995469987840 ddar.py:60] Depth 1/1000 time = 2.5544087886810303
I0123 13:50:49.977714 139995469987840 ddar.py:60] Depth 2/1000 time = 4.066564083099365
I0123 13:50:54.267506 139995469987840 ddar.py:60] Depth 3/1000 time = 4.289549112319946
I0123 13:51:00.889788 139995469987840 ddar.py:60] Depth 4/1000 time = 6.61630916595459
I0123 13:51:08.859083 139995469987840 ddar.py:60] Depth 5/1000 time = 7.96895432472229
I0123 13:51:17.596959 139995469987840 ddar.py:60] Depth 6/1000 time = 8.737560510635376
I0123 13:51:31.679078 139995469987840 ddar.py:60] Depth 7/1000 time = 14.0818190574646
I0123 13:51:44.251621 139995469987840 ddar.py:60] Depth 8/1000 time = 12.57209587097168
I0123 13:52:02.644665 139995469987840 ddar.py:60] Depth 9/1000 time = 18.39238452911377
I0123 13:52:20.675539 139995469987840 ddar.py:60] Depth 10/1000 time = 18.030275344848633
I0123 13:52:40.053403 139995469987840 ddar.py:60] Depth 11/1000 time = 19.37742805480957
I0123 13:52:59.698592 139995469987840 ddar.py:60] Depth 12/1000 time = 19.574090719223022
I0123 13:53:19.476370 139995469987840 ddar.py:60] Depth 13/1000 time = 19.64350414276123
I0123 13:53:38.645389 139995469987840 ddar.py:60] Depth 14/1000 time = 19.15453600883484
I0123 13:53:38.646115 139995469987840 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:53:38.646282 139995469987840 alphageometry.py:566] LM output (score=-2.860418): "n : C e f n 22 D e f f n 23 ;"
I0123 13:53:38.646328 139995469987840 alphageometry.py:567] Translation: "n = on_line n e f, on_circle n f e"

I0123 13:53:38.646377 139995469987840 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = lc_tangent h g c, on_line h a c; i = lc_tangent i g c, on_line i b c; j = on_circle j d c, on_bline j c b; k = on_circle k d a, on_bline k a c; l = on_line l j i, on_line l k h; m = on_line m c l, on_line m i h; n = on_line n e f, on_circle n f e ? eqangle m b m i m h m a"
I0123 13:53:38.646602 139995469987840 graph.py:498] 
I0123 13:53:38.646672 139995469987840 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = lc_tangent h g c, on_line h a c; i = lc_tangent i g c, on_line i b c; j = on_circle j d c, on_bline j c b; k = on_circle k d a, on_bline k a c; l = on_line l j i, on_line l k h; m = on_line m c l, on_line m i h; n = on_line n e f, on_circle n f e ? eqangle m b m i m h m a
I0123 13:53:41.369893 139995469987840 ddar.py:60] Depth 1/1000 time = 2.6423208713531494
I0123 13:53:45.121156 139995469987840 ddar.py:60] Depth 2/1000 time = 3.750971555709839
I0123 13:53:49.434322 139995469987840 ddar.py:60] Depth 3/1000 time = 4.312784910202026
I0123 13:53:53.740930 139995469987840 ddar.py:60] Depth 4/1000 time = 4.306114912033081
I0123 13:53:59.005687 139995469987840 ddar.py:60] Depth 5/1000 time = 5.262298107147217
I0123 13:54:06.620230 139995469987840 ddar.py:60] Depth 6/1000 time = 7.614239931106567
I0123 13:54:14.000714 139995469987840 ddar.py:60] Depth 7/1000 time = 7.380083799362183
I0123 13:54:27.764273 139995469987840 ddar.py:60] Depth 8/1000 time = 13.763170957565308
I0123 13:54:40.318360 139995469987840 ddar.py:60] Depth 9/1000 time = 12.553588151931763
I0123 13:54:57.310707 139995469987840 ddar.py:60] Depth 10/1000 time = 16.991989135742188
I0123 13:55:14.838491 139995469987840 ddar.py:60] Depth 11/1000 time = 17.52725863456726
I0123 13:55:33.201370 139995469987840 ddar.py:60] Depth 12/1000 time = 18.36220121383667
I0123 13:55:52.110176 139995469987840 ddar.py:60] Depth 13/1000 time = 18.867144107818604
I0123 13:56:10.453434 139995469987840 ddar.py:60] Depth 14/1000 time = 18.322705507278442
I0123 13:56:28.867407 139995469987840 ddar.py:60] Depth 15/1000 time = 18.284297466278076
I0123 13:56:47.887061 139995469987840 ddar.py:60] Depth 16/1000 time = 19.00638699531555
I0123 13:56:47.887768 139995469987840 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 13:56:47.887899 139995469987840 alphageometry.py:566] LM output (score=-2.870911): "n : C d j n 22 D d j j n 23 ;"
I0123 13:56:47.887937 139995469987840 alphageometry.py:567] Translation: "n = on_line n d j, on_circle n j d"

I0123 13:56:47.887983 139995469987840 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = lc_tangent h g c, on_line h a c; i = lc_tangent i g c, on_line i b c; j = on_circle j d c, on_bline j c b; k = on_circle k d a, on_bline k a c; l = on_line l j i, on_line l k h; m = on_line m c l, on_line m i h; n = on_line n d j, on_circle n j d ? eqangle m b m i m h m a"
I0123 13:56:47.888196 139995469987840 graph.py:498] 
I0123 13:56:47.888261 139995469987840 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = lc_tangent h g c, on_line h a c; i = lc_tangent i g c, on_line i b c; j = on_circle j d c, on_bline j c b; k = on_circle k d a, on_bline k a c; l = on_line l j i, on_line l k h; m = on_line m c l, on_line m i h; n = on_line n d j, on_circle n j d ? eqangle m b m i m h m a
I0123 13:56:50.905094 139995469987840 ddar.py:60] Depth 1/1000 time = 2.981489896774292
I0123 13:56:55.633183 139995469987840 ddar.py:60] Depth 2/1000 time = 4.7278358936309814
I0123 13:57:00.560509 139995469987840 ddar.py:60] Depth 3/1000 time = 4.927037477493286
I0123 13:57:05.491421 139995469987840 ddar.py:60] Depth 4/1000 time = 4.9306323528289795
I0123 13:57:10.452891 139995469987840 ddar.py:60] Depth 5/1000 time = 4.960938453674316
I0123 13:57:17.140337 139995469987840 ddar.py:60] Depth 6/1000 time = 6.685032367706299
I0123 13:57:25.523684 139995469987840 ddar.py:60] Depth 7/1000 time = 8.383067846298218
I0123 13:57:33.622026 139995469987840 ddar.py:60] Depth 8/1000 time = 8.098097562789917
I0123 13:57:48.520089 139995469987840 ddar.py:60] Depth 9/1000 time = 14.897719144821167
I0123 13:58:01.377173 139995469987840 ddar.py:60] Depth 10/1000 time = 12.856592178344727
I0123 13:58:21.011703 139995469987840 ddar.py:60] Depth 11/1000 time = 19.634036540985107
I0123 13:58:39.807349 139995469987840 ddar.py:60] Depth 12/1000 time = 18.795113801956177
I0123 13:59:00.149288 139995469987840 ddar.py:60] Depth 13/1000 time = 20.341314792633057
I0123 13:59:19.748676 139995469987840 ddar.py:60] Depth 14/1000 time = 19.55662775039673
I0123 13:59:40.139480 139995469987840 ddar.py:60] Depth 15/1000 time = 20.365994453430176
I0123 14:00:00.774065 139995469987840 ddar.py:60] Depth 16/1000 time = 20.578975439071655
I0123 14:00:20.569724 139995469987840 ddar.py:60] Depth 17/1000 time = 19.7129385471344
I0123 14:00:40.372181 139995469987840 ddar.py:60] Depth 18/1000 time = 19.78835439682007
I0123 14:00:40.372571 139995469987840 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:00:40.372707 139995469987840 alphageometry.py:566] LM output (score=-2.871393): "n : T b j e n 22 ;"
I0123 14:00:40.372749 139995469987840 alphageometry.py:567] Translation: "n = on_tline n e b j"

I0123 14:00:40.372789 139995469987840 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = lc_tangent h g c, on_line h a c; i = lc_tangent i g c, on_line i b c; j = on_circle j d c, on_bline j c b; k = on_circle k d a, on_bline k a c; l = on_line l j i, on_line l k h; m = on_line m c l, on_line m i h; n = on_tline n e b j ? eqangle m b m i m h m a"
I0123 14:00:40.372993 139995469987840 graph.py:498] 
I0123 14:00:40.373059 139995469987840 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = lc_tangent h g c, on_line h a c; i = lc_tangent i g c, on_line i b c; j = on_circle j d c, on_bline j c b; k = on_circle k d a, on_bline k a c; l = on_line l j i, on_line l k h; m = on_line m c l, on_line m i h; n = on_tline n e b j ? eqangle m b m i m h m a
I0123 14:00:43.637157 139995469987840 ddar.py:60] Depth 1/1000 time = 2.565854072570801
I0123 14:00:46.794190 139995469987840 ddar.py:60] Depth 2/1000 time = 3.1567461490631104
I0123 14:00:51.212237 139995469987840 ddar.py:60] Depth 3/1000 time = 4.4178102016448975
I0123 14:00:55.600568 139995469987840 ddar.py:60] Depth 4/1000 time = 4.388055086135864
I0123 14:01:00.996550 139995469987840 ddar.py:60] Depth 5/1000 time = 5.393069505691528
I0123 14:01:08.951823 139995469987840 ddar.py:60] Depth 6/1000 time = 7.9550230503082275
I0123 14:01:16.553249 139995469987840 ddar.py:60] Depth 7/1000 time = 7.601141452789307
I0123 14:01:29.100857 139995469987840 ddar.py:60] Depth 8/1000 time = 12.547214031219482
I0123 14:01:40.868613 139995469987840 ddar.py:60] Depth 9/1000 time = 11.767189264297485
I0123 14:01:59.906706 139995469987840 ddar.py:60] Depth 10/1000 time = 19.037709712982178
I0123 14:02:18.275481 139995469987840 ddar.py:60] Depth 11/1000 time = 18.368430376052856
I0123 14:02:36.570926 139995469987840 ddar.py:60] Depth 12/1000 time = 18.29505205154419
I0123 14:02:56.336810 139995469987840 ddar.py:60] Depth 13/1000 time = 19.69596529006958
I0123 14:03:15.377335 139995469987840 ddar.py:60] Depth 14/1000 time = 18.90314030647278
I0123 14:03:34.544341 139995469987840 ddar.py:60] Depth 15/1000 time = 19.152440309524536
I0123 14:03:34.545039 139995469987840 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:03:34.545173 139995469987840 alphageometry.py:566] LM output (score=-2.921773): "n : T b c e n 22 ;"
I0123 14:03:34.545211 139995469987840 alphageometry.py:567] Translation: "n = on_tline n e b c"

I0123 14:03:34.545256 139995469987840 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = lc_tangent h g c, on_line h a c; i = lc_tangent i g c, on_line i b c; j = on_circle j d c, on_bline j c b; k = on_circle k d a, on_bline k a c; l = on_line l j i, on_line l k h; m = on_line m c l, on_line m i h; n = on_tline n e b c ? eqangle m b m i m h m a"
I0123 14:03:34.545469 139995469987840 graph.py:498] 
I0123 14:03:34.545537 139995469987840 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = lc_tangent h g c, on_line h a c; i = lc_tangent i g c, on_line i b c; j = on_circle j d c, on_bline j c b; k = on_circle k d a, on_bline k a c; l = on_line l j i, on_line l k h; m = on_line m c l, on_line m i h; n = on_tline n e b c ? eqangle m b m i m h m a
I0123 14:03:36.492144 139995469987840 ddar.py:60] Depth 1/1000 time = 1.9099323749542236
I0123 14:03:40.782959 139995469987840 ddar.py:60] Depth 2/1000 time = 4.290586233139038
I0123 14:03:45.114633 139995469987840 ddar.py:60] Depth 3/1000 time = 4.331444501876831
I0123 14:03:50.527685 139995469987840 ddar.py:60] Depth 4/1000 time = 5.4101903438568115
I0123 14:03:57.692941 139995469987840 ddar.py:60] Depth 5/1000 time = 7.164876699447632
I0123 14:04:05.128903 139995469987840 ddar.py:60] Depth 6/1000 time = 7.435718774795532
I0123 14:04:18.769901 139995469987840 ddar.py:60] Depth 7/1000 time = 13.640721082687378
I0123 14:04:31.017962 139995469987840 ddar.py:60] Depth 8/1000 time = 12.247613430023193
I0123 14:04:48.927147 139995469987840 ddar.py:60] Depth 9/1000 time = 17.90860366821289
I0123 14:05:06.513489 139995469987840 ddar.py:60] Depth 10/1000 time = 17.585890293121338
I0123 14:05:24.929657 139995469987840 ddar.py:60] Depth 11/1000 time = 18.415679454803467
I0123 14:05:43.520541 139995469987840 ddar.py:60] Depth 12/1000 time = 18.52498507499695
I0123 14:06:01.443506 139995469987840 ddar.py:60] Depth 13/1000 time = 17.79660201072693
I0123 14:06:20.811302 139995469987840 ddar.py:60] Depth 14/1000 time = 19.35481548309326
I0123 14:06:20.811734 139995469987840 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:06:20.811861 139995469987840 alphageometry.py:566] LM output (score=-2.956383): "n : T a b b n 22 ;"
I0123 14:06:20.811899 139995469987840 alphageometry.py:567] Translation: "n = on_tline n b a b"

I0123 14:06:20.811944 139995469987840 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = lc_tangent h g c, on_line h a c; i = lc_tangent i g c, on_line i b c; j = on_circle j d c, on_bline j c b; k = on_circle k d a, on_bline k a c; l = on_line l j i, on_line l k h; m = on_line m c l, on_line m i h; n = on_tline n b a b ? eqangle m b m i m h m a"
I0123 14:06:20.812153 139995469987840 graph.py:498] 
I0123 14:06:20.812222 139995469987840 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = lc_tangent h g c, on_line h a c; i = lc_tangent i g c, on_line i b c; j = on_circle j d c, on_bline j c b; k = on_circle k d a, on_bline k a c; l = on_line l j i, on_line l k h; m = on_line m c l, on_line m i h; n = on_tline n b a b ? eqangle m b m i m h m a
I0123 14:06:23.483909 139995469987840 ddar.py:60] Depth 1/1000 time = 2.6109635829925537
I0123 14:06:27.366832 139995469987840 ddar.py:60] Depth 2/1000 time = 3.8827006816864014
I0123 14:06:31.122007 139995469987840 ddar.py:60] Depth 3/1000 time = 3.7549362182617188
I0123 14:06:35.659435 139995469987840 ddar.py:60] Depth 4/1000 time = 4.5371479988098145
I0123 14:06:41.206871 139995469987840 ddar.py:60] Depth 5/1000 time = 5.544666767120361
I0123 14:06:49.243672 139995469987840 ddar.py:60] Depth 6/1000 time = 8.03652811050415
I0123 14:06:56.873391 139995469987840 ddar.py:60] Depth 7/1000 time = 7.629418611526489
I0123 14:07:09.807296 139995469987840 ddar.py:60] Depth 8/1000 time = 12.933465480804443
I0123 14:07:21.274064 139995469987840 ddar.py:60] Depth 9/1000 time = 11.466269254684448
I0123 14:07:40.785234 139995469987840 ddar.py:60] Depth 10/1000 time = 19.51055908203125
I0123 14:07:58.860368 139995469987840 ddar.py:60] Depth 11/1000 time = 18.074657678604126
I0123 14:08:17.300462 139995469987840 ddar.py:60] Depth 12/1000 time = 18.43973994255066
I0123 14:08:35.104241 139995469987840 ddar.py:60] Depth 13/1000 time = 17.75269389152527
I0123 14:08:53.593271 139995469987840 ddar.py:60] Depth 14/1000 time = 18.464669704437256
I0123 14:09:12.342630 139995469987840 ddar.py:60] Depth 15/1000 time = 18.61227774620056
I0123 14:09:30.317807 139995469987840 ddar.py:60] Depth 16/1000 time = 17.9606614112854
I0123 14:09:30.318711 139995469987840 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:09:30.318873 139995469987840 alphageometry.py:566] LM output (score=-2.956384): "n : T b f d n 22 ;"
I0123 14:09:30.318911 139995469987840 alphageometry.py:567] Translation: "n = on_tline n d b f"

I0123 14:09:30.318973 139995469987840 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = lc_tangent h g c, on_line h a c; i = lc_tangent i g c, on_line i b c; j = on_circle j d c, on_bline j c b; k = on_circle k d a, on_bline k a c; l = on_line l j i, on_line l k h; m = on_line m c l, on_line m i h; n = on_tline n d b f ? eqangle m b m i m h m a"
I0123 14:09:30.319205 139995469987840 graph.py:498] 
I0123 14:09:30.319271 139995469987840 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = lc_tangent h g c, on_line h a c; i = lc_tangent i g c, on_line i b c; j = on_circle j d c, on_bline j c b; k = on_circle k d a, on_bline k a c; l = on_line l j i, on_line l k h; m = on_line m c l, on_line m i h; n = on_tline n d b f ? eqangle m b m i m h m a
I0123 14:09:33.027404 139995469987840 ddar.py:60] Depth 1/1000 time = 2.6591548919677734
I0123 14:09:37.207954 139995469987840 ddar.py:60] Depth 2/1000 time = 4.180322170257568
I0123 14:09:40.982225 139995469987840 ddar.py:60] Depth 3/1000 time = 3.7740323543548584
I0123 14:09:48.689244 139995469987840 ddar.py:60] Depth 4/1000 time = 7.701233625411987
I0123 14:09:56.125826 139995469987840 ddar.py:60] Depth 5/1000 time = 7.436335563659668
I0123 14:10:05.475213 139995469987840 ddar.py:60] Depth 6/1000 time = 9.34906816482544
I0123 14:10:20.312762 139995469987840 ddar.py:60] Depth 7/1000 time = 14.837160587310791
I0123 14:10:33.947171 139995469987840 ddar.py:60] Depth 8/1000 time = 13.634058952331543
I0123 14:10:52.755417 139995469987840 ddar.py:60] Depth 9/1000 time = 18.807735443115234
I0123 14:11:12.253749 139995469987840 ddar.py:60] Depth 10/1000 time = 19.497655153274536
I0123 14:11:31.780700 139995469987840 ddar.py:60] Depth 11/1000 time = 19.526256799697876
I0123 14:11:52.247443 139995469987840 ddar.py:60] Depth 12/1000 time = 20.394206762313843
I0123 14:12:12.780599 139995469987840 ddar.py:60] Depth 13/1000 time = 20.50688409805298
I0123 14:12:32.700306 139995469987840 ddar.py:60] Depth 14/1000 time = 19.79262399673462
I0123 14:12:53.346674 139995469987840 ddar.py:60] Depth 15/1000 time = 20.630974531173706
I0123 14:12:53.347543 139995469987840 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:12:53.347749 139995469987840 alphageometry.py:566] LM output (score=-2.957573): "n : T b g b n 22 ;"
I0123 14:12:53.347793 139995469987840 alphageometry.py:567] Translation: "n = on_tline n b b g"

I0123 14:12:53.347862 139995469987840 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = lc_tangent h g c, on_line h a c; i = lc_tangent i g c, on_line i b c; j = on_circle j d c, on_bline j c b; k = on_circle k d a, on_bline k a c; l = on_line l j i, on_line l k h; m = on_line m c l, on_line m i h; n = on_tline n b b g ? eqangle m b m i m h m a"
I0123 14:12:53.348108 139995469987840 graph.py:498] 
I0123 14:12:53.348177 139995469987840 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = lc_tangent h g c, on_line h a c; i = lc_tangent i g c, on_line i b c; j = on_circle j d c, on_bline j c b; k = on_circle k d a, on_bline k a c; l = on_line l j i, on_line l k h; m = on_line m c l, on_line m i h; n = on_tline n b b g ? eqangle m b m i m h m a
I0123 14:12:55.341025 139995469987840 ddar.py:60] Depth 1/1000 time = 1.9080867767333984
I0123 14:12:59.846492 139995469987840 ddar.py:60] Depth 2/1000 time = 4.5052289962768555
I0123 14:13:04.362339 139995469987840 ddar.py:60] Depth 3/1000 time = 4.515607833862305
I0123 14:13:10.534974 139995469987840 ddar.py:60] Depth 4/1000 time = 6.166807413101196
I0123 14:13:18.624851 139995469987840 ddar.py:60] Depth 5/1000 time = 8.089598655700684
I0123 14:13:27.742983 139995469987840 ddar.py:60] Depth 6/1000 time = 9.117814064025879
I0123 14:13:41.983316 139995469987840 ddar.py:60] Depth 7/1000 time = 14.239935636520386
I0123 14:13:55.163065 139995469987840 ddar.py:60] Depth 8/1000 time = 13.179212093353271
I0123 14:14:13.344469 139995469987840 ddar.py:60] Depth 9/1000 time = 18.1808443069458
I0123 14:14:32.122526 139995469987840 ddar.py:60] Depth 10/1000 time = 18.777623414993286
I0123 14:14:51.809080 139995469987840 ddar.py:60] Depth 11/1000 time = 19.686110973358154
I0123 14:15:11.475956 139995469987840 ddar.py:60] Depth 12/1000 time = 19.59643316268921
I0123 14:15:31.265161 139995469987840 ddar.py:60] Depth 13/1000 time = 19.647374391555786
I0123 14:15:51.133239 139995469987840 ddar.py:60] Depth 14/1000 time = 19.853209733963013
I0123 14:15:51.133925 139995469987840 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:15:51.134086 139995469987840 alphageometry.py:566] LM output (score=-2.989007): "n : T b i i n 22 ;"
I0123 14:15:51.134130 139995469987840 alphageometry.py:567] Translation: "n = on_tline n i b i"

I0123 14:15:51.134172 139995469987840 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = lc_tangent h g c, on_line h a c; i = lc_tangent i g c, on_line i b c; j = on_circle j d c, on_bline j c b; k = on_circle k d a, on_bline k a c; l = on_line l j i, on_line l k h; m = on_line m c l, on_line m i h; n = on_tline n i b i ? eqangle m b m i m h m a"
I0123 14:15:51.134379 139995469987840 graph.py:498] 
I0123 14:15:51.134447 139995469987840 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = lc_tangent h g c, on_line h a c; i = lc_tangent i g c, on_line i b c; j = on_circle j d c, on_bline j c b; k = on_circle k d a, on_bline k a c; l = on_line l j i, on_line l k h; m = on_line m c l, on_line m i h; n = on_tline n i b i ? eqangle m b m i m h m a
I0123 14:15:53.875482 139995469987840 ddar.py:60] Depth 1/1000 time = 2.676797389984131
I0123 14:15:57.243283 139995469987840 ddar.py:60] Depth 2/1000 time = 3.3675715923309326
I0123 14:16:01.811383 139995469987840 ddar.py:60] Depth 3/1000 time = 4.567666053771973
I0123 14:16:07.333065 139995469987840 ddar.py:60] Depth 4/1000 time = 5.519185304641724
I0123 14:16:14.678565 139995469987840 ddar.py:60] Depth 5/1000 time = 7.345257997512817
I0123 14:16:22.426155 139995469987840 ddar.py:60] Depth 6/1000 time = 7.747259616851807
I0123 14:16:35.487307 139995469987840 ddar.py:60] Depth 7/1000 time = 13.060738325119019
I0123 14:16:47.937543 139995469987840 ddar.py:60] Depth 8/1000 time = 12.449887990951538
I0123 14:17:05.125787 139995469987840 ddar.py:60] Depth 9/1000 time = 17.187748193740845
I0123 14:17:23.368961 139995469987840 ddar.py:60] Depth 10/1000 time = 18.242638111114502
I0123 14:17:41.606448 139995469987840 ddar.py:60] Depth 11/1000 time = 18.237005710601807
I0123 14:17:59.848688 139995469987840 ddar.py:60] Depth 12/1000 time = 18.177140474319458
I0123 14:18:19.011287 139995469987840 ddar.py:60] Depth 13/1000 time = 19.035089254379272
I0123 14:18:38.110337 139995469987840 ddar.py:60] Depth 14/1000 time = 19.085309743881226
I0123 14:18:38.111027 139995469987840 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:18:38.111156 139995469987840 alphageometry.py:566] LM output (score=-2.997913): "n : T d n i l 22 ;"
I0123 14:18:38.111194 139995469987840 alphageometry.py:567] Translation: "n = on_tline n d i l"

I0123 14:18:38.111260 139995469987840 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = lc_tangent h g c, on_line h a c; i = lc_tangent i g c, on_line i b c; j = on_circle j d c, on_bline j c b; k = on_circle k d a, on_bline k a c; l = on_line l j i, on_line l k h; m = on_line m c l, on_line m i h; n = on_tline n d i l ? eqangle m b m i m h m a"
I0123 14:18:38.111469 139995469987840 graph.py:498] 
I0123 14:18:38.111537 139995469987840 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = angle_bisector e b c a, on_line e b a; f = angle_bisector f c b a, on_line f c a; g = on_line g c e, on_line g b f; h = lc_tangent h g c, on_line h a c; i = lc_tangent i g c, on_line i b c; j = on_circle j d c, on_bline j c b; k = on_circle k d a, on_bline k a c; l = on_line l j i, on_line l k h; m = on_line m c l, on_line m i h; n = on_tline n d i l ? eqangle m b m i m h m a
I0123 14:18:40.871708 139995469987840 ddar.py:60] Depth 1/1000 time = 2.688032388687134
I0123 14:18:44.565948 139995469987840 ddar.py:60] Depth 2/1000 time = 3.69400954246521
I0123 14:18:49.257354 139995469987840 ddar.py:60] Depth 3/1000 time = 4.691143989562988
I0123 14:18:54.914620 139995469987840 ddar.py:60] Depth 4/1000 time = 5.654449939727783
I0123 14:19:01.621824 139995469987840 ddar.py:60] Depth 5/1000 time = 6.706963300704956
I0123 14:19:09.433002 139995469987840 ddar.py:60] Depth 6/1000 time = 7.810859203338623
I0123 14:19:22.472057 139995469987840 ddar.py:60] Depth 7/1000 time = 13.03864312171936
I0123 14:19:34.972355 139995469987840 ddar.py:60] Depth 8/1000 time = 12.49994444847107
I0123 14:19:53.911878 139995469987840 ddar.py:60] Depth 9/1000 time = 18.939138650894165
I0123 14:20:12.435224 139995469987840 ddar.py:60] Depth 10/1000 time = 18.52292561531067
I0123 14:20:30.937293 139995469987840 ddar.py:60] Depth 11/1000 time = 18.501631021499634
I0123 14:20:50.496542 139995469987840 ddar.py:60] Depth 12/1000 time = 19.493680000305176
I0123 14:21:09.499994 139995469987840 ddar.py:60] Depth 13/1000 time = 18.856444120407104
I0123 14:21:09.514553 139995469987840 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 14:21:09.514603 139995469987840 alphageometry.py:585] Timeout.
