I0123 13:25:59.947128 139700209868800 inference_utils.py:69] Parsing gin configuration.
I0123 13:25:59.947226 139700209868800 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0123 13:25:59.947420 139700209868800 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0123 13:25:59.947451 139700209868800 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0123 13:25:59.947479 139700209868800 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0123 13:25:59.947506 139700209868800 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0123 13:25:59.947531 139700209868800 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0123 13:25:59.947557 139700209868800 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0123 13:25:59.947583 139700209868800 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0123 13:25:59.947608 139700209868800 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=32
I0123 13:25:59.947633 139700209868800 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0123 13:25:59.947658 139700209868800 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0123 13:25:59.947702 139700209868800 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0123 13:25:59.947835 139700209868800 resource_reader.py:55] Path not found: base_htrans.gin
I0123 13:25:59.948037 139700209868800 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0123 13:25:59.948133 139700209868800 resource_reader.py:55] Path not found: trainer_configuration.gin
I0123 13:25:59.954339 139700209868800 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0123 13:25:59.954456 139700209868800 resource_reader.py:55] Path not found: size/medium_150M.gin
I0123 13:25:59.954768 139700209868800 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0123 13:25:59.954870 139700209868800 resource_reader.py:55] Path not found: options/positions_t5.gin
I0123 13:25:59.955141 139700209868800 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0123 13:25:59.955240 139700209868800 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0123 13:25:59.955640 139700209868800 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0123 13:25:59.955737 139700209868800 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0123 13:25:59.959490 139700209868800 training_loop.py:334] ==== Training loop: initializing model ====
I0123 13:26:00.054341 139700209868800 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0123 13:26:00.055052 139700209868800 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0123 13:26:00.061786 139700209868800 training_loop.py:335] Process 0 of 1
I0123 13:26:00.061841 139700209868800 training_loop.py:336] Local device count = 1
I0123 13:26:00.061881 139700209868800 training_loop.py:337] Number of replicas = 1
I0123 13:26:00.061913 139700209868800 training_loop.py:339] Using random number seed 42
I0123 13:26:00.535080 139700209868800 training_loop.py:359] Initializing the model.
I0123 13:26:00.958754 139700209868800 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:00.958984 139700209868800 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 13:26:00.959084 139700209868800 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 13:26:00.959161 139700209868800 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 13:26:00.959236 139700209868800 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 13:26:00.959315 139700209868800 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 13:26:00.959387 139700209868800 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 13:26:00.959456 139700209868800 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 13:26:00.959525 139700209868800 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 13:26:00.959594 139700209868800 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 13:26:00.959662 139700209868800 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 13:26:00.959730 139700209868800 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 13:26:00.959798 139700209868800 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 13:26:00.959864 139700209868800 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 13:26:00.959903 139700209868800 attention.py:418] Single window, no scan.
I0123 13:26:00.959947 139700209868800 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 13:26:00.960060 139700209868800 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:26:00.960098 139700209868800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:26:00.960129 139700209868800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:26:00.962140 139700209868800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:00.967340 139700209868800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:26:00.978311 139700209868800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:00.978592 139700209868800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:26:00.982910 139700209868800 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:26:00.993482 139700209868800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:26:00.993538 139700209868800 attention.py:418] Single window, no scan.
I0123 13:26:00.993575 139700209868800 transformer_layer.py:389] tlayer: self-attention.
I0123 13:26:00.993607 139700209868800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:00.993677 139700209868800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:00.994858 139700209868800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:00.994935 139700209868800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:00.995627 139700209868800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:00.998056 139700209868800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.003693 139700209868800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.005392 139700209868800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.005471 139700209868800 transformer_layer.py:468] tlayer: End windows.
I0123 13:26:01.005507 139700209868800 transformer_layer.py:472] tlayer: final FFN.
I0123 13:26:01.005567 139700209868800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.005701 139700209868800 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:26:01.006036 139700209868800 nn_components.py:325] mlp: activation = None
I0123 13:26:01.006081 139700209868800 nn_components.py:329] mlp: final activation = None
I0123 13:26:01.007997 139700209868800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.008097 139700209868800 nn_components.py:261] mlp: residual
I0123 13:26:01.010957 139700209868800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.011038 139700209868800 transformer_base.py:443] tbase: final FFN
I0123 13:26:01.011535 139700209868800 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:26:01.021485 139700209868800 nn_components.py:329] mlp: final activation = None
I0123 13:26:01.030176 139700209868800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.030275 139700209868800 nn_components.py:261] mlp: residual
I0123 13:26:01.030567 139700209868800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.030649 139700209868800 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 13:26:01.030761 139700209868800 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:26:01.030799 139700209868800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:26:01.030830 139700209868800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:26:01.032681 139700209868800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.035142 139700209868800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:26:01.040704 139700209868800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.040968 139700209868800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:26:01.043584 139700209868800 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:26:01.047403 139700209868800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:26:01.047457 139700209868800 attention.py:418] Single window, no scan.
I0123 13:26:01.047493 139700209868800 transformer_layer.py:389] tlayer: self-attention.
I0123 13:26:01.047524 139700209868800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.047587 139700209868800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.048152 139700209868800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.048227 139700209868800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.048579 139700209868800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.049344 139700209868800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.051789 139700209868800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.052409 139700209868800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.052484 139700209868800 transformer_layer.py:468] tlayer: End windows.
I0123 13:26:01.052518 139700209868800 transformer_layer.py:472] tlayer: final FFN.
I0123 13:26:01.052575 139700209868800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.052699 139700209868800 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:26:01.053026 139700209868800 nn_components.py:325] mlp: activation = None
I0123 13:26:01.053068 139700209868800 nn_components.py:329] mlp: final activation = None
I0123 13:26:01.054999 139700209868800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.055096 139700209868800 nn_components.py:261] mlp: residual
I0123 13:26:01.057576 139700209868800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.057663 139700209868800 transformer_base.py:443] tbase: final FFN
I0123 13:26:01.058099 139700209868800 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:26:01.060384 139700209868800 nn_components.py:329] mlp: final activation = None
I0123 13:26:01.062823 139700209868800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.062975 139700209868800 nn_components.py:261] mlp: residual
I0123 13:26:01.063271 139700209868800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.063354 139700209868800 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 13:26:01.063464 139700209868800 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:26:01.063503 139700209868800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:26:01.063534 139700209868800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:26:01.065485 139700209868800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.067848 139700209868800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:26:01.073764 139700209868800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.074036 139700209868800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:26:01.076691 139700209868800 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:26:01.080857 139700209868800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:26:01.080916 139700209868800 attention.py:418] Single window, no scan.
I0123 13:26:01.080952 139700209868800 transformer_layer.py:389] tlayer: self-attention.
I0123 13:26:01.080983 139700209868800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.081045 139700209868800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.081623 139700209868800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.081706 139700209868800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.082062 139700209868800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.082828 139700209868800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.085298 139700209868800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.085977 139700209868800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.086055 139700209868800 transformer_layer.py:468] tlayer: End windows.
I0123 13:26:01.086090 139700209868800 transformer_layer.py:472] tlayer: final FFN.
I0123 13:26:01.086146 139700209868800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.086276 139700209868800 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:26:01.086602 139700209868800 nn_components.py:325] mlp: activation = None
I0123 13:26:01.086645 139700209868800 nn_components.py:329] mlp: final activation = None
I0123 13:26:01.088541 139700209868800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.088633 139700209868800 nn_components.py:261] mlp: residual
I0123 13:26:01.091114 139700209868800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.091200 139700209868800 transformer_base.py:443] tbase: final FFN
I0123 13:26:01.091695 139700209868800 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:26:01.093952 139700209868800 nn_components.py:329] mlp: final activation = None
I0123 13:26:01.095837 139700209868800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.095934 139700209868800 nn_components.py:261] mlp: residual
I0123 13:26:01.096217 139700209868800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.096296 139700209868800 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 13:26:01.096404 139700209868800 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:26:01.096442 139700209868800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:26:01.096472 139700209868800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:26:01.098380 139700209868800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.100750 139700209868800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:26:01.106345 139700209868800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.106603 139700209868800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:26:01.109211 139700209868800 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:26:01.113007 139700209868800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:26:01.113063 139700209868800 attention.py:418] Single window, no scan.
I0123 13:26:01.113099 139700209868800 transformer_layer.py:389] tlayer: self-attention.
I0123 13:26:01.113131 139700209868800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.113193 139700209868800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.113770 139700209868800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.113847 139700209868800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.114202 139700209868800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.114969 139700209868800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.117482 139700209868800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.118109 139700209868800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.118188 139700209868800 transformer_layer.py:468] tlayer: End windows.
I0123 13:26:01.118223 139700209868800 transformer_layer.py:472] tlayer: final FFN.
I0123 13:26:01.118281 139700209868800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.118409 139700209868800 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:26:01.118737 139700209868800 nn_components.py:325] mlp: activation = None
I0123 13:26:01.118780 139700209868800 nn_components.py:329] mlp: final activation = None
I0123 13:26:01.120669 139700209868800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.120762 139700209868800 nn_components.py:261] mlp: residual
I0123 13:26:01.123288 139700209868800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.123375 139700209868800 transformer_base.py:443] tbase: final FFN
I0123 13:26:01.123807 139700209868800 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:26:01.126074 139700209868800 nn_components.py:329] mlp: final activation = None
I0123 13:26:01.127962 139700209868800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.128059 139700209868800 nn_components.py:261] mlp: residual
I0123 13:26:01.128341 139700209868800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.128421 139700209868800 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 13:26:01.128529 139700209868800 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:26:01.128567 139700209868800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:26:01.128597 139700209868800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:26:01.130500 139700209868800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.132870 139700209868800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:26:01.138472 139700209868800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.138735 139700209868800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:26:01.141383 139700209868800 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:26:01.145135 139700209868800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:26:01.145190 139700209868800 attention.py:418] Single window, no scan.
I0123 13:26:01.145226 139700209868800 transformer_layer.py:389] tlayer: self-attention.
I0123 13:26:01.145258 139700209868800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.145320 139700209868800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.145891 139700209868800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.145970 139700209868800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.146332 139700209868800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.147091 139700209868800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.149934 139700209868800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.150552 139700209868800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.150630 139700209868800 transformer_layer.py:468] tlayer: End windows.
I0123 13:26:01.150665 139700209868800 transformer_layer.py:472] tlayer: final FFN.
I0123 13:26:01.150723 139700209868800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.150853 139700209868800 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:26:01.151180 139700209868800 nn_components.py:325] mlp: activation = None
I0123 13:26:01.151222 139700209868800 nn_components.py:329] mlp: final activation = None
I0123 13:26:01.153109 139700209868800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.153203 139700209868800 nn_components.py:261] mlp: residual
I0123 13:26:01.155733 139700209868800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.155811 139700209868800 transformer_base.py:443] tbase: final FFN
I0123 13:26:01.156238 139700209868800 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:26:01.158507 139700209868800 nn_components.py:329] mlp: final activation = None
I0123 13:26:01.160447 139700209868800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.160541 139700209868800 nn_components.py:261] mlp: residual
I0123 13:26:01.160831 139700209868800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.160911 139700209868800 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 13:26:01.161021 139700209868800 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:26:01.161059 139700209868800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:26:01.161090 139700209868800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:26:01.162941 139700209868800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.165298 139700209868800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:26:01.170879 139700209868800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.171139 139700209868800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:26:01.173812 139700209868800 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:26:01.177580 139700209868800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:26:01.177634 139700209868800 attention.py:418] Single window, no scan.
I0123 13:26:01.177678 139700209868800 transformer_layer.py:389] tlayer: self-attention.
I0123 13:26:01.177709 139700209868800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.177772 139700209868800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.178377 139700209868800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.178453 139700209868800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.178804 139700209868800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.179570 139700209868800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.182051 139700209868800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.182672 139700209868800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.182748 139700209868800 transformer_layer.py:468] tlayer: End windows.
I0123 13:26:01.182783 139700209868800 transformer_layer.py:472] tlayer: final FFN.
I0123 13:26:01.182841 139700209868800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.182965 139700209868800 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:26:01.183290 139700209868800 nn_components.py:325] mlp: activation = None
I0123 13:26:01.183333 139700209868800 nn_components.py:329] mlp: final activation = None
I0123 13:26:01.185218 139700209868800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.185310 139700209868800 nn_components.py:261] mlp: residual
I0123 13:26:01.187842 139700209868800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.187925 139700209868800 transformer_base.py:443] tbase: final FFN
I0123 13:26:01.188369 139700209868800 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:26:01.190661 139700209868800 nn_components.py:329] mlp: final activation = None
I0123 13:26:01.192562 139700209868800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.192655 139700209868800 nn_components.py:261] mlp: residual
I0123 13:26:01.192941 139700209868800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.193021 139700209868800 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 13:26:01.193129 139700209868800 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:26:01.193168 139700209868800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:26:01.193199 139700209868800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:26:01.195050 139700209868800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.197481 139700209868800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:26:01.203032 139700209868800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.203295 139700209868800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:26:01.205874 139700209868800 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:26:01.209617 139700209868800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:26:01.209678 139700209868800 attention.py:418] Single window, no scan.
I0123 13:26:01.209714 139700209868800 transformer_layer.py:389] tlayer: self-attention.
I0123 13:26:01.209744 139700209868800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.209808 139700209868800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.210373 139700209868800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.210448 139700209868800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.210798 139700209868800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.211553 139700209868800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.214012 139700209868800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.214626 139700209868800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.214705 139700209868800 transformer_layer.py:468] tlayer: End windows.
I0123 13:26:01.214740 139700209868800 transformer_layer.py:472] tlayer: final FFN.
I0123 13:26:01.214797 139700209868800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.214921 139700209868800 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:26:01.215245 139700209868800 nn_components.py:325] mlp: activation = None
I0123 13:26:01.215288 139700209868800 nn_components.py:329] mlp: final activation = None
I0123 13:26:01.217225 139700209868800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.217318 139700209868800 nn_components.py:261] mlp: residual
I0123 13:26:01.219852 139700209868800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.219933 139700209868800 transformer_base.py:443] tbase: final FFN
I0123 13:26:01.220367 139700209868800 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:26:01.222971 139700209868800 nn_components.py:329] mlp: final activation = None
I0123 13:26:01.224850 139700209868800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.224951 139700209868800 nn_components.py:261] mlp: residual
I0123 13:26:01.225236 139700209868800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.225316 139700209868800 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 13:26:01.225425 139700209868800 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:26:01.225463 139700209868800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:26:01.225494 139700209868800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:26:01.364461 139700209868800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.367604 139700209868800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:26:01.373900 139700209868800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.374198 139700209868800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:26:01.376880 139700209868800 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:26:01.380834 139700209868800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:26:01.380893 139700209868800 attention.py:418] Single window, no scan.
I0123 13:26:01.380930 139700209868800 transformer_layer.py:389] tlayer: self-attention.
I0123 13:26:01.380962 139700209868800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.381029 139700209868800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.381652 139700209868800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.381731 139700209868800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.382088 139700209868800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.382867 139700209868800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.385411 139700209868800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.386056 139700209868800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.386135 139700209868800 transformer_layer.py:468] tlayer: End windows.
I0123 13:26:01.386169 139700209868800 transformer_layer.py:472] tlayer: final FFN.
I0123 13:26:01.386228 139700209868800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.386355 139700209868800 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:26:01.386701 139700209868800 nn_components.py:325] mlp: activation = None
I0123 13:26:01.386744 139700209868800 nn_components.py:329] mlp: final activation = None
I0123 13:26:01.388639 139700209868800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.388732 139700209868800 nn_components.py:261] mlp: residual
I0123 13:26:01.391279 139700209868800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.391360 139700209868800 transformer_base.py:443] tbase: final FFN
I0123 13:26:01.391805 139700209868800 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:26:01.394112 139700209868800 nn_components.py:329] mlp: final activation = None
I0123 13:26:01.396002 139700209868800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.396106 139700209868800 nn_components.py:261] mlp: residual
I0123 13:26:01.396390 139700209868800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.396473 139700209868800 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 13:26:01.396584 139700209868800 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:26:01.396621 139700209868800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:26:01.396652 139700209868800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:26:01.398609 139700209868800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.400975 139700209868800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:26:01.406584 139700209868800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.406848 139700209868800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:26:01.409505 139700209868800 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:26:01.413310 139700209868800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:26:01.413364 139700209868800 attention.py:418] Single window, no scan.
I0123 13:26:01.413401 139700209868800 transformer_layer.py:389] tlayer: self-attention.
I0123 13:26:01.413433 139700209868800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.413495 139700209868800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.414067 139700209868800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.414143 139700209868800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.414492 139700209868800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.415266 139700209868800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.417808 139700209868800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.418425 139700209868800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.418503 139700209868800 transformer_layer.py:468] tlayer: End windows.
I0123 13:26:01.418538 139700209868800 transformer_layer.py:472] tlayer: final FFN.
I0123 13:26:01.418596 139700209868800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.418720 139700209868800 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:26:01.419044 139700209868800 nn_components.py:325] mlp: activation = None
I0123 13:26:01.419087 139700209868800 nn_components.py:329] mlp: final activation = None
I0123 13:26:01.420961 139700209868800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.421052 139700209868800 nn_components.py:261] mlp: residual
I0123 13:26:01.423597 139700209868800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.423678 139700209868800 transformer_base.py:443] tbase: final FFN
I0123 13:26:01.424117 139700209868800 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:26:01.426387 139700209868800 nn_components.py:329] mlp: final activation = None
I0123 13:26:01.428332 139700209868800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.428426 139700209868800 nn_components.py:261] mlp: residual
I0123 13:26:01.428725 139700209868800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.428816 139700209868800 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 13:26:01.428930 139700209868800 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:26:01.428970 139700209868800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:26:01.429000 139700209868800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:26:01.430869 139700209868800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.433301 139700209868800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:26:01.438788 139700209868800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.439050 139700209868800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:26:01.442047 139700209868800 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:26:01.445786 139700209868800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:26:01.445842 139700209868800 attention.py:418] Single window, no scan.
I0123 13:26:01.445878 139700209868800 transformer_layer.py:389] tlayer: self-attention.
I0123 13:26:01.445910 139700209868800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.445972 139700209868800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.446572 139700209868800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.446648 139700209868800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.447001 139700209868800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.447768 139700209868800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.450201 139700209868800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.450817 139700209868800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.450897 139700209868800 transformer_layer.py:468] tlayer: End windows.
I0123 13:26:01.450932 139700209868800 transformer_layer.py:472] tlayer: final FFN.
I0123 13:26:01.450991 139700209868800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.451119 139700209868800 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:26:01.451446 139700209868800 nn_components.py:325] mlp: activation = None
I0123 13:26:01.451488 139700209868800 nn_components.py:329] mlp: final activation = None
I0123 13:26:01.453384 139700209868800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.453476 139700209868800 nn_components.py:261] mlp: residual
I0123 13:26:01.455994 139700209868800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.456077 139700209868800 transformer_base.py:443] tbase: final FFN
I0123 13:26:01.456511 139700209868800 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:26:01.458799 139700209868800 nn_components.py:329] mlp: final activation = None
I0123 13:26:01.460685 139700209868800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.460784 139700209868800 nn_components.py:261] mlp: residual
I0123 13:26:01.461065 139700209868800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.461149 139700209868800 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 13:26:01.461261 139700209868800 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:26:01.461299 139700209868800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:26:01.461330 139700209868800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:26:01.463169 139700209868800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.465590 139700209868800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:26:01.471376 139700209868800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.471638 139700209868800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:26:01.474229 139700209868800 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:26:01.477970 139700209868800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:26:01.478026 139700209868800 attention.py:418] Single window, no scan.
I0123 13:26:01.478062 139700209868800 transformer_layer.py:389] tlayer: self-attention.
I0123 13:26:01.478093 139700209868800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.478154 139700209868800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.478713 139700209868800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.478789 139700209868800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.479141 139700209868800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.479908 139700209868800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.482333 139700209868800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.482954 139700209868800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.483029 139700209868800 transformer_layer.py:468] tlayer: End windows.
I0123 13:26:01.483062 139700209868800 transformer_layer.py:472] tlayer: final FFN.
I0123 13:26:01.483124 139700209868800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.483248 139700209868800 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:26:01.483565 139700209868800 nn_components.py:325] mlp: activation = None
I0123 13:26:01.483606 139700209868800 nn_components.py:329] mlp: final activation = None
I0123 13:26:01.485533 139700209868800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.485626 139700209868800 nn_components.py:261] mlp: residual
I0123 13:26:01.488325 139700209868800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.488405 139700209868800 transformer_base.py:443] tbase: final FFN
I0123 13:26:01.488835 139700209868800 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:26:01.491118 139700209868800 nn_components.py:329] mlp: final activation = None
I0123 13:26:01.492980 139700209868800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.493075 139700209868800 nn_components.py:261] mlp: residual
I0123 13:26:01.493372 139700209868800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.493452 139700209868800 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 13:26:01.493566 139700209868800 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:26:01.493605 139700209868800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:26:01.493636 139700209868800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:26:01.495540 139700209868800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.497884 139700209868800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:26:01.503401 139700209868800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.503661 139700209868800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:26:01.506238 139700209868800 transformer_layer.py:213] tlayer: windowed attention.
I0123 13:26:01.510059 139700209868800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:26:01.510113 139700209868800 attention.py:418] Single window, no scan.
I0123 13:26:01.510149 139700209868800 transformer_layer.py:389] tlayer: self-attention.
I0123 13:26:01.510180 139700209868800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.510240 139700209868800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.510801 139700209868800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.510876 139700209868800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.511220 139700209868800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.511991 139700209868800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.514437 139700209868800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.515424 139700209868800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.515503 139700209868800 transformer_layer.py:468] tlayer: End windows.
I0123 13:26:01.515538 139700209868800 transformer_layer.py:472] tlayer: final FFN.
I0123 13:26:01.515598 139700209868800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.515727 139700209868800 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:26:01.516052 139700209868800 nn_components.py:325] mlp: activation = None
I0123 13:26:01.516094 139700209868800 nn_components.py:329] mlp: final activation = None
I0123 13:26:01.517983 139700209868800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.518076 139700209868800 nn_components.py:261] mlp: residual
I0123 13:26:01.520506 139700209868800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.520585 139700209868800 transformer_base.py:443] tbase: final FFN
I0123 13:26:01.521067 139700209868800 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:26:01.523318 139700209868800 nn_components.py:329] mlp: final activation = None
I0123 13:26:01.525195 139700209868800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.525289 139700209868800 nn_components.py:261] mlp: residual
I0123 13:26:01.525568 139700209868800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.525858 139700209868800 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 13:26:01.525929 139700209868800 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 13:26:01.525994 139700209868800 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 13:26:01.526053 139700209868800 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 13:26:01.526109 139700209868800 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 13:26:01.526165 139700209868800 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 13:26:01.526220 139700209868800 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 13:26:01.526274 139700209868800 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 13:26:01.526327 139700209868800 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 13:26:01.526378 139700209868800 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 13:26:01.526430 139700209868800 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 13:26:01.526483 139700209868800 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 13:26:01.526520 139700209868800 decoder_stack.py:344] dstack: Final layernorm.
I0123 13:26:01.529998 139700209868800 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 13:26:01.577906 139700209868800 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.577990 139700209868800 decoder_stack.py:333] dstack: autoregressive generator.
I0123 13:26:01.578044 139700209868800 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 13:26:01.578148 139700209868800 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:26:01.578186 139700209868800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:26:01.578216 139700209868800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:26:01.578282 139700209868800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.580668 139700209868800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:26:01.586091 139700209868800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.586351 139700209868800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:26:01.588977 139700209868800 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:26:01.605397 139700209868800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:26:01.605453 139700209868800 attention.py:418] Single window, no scan.
I0123 13:26:01.605488 139700209868800 transformer_layer.py:389] tlayer: self-attention.
I0123 13:26:01.605518 139700209868800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.605580 139700209868800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.606694 139700209868800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.606772 139700209868800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.607456 139700209868800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.609418 139700209868800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.614125 139700209868800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.615417 139700209868800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.615503 139700209868800 transformer_layer.py:468] tlayer: End windows.
I0123 13:26:01.615540 139700209868800 transformer_layer.py:472] tlayer: final FFN.
I0123 13:26:01.615599 139700209868800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.615731 139700209868800 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:26:01.615843 139700209868800 nn_components.py:325] mlp: activation = None
I0123 13:26:01.615883 139700209868800 nn_components.py:329] mlp: final activation = None
I0123 13:26:01.617761 139700209868800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.617856 139700209868800 nn_components.py:261] mlp: residual
I0123 13:26:01.620243 139700209868800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.620325 139700209868800 transformer_base.py:443] tbase: final FFN
I0123 13:26:01.620434 139700209868800 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:26:01.622648 139700209868800 nn_components.py:329] mlp: final activation = None
I0123 13:26:01.624580 139700209868800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.624675 139700209868800 nn_components.py:261] mlp: residual
I0123 13:26:01.624961 139700209868800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.625042 139700209868800 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 13:26:01.625152 139700209868800 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:26:01.625192 139700209868800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:26:01.625225 139700209868800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:26:01.625289 139700209868800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.627529 139700209868800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:26:01.632961 139700209868800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.633221 139700209868800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:26:01.635863 139700209868800 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:26:01.648949 139700209868800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:26:01.649005 139700209868800 attention.py:418] Single window, no scan.
I0123 13:26:01.649041 139700209868800 transformer_layer.py:389] tlayer: self-attention.
I0123 13:26:01.649072 139700209868800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.649135 139700209868800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.649698 139700209868800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.649775 139700209868800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.650123 139700209868800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.650796 139700209868800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.653244 139700209868800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.653864 139700209868800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.653941 139700209868800 transformer_layer.py:468] tlayer: End windows.
I0123 13:26:01.653982 139700209868800 transformer_layer.py:472] tlayer: final FFN.
I0123 13:26:01.654041 139700209868800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.654171 139700209868800 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:26:01.654280 139700209868800 nn_components.py:325] mlp: activation = None
I0123 13:26:01.654317 139700209868800 nn_components.py:329] mlp: final activation = None
I0123 13:26:01.656226 139700209868800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.656319 139700209868800 nn_components.py:261] mlp: residual
I0123 13:26:01.658684 139700209868800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.658764 139700209868800 transformer_base.py:443] tbase: final FFN
I0123 13:26:01.658870 139700209868800 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:26:01.661073 139700209868800 nn_components.py:329] mlp: final activation = None
I0123 13:26:01.662977 139700209868800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.663072 139700209868800 nn_components.py:261] mlp: residual
I0123 13:26:01.663353 139700209868800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.663432 139700209868800 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 13:26:01.663540 139700209868800 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:26:01.663579 139700209868800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:26:01.663610 139700209868800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:26:01.663673 139700209868800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.665875 139700209868800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:26:01.671407 139700209868800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.671667 139700209868800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:26:01.674328 139700209868800 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:26:01.686879 139700209868800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:26:01.686935 139700209868800 attention.py:418] Single window, no scan.
I0123 13:26:01.686971 139700209868800 transformer_layer.py:389] tlayer: self-attention.
I0123 13:26:01.687002 139700209868800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.687065 139700209868800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.687616 139700209868800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.687692 139700209868800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.688040 139700209868800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.688724 139700209868800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.691135 139700209868800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.691742 139700209868800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.691819 139700209868800 transformer_layer.py:468] tlayer: End windows.
I0123 13:26:01.691853 139700209868800 transformer_layer.py:472] tlayer: final FFN.
I0123 13:26:01.691918 139700209868800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.692048 139700209868800 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:26:01.692158 139700209868800 nn_components.py:325] mlp: activation = None
I0123 13:26:01.692196 139700209868800 nn_components.py:329] mlp: final activation = None
I0123 13:26:01.694111 139700209868800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.694206 139700209868800 nn_components.py:261] mlp: residual
I0123 13:26:01.696578 139700209868800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.696656 139700209868800 transformer_base.py:443] tbase: final FFN
I0123 13:26:01.696763 139700209868800 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:26:01.698950 139700209868800 nn_components.py:329] mlp: final activation = None
I0123 13:26:01.700835 139700209868800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.700928 139700209868800 nn_components.py:261] mlp: residual
I0123 13:26:01.701206 139700209868800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.701286 139700209868800 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 13:26:01.701393 139700209868800 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:26:01.701431 139700209868800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:26:01.701463 139700209868800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:26:01.701525 139700209868800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.703707 139700209868800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:26:01.709042 139700209868800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.709300 139700209868800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:26:01.711919 139700209868800 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:26:01.728609 139700209868800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:26:01.728691 139700209868800 attention.py:418] Single window, no scan.
I0123 13:26:01.728729 139700209868800 transformer_layer.py:389] tlayer: self-attention.
I0123 13:26:01.728761 139700209868800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.728835 139700209868800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.729433 139700209868800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.729509 139700209868800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.729883 139700209868800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.730588 139700209868800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.733066 139700209868800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.733692 139700209868800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.733773 139700209868800 transformer_layer.py:468] tlayer: End windows.
I0123 13:26:01.733808 139700209868800 transformer_layer.py:472] tlayer: final FFN.
I0123 13:26:01.733870 139700209868800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.734008 139700209868800 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:26:01.734121 139700209868800 nn_components.py:325] mlp: activation = None
I0123 13:26:01.734159 139700209868800 nn_components.py:329] mlp: final activation = None
I0123 13:26:01.736159 139700209868800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.736254 139700209868800 nn_components.py:261] mlp: residual
I0123 13:26:01.738644 139700209868800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.738723 139700209868800 transformer_base.py:443] tbase: final FFN
I0123 13:26:01.738833 139700209868800 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:26:01.741057 139700209868800 nn_components.py:329] mlp: final activation = None
I0123 13:26:01.742921 139700209868800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.743016 139700209868800 nn_components.py:261] mlp: residual
I0123 13:26:01.743294 139700209868800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.743377 139700209868800 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 13:26:01.743490 139700209868800 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:26:01.743532 139700209868800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:26:01.743563 139700209868800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:26:01.743630 139700209868800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.746200 139700209868800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:26:01.751570 139700209868800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.751829 139700209868800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:26:01.754436 139700209868800 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:26:01.767146 139700209868800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:26:01.767200 139700209868800 attention.py:418] Single window, no scan.
I0123 13:26:01.767235 139700209868800 transformer_layer.py:389] tlayer: self-attention.
I0123 13:26:01.767265 139700209868800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.767327 139700209868800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.767880 139700209868800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.767959 139700209868800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.768314 139700209868800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.768995 139700209868800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.771488 139700209868800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.772270 139700209868800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.772348 139700209868800 transformer_layer.py:468] tlayer: End windows.
I0123 13:26:01.772382 139700209868800 transformer_layer.py:472] tlayer: final FFN.
I0123 13:26:01.772440 139700209868800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.772576 139700209868800 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:26:01.772685 139700209868800 nn_components.py:325] mlp: activation = None
I0123 13:26:01.772723 139700209868800 nn_components.py:329] mlp: final activation = None
I0123 13:26:01.774708 139700209868800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.774801 139700209868800 nn_components.py:261] mlp: residual
I0123 13:26:01.777159 139700209868800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.777237 139700209868800 transformer_base.py:443] tbase: final FFN
I0123 13:26:01.777344 139700209868800 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:26:01.779620 139700209868800 nn_components.py:329] mlp: final activation = None
I0123 13:26:01.781461 139700209868800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.781555 139700209868800 nn_components.py:261] mlp: residual
I0123 13:26:01.781839 139700209868800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.781920 139700209868800 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 13:26:01.782027 139700209868800 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:26:01.782066 139700209868800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:26:01.782097 139700209868800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:26:01.782161 139700209868800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.784367 139700209868800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:26:01.789739 139700209868800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.789996 139700209868800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:26:01.792636 139700209868800 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:26:01.805289 139700209868800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:26:01.805344 139700209868800 attention.py:418] Single window, no scan.
I0123 13:26:01.805380 139700209868800 transformer_layer.py:389] tlayer: self-attention.
I0123 13:26:01.805411 139700209868800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.805472 139700209868800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.806033 139700209868800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.806112 139700209868800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.806464 139700209868800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.807159 139700209868800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.809591 139700209868800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.810235 139700209868800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.810318 139700209868800 transformer_layer.py:468] tlayer: End windows.
I0123 13:26:01.810354 139700209868800 transformer_layer.py:472] tlayer: final FFN.
I0123 13:26:01.810416 139700209868800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.810549 139700209868800 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:26:01.810671 139700209868800 nn_components.py:325] mlp: activation = None
I0123 13:26:01.810712 139700209868800 nn_components.py:329] mlp: final activation = None
I0123 13:26:01.812701 139700209868800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.812794 139700209868800 nn_components.py:261] mlp: residual
I0123 13:26:01.815170 139700209868800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.815250 139700209868800 transformer_base.py:443] tbase: final FFN
I0123 13:26:01.815360 139700209868800 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:26:01.817569 139700209868800 nn_components.py:329] mlp: final activation = None
I0123 13:26:01.819417 139700209868800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.819512 139700209868800 nn_components.py:261] mlp: residual
I0123 13:26:01.819794 139700209868800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.819876 139700209868800 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 13:26:01.819984 139700209868800 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:26:01.820023 139700209868800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:26:01.820055 139700209868800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:26:01.820119 139700209868800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.822345 139700209868800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:26:01.827820 139700209868800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.828078 139700209868800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:26:01.830651 139700209868800 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:26:01.843310 139700209868800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:26:01.843368 139700209868800 attention.py:418] Single window, no scan.
I0123 13:26:01.843404 139700209868800 transformer_layer.py:389] tlayer: self-attention.
I0123 13:26:01.843435 139700209868800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.843497 139700209868800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.844052 139700209868800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.844128 139700209868800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.844475 139700209868800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.845167 139700209868800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.847607 139700209868800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.848590 139700209868800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.848670 139700209868800 transformer_layer.py:468] tlayer: End windows.
I0123 13:26:01.848706 139700209868800 transformer_layer.py:472] tlayer: final FFN.
I0123 13:26:01.848766 139700209868800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.848897 139700209868800 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:26:01.849006 139700209868800 nn_components.py:325] mlp: activation = None
I0123 13:26:01.849050 139700209868800 nn_components.py:329] mlp: final activation = None
I0123 13:26:01.850937 139700209868800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.851032 139700209868800 nn_components.py:261] mlp: residual
I0123 13:26:01.853399 139700209868800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.853479 139700209868800 transformer_base.py:443] tbase: final FFN
I0123 13:26:01.853586 139700209868800 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:26:01.855778 139700209868800 nn_components.py:329] mlp: final activation = None
I0123 13:26:01.857686 139700209868800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.857782 139700209868800 nn_components.py:261] mlp: residual
I0123 13:26:01.858062 139700209868800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.858144 139700209868800 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 13:26:01.858253 139700209868800 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:26:01.858291 139700209868800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:26:01.858322 139700209868800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:26:01.858384 139700209868800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.860582 139700209868800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:26:01.865962 139700209868800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.866232 139700209868800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:26:01.868871 139700209868800 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:26:01.881744 139700209868800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:26:01.881801 139700209868800 attention.py:418] Single window, no scan.
I0123 13:26:01.881837 139700209868800 transformer_layer.py:389] tlayer: self-attention.
I0123 13:26:01.881868 139700209868800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.881930 139700209868800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.882527 139700209868800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.882604 139700209868800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.882957 139700209868800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.883640 139700209868800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.886080 139700209868800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.886698 139700209868800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.886777 139700209868800 transformer_layer.py:468] tlayer: End windows.
I0123 13:26:01.886812 139700209868800 transformer_layer.py:472] tlayer: final FFN.
I0123 13:26:01.886874 139700209868800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.887002 139700209868800 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:26:01.887114 139700209868800 nn_components.py:325] mlp: activation = None
I0123 13:26:01.887158 139700209868800 nn_components.py:329] mlp: final activation = None
I0123 13:26:01.889009 139700209868800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.889101 139700209868800 nn_components.py:261] mlp: residual
I0123 13:26:01.891513 139700209868800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.891593 139700209868800 transformer_base.py:443] tbase: final FFN
I0123 13:26:01.891700 139700209868800 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:26:01.893903 139700209868800 nn_components.py:329] mlp: final activation = None
I0123 13:26:01.895745 139700209868800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.895842 139700209868800 nn_components.py:261] mlp: residual
I0123 13:26:01.896123 139700209868800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.896203 139700209868800 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 13:26:01.896313 139700209868800 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:26:01.896353 139700209868800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:26:01.896384 139700209868800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:26:01.896448 139700209868800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.898658 139700209868800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:26:01.904091 139700209868800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.904355 139700209868800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:26:01.906948 139700209868800 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:26:01.919604 139700209868800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:26:01.919660 139700209868800 attention.py:418] Single window, no scan.
I0123 13:26:01.919695 139700209868800 transformer_layer.py:389] tlayer: self-attention.
I0123 13:26:01.919726 139700209868800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.919788 139700209868800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.920343 139700209868800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.920420 139700209868800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.920769 139700209868800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.921446 139700209868800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.923899 139700209868800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.924561 139700209868800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.924639 139700209868800 transformer_layer.py:468] tlayer: End windows.
I0123 13:26:01.924673 139700209868800 transformer_layer.py:472] tlayer: final FFN.
I0123 13:26:01.924731 139700209868800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.924864 139700209868800 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:26:01.924975 139700209868800 nn_components.py:325] mlp: activation = None
I0123 13:26:01.925013 139700209868800 nn_components.py:329] mlp: final activation = None
I0123 13:26:01.926876 139700209868800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.926969 139700209868800 nn_components.py:261] mlp: residual
I0123 13:26:01.929315 139700209868800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.929393 139700209868800 transformer_base.py:443] tbase: final FFN
I0123 13:26:01.929500 139700209868800 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:26:01.931714 139700209868800 nn_components.py:329] mlp: final activation = None
I0123 13:26:01.933617 139700209868800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.933722 139700209868800 nn_components.py:261] mlp: residual
I0123 13:26:01.934001 139700209868800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.934081 139700209868800 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 13:26:01.934189 139700209868800 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:26:01.934228 139700209868800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:26:01.934259 139700209868800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:26:01.934322 139700209868800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.936538 139700209868800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:26:01.941887 139700209868800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.942142 139700209868800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:26:01.944789 139700209868800 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:26:01.957688 139700209868800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:26:01.957744 139700209868800 attention.py:418] Single window, no scan.
I0123 13:26:01.957781 139700209868800 transformer_layer.py:389] tlayer: self-attention.
I0123 13:26:01.957811 139700209868800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.957873 139700209868800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.958471 139700209868800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.958548 139700209868800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.958901 139700209868800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.959580 139700209868800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.962037 139700209868800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.962656 139700209868800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.962733 139700209868800 transformer_layer.py:468] tlayer: End windows.
I0123 13:26:01.962768 139700209868800 transformer_layer.py:472] tlayer: final FFN.
I0123 13:26:01.962827 139700209868800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.962957 139700209868800 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:26:01.963066 139700209868800 nn_components.py:325] mlp: activation = None
I0123 13:26:01.963103 139700209868800 nn_components.py:329] mlp: final activation = None
I0123 13:26:01.964964 139700209868800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.965065 139700209868800 nn_components.py:261] mlp: residual
I0123 13:26:01.967493 139700209868800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.967573 139700209868800 transformer_base.py:443] tbase: final FFN
I0123 13:26:01.967681 139700209868800 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:26:01.969921 139700209868800 nn_components.py:329] mlp: final activation = None
I0123 13:26:01.971769 139700209868800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.971863 139700209868800 nn_components.py:261] mlp: residual
I0123 13:26:01.972141 139700209868800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.972222 139700209868800 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 13:26:01.972332 139700209868800 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:26:01.972372 139700209868800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:26:01.972402 139700209868800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:26:01.972465 139700209868800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.974854 139700209868800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:26:01.980334 139700209868800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.980593 139700209868800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:26:01.983205 139700209868800 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:26:01.995829 139700209868800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:26:01.995885 139700209868800 attention.py:418] Single window, no scan.
I0123 13:26:01.995921 139700209868800 transformer_layer.py:389] tlayer: self-attention.
I0123 13:26:01.995952 139700209868800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.996013 139700209868800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.996567 139700209868800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.996643 139700209868800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.996990 139700209868800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:01.997683 139700209868800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:02.000158 139700209868800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:02.000818 139700209868800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:02.000896 139700209868800 transformer_layer.py:468] tlayer: End windows.
I0123 13:26:02.000931 139700209868800 transformer_layer.py:472] tlayer: final FFN.
I0123 13:26:02.000990 139700209868800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:02.001117 139700209868800 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:26:02.001226 139700209868800 nn_components.py:325] mlp: activation = None
I0123 13:26:02.001264 139700209868800 nn_components.py:329] mlp: final activation = None
I0123 13:26:02.003149 139700209868800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:02.003249 139700209868800 nn_components.py:261] mlp: residual
I0123 13:26:02.005620 139700209868800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:02.005705 139700209868800 transformer_base.py:443] tbase: final FFN
I0123 13:26:02.005815 139700209868800 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:26:02.007998 139700209868800 nn_components.py:329] mlp: final activation = None
I0123 13:26:02.009912 139700209868800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:02.010008 139700209868800 nn_components.py:261] mlp: residual
I0123 13:26:02.010286 139700209868800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:02.010367 139700209868800 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 13:26:02.010476 139700209868800 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:26:02.010515 139700209868800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:26:02.010546 139700209868800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:26:02.010608 139700209868800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:02.012838 139700209868800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:26:02.018239 139700209868800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:02.018495 139700209868800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:26:02.021153 139700209868800 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:26:02.033770 139700209868800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:26:02.033826 139700209868800 attention.py:418] Single window, no scan.
I0123 13:26:02.033862 139700209868800 transformer_layer.py:389] tlayer: self-attention.
I0123 13:26:02.033892 139700209868800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:02.033954 139700209868800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:02.034501 139700209868800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:02.034578 139700209868800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:02.034930 139700209868800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:02.035662 139700209868800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:02.038123 139700209868800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:02.038737 139700209868800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:02.038814 139700209868800 transformer_layer.py:468] tlayer: End windows.
I0123 13:26:02.038848 139700209868800 transformer_layer.py:472] tlayer: final FFN.
I0123 13:26:02.038908 139700209868800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:02.039036 139700209868800 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:26:02.039149 139700209868800 nn_components.py:325] mlp: activation = None
I0123 13:26:02.039188 139700209868800 nn_components.py:329] mlp: final activation = None
I0123 13:26:02.041059 139700209868800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:02.041152 139700209868800 nn_components.py:261] mlp: residual
I0123 13:26:02.043540 139700209868800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:02.043620 139700209868800 transformer_base.py:443] tbase: final FFN
I0123 13:26:02.043727 139700209868800 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:26:02.046003 139700209868800 nn_components.py:329] mlp: final activation = None
I0123 13:26:02.047859 139700209868800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:02.047955 139700209868800 nn_components.py:261] mlp: residual
I0123 13:26:02.048233 139700209868800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:02.048321 139700209868800 decoder_stack.py:344] dstack: Final layernorm.
I0123 13:26:02.051176 139700209868800 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 13:26:02.107070 139700209868800 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.107154 139700209868800 decoder_stack.py:333] dstack: autoregressive generator.
I0123 13:26:02.107208 139700209868800 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 13:26:02.107312 139700209868800 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:26:02.107351 139700209868800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:26:02.107381 139700209868800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:26:02.107445 139700209868800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.110095 139700209868800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:26:02.115417 139700209868800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.115675 139700209868800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:26:02.118252 139700209868800 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:26:02.130596 139700209868800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:26:02.130652 139700209868800 attention.py:418] Single window, no scan.
I0123 13:26:02.130688 139700209868800 transformer_layer.py:389] tlayer: self-attention.
I0123 13:26:02.130719 139700209868800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.130781 139700209868800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.131329 139700209868800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.131405 139700209868800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.131752 139700209868800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.132419 139700209868800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.134875 139700209868800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.135477 139700209868800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.135554 139700209868800 transformer_layer.py:468] tlayer: End windows.
I0123 13:26:02.135588 139700209868800 transformer_layer.py:472] tlayer: final FFN.
I0123 13:26:02.135646 139700209868800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.135772 139700209868800 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:26:02.135886 139700209868800 nn_components.py:325] mlp: activation = None
I0123 13:26:02.135925 139700209868800 nn_components.py:329] mlp: final activation = None
I0123 13:26:02.137748 139700209868800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.137842 139700209868800 nn_components.py:261] mlp: residual
I0123 13:26:02.140193 139700209868800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.140271 139700209868800 transformer_base.py:443] tbase: final FFN
I0123 13:26:02.140379 139700209868800 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:26:02.142609 139700209868800 nn_components.py:329] mlp: final activation = None
I0123 13:26:02.144445 139700209868800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.144541 139700209868800 nn_components.py:261] mlp: residual
I0123 13:26:02.144824 139700209868800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.144905 139700209868800 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 13:26:02.145013 139700209868800 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:26:02.145052 139700209868800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:26:02.145082 139700209868800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:26:02.145147 139700209868800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.147346 139700209868800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:26:02.152648 139700209868800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.152905 139700209868800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:26:02.155512 139700209868800 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:26:02.167794 139700209868800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:26:02.167851 139700209868800 attention.py:418] Single window, no scan.
I0123 13:26:02.167887 139700209868800 transformer_layer.py:389] tlayer: self-attention.
I0123 13:26:02.167917 139700209868800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.167980 139700209868800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.168534 139700209868800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.168611 139700209868800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.168959 139700209868800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.169616 139700209868800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.172077 139700209868800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.172686 139700209868800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.172764 139700209868800 transformer_layer.py:468] tlayer: End windows.
I0123 13:26:02.172799 139700209868800 transformer_layer.py:472] tlayer: final FFN.
I0123 13:26:02.172857 139700209868800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.172985 139700209868800 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:26:02.173094 139700209868800 nn_components.py:325] mlp: activation = None
I0123 13:26:02.173138 139700209868800 nn_components.py:329] mlp: final activation = None
I0123 13:26:02.174973 139700209868800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.175067 139700209868800 nn_components.py:261] mlp: residual
I0123 13:26:02.177407 139700209868800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.177487 139700209868800 transformer_base.py:443] tbase: final FFN
I0123 13:26:02.177595 139700209868800 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:26:02.179830 139700209868800 nn_components.py:329] mlp: final activation = None
I0123 13:26:02.181660 139700209868800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.181758 139700209868800 nn_components.py:261] mlp: residual
I0123 13:26:02.182040 139700209868800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.182121 139700209868800 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 13:26:02.182228 139700209868800 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:26:02.182267 139700209868800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:26:02.182297 139700209868800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:26:02.182360 139700209868800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.184539 139700209868800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:26:02.189821 139700209868800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.190078 139700209868800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:26:02.192672 139700209868800 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:26:02.204898 139700209868800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:26:02.204953 139700209868800 attention.py:418] Single window, no scan.
I0123 13:26:02.204989 139700209868800 transformer_layer.py:389] tlayer: self-attention.
I0123 13:26:02.205020 139700209868800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.205082 139700209868800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.205632 139700209868800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.205715 139700209868800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.206071 139700209868800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.206748 139700209868800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.209221 139700209868800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.209835 139700209868800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.209913 139700209868800 transformer_layer.py:468] tlayer: End windows.
I0123 13:26:02.209948 139700209868800 transformer_layer.py:472] tlayer: final FFN.
I0123 13:26:02.210006 139700209868800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.210287 139700209868800 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:26:02.210396 139700209868800 nn_components.py:325] mlp: activation = None
I0123 13:26:02.210435 139700209868800 nn_components.py:329] mlp: final activation = None
I0123 13:26:02.212244 139700209868800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.212527 139700209868800 nn_components.py:261] mlp: residual
I0123 13:26:02.214853 139700209868800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.214934 139700209868800 transformer_base.py:443] tbase: final FFN
I0123 13:26:02.215043 139700209868800 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:26:02.217712 139700209868800 nn_components.py:329] mlp: final activation = None
I0123 13:26:02.219524 139700209868800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.219619 139700209868800 nn_components.py:261] mlp: residual
I0123 13:26:02.219902 139700209868800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.219982 139700209868800 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 13:26:02.220089 139700209868800 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:26:02.220127 139700209868800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:26:02.220157 139700209868800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:26:02.220218 139700209868800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.222408 139700209868800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:26:02.227644 139700209868800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.227901 139700209868800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:26:02.230550 139700209868800 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:26:02.242911 139700209868800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:26:02.242968 139700209868800 attention.py:418] Single window, no scan.
I0123 13:26:02.243008 139700209868800 transformer_layer.py:389] tlayer: self-attention.
I0123 13:26:02.243047 139700209868800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.243110 139700209868800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.243672 139700209868800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.243746 139700209868800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.244094 139700209868800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.244771 139700209868800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.247252 139700209868800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.247863 139700209868800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.247939 139700209868800 transformer_layer.py:468] tlayer: End windows.
I0123 13:26:02.247973 139700209868800 transformer_layer.py:472] tlayer: final FFN.
I0123 13:26:02.248031 139700209868800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.248157 139700209868800 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:26:02.248264 139700209868800 nn_components.py:325] mlp: activation = None
I0123 13:26:02.248303 139700209868800 nn_components.py:329] mlp: final activation = None
I0123 13:26:02.250151 139700209868800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.250244 139700209868800 nn_components.py:261] mlp: residual
I0123 13:26:02.252582 139700209868800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.252658 139700209868800 transformer_base.py:443] tbase: final FFN
I0123 13:26:02.252765 139700209868800 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:26:02.255039 139700209868800 nn_components.py:329] mlp: final activation = None
I0123 13:26:02.256873 139700209868800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.256968 139700209868800 nn_components.py:261] mlp: residual
I0123 13:26:02.257247 139700209868800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.257326 139700209868800 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 13:26:02.257434 139700209868800 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:26:02.257472 139700209868800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:26:02.257502 139700209868800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:26:02.257564 139700209868800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.259776 139700209868800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:26:02.265087 139700209868800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.265340 139700209868800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:26:02.267965 139700209868800 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:26:02.280316 139700209868800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:26:02.280370 139700209868800 attention.py:418] Single window, no scan.
I0123 13:26:02.280405 139700209868800 transformer_layer.py:389] tlayer: self-attention.
I0123 13:26:02.280434 139700209868800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.280494 139700209868800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.281043 139700209868800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.281117 139700209868800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.281461 139700209868800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.282143 139700209868800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.284613 139700209868800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.285233 139700209868800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.285309 139700209868800 transformer_layer.py:468] tlayer: End windows.
I0123 13:26:02.285342 139700209868800 transformer_layer.py:472] tlayer: final FFN.
I0123 13:26:02.285400 139700209868800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.285533 139700209868800 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:26:02.285650 139700209868800 nn_components.py:325] mlp: activation = None
I0123 13:26:02.285690 139700209868800 nn_components.py:329] mlp: final activation = None
I0123 13:26:02.287522 139700209868800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.287618 139700209868800 nn_components.py:261] mlp: residual
I0123 13:26:02.289980 139700209868800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.290059 139700209868800 transformer_base.py:443] tbase: final FFN
I0123 13:26:02.290166 139700209868800 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:26:02.292404 139700209868800 nn_components.py:329] mlp: final activation = None
I0123 13:26:02.294247 139700209868800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.294341 139700209868800 nn_components.py:261] mlp: residual
I0123 13:26:02.294618 139700209868800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.294697 139700209868800 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 13:26:02.294806 139700209868800 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:26:02.294843 139700209868800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:26:02.294873 139700209868800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:26:02.294934 139700209868800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.297138 139700209868800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:26:02.302465 139700209868800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.302721 139700209868800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:26:02.305370 139700209868800 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:26:02.317811 139700209868800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:26:02.317865 139700209868800 attention.py:418] Single window, no scan.
I0123 13:26:02.317899 139700209868800 transformer_layer.py:389] tlayer: self-attention.
I0123 13:26:02.317928 139700209868800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.317992 139700209868800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.318537 139700209868800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.318612 139700209868800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.318956 139700209868800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.319625 139700209868800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.322124 139700209868800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.322743 139700209868800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.322820 139700209868800 transformer_layer.py:468] tlayer: End windows.
I0123 13:26:02.322854 139700209868800 transformer_layer.py:472] tlayer: final FFN.
I0123 13:26:02.322911 139700209868800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.323034 139700209868800 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:26:02.323140 139700209868800 nn_components.py:325] mlp: activation = None
I0123 13:26:02.323177 139700209868800 nn_components.py:329] mlp: final activation = None
I0123 13:26:02.325005 139700209868800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.325102 139700209868800 nn_components.py:261] mlp: residual
I0123 13:26:02.327448 139700209868800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.327526 139700209868800 transformer_base.py:443] tbase: final FFN
I0123 13:26:02.327632 139700209868800 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:26:02.330271 139700209868800 nn_components.py:329] mlp: final activation = None
I0123 13:26:02.332087 139700209868800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.332180 139700209868800 nn_components.py:261] mlp: residual
I0123 13:26:02.332454 139700209868800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.332534 139700209868800 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 13:26:02.332641 139700209868800 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:26:02.332678 139700209868800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:26:02.332707 139700209868800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:26:02.332770 139700209868800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.334968 139700209868800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:26:02.340423 139700209868800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.340679 139700209868800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:26:02.343495 139700209868800 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:26:02.355919 139700209868800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:26:02.355972 139700209868800 attention.py:418] Single window, no scan.
I0123 13:26:02.356008 139700209868800 transformer_layer.py:389] tlayer: self-attention.
I0123 13:26:02.356037 139700209868800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.356097 139700209868800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.356644 139700209868800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.356719 139700209868800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.357065 139700209868800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.357744 139700209868800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.360226 139700209868800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.360840 139700209868800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.360916 139700209868800 transformer_layer.py:468] tlayer: End windows.
I0123 13:26:02.360949 139700209868800 transformer_layer.py:472] tlayer: final FFN.
I0123 13:26:02.361007 139700209868800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.361130 139700209868800 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:26:02.361237 139700209868800 nn_components.py:325] mlp: activation = None
I0123 13:26:02.361274 139700209868800 nn_components.py:329] mlp: final activation = None
I0123 13:26:02.363116 139700209868800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.363208 139700209868800 nn_components.py:261] mlp: residual
I0123 13:26:02.365544 139700209868800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.365622 139700209868800 transformer_base.py:443] tbase: final FFN
I0123 13:26:02.365735 139700209868800 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:26:02.367986 139700209868800 nn_components.py:329] mlp: final activation = None
I0123 13:26:02.369826 139700209868800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.369920 139700209868800 nn_components.py:261] mlp: residual
I0123 13:26:02.370199 139700209868800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.370280 139700209868800 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 13:26:02.370386 139700209868800 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:26:02.370423 139700209868800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:26:02.370452 139700209868800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:26:02.370514 139700209868800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.372696 139700209868800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:26:02.378036 139700209868800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.378289 139700209868800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:26:02.380921 139700209868800 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:26:02.393301 139700209868800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:26:02.393356 139700209868800 attention.py:418] Single window, no scan.
I0123 13:26:02.393390 139700209868800 transformer_layer.py:389] tlayer: self-attention.
I0123 13:26:02.393419 139700209868800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.393479 139700209868800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.394036 139700209868800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.394111 139700209868800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.394455 139700209868800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.395131 139700209868800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.397606 139700209868800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.398232 139700209868800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.398307 139700209868800 transformer_layer.py:468] tlayer: End windows.
I0123 13:26:02.398341 139700209868800 transformer_layer.py:472] tlayer: final FFN.
I0123 13:26:02.398397 139700209868800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.398522 139700209868800 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:26:02.398629 139700209868800 nn_components.py:325] mlp: activation = None
I0123 13:26:02.398665 139700209868800 nn_components.py:329] mlp: final activation = None
I0123 13:26:02.400474 139700209868800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.400565 139700209868800 nn_components.py:261] mlp: residual
I0123 13:26:02.402888 139700209868800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.402973 139700209868800 transformer_base.py:443] tbase: final FFN
I0123 13:26:02.403080 139700209868800 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:26:02.405306 139700209868800 nn_components.py:329] mlp: final activation = None
I0123 13:26:02.407125 139700209868800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.407219 139700209868800 nn_components.py:261] mlp: residual
I0123 13:26:02.407497 139700209868800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.407576 139700209868800 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 13:26:02.407683 139700209868800 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:26:02.407721 139700209868800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:26:02.407750 139700209868800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:26:02.407813 139700209868800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.410020 139700209868800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:26:02.415291 139700209868800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.415546 139700209868800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:26:02.418193 139700209868800 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:26:02.430606 139700209868800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:26:02.430659 139700209868800 attention.py:418] Single window, no scan.
I0123 13:26:02.430693 139700209868800 transformer_layer.py:389] tlayer: self-attention.
I0123 13:26:02.430723 139700209868800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.430782 139700209868800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.431328 139700209868800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.431402 139700209868800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.431751 139700209868800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.432429 139700209868800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.434910 139700209868800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.435534 139700209868800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.435610 139700209868800 transformer_layer.py:468] tlayer: End windows.
I0123 13:26:02.435644 139700209868800 transformer_layer.py:472] tlayer: final FFN.
I0123 13:26:02.435702 139700209868800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.435826 139700209868800 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:26:02.435933 139700209868800 nn_components.py:325] mlp: activation = None
I0123 13:26:02.435969 139700209868800 nn_components.py:329] mlp: final activation = None
I0123 13:26:02.437810 139700209868800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.437901 139700209868800 nn_components.py:261] mlp: residual
I0123 13:26:02.440223 139700209868800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.440304 139700209868800 transformer_base.py:443] tbase: final FFN
I0123 13:26:02.440412 139700209868800 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:26:02.443051 139700209868800 nn_components.py:329] mlp: final activation = None
I0123 13:26:02.444950 139700209868800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.445043 139700209868800 nn_components.py:261] mlp: residual
I0123 13:26:02.445319 139700209868800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.445398 139700209868800 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 13:26:02.445505 139700209868800 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:26:02.445542 139700209868800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:26:02.445571 139700209868800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:26:02.445633 139700209868800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.447831 139700209868800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:26:02.453115 139700209868800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.453369 139700209868800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:26:02.455983 139700209868800 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:26:02.468381 139700209868800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:26:02.468436 139700209868800 attention.py:418] Single window, no scan.
I0123 13:26:02.468470 139700209868800 transformer_layer.py:389] tlayer: self-attention.
I0123 13:26:02.468499 139700209868800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.468560 139700209868800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.469114 139700209868800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.469189 139700209868800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.469534 139700209868800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.470214 139700209868800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.472685 139700209868800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.473299 139700209868800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.473374 139700209868800 transformer_layer.py:468] tlayer: End windows.
I0123 13:26:02.473407 139700209868800 transformer_layer.py:472] tlayer: final FFN.
I0123 13:26:02.473463 139700209868800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.473591 139700209868800 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:26:02.473710 139700209868800 nn_components.py:325] mlp: activation = None
I0123 13:26:02.473750 139700209868800 nn_components.py:329] mlp: final activation = None
I0123 13:26:02.476047 139700209868800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.476140 139700209868800 nn_components.py:261] mlp: residual
I0123 13:26:02.478463 139700209868800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.478542 139700209868800 transformer_base.py:443] tbase: final FFN
I0123 13:26:02.478652 139700209868800 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:26:02.480859 139700209868800 nn_components.py:329] mlp: final activation = None
I0123 13:26:02.482659 139700209868800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.482753 139700209868800 nn_components.py:261] mlp: residual
I0123 13:26:02.483036 139700209868800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.483115 139700209868800 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 13:26:02.483221 139700209868800 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:26:02.483259 139700209868800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:26:02.483288 139700209868800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:26:02.483349 139700209868800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.485528 139700209868800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:26:02.490813 139700209868800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.491067 139700209868800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:26:02.493699 139700209868800 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:26:02.506058 139700209868800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:26:02.506112 139700209868800 attention.py:418] Single window, no scan.
I0123 13:26:02.506145 139700209868800 transformer_layer.py:389] tlayer: self-attention.
I0123 13:26:02.506175 139700209868800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.506236 139700209868800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.506789 139700209868800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.506863 139700209868800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.507211 139700209868800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.507881 139700209868800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.510387 139700209868800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.511014 139700209868800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.511090 139700209868800 transformer_layer.py:468] tlayer: End windows.
I0123 13:26:02.511123 139700209868800 transformer_layer.py:472] tlayer: final FFN.
I0123 13:26:02.511179 139700209868800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.511304 139700209868800 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:26:02.511411 139700209868800 nn_components.py:325] mlp: activation = None
I0123 13:26:02.511448 139700209868800 nn_components.py:329] mlp: final activation = None
I0123 13:26:02.513274 139700209868800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.513366 139700209868800 nn_components.py:261] mlp: residual
I0123 13:26:02.515704 139700209868800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.515782 139700209868800 transformer_base.py:443] tbase: final FFN
I0123 13:26:02.515887 139700209868800 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:26:02.518123 139700209868800 nn_components.py:329] mlp: final activation = None
I0123 13:26:02.519942 139700209868800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.520035 139700209868800 nn_components.py:261] mlp: residual
I0123 13:26:02.520313 139700209868800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.520392 139700209868800 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 13:26:02.520498 139700209868800 transformer_layer.py:154] tlayer: recurrent = False
I0123 13:26:02.520535 139700209868800 transformer_layer.py:155] tlayer: compute_importance = False
I0123 13:26:02.520565 139700209868800 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 13:26:02.520625 139700209868800 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.522819 139700209868800 transformer_base.py:161] kvq: pre_attn dropout.
I0123 13:26:02.528093 139700209868800 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.528344 139700209868800 transformer_base.py:194] kvq: normalize keys, queries.
I0123 13:26:02.530967 139700209868800 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 13:26:02.543489 139700209868800 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 13:26:02.543543 139700209868800 attention.py:418] Single window, no scan.
I0123 13:26:02.543578 139700209868800 transformer_layer.py:389] tlayer: self-attention.
I0123 13:26:02.543607 139700209868800 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.543668 139700209868800 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.544396 139700209868800 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.544470 139700209868800 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.544821 139700209868800 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.545500 139700209868800 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.548048 139700209868800 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.548664 139700209868800 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.548739 139700209868800 transformer_layer.py:468] tlayer: End windows.
I0123 13:26:02.548773 139700209868800 transformer_layer.py:472] tlayer: final FFN.
I0123 13:26:02.548828 139700209868800 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.548952 139700209868800 transformer_base.py:410] tbase: post-attention MLP.
I0123 13:26:02.549060 139700209868800 nn_components.py:325] mlp: activation = None
I0123 13:26:02.549097 139700209868800 nn_components.py:329] mlp: final activation = None
I0123 13:26:02.550970 139700209868800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.551066 139700209868800 nn_components.py:261] mlp: residual
I0123 13:26:02.553419 139700209868800 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.553496 139700209868800 transformer_base.py:443] tbase: final FFN
I0123 13:26:02.553599 139700209868800 nn_components.py:320] mlp: hidden 4096, relu
I0123 13:26:02.556200 139700209868800 nn_components.py:329] mlp: final activation = None
I0123 13:26:02.558037 139700209868800 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.558131 139700209868800 nn_components.py:261] mlp: residual
I0123 13:26:02.558408 139700209868800 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:02.558492 139700209868800 decoder_stack.py:344] dstack: Final layernorm.
I0123 13:26:02.561261 139700209868800 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 13:26:06.978066 139700209868800 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
I0123 13:26:07.502314 139700209868800 training_loop.py:409] No working directory specified.
I0123 13:26:07.502430 139700209868800 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0123 13:26:07.503188 139700209868800 checkpoints.py:1062] Restoring legacy Flax checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0123 13:26:10.349687 139700209868800 training_loop.py:447] Only restoring trainable parameters.
I0123 13:26:10.350359 139700209868800 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0123 13:26:10.350416 139700209868800 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0123 13:26:10.350462 139700209868800 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 13:26:10.350505 139700209868800 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 13:26:10.350545 139700209868800 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:10.350583 139700209868800 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 13:26:10.350621 139700209868800 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:10.350659 139700209868800 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:10.350698 139700209868800 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 13:26:10.350736 139700209868800 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 13:26:10.350774 139700209868800 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:10.350811 139700209868800 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 13:26:10.350848 139700209868800 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 13:26:10.350884 139700209868800 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 13:26:10.350921 139700209868800 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:10.350957 139700209868800 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 13:26:10.350993 139700209868800 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:10.351029 139700209868800 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:10.351065 139700209868800 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 13:26:10.351101 139700209868800 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 13:26:10.351150 139700209868800 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:10.351189 139700209868800 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 13:26:10.351226 139700209868800 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 13:26:10.351263 139700209868800 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 13:26:10.351299 139700209868800 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:10.351335 139700209868800 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 13:26:10.351371 139700209868800 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:10.351408 139700209868800 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:10.351444 139700209868800 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 13:26:10.351480 139700209868800 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 13:26:10.351516 139700209868800 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:10.351552 139700209868800 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 13:26:10.351588 139700209868800 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 13:26:10.351625 139700209868800 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 13:26:10.351662 139700209868800 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:10.351699 139700209868800 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 13:26:10.351735 139700209868800 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:10.351770 139700209868800 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:10.351806 139700209868800 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 13:26:10.351842 139700209868800 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 13:26:10.351878 139700209868800 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:10.351914 139700209868800 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 13:26:10.351950 139700209868800 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 13:26:10.351986 139700209868800 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 13:26:10.352022 139700209868800 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:10.352058 139700209868800 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 13:26:10.352099 139700209868800 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:10.352138 139700209868800 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:10.352174 139700209868800 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 13:26:10.352211 139700209868800 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 13:26:10.352247 139700209868800 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:10.352284 139700209868800 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 13:26:10.352319 139700209868800 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 13:26:10.352355 139700209868800 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 13:26:10.352391 139700209868800 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:10.352427 139700209868800 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 13:26:10.352463 139700209868800 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:10.352499 139700209868800 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:10.352536 139700209868800 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 13:26:10.352572 139700209868800 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 13:26:10.352607 139700209868800 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:10.352643 139700209868800 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 13:26:10.352679 139700209868800 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 13:26:10.352715 139700209868800 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 13:26:10.352751 139700209868800 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:10.352786 139700209868800 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 13:26:10.352822 139700209868800 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:10.352858 139700209868800 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:10.352894 139700209868800 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 13:26:10.352930 139700209868800 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 13:26:10.352967 139700209868800 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:10.353002 139700209868800 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 13:26:10.353039 139700209868800 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 13:26:10.353079 139700209868800 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 13:26:10.353117 139700209868800 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:10.353154 139700209868800 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 13:26:10.353190 139700209868800 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:10.353226 139700209868800 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:10.353262 139700209868800 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 13:26:10.353299 139700209868800 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 13:26:10.353335 139700209868800 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:10.353371 139700209868800 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 13:26:10.353407 139700209868800 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 13:26:10.353443 139700209868800 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 13:26:10.353480 139700209868800 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:10.353516 139700209868800 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 13:26:10.353553 139700209868800 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:10.353589 139700209868800 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:10.353626 139700209868800 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 13:26:10.353673 139700209868800 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 13:26:10.353711 139700209868800 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:10.353747 139700209868800 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 13:26:10.353784 139700209868800 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 13:26:10.353821 139700209868800 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 13:26:10.353857 139700209868800 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:10.353893 139700209868800 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 13:26:10.353929 139700209868800 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:10.353964 139700209868800 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:10.354001 139700209868800 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 13:26:10.354036 139700209868800 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 13:26:10.354077 139700209868800 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:10.354114 139700209868800 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 13:26:10.354151 139700209868800 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 13:26:10.354186 139700209868800 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 13:26:10.354222 139700209868800 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:10.354257 139700209868800 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 13:26:10.354292 139700209868800 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:10.354326 139700209868800 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:10.354362 139700209868800 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 13:26:10.354398 139700209868800 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 13:26:10.354432 139700209868800 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:10.354468 139700209868800 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 13:26:10.354504 139700209868800 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 13:26:10.354539 139700209868800 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 13:26:10.354574 139700209868800 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:10.354609 139700209868800 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 13:26:10.354644 139700209868800 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:10.354679 139700209868800 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:10.354715 139700209868800 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 13:26:10.354751 139700209868800 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 13:26:10.354786 139700209868800 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 13:26:10.354822 139700209868800 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 13:26:10.354851 139700209868800 training_loop.py:725] Total parameters: 152072288
I0123 13:26:10.355061 139700209868800 training_loop.py:739] Total state size: 0
I0123 13:26:10.375961 139700209868800 training_loop.py:492] Training loop: creating task for mode beam_search
I0123 13:26:10.376194 139700209868800 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0123 13:26:10.376455 139700209868800 training_loop.py:652] Compiling mode beam_search with jit.
I0123 13:26:10.376763 139700209868800 training_loop.py:89] registering functions: dict_keys([])
I0123 13:26:10.392657 139700209868800 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_line e c d, on_line e b a; f = lc_tangent f c e, on_line f b e; g = on_circle g f c, on_line g b a; h = on_circle h f c, on_line h b a; i = on_circle i f c ? eqratio a g g b a h b h
I0123 13:26:10.931019 139700209868800 ddar.py:60] Depth 1/1000 time = 0.5153124332427979
I0123 13:26:12.204027 139700209868800 ddar.py:60] Depth 2/1000 time = 1.2728281021118164
I0123 13:26:13.363248 139700209868800 ddar.py:60] Depth 3/1000 time = 1.1588728427886963
I0123 13:26:14.786978 139700209868800 ddar.py:60] Depth 4/1000 time = 1.4235515594482422
I0123 13:26:16.045606 139700209868800 ddar.py:60] Depth 5/1000 time = 1.2584507465362549
I0123 13:26:17.299734 139700209868800 ddar.py:60] Depth 6/1000 time = 1.2537970542907715
I0123 13:26:18.716605 139700209868800 ddar.py:60] Depth 7/1000 time = 1.4140269756317139
I0123 13:26:18.720188 139700209868800 alphageometry.py:191] 
==========================
 * From theorem premises:
A B C D E F G H : Points
DB = DA [00]
DC = DB [01]
D,E,C are collinear [02]
A,B,E are collinear [03]
CF  CE [04]
F,B,E are collinear [05]
FG = FC [06]
A,G,B are collinear [07]
FH = FC [08]
A,B,H are collinear [09]

 * Auxiliary Constructions:
I : Points
FI = FC [10]

 * Proof steps:
001. CF  CE [04] & D,E,C are collinear [02]   CF  CD [11]
002. DB = DA [00]   DBA = BAD [12]
003. DC = DB [01]   DBC = BCD [13]
004. DC = DB [01] & DB = DA [00]   DA = DC [14]
005. DA = DC [14]   DAC = ACD [15]
006. FH = FC [08]   FHC = HCF [16]
007. FHC = HCF [16] & A,B,H are collinear [09] & F,B,E are collinear [05] & A,B,E are collinear [03]   (AB-CH) = HCF [17]
008. CF  CD [11] & DBA = BAD [12] & DBC = BCD [13] & DAC = ACD [15] & (AB-CH) = HCF [17] (Angle chase)  HCB = ACH [18]
009. HCB = ACH [18] & A,B,H are collinear [09]   AH:BH = AC:BC [19]
010. FG = FC [06] & FI = FC [10] & FH = FC [08]   G,I,H,C are concyclic [20]
011. G,I,H,C are concyclic [20]   GIH = GCH [21]
012. FI = FC [10] & FH = FC [08]   FH = FI [22]
013. FH = FI [22]   FHI = HIF [23]
014. FHI = HIF [23] & A,B,H are collinear [09] & F,B,E are collinear [05] & A,B,E are collinear [03]   (AB-HI) = HIF [24]
015. FG = FC [06] & FI = FC [10]   FI = FG [25]
016. FI = FG [25]   FIG = IGF [26]
017. FIG = IGF [26] & A,G,B are collinear [07] & F,B,E are collinear [05] & A,B,E are collinear [03]   FIG = (GI-AB) [27]
018. CF  CD [11] & GIH = GCH [21] & DBA = BAD [12] & DBC = BCD [13] & DAC = ACD [15] & (AB-HI) = HIF [24] & (AB-CH) = HCF [17] & FIG = (GI-AB) [27] (Angle chase)  GCB = ACG [28]
019. GCB = ACG [28] & A,G,B are collinear [07]   AG:GB = AC:BC [29]
020. AH:BH = AC:BC [19] & AG:GB = AC:BC [29]   AG:GB = AH:BH
==========================

