I0123 19:25:33.216028 139637719117824 inference_utils.py:69] Parsing gin configuration.
I0123 19:25:33.216127 139637719117824 inference_utils.py:71] Added Gin search path meliad_lib/meliad/transformer/configs
I0123 19:25:33.216324 139637719117824 inference_utils.py:74] Loading Gin config file base_htrans.gin
I0123 19:25:33.216358 139637719117824 inference_utils.py:74] Loading Gin config file size/medium_150M.gin
I0123 19:25:33.216386 139637719117824 inference_utils.py:74] Loading Gin config file options/positions_t5.gin
I0123 19:25:33.216413 139637719117824 inference_utils.py:74] Loading Gin config file options/lr_cosine_decay.gin
I0123 19:25:33.216439 139637719117824 inference_utils.py:74] Loading Gin config file options/seq_1024_nocache.gin
I0123 19:25:33.216465 139637719117824 inference_utils.py:74] Loading Gin config file geometry_150M_generate.gin
I0123 19:25:33.216493 139637719117824 inference_utils.py:76] Overriding Gin param DecoderOnlyLanguageModelGenerate.output_token_losses=True
I0123 19:25:33.216518 139637719117824 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.batch_size=32
I0123 19:25:33.216543 139637719117824 inference_utils.py:76] Overriding Gin param TransformerTaskConfig.sequence_length=128
I0123 19:25:33.216568 139637719117824 inference_utils.py:76] Overriding Gin param Trainer.restore_state_variables=False
I0123 19:25:33.216613 139637719117824 resource_reader.py:50] system_path_file_exists:base_htrans.gin
E0123 19:25:33.216742 139637719117824 resource_reader.py:55] Path not found: base_htrans.gin
I0123 19:25:33.216945 139637719117824 resource_reader.py:50] system_path_file_exists:trainer_configuration.gin
E0123 19:25:33.217042 139637719117824 resource_reader.py:55] Path not found: trainer_configuration.gin
I0123 19:25:33.223351 139637719117824 resource_reader.py:50] system_path_file_exists:size/medium_150M.gin
E0123 19:25:33.223467 139637719117824 resource_reader.py:55] Path not found: size/medium_150M.gin
I0123 19:25:33.223789 139637719117824 resource_reader.py:50] system_path_file_exists:options/positions_t5.gin
E0123 19:25:33.223891 139637719117824 resource_reader.py:55] Path not found: options/positions_t5.gin
I0123 19:25:33.224171 139637719117824 resource_reader.py:50] system_path_file_exists:options/lr_cosine_decay.gin
E0123 19:25:33.224269 139637719117824 resource_reader.py:55] Path not found: options/lr_cosine_decay.gin
I0123 19:25:33.224679 139637719117824 resource_reader.py:50] system_path_file_exists:options/seq_1024_nocache.gin
E0123 19:25:33.224777 139637719117824 resource_reader.py:55] Path not found: options/seq_1024_nocache.gin
I0123 19:25:33.228459 139637719117824 training_loop.py:334] ==== Training loop: initializing model ====
I0123 19:25:33.329301 139637719117824 xla_bridge.py:660] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA
I0123 19:25:33.330035 139637719117824 xla_bridge.py:660] Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory
I0123 19:25:33.336875 139637719117824 training_loop.py:335] Process 0 of 1
I0123 19:25:33.336927 139637719117824 training_loop.py:336] Local device count = 1
I0123 19:25:33.336966 139637719117824 training_loop.py:337] Number of replicas = 1
I0123 19:25:33.336998 139637719117824 training_loop.py:339] Using random number seed 42
I0123 19:25:33.820613 139637719117824 training_loop.py:359] Initializing the model.
I0123 19:25:34.193342 139637719117824 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.193632 139637719117824 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 19:25:34.193751 139637719117824 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 19:25:34.193829 139637719117824 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 19:25:34.193908 139637719117824 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 19:25:34.193988 139637719117824 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 19:25:34.194062 139637719117824 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 19:25:34.194131 139637719117824 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 19:25:34.194202 139637719117824 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 19:25:34.194269 139637719117824 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 19:25:34.194337 139637719117824 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 19:25:34.194405 139637719117824 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 19:25:34.194472 139637719117824 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 19:25:34.194538 139637719117824 transformer_layer.py:657] tlayer: Skipping XL cache for mode init.
I0123 19:25:34.194577 139637719117824 attention.py:418] Single window, no scan.
I0123 19:25:34.194621 139637719117824 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 19:25:34.194735 139637719117824 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:25:34.194776 139637719117824 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:25:34.194806 139637719117824 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:25:34.196793 139637719117824 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.202036 139637719117824 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:25:34.212548 139637719117824 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.212824 139637719117824 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:25:34.217107 139637719117824 transformer_layer.py:213] tlayer: windowed attention.
I0123 19:25:34.227650 139637719117824 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:25:34.227707 139637719117824 attention.py:418] Single window, no scan.
I0123 19:25:34.227745 139637719117824 transformer_layer.py:389] tlayer: self-attention.
I0123 19:25:34.227777 139637719117824 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.227839 139637719117824 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.229010 139637719117824 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.229086 139637719117824 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.229788 139637719117824 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.232181 139637719117824 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.237814 139637719117824 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.239517 139637719117824 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.239596 139637719117824 transformer_layer.py:468] tlayer: End windows.
I0123 19:25:34.239630 139637719117824 transformer_layer.py:472] tlayer: final FFN.
I0123 19:25:34.239690 139637719117824 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.239815 139637719117824 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:25:34.240147 139637719117824 nn_components.py:325] mlp: activation = None
I0123 19:25:34.240194 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:25:34.242099 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.242199 139637719117824 nn_components.py:261] mlp: residual
I0123 19:25:34.245030 139637719117824 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.245108 139637719117824 transformer_base.py:443] tbase: final FFN
I0123 19:25:34.245594 139637719117824 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:25:34.255662 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:25:34.264406 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.264504 139637719117824 nn_components.py:261] mlp: residual
I0123 19:25:34.264799 139637719117824 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.264878 139637719117824 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 19:25:34.264986 139637719117824 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:25:34.265024 139637719117824 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:25:34.265054 139637719117824 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:25:34.266903 139637719117824 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.269391 139637719117824 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:25:34.274979 139637719117824 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.275239 139637719117824 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:25:34.277870 139637719117824 transformer_layer.py:213] tlayer: windowed attention.
I0123 19:25:34.281689 139637719117824 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:25:34.281744 139637719117824 attention.py:418] Single window, no scan.
I0123 19:25:34.281780 139637719117824 transformer_layer.py:389] tlayer: self-attention.
I0123 19:25:34.281810 139637719117824 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.281872 139637719117824 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.282438 139637719117824 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.282511 139637719117824 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.282871 139637719117824 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.283638 139637719117824 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.286107 139637719117824 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.286737 139637719117824 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.286815 139637719117824 transformer_layer.py:468] tlayer: End windows.
I0123 19:25:34.286850 139637719117824 transformer_layer.py:472] tlayer: final FFN.
I0123 19:25:34.286908 139637719117824 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.287034 139637719117824 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:25:34.287355 139637719117824 nn_components.py:325] mlp: activation = None
I0123 19:25:34.287398 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:25:34.289327 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.289418 139637719117824 nn_components.py:261] mlp: residual
I0123 19:25:34.291920 139637719117824 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.291998 139637719117824 transformer_base.py:443] tbase: final FFN
I0123 19:25:34.292422 139637719117824 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:25:34.294749 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:25:34.296657 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.296749 139637719117824 nn_components.py:261] mlp: residual
I0123 19:25:34.297035 139637719117824 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.297113 139637719117824 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 19:25:34.297222 139637719117824 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:25:34.297260 139637719117824 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:25:34.297290 139637719117824 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:25:34.299196 139637719117824 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.301557 139637719117824 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:25:34.307496 139637719117824 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.307759 139637719117824 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:25:34.310403 139637719117824 transformer_layer.py:213] tlayer: windowed attention.
I0123 19:25:34.314247 139637719117824 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:25:34.314302 139637719117824 attention.py:418] Single window, no scan.
I0123 19:25:34.314337 139637719117824 transformer_layer.py:389] tlayer: self-attention.
I0123 19:25:34.314368 139637719117824 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.314429 139637719117824 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.314992 139637719117824 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.315067 139637719117824 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.315423 139637719117824 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.316173 139637719117824 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.318645 139637719117824 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.319315 139637719117824 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.319390 139637719117824 transformer_layer.py:468] tlayer: End windows.
I0123 19:25:34.319423 139637719117824 transformer_layer.py:472] tlayer: final FFN.
I0123 19:25:34.319480 139637719117824 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.319603 139637719117824 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:25:34.319927 139637719117824 nn_components.py:325] mlp: activation = None
I0123 19:25:34.319969 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:25:34.321866 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.321957 139637719117824 nn_components.py:261] mlp: residual
I0123 19:25:34.324451 139637719117824 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.324537 139637719117824 transformer_base.py:443] tbase: final FFN
I0123 19:25:34.325016 139637719117824 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:25:34.327283 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:25:34.329178 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.329270 139637719117824 nn_components.py:261] mlp: residual
I0123 19:25:34.329561 139637719117824 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.329644 139637719117824 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 19:25:34.329754 139637719117824 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:25:34.329792 139637719117824 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:25:34.329822 139637719117824 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:25:34.331703 139637719117824 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.334072 139637719117824 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:25:34.339634 139637719117824 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.339897 139637719117824 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:25:34.342508 139637719117824 transformer_layer.py:213] tlayer: windowed attention.
I0123 19:25:34.346278 139637719117824 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:25:34.346332 139637719117824 attention.py:418] Single window, no scan.
I0123 19:25:34.346366 139637719117824 transformer_layer.py:389] tlayer: self-attention.
I0123 19:25:34.346397 139637719117824 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.346458 139637719117824 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.347018 139637719117824 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.347092 139637719117824 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.347448 139637719117824 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.348222 139637719117824 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.350732 139637719117824 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.351349 139637719117824 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.351424 139637719117824 transformer_layer.py:468] tlayer: End windows.
I0123 19:25:34.351457 139637719117824 transformer_layer.py:472] tlayer: final FFN.
I0123 19:25:34.351518 139637719117824 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.351646 139637719117824 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:25:34.351973 139637719117824 nn_components.py:325] mlp: activation = None
I0123 19:25:34.352016 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:25:34.353900 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.353992 139637719117824 nn_components.py:261] mlp: residual
I0123 19:25:34.356530 139637719117824 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.356616 139637719117824 transformer_base.py:443] tbase: final FFN
I0123 19:25:34.357044 139637719117824 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:25:34.359287 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:25:34.361154 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.361251 139637719117824 nn_components.py:261] mlp: residual
I0123 19:25:34.361537 139637719117824 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.361619 139637719117824 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 19:25:34.361734 139637719117824 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:25:34.361773 139637719117824 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:25:34.361803 139637719117824 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:25:34.363703 139637719117824 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.366079 139637719117824 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:25:34.371636 139637719117824 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.371893 139637719117824 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:25:34.374552 139637719117824 transformer_layer.py:213] tlayer: windowed attention.
I0123 19:25:34.378282 139637719117824 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:25:34.378339 139637719117824 attention.py:418] Single window, no scan.
I0123 19:25:34.378374 139637719117824 transformer_layer.py:389] tlayer: self-attention.
I0123 19:25:34.378404 139637719117824 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.378465 139637719117824 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.379026 139637719117824 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.379100 139637719117824 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.379455 139637719117824 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.380203 139637719117824 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.383057 139637719117824 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.383676 139637719117824 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.383751 139637719117824 transformer_layer.py:468] tlayer: End windows.
I0123 19:25:34.383785 139637719117824 transformer_layer.py:472] tlayer: final FFN.
I0123 19:25:34.383843 139637719117824 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.383976 139637719117824 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:25:34.384296 139637719117824 nn_components.py:325] mlp: activation = None
I0123 19:25:34.384340 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:25:34.386235 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.386330 139637719117824 nn_components.py:261] mlp: residual
I0123 19:25:34.388851 139637719117824 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.388927 139637719117824 transformer_base.py:443] tbase: final FFN
I0123 19:25:34.389352 139637719117824 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:25:34.391609 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:25:34.393557 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.393654 139637719117824 nn_components.py:261] mlp: residual
I0123 19:25:34.393948 139637719117824 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.394026 139637719117824 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 19:25:34.394134 139637719117824 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:25:34.394172 139637719117824 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:25:34.394201 139637719117824 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:25:34.396038 139637719117824 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.398416 139637719117824 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:25:34.404021 139637719117824 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.404274 139637719117824 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:25:34.406934 139637719117824 transformer_layer.py:213] tlayer: windowed attention.
I0123 19:25:34.410668 139637719117824 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:25:34.410723 139637719117824 attention.py:418] Single window, no scan.
I0123 19:25:34.410758 139637719117824 transformer_layer.py:389] tlayer: self-attention.
I0123 19:25:34.410788 139637719117824 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.410849 139637719117824 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.411451 139637719117824 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.411526 139637719117824 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.411878 139637719117824 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.412653 139637719117824 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.415108 139637719117824 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.415721 139637719117824 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.415796 139637719117824 transformer_layer.py:468] tlayer: End windows.
I0123 19:25:34.415830 139637719117824 transformer_layer.py:472] tlayer: final FFN.
I0123 19:25:34.415888 139637719117824 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.416011 139637719117824 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:25:34.416328 139637719117824 nn_components.py:325] mlp: activation = None
I0123 19:25:34.416370 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:25:34.418267 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.418359 139637719117824 nn_components.py:261] mlp: residual
I0123 19:25:34.420866 139637719117824 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.420943 139637719117824 transformer_base.py:443] tbase: final FFN
I0123 19:25:34.421371 139637719117824 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:25:34.423682 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:25:34.425566 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.425664 139637719117824 nn_components.py:261] mlp: residual
I0123 19:25:34.425947 139637719117824 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.426025 139637719117824 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 19:25:34.426132 139637719117824 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:25:34.426171 139637719117824 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:25:34.426202 139637719117824 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:25:34.428052 139637719117824 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.430468 139637719117824 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:25:34.436036 139637719117824 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.436300 139637719117824 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:25:34.438915 139637719117824 transformer_layer.py:213] tlayer: windowed attention.
I0123 19:25:34.442687 139637719117824 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:25:34.442743 139637719117824 attention.py:418] Single window, no scan.
I0123 19:25:34.442777 139637719117824 transformer_layer.py:389] tlayer: self-attention.
I0123 19:25:34.442808 139637719117824 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.442868 139637719117824 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.443428 139637719117824 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.443504 139637719117824 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.443867 139637719117824 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.444630 139637719117824 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.447090 139637719117824 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.447719 139637719117824 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.447794 139637719117824 transformer_layer.py:468] tlayer: End windows.
I0123 19:25:34.447828 139637719117824 transformer_layer.py:472] tlayer: final FFN.
I0123 19:25:34.447885 139637719117824 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.448010 139637719117824 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:25:34.448331 139637719117824 nn_components.py:325] mlp: activation = None
I0123 19:25:34.448374 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:25:34.451043 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.451198 139637719117824 nn_components.py:261] mlp: residual
I0123 19:25:34.453758 139637719117824 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.453838 139637719117824 transformer_base.py:443] tbase: final FFN
I0123 19:25:34.454272 139637719117824 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:25:34.456937 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:25:34.458843 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.458955 139637719117824 nn_components.py:261] mlp: residual
I0123 19:25:34.459248 139637719117824 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.459329 139637719117824 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 19:25:34.459437 139637719117824 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:25:34.459475 139637719117824 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:25:34.459506 139637719117824 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:25:34.598527 139637719117824 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.601598 139637719117824 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:25:34.607522 139637719117824 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.607818 139637719117824 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:25:34.610593 139637719117824 transformer_layer.py:213] tlayer: windowed attention.
I0123 19:25:34.614630 139637719117824 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:25:34.614687 139637719117824 attention.py:418] Single window, no scan.
I0123 19:25:34.614727 139637719117824 transformer_layer.py:389] tlayer: self-attention.
I0123 19:25:34.614758 139637719117824 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.614823 139637719117824 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.615435 139637719117824 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.615510 139637719117824 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.615876 139637719117824 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.616665 139637719117824 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.619242 139637719117824 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.619886 139637719117824 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.619966 139637719117824 transformer_layer.py:468] tlayer: End windows.
I0123 19:25:34.620000 139637719117824 transformer_layer.py:472] tlayer: final FFN.
I0123 19:25:34.620061 139637719117824 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.620189 139637719117824 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:25:34.620521 139637719117824 nn_components.py:325] mlp: activation = None
I0123 19:25:34.620564 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:25:34.622473 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.622565 139637719117824 nn_components.py:261] mlp: residual
I0123 19:25:34.625135 139637719117824 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.625216 139637719117824 transformer_base.py:443] tbase: final FFN
I0123 19:25:34.625649 139637719117824 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:25:34.627948 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:25:34.629849 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.629953 139637719117824 nn_components.py:261] mlp: residual
I0123 19:25:34.630244 139637719117824 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.630326 139637719117824 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 19:25:34.630436 139637719117824 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:25:34.630475 139637719117824 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:25:34.630506 139637719117824 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:25:34.632434 139637719117824 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.634804 139637719117824 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:25:34.640398 139637719117824 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.640661 139637719117824 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:25:34.643355 139637719117824 transformer_layer.py:213] tlayer: windowed attention.
I0123 19:25:34.647100 139637719117824 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:25:34.647154 139637719117824 attention.py:418] Single window, no scan.
I0123 19:25:34.647189 139637719117824 transformer_layer.py:389] tlayer: self-attention.
I0123 19:25:34.647220 139637719117824 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.647280 139637719117824 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.647841 139637719117824 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.647916 139637719117824 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.648270 139637719117824 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.649023 139637719117824 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.651572 139637719117824 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.652190 139637719117824 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.652266 139637719117824 transformer_layer.py:468] tlayer: End windows.
I0123 19:25:34.652305 139637719117824 transformer_layer.py:472] tlayer: final FFN.
I0123 19:25:34.652364 139637719117824 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.652490 139637719117824 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:25:34.652813 139637719117824 nn_components.py:325] mlp: activation = None
I0123 19:25:34.652856 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:25:34.654754 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.654847 139637719117824 nn_components.py:261] mlp: residual
I0123 19:25:34.657427 139637719117824 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.657506 139637719117824 transformer_base.py:443] tbase: final FFN
I0123 19:25:34.657938 139637719117824 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:25:34.660220 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:25:34.662169 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.662264 139637719117824 nn_components.py:261] mlp: residual
I0123 19:25:34.662553 139637719117824 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.662639 139637719117824 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 19:25:34.662751 139637719117824 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:25:34.662790 139637719117824 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:25:34.662820 139637719117824 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:25:34.664639 139637719117824 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.667226 139637719117824 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:25:34.672730 139637719117824 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.672987 139637719117824 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:25:34.675999 139637719117824 transformer_layer.py:213] tlayer: windowed attention.
I0123 19:25:34.679695 139637719117824 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:25:34.679751 139637719117824 attention.py:418] Single window, no scan.
I0123 19:25:34.679786 139637719117824 transformer_layer.py:389] tlayer: self-attention.
I0123 19:25:34.679816 139637719117824 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.679875 139637719117824 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.680478 139637719117824 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.680552 139637719117824 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.680903 139637719117824 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.681670 139637719117824 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.684096 139637719117824 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.684714 139637719117824 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.684789 139637719117824 transformer_layer.py:468] tlayer: End windows.
I0123 19:25:34.684823 139637719117824 transformer_layer.py:472] tlayer: final FFN.
I0123 19:25:34.684880 139637719117824 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.685007 139637719117824 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:25:34.685326 139637719117824 nn_components.py:325] mlp: activation = None
I0123 19:25:34.685369 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:25:34.687259 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.687352 139637719117824 nn_components.py:261] mlp: residual
I0123 19:25:34.689903 139637719117824 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.689986 139637719117824 transformer_base.py:443] tbase: final FFN
I0123 19:25:34.690411 139637719117824 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:25:34.692708 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:25:34.694646 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.694741 139637719117824 nn_components.py:261] mlp: residual
I0123 19:25:34.695033 139637719117824 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.695120 139637719117824 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 19:25:34.695230 139637719117824 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:25:34.695270 139637719117824 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:25:34.695302 139637719117824 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:25:34.697145 139637719117824 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.699604 139637719117824 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:25:34.705173 139637719117824 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.705443 139637719117824 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:25:34.708086 139637719117824 transformer_layer.py:213] tlayer: windowed attention.
I0123 19:25:34.711869 139637719117824 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:25:34.711925 139637719117824 attention.py:418] Single window, no scan.
I0123 19:25:34.711960 139637719117824 transformer_layer.py:389] tlayer: self-attention.
I0123 19:25:34.711991 139637719117824 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.712052 139637719117824 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.712615 139637719117824 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.712690 139637719117824 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.713049 139637719117824 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.713835 139637719117824 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.716292 139637719117824 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.716916 139637719117824 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.716992 139637719117824 transformer_layer.py:468] tlayer: End windows.
I0123 19:25:34.717027 139637719117824 transformer_layer.py:472] tlayer: final FFN.
I0123 19:25:34.717086 139637719117824 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.717214 139637719117824 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:25:34.717534 139637719117824 nn_components.py:325] mlp: activation = None
I0123 19:25:34.717576 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:25:34.719522 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.719619 139637719117824 nn_components.py:261] mlp: residual
I0123 19:25:34.722378 139637719117824 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.722457 139637719117824 transformer_base.py:443] tbase: final FFN
I0123 19:25:34.722884 139637719117824 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:25:34.725195 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:25:34.727077 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.727171 139637719117824 nn_components.py:261] mlp: residual
I0123 19:25:34.727460 139637719117824 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.727540 139637719117824 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 19:25:34.727654 139637719117824 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:25:34.727693 139637719117824 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:25:34.727723 139637719117824 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:25:34.729609 139637719117824 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.731981 139637719117824 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:25:34.737538 139637719117824 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.737802 139637719117824 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:25:34.740400 139637719117824 transformer_layer.py:213] tlayer: windowed attention.
I0123 19:25:34.744201 139637719117824 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:25:34.744257 139637719117824 attention.py:418] Single window, no scan.
I0123 19:25:34.744293 139637719117824 transformer_layer.py:389] tlayer: self-attention.
I0123 19:25:34.744323 139637719117824 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.744383 139637719117824 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.744950 139637719117824 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.745026 139637719117824 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.745383 139637719117824 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.746189 139637719117824 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.748663 139637719117824 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.749634 139637719117824 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.749717 139637719117824 transformer_layer.py:468] tlayer: End windows.
I0123 19:25:34.749752 139637719117824 transformer_layer.py:472] tlayer: final FFN.
I0123 19:25:34.749811 139637719117824 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.749947 139637719117824 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:25:34.750268 139637719117824 nn_components.py:325] mlp: activation = None
I0123 19:25:34.750311 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:25:34.752202 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.752299 139637719117824 nn_components.py:261] mlp: residual
I0123 19:25:34.754784 139637719117824 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.754865 139637719117824 transformer_base.py:443] tbase: final FFN
I0123 19:25:34.755345 139637719117824 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:25:34.757602 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:25:34.759488 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.759581 139637719117824 nn_components.py:261] mlp: residual
I0123 19:25:34.759870 139637719117824 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.760147 139637719117824 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 19:25:34.760215 139637719117824 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 19:25:34.760280 139637719117824 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 19:25:34.760337 139637719117824 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 19:25:34.760390 139637719117824 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 19:25:34.760443 139637719117824 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 19:25:34.760496 139637719117824 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 19:25:34.760548 139637719117824 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 19:25:34.760600 139637719117824 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 19:25:34.760651 139637719117824 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 19:25:34.760703 139637719117824 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 19:25:34.760753 139637719117824 transformer_layer.py:673] tlayer: Skipping XL cache update for mode init.
I0123 19:25:34.760790 139637719117824 decoder_stack.py:344] dstack: Final layernorm.
I0123 19:25:34.764274 139637719117824 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:25:34.811714 139637719117824 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:34.811799 139637719117824 decoder_stack.py:333] dstack: autoregressive generator.
I0123 19:25:34.811853 139637719117824 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 19:25:34.811958 139637719117824 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:25:34.811996 139637719117824 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:25:34.812026 139637719117824 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:25:34.812088 139637719117824 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:34.814514 139637719117824 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:25:34.819970 139637719117824 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:34.820226 139637719117824 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:25:34.822871 139637719117824 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:25:34.839341 139637719117824 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:25:34.839396 139637719117824 attention.py:418] Single window, no scan.
I0123 19:25:34.839432 139637719117824 transformer_layer.py:389] tlayer: self-attention.
I0123 19:25:34.839462 139637719117824 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:34.839523 139637719117824 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:34.840650 139637719117824 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:34.840727 139637719117824 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:34.841428 139637719117824 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:34.843425 139637719117824 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:34.848152 139637719117824 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:34.849448 139637719117824 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:34.849532 139637719117824 transformer_layer.py:468] tlayer: End windows.
I0123 19:25:34.849567 139637719117824 transformer_layer.py:472] tlayer: final FFN.
I0123 19:25:34.849626 139637719117824 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:34.849765 139637719117824 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:25:34.849875 139637719117824 nn_components.py:325] mlp: activation = None
I0123 19:25:34.849914 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:25:34.851804 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:34.851896 139637719117824 nn_components.py:261] mlp: residual
I0123 19:25:34.854310 139637719117824 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:34.854389 139637719117824 transformer_base.py:443] tbase: final FFN
I0123 19:25:34.854496 139637719117824 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:25:34.856703 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:25:34.858649 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:34.858745 139637719117824 nn_components.py:261] mlp: residual
I0123 19:25:34.859035 139637719117824 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:34.859116 139637719117824 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 19:25:34.859223 139637719117824 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:25:34.859261 139637719117824 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:25:34.859291 139637719117824 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:25:34.859355 139637719117824 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:34.861575 139637719117824 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:25:34.867073 139637719117824 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:34.867328 139637719117824 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:25:34.869975 139637719117824 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:25:34.882991 139637719117824 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:25:34.883047 139637719117824 attention.py:418] Single window, no scan.
I0123 19:25:34.883082 139637719117824 transformer_layer.py:389] tlayer: self-attention.
I0123 19:25:34.883111 139637719117824 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:34.883174 139637719117824 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:34.883730 139637719117824 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:34.883805 139637719117824 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:34.884163 139637719117824 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:34.884845 139637719117824 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:34.887353 139637719117824 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:34.887966 139637719117824 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:34.888042 139637719117824 transformer_layer.py:468] tlayer: End windows.
I0123 19:25:34.888082 139637719117824 transformer_layer.py:472] tlayer: final FFN.
I0123 19:25:34.888141 139637719117824 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:34.888267 139637719117824 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:25:34.888374 139637719117824 nn_components.py:325] mlp: activation = None
I0123 19:25:34.888411 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:25:34.890346 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:34.890442 139637719117824 nn_components.py:261] mlp: residual
I0123 19:25:34.892842 139637719117824 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:34.892919 139637719117824 transformer_base.py:443] tbase: final FFN
I0123 19:25:34.893026 139637719117824 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:25:34.895239 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:25:34.897143 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:34.897238 139637719117824 nn_components.py:261] mlp: residual
I0123 19:25:34.897522 139637719117824 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:34.897602 139637719117824 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 19:25:34.897718 139637719117824 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:25:34.897759 139637719117824 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:25:34.897789 139637719117824 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:25:34.897852 139637719117824 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:34.900084 139637719117824 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:25:34.905507 139637719117824 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:34.905775 139637719117824 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:25:34.908428 139637719117824 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:25:34.921091 139637719117824 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:25:34.921146 139637719117824 attention.py:418] Single window, no scan.
I0123 19:25:34.921181 139637719117824 transformer_layer.py:389] tlayer: self-attention.
I0123 19:25:34.921212 139637719117824 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:34.921274 139637719117824 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:34.921840 139637719117824 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:34.921918 139637719117824 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:34.922282 139637719117824 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:34.922971 139637719117824 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:34.925425 139637719117824 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:34.926053 139637719117824 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:34.926130 139637719117824 transformer_layer.py:468] tlayer: End windows.
I0123 19:25:34.926163 139637719117824 transformer_layer.py:472] tlayer: final FFN.
I0123 19:25:34.926229 139637719117824 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:34.926356 139637719117824 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:25:34.926462 139637719117824 nn_components.py:325] mlp: activation = None
I0123 19:25:34.926500 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:25:34.928429 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:34.928523 139637719117824 nn_components.py:261] mlp: residual
I0123 19:25:34.930957 139637719117824 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:34.931035 139637719117824 transformer_base.py:443] tbase: final FFN
I0123 19:25:34.931144 139637719117824 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:25:34.933353 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:25:34.935282 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:34.935376 139637719117824 nn_components.py:261] mlp: residual
I0123 19:25:34.935656 139637719117824 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:34.935734 139637719117824 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 19:25:34.935841 139637719117824 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:25:34.935879 139637719117824 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:25:34.935909 139637719117824 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:25:34.935969 139637719117824 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:34.938188 139637719117824 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:25:34.943646 139637719117824 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:34.943907 139637719117824 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:25:34.946596 139637719117824 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:25:34.959276 139637719117824 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:25:34.959332 139637719117824 attention.py:418] Single window, no scan.
I0123 19:25:34.959367 139637719117824 transformer_layer.py:389] tlayer: self-attention.
I0123 19:25:34.959396 139637719117824 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:34.959457 139637719117824 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:34.960019 139637719117824 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:34.960098 139637719117824 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:34.960449 139637719117824 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:34.961129 139637719117824 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:34.963608 139637719117824 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:34.964233 139637719117824 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:34.964312 139637719117824 transformer_layer.py:468] tlayer: End windows.
I0123 19:25:34.964346 139637719117824 transformer_layer.py:472] tlayer: final FFN.
I0123 19:25:34.964405 139637719117824 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:34.964538 139637719117824 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:25:34.964646 139637719117824 nn_components.py:325] mlp: activation = None
I0123 19:25:34.964684 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:25:34.966651 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:34.966744 139637719117824 nn_components.py:261] mlp: residual
I0123 19:25:34.969137 139637719117824 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:34.969218 139637719117824 transformer_base.py:443] tbase: final FFN
I0123 19:25:34.969324 139637719117824 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:25:34.971546 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:25:34.973399 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:34.973491 139637719117824 nn_components.py:261] mlp: residual
I0123 19:25:34.973784 139637719117824 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:34.973863 139637719117824 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 19:25:34.973971 139637719117824 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:25:34.974008 139637719117824 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:25:34.974038 139637719117824 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:25:34.974099 139637719117824 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:34.976669 139637719117824 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:25:34.982169 139637719117824 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:34.982429 139637719117824 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:25:34.985036 139637719117824 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:25:34.997752 139637719117824 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:25:34.997807 139637719117824 attention.py:418] Single window, no scan.
I0123 19:25:34.997846 139637719117824 transformer_layer.py:389] tlayer: self-attention.
I0123 19:25:34.997876 139637719117824 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:34.997940 139637719117824 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:34.998501 139637719117824 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:34.998578 139637719117824 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:34.998934 139637719117824 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:34.999631 139637719117824 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.002162 139637719117824 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.002787 139637719117824 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.002862 139637719117824 transformer_layer.py:468] tlayer: End windows.
I0123 19:25:35.002895 139637719117824 transformer_layer.py:472] tlayer: final FFN.
I0123 19:25:35.002952 139637719117824 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.003085 139637719117824 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:25:35.003192 139637719117824 nn_components.py:325] mlp: activation = None
I0123 19:25:35.003230 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:25:35.005115 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.005208 139637719117824 nn_components.py:261] mlp: residual
I0123 19:25:35.007605 139637719117824 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.007684 139637719117824 transformer_base.py:443] tbase: final FFN
I0123 19:25:35.007792 139637719117824 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:25:35.010087 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:25:35.011946 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.012039 139637719117824 nn_components.py:261] mlp: residual
I0123 19:25:35.012319 139637719117824 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.012399 139637719117824 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 19:25:35.012506 139637719117824 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:25:35.012545 139637719117824 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:25:35.012575 139637719117824 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:25:35.012637 139637719117824 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.014880 139637719117824 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:25:35.020327 139637719117824 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.020583 139637719117824 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:25:35.023250 139637719117824 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:25:35.035924 139637719117824 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:25:35.035980 139637719117824 attention.py:418] Single window, no scan.
I0123 19:25:35.036015 139637719117824 transformer_layer.py:389] tlayer: self-attention.
I0123 19:25:35.036044 139637719117824 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.036104 139637719117824 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.036658 139637719117824 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.036731 139637719117824 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.037079 139637719117824 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.037777 139637719117824 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.040220 139637719117824 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.040835 139637719117824 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.040909 139637719117824 transformer_layer.py:468] tlayer: End windows.
I0123 19:25:35.040942 139637719117824 transformer_layer.py:472] tlayer: final FFN.
I0123 19:25:35.040999 139637719117824 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.041131 139637719117824 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:25:35.041244 139637719117824 nn_components.py:325] mlp: activation = None
I0123 19:25:35.041283 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:25:35.043223 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.043315 139637719117824 nn_components.py:261] mlp: residual
I0123 19:25:35.045698 139637719117824 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.045776 139637719117824 transformer_base.py:443] tbase: final FFN
I0123 19:25:35.045882 139637719117824 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:25:35.048093 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:25:35.049942 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.050037 139637719117824 nn_components.py:261] mlp: residual
I0123 19:25:35.050320 139637719117824 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.050400 139637719117824 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 19:25:35.050507 139637719117824 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:25:35.050544 139637719117824 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:25:35.050574 139637719117824 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:25:35.050635 139637719117824 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.052848 139637719117824 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:25:35.058372 139637719117824 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.058629 139637719117824 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:25:35.061239 139637719117824 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:25:35.073910 139637719117824 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:25:35.073965 139637719117824 attention.py:418] Single window, no scan.
I0123 19:25:35.074000 139637719117824 transformer_layer.py:389] tlayer: self-attention.
I0123 19:25:35.074030 139637719117824 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.074090 139637719117824 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.074646 139637719117824 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.074720 139637719117824 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.075068 139637719117824 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.075751 139637719117824 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.078229 139637719117824 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.079212 139637719117824 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.079289 139637719117824 transformer_layer.py:468] tlayer: End windows.
I0123 19:25:35.079323 139637719117824 transformer_layer.py:472] tlayer: final FFN.
I0123 19:25:35.079380 139637719117824 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.079509 139637719117824 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:25:35.079617 139637719117824 nn_components.py:325] mlp: activation = None
I0123 19:25:35.079659 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:25:35.081544 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.081638 139637719117824 nn_components.py:261] mlp: residual
I0123 19:25:35.084028 139637719117824 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.084107 139637719117824 transformer_base.py:443] tbase: final FFN
I0123 19:25:35.084219 139637719117824 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:25:35.086429 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:25:35.088345 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.088439 139637719117824 nn_components.py:261] mlp: residual
I0123 19:25:35.088728 139637719117824 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.088808 139637719117824 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 19:25:35.088916 139637719117824 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:25:35.088955 139637719117824 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:25:35.088985 139637719117824 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:25:35.089048 139637719117824 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.091286 139637719117824 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:25:35.096718 139637719117824 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.096983 139637719117824 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:25:35.099632 139637719117824 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:25:35.117172 139637719117824 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:25:35.117258 139637719117824 attention.py:418] Single window, no scan.
I0123 19:25:35.117294 139637719117824 transformer_layer.py:389] tlayer: self-attention.
I0123 19:25:35.117327 139637719117824 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.117400 139637719117824 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.118085 139637719117824 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.118162 139637719117824 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.118526 139637719117824 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.119226 139637719117824 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.121733 139637719117824 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.122363 139637719117824 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.122438 139637719117824 transformer_layer.py:468] tlayer: End windows.
I0123 19:25:35.122473 139637719117824 transformer_layer.py:472] tlayer: final FFN.
I0123 19:25:35.122534 139637719117824 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.122664 139637719117824 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:25:35.122774 139637719117824 nn_components.py:325] mlp: activation = None
I0123 19:25:35.122817 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:25:35.124808 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.124901 139637719117824 nn_components.py:261] mlp: residual
I0123 19:25:35.127398 139637719117824 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.127477 139637719117824 transformer_base.py:443] tbase: final FFN
I0123 19:25:35.127587 139637719117824 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:25:35.129844 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:25:35.131706 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.131802 139637719117824 nn_components.py:261] mlp: residual
I0123 19:25:35.132087 139637719117824 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.132170 139637719117824 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 19:25:35.132282 139637719117824 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:25:35.132322 139637719117824 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:25:35.132353 139637719117824 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:25:35.132419 139637719117824 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.134666 139637719117824 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:25:35.140177 139637719117824 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.140433 139637719117824 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:25:35.143070 139637719117824 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:25:35.155739 139637719117824 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:25:35.155796 139637719117824 attention.py:418] Single window, no scan.
I0123 19:25:35.155831 139637719117824 transformer_layer.py:389] tlayer: self-attention.
I0123 19:25:35.155861 139637719117824 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.155921 139637719117824 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.156482 139637719117824 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.156556 139637719117824 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.156911 139637719117824 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.157595 139637719117824 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.160052 139637719117824 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.160712 139637719117824 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.160789 139637719117824 transformer_layer.py:468] tlayer: End windows.
I0123 19:25:35.160824 139637719117824 transformer_layer.py:472] tlayer: final FFN.
I0123 19:25:35.160882 139637719117824 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.161012 139637719117824 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:25:35.161121 139637719117824 nn_components.py:325] mlp: activation = None
I0123 19:25:35.161159 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:25:35.163057 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.163149 139637719117824 nn_components.py:261] mlp: residual
I0123 19:25:35.165527 139637719117824 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.165611 139637719117824 transformer_base.py:443] tbase: final FFN
I0123 19:25:35.165723 139637719117824 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:25:35.167928 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:25:35.169868 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.169963 139637719117824 nn_components.py:261] mlp: residual
I0123 19:25:35.170248 139637719117824 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.170328 139637719117824 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 19:25:35.170438 139637719117824 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:25:35.170477 139637719117824 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:25:35.170507 139637719117824 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:25:35.170569 139637719117824 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.172788 139637719117824 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:25:35.178216 139637719117824 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.178470 139637719117824 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:25:35.181124 139637719117824 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:25:35.194042 139637719117824 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:25:35.194097 139637719117824 attention.py:418] Single window, no scan.
I0123 19:25:35.194132 139637719117824 transformer_layer.py:389] tlayer: self-attention.
I0123 19:25:35.194162 139637719117824 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.194224 139637719117824 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.194831 139637719117824 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.194905 139637719117824 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.195259 139637719117824 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.195945 139637719117824 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.198400 139637719117824 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.199028 139637719117824 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.199103 139637719117824 transformer_layer.py:468] tlayer: End windows.
I0123 19:25:35.199138 139637719117824 transformer_layer.py:472] tlayer: final FFN.
I0123 19:25:35.199193 139637719117824 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.199319 139637719117824 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:25:35.199427 139637719117824 nn_components.py:325] mlp: activation = None
I0123 19:25:35.199465 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:25:35.201319 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.201416 139637719117824 nn_components.py:261] mlp: residual
I0123 19:25:35.203859 139637719117824 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.203938 139637719117824 transformer_base.py:443] tbase: final FFN
I0123 19:25:35.204042 139637719117824 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:25:35.206264 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:25:35.208110 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.208201 139637719117824 nn_components.py:261] mlp: residual
I0123 19:25:35.208480 139637719117824 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.208559 139637719117824 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 19:25:35.208667 139637719117824 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:25:35.208705 139637719117824 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:25:35.208734 139637719117824 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:25:35.208796 139637719117824 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.211029 139637719117824 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:25:35.216572 139637719117824 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.216828 139637719117824 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:25:35.219463 139637719117824 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:25:35.232039 139637719117824 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:25:35.232094 139637719117824 attention.py:418] Single window, no scan.
I0123 19:25:35.232128 139637719117824 transformer_layer.py:389] tlayer: self-attention.
I0123 19:25:35.232158 139637719117824 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.232218 139637719117824 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.232773 139637719117824 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.232847 139637719117824 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.233191 139637719117824 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.233897 139637719117824 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.236347 139637719117824 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.237005 139637719117824 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.237081 139637719117824 transformer_layer.py:468] tlayer: End windows.
I0123 19:25:35.237119 139637719117824 transformer_layer.py:472] tlayer: final FFN.
I0123 19:25:35.237177 139637719117824 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.237310 139637719117824 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:25:35.237418 139637719117824 nn_components.py:325] mlp: activation = None
I0123 19:25:35.237456 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:25:35.239319 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.239417 139637719117824 nn_components.py:261] mlp: residual
I0123 19:25:35.241827 139637719117824 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.241906 139637719117824 transformer_base.py:443] tbase: final FFN
I0123 19:25:35.242011 139637719117824 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:25:35.244211 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:25:35.246138 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.246232 139637719117824 nn_components.py:261] mlp: residual
I0123 19:25:35.246514 139637719117824 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.246594 139637719117824 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 19:25:35.246701 139637719117824 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:25:35.246739 139637719117824 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:25:35.246769 139637719117824 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:25:35.246830 139637719117824 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.249062 139637719117824 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:25:35.254483 139637719117824 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.254737 139637719117824 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:25:35.257391 139637719117824 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:25:35.269855 139637719117824 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:25:35.269910 139637719117824 attention.py:418] Single window, no scan.
I0123 19:25:35.269944 139637719117824 transformer_layer.py:389] tlayer: self-attention.
I0123 19:25:35.269974 139637719117824 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.270036 139637719117824 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.270591 139637719117824 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.270665 139637719117824 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.271016 139637719117824 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.271739 139637719117824 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.274211 139637719117824 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.274829 139637719117824 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.274905 139637719117824 transformer_layer.py:468] tlayer: End windows.
I0123 19:25:35.274938 139637719117824 transformer_layer.py:472] tlayer: final FFN.
I0123 19:25:35.274994 139637719117824 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.275122 139637719117824 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:25:35.275231 139637719117824 nn_components.py:325] mlp: activation = None
I0123 19:25:35.275269 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:25:35.277136 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.277229 139637719117824 nn_components.py:261] mlp: residual
I0123 19:25:35.279633 139637719117824 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.279711 139637719117824 transformer_base.py:443] tbase: final FFN
I0123 19:25:35.279817 139637719117824 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:25:35.282095 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:25:35.283946 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.284040 139637719117824 nn_components.py:261] mlp: residual
I0123 19:25:35.284320 139637719117824 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.284409 139637719117824 decoder_stack.py:344] dstack: Final layernorm.
I0123 19:25:35.287271 139637719117824 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:25:35.342941 139637719117824 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.343025 139637719117824 decoder_stack.py:333] dstack: autoregressive generator.
I0123 19:25:35.343080 139637719117824 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 19:25:35.343187 139637719117824 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:25:35.343225 139637719117824 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:25:35.343255 139637719117824 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:25:35.343316 139637719117824 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.345991 139637719117824 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:25:35.351370 139637719117824 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.351627 139637719117824 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:25:35.354196 139637719117824 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:25:35.366592 139637719117824 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:25:35.366647 139637719117824 attention.py:418] Single window, no scan.
I0123 19:25:35.366682 139637719117824 transformer_layer.py:389] tlayer: self-attention.
I0123 19:25:35.366712 139637719117824 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.366772 139637719117824 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.367324 139637719117824 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.367399 139637719117824 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.367752 139637719117824 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.368421 139637719117824 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.370892 139637719117824 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.371502 139637719117824 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.371578 139637719117824 transformer_layer.py:468] tlayer: End windows.
I0123 19:25:35.371611 139637719117824 transformer_layer.py:472] tlayer: final FFN.
I0123 19:25:35.371670 139637719117824 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.371799 139637719117824 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:25:35.371913 139637719117824 nn_components.py:325] mlp: activation = None
I0123 19:25:35.371952 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:25:35.373795 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.373888 139637719117824 nn_components.py:261] mlp: residual
I0123 19:25:35.376288 139637719117824 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.376365 139637719117824 transformer_base.py:443] tbase: final FFN
I0123 19:25:35.376471 139637719117824 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:25:35.378718 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:25:35.380569 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.380663 139637719117824 nn_components.py:261] mlp: residual
I0123 19:25:35.380946 139637719117824 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.381025 139637719117824 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 19:25:35.381132 139637719117824 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:25:35.381170 139637719117824 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:25:35.381200 139637719117824 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:25:35.381263 139637719117824 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.383523 139637719117824 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:25:35.388930 139637719117824 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.389188 139637719117824 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:25:35.391843 139637719117824 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:25:35.404143 139637719117824 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:25:35.404198 139637719117824 attention.py:418] Single window, no scan.
I0123 19:25:35.404233 139637719117824 transformer_layer.py:389] tlayer: self-attention.
I0123 19:25:35.404263 139637719117824 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.404323 139637719117824 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.404875 139637719117824 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.404950 139637719117824 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.405304 139637719117824 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.405980 139637719117824 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.408445 139637719117824 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.409053 139637719117824 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.409128 139637719117824 transformer_layer.py:468] tlayer: End windows.
I0123 19:25:35.409162 139637719117824 transformer_layer.py:472] tlayer: final FFN.
I0123 19:25:35.409221 139637719117824 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.409346 139637719117824 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:25:35.409452 139637719117824 nn_components.py:325] mlp: activation = None
I0123 19:25:35.409496 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:25:35.411347 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.411440 139637719117824 nn_components.py:261] mlp: residual
I0123 19:25:35.413823 139637719117824 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.413903 139637719117824 transformer_base.py:443] tbase: final FFN
I0123 19:25:35.414012 139637719117824 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:25:35.416261 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:25:35.418104 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.418200 139637719117824 nn_components.py:261] mlp: residual
I0123 19:25:35.418485 139637719117824 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.418564 139637719117824 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 19:25:35.418669 139637719117824 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:25:35.418707 139637719117824 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:25:35.418737 139637719117824 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:25:35.418797 139637719117824 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.420995 139637719117824 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:25:35.426365 139637719117824 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.426619 139637719117824 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:25:35.429246 139637719117824 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:25:35.441524 139637719117824 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:25:35.441579 139637719117824 attention.py:418] Single window, no scan.
I0123 19:25:35.441613 139637719117824 transformer_layer.py:389] tlayer: self-attention.
I0123 19:25:35.441649 139637719117824 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.441713 139637719117824 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.442263 139637719117824 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.442338 139637719117824 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.442688 139637719117824 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.443363 139637719117824 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.445823 139637719117824 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.446428 139637719117824 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.446503 139637719117824 transformer_layer.py:468] tlayer: End windows.
I0123 19:25:35.446536 139637719117824 transformer_layer.py:472] tlayer: final FFN.
I0123 19:25:35.446593 139637719117824 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.446717 139637719117824 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:25:35.446821 139637719117824 nn_components.py:325] mlp: activation = None
I0123 19:25:35.446859 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:25:35.448707 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.448800 139637719117824 nn_components.py:261] mlp: residual
I0123 19:25:35.451162 139637719117824 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.451240 139637719117824 transformer_base.py:443] tbase: final FFN
I0123 19:25:35.451347 139637719117824 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:25:35.454035 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:25:35.455866 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.455961 139637719117824 nn_components.py:261] mlp: residual
I0123 19:25:35.456247 139637719117824 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.456327 139637719117824 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 19:25:35.456433 139637719117824 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:25:35.456471 139637719117824 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:25:35.456501 139637719117824 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:25:35.456563 139637719117824 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.458775 139637719117824 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:25:35.464090 139637719117824 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.464346 139637719117824 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:25:35.466967 139637719117824 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:25:35.479334 139637719117824 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:25:35.479389 139637719117824 attention.py:418] Single window, no scan.
I0123 19:25:35.479426 139637719117824 transformer_layer.py:389] tlayer: self-attention.
I0123 19:25:35.479462 139637719117824 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.479524 139637719117824 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.480085 139637719117824 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.480158 139637719117824 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.480512 139637719117824 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.481188 139637719117824 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.483683 139637719117824 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.484312 139637719117824 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.484387 139637719117824 transformer_layer.py:468] tlayer: End windows.
I0123 19:25:35.484419 139637719117824 transformer_layer.py:472] tlayer: final FFN.
I0123 19:25:35.484475 139637719117824 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.484601 139637719117824 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:25:35.484704 139637719117824 nn_components.py:325] mlp: activation = None
I0123 19:25:35.484743 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:25:35.486595 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.486686 139637719117824 nn_components.py:261] mlp: residual
I0123 19:25:35.489056 139637719117824 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.489132 139637719117824 transformer_base.py:443] tbase: final FFN
I0123 19:25:35.489237 139637719117824 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:25:35.491494 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:25:35.493326 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.493417 139637719117824 nn_components.py:261] mlp: residual
I0123 19:25:35.493702 139637719117824 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.493781 139637719117824 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 19:25:35.493885 139637719117824 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:25:35.493922 139637719117824 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:25:35.493950 139637719117824 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:25:35.494010 139637719117824 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.496201 139637719117824 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:25:35.501565 139637719117824 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.501821 139637719117824 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:25:35.504465 139637719117824 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:25:35.516851 139637719117824 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:25:35.516906 139637719117824 attention.py:418] Single window, no scan.
I0123 19:25:35.516939 139637719117824 transformer_layer.py:389] tlayer: self-attention.
I0123 19:25:35.516967 139637719117824 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.517026 139637719117824 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.517575 139637719117824 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.517651 139637719117824 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.518004 139637719117824 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.518673 139637719117824 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.521156 139637719117824 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.521781 139637719117824 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.521856 139637719117824 transformer_layer.py:468] tlayer: End windows.
I0123 19:25:35.521890 139637719117824 transformer_layer.py:472] tlayer: final FFN.
I0123 19:25:35.521947 139637719117824 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.522070 139637719117824 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:25:35.522175 139637719117824 nn_components.py:325] mlp: activation = None
I0123 19:25:35.522212 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:25:35.524060 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.524158 139637719117824 nn_components.py:261] mlp: residual
I0123 19:25:35.526537 139637719117824 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.526616 139637719117824 transformer_base.py:443] tbase: final FFN
I0123 19:25:35.526721 139637719117824 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:25:35.528975 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:25:35.530836 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.530930 139637719117824 nn_components.py:261] mlp: residual
I0123 19:25:35.531211 139637719117824 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.531289 139637719117824 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 19:25:35.531393 139637719117824 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:25:35.531430 139637719117824 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:25:35.531458 139637719117824 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:25:35.531519 139637719117824 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.533731 139637719117824 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:25:35.539088 139637719117824 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.539344 139637719117824 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:25:35.542031 139637719117824 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:25:35.554429 139637719117824 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:25:35.554483 139637719117824 attention.py:418] Single window, no scan.
I0123 19:25:35.554517 139637719117824 transformer_layer.py:389] tlayer: self-attention.
I0123 19:25:35.554546 139637719117824 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.554605 139637719117824 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.555160 139637719117824 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.555234 139637719117824 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.555589 139637719117824 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.556269 139637719117824 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.558785 139637719117824 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.559401 139637719117824 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.559476 139637719117824 transformer_layer.py:468] tlayer: End windows.
I0123 19:25:35.559509 139637719117824 transformer_layer.py:472] tlayer: final FFN.
I0123 19:25:35.559566 139637719117824 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.559690 139637719117824 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:25:35.559796 139637719117824 nn_components.py:325] mlp: activation = None
I0123 19:25:35.559832 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:25:35.561678 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.561774 139637719117824 nn_components.py:261] mlp: residual
I0123 19:25:35.564131 139637719117824 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.564211 139637719117824 transformer_base.py:443] tbase: final FFN
I0123 19:25:35.564318 139637719117824 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:25:35.566982 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:25:35.568825 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.568917 139637719117824 nn_components.py:261] mlp: residual
I0123 19:25:35.569197 139637719117824 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.569276 139637719117824 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 19:25:35.569382 139637719117824 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:25:35.569420 139637719117824 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:25:35.569449 139637719117824 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:25:35.569508 139637719117824 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.571718 139637719117824 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:25:35.577067 139637719117824 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.577318 139637719117824 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:25:35.579960 139637719117824 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:25:35.592413 139637719117824 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:25:35.592467 139637719117824 attention.py:418] Single window, no scan.
I0123 19:25:35.592501 139637719117824 transformer_layer.py:389] tlayer: self-attention.
I0123 19:25:35.592530 139637719117824 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.592588 139637719117824 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.593139 139637719117824 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.593212 139637719117824 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.593561 139637719117824 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.594253 139637719117824 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.596728 139637719117824 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.597348 139637719117824 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.597421 139637719117824 transformer_layer.py:468] tlayer: End windows.
I0123 19:25:35.597454 139637719117824 transformer_layer.py:472] tlayer: final FFN.
I0123 19:25:35.597511 139637719117824 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.597635 139637719117824 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:25:35.597753 139637719117824 nn_components.py:325] mlp: activation = None
I0123 19:25:35.597790 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:25:35.599638 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.599729 139637719117824 nn_components.py:261] mlp: residual
I0123 19:25:35.602081 139637719117824 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.602162 139637719117824 transformer_base.py:443] tbase: final FFN
I0123 19:25:35.602268 139637719117824 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:25:35.604507 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:25:35.606348 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.606441 139637719117824 nn_components.py:261] mlp: residual
I0123 19:25:35.606717 139637719117824 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.606795 139637719117824 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 19:25:35.606900 139637719117824 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:25:35.606937 139637719117824 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:25:35.606965 139637719117824 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:25:35.607025 139637719117824 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.609217 139637719117824 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:25:35.614562 139637719117824 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.614815 139637719117824 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:25:35.617462 139637719117824 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:25:35.629831 139637719117824 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:25:35.629884 139637719117824 attention.py:418] Single window, no scan.
I0123 19:25:35.629917 139637719117824 transformer_layer.py:389] tlayer: self-attention.
I0123 19:25:35.629945 139637719117824 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.630004 139637719117824 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.630557 139637719117824 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.630630 139637719117824 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.630981 139637719117824 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.631653 139637719117824 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.634132 139637719117824 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.634753 139637719117824 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.634826 139637719117824 transformer_layer.py:468] tlayer: End windows.
I0123 19:25:35.634860 139637719117824 transformer_layer.py:472] tlayer: final FFN.
I0123 19:25:35.634915 139637719117824 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.635037 139637719117824 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:25:35.635141 139637719117824 nn_components.py:325] mlp: activation = None
I0123 19:25:35.635177 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:25:35.637032 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.637122 139637719117824 nn_components.py:261] mlp: residual
I0123 19:25:35.639477 139637719117824 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.639560 139637719117824 transformer_base.py:443] tbase: final FFN
I0123 19:25:35.639671 139637719117824 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:25:35.641918 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:25:35.643743 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.643836 139637719117824 nn_components.py:261] mlp: residual
I0123 19:25:35.644113 139637719117824 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.644190 139637719117824 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 19:25:35.644294 139637719117824 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:25:35.644331 139637719117824 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:25:35.644358 139637719117824 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:25:35.644416 139637719117824 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.646612 139637719117824 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:25:35.651970 139637719117824 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.652227 139637719117824 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:25:35.654881 139637719117824 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:25:35.667321 139637719117824 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:25:35.667378 139637719117824 attention.py:418] Single window, no scan.
I0123 19:25:35.667412 139637719117824 transformer_layer.py:389] tlayer: self-attention.
I0123 19:25:35.667441 139637719117824 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.667500 139637719117824 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.668061 139637719117824 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.668135 139637719117824 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.668481 139637719117824 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.669160 139637719117824 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.671665 139637719117824 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.672281 139637719117824 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.672355 139637719117824 transformer_layer.py:468] tlayer: End windows.
I0123 19:25:35.672388 139637719117824 transformer_layer.py:472] tlayer: final FFN.
I0123 19:25:35.672443 139637719117824 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.672567 139637719117824 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:25:35.672672 139637719117824 nn_components.py:325] mlp: activation = None
I0123 19:25:35.672709 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:25:35.674570 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.674660 139637719117824 nn_components.py:261] mlp: residual
I0123 19:25:35.677020 139637719117824 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.677102 139637719117824 transformer_base.py:443] tbase: final FFN
I0123 19:25:35.677209 139637719117824 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:25:35.679846 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:25:35.681690 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.681782 139637719117824 nn_components.py:261] mlp: residual
I0123 19:25:35.682063 139637719117824 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.682142 139637719117824 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 19:25:35.682247 139637719117824 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:25:35.682284 139637719117824 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:25:35.682312 139637719117824 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:25:35.682373 139637719117824 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.684572 139637719117824 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:25:35.689938 139637719117824 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.690192 139637719117824 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:25:35.692833 139637719117824 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:25:35.705324 139637719117824 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:25:35.705378 139637719117824 attention.py:418] Single window, no scan.
I0123 19:25:35.705411 139637719117824 transformer_layer.py:389] tlayer: self-attention.
I0123 19:25:35.705439 139637719117824 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.705499 139637719117824 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.706059 139637719117824 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.706133 139637719117824 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.706484 139637719117824 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.707160 139637719117824 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.709651 139637719117824 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.710271 139637719117824 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.710347 139637719117824 transformer_layer.py:468] tlayer: End windows.
I0123 19:25:35.710380 139637719117824 transformer_layer.py:472] tlayer: final FFN.
I0123 19:25:35.710435 139637719117824 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.710557 139637719117824 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:25:35.710661 139637719117824 nn_components.py:325] mlp: activation = None
I0123 19:25:35.710697 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:25:35.713091 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.713183 139637719117824 nn_components.py:261] mlp: residual
I0123 19:25:35.715530 139637719117824 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.715608 139637719117824 transformer_base.py:443] tbase: final FFN
I0123 19:25:35.715719 139637719117824 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:25:35.717938 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:25:35.719766 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.719858 139637719117824 nn_components.py:261] mlp: residual
I0123 19:25:35.720138 139637719117824 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.720216 139637719117824 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 19:25:35.720321 139637719117824 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:25:35.720358 139637719117824 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:25:35.720386 139637719117824 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:25:35.720446 139637719117824 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.722649 139637719117824 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:25:35.727998 139637719117824 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.728251 139637719117824 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:25:35.730860 139637719117824 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:25:35.743158 139637719117824 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:25:35.743211 139637719117824 attention.py:418] Single window, no scan.
I0123 19:25:35.743245 139637719117824 transformer_layer.py:389] tlayer: self-attention.
I0123 19:25:35.743273 139637719117824 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.743333 139637719117824 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.743885 139637719117824 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.743958 139637719117824 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.744311 139637719117824 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.744982 139637719117824 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.747463 139637719117824 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.748089 139637719117824 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.748165 139637719117824 transformer_layer.py:468] tlayer: End windows.
I0123 19:25:35.748198 139637719117824 transformer_layer.py:472] tlayer: final FFN.
I0123 19:25:35.748253 139637719117824 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.748379 139637719117824 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:25:35.748484 139637719117824 nn_components.py:325] mlp: activation = None
I0123 19:25:35.748520 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:25:35.750366 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.750456 139637719117824 nn_components.py:261] mlp: residual
I0123 19:25:35.752791 139637719117824 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.752866 139637719117824 transformer_base.py:443] tbase: final FFN
I0123 19:25:35.752970 139637719117824 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:25:35.755236 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:25:35.757068 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.757160 139637719117824 nn_components.py:261] mlp: residual
I0123 19:25:35.757439 139637719117824 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.757518 139637719117824 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 19:25:35.757622 139637719117824 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:25:35.757664 139637719117824 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:25:35.757695 139637719117824 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:25:35.757756 139637719117824 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.759950 139637719117824 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:25:35.765272 139637719117824 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.765522 139637719117824 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:25:35.768158 139637719117824 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:25:35.780486 139637719117824 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:25:35.780541 139637719117824 attention.py:418] Single window, no scan.
I0123 19:25:35.780574 139637719117824 transformer_layer.py:389] tlayer: self-attention.
I0123 19:25:35.780603 139637719117824 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.780663 139637719117824 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.781210 139637719117824 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.781284 139637719117824 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.781628 139637719117824 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.782324 139637719117824 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.784803 139637719117824 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.785417 139637719117824 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.785493 139637719117824 transformer_layer.py:468] tlayer: End windows.
I0123 19:25:35.785526 139637719117824 transformer_layer.py:472] tlayer: final FFN.
I0123 19:25:35.785582 139637719117824 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.785715 139637719117824 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:25:35.785827 139637719117824 nn_components.py:325] mlp: activation = None
I0123 19:25:35.785867 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:25:35.787703 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.787792 139637719117824 nn_components.py:261] mlp: residual
I0123 19:25:35.790164 139637719117824 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.790245 139637719117824 transformer_base.py:443] tbase: final FFN
I0123 19:25:35.790352 139637719117824 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:25:35.792994 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:25:35.794852 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.794945 139637719117824 nn_components.py:261] mlp: residual
I0123 19:25:35.795227 139637719117824 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:35.795309 139637719117824 decoder_stack.py:344] dstack: Final layernorm.
I0123 19:25:35.798146 139637719117824 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:25:40.206189 139637719117824 optimizer_config.py:74] Using Flax Adafactor Optimizer. lr=1.000000, b1=0.900000
I0123 19:25:40.747165 139637719117824 training_loop.py:409] No working directory specified.
I0123 19:25:40.747281 139637719117824 training_loop.py:431] Loading pre-trained model from ag_ckpt_vocab:
I0123 19:25:40.748050 139637719117824 checkpoints.py:1062] Restoring legacy Flax checkpoint from ag_ckpt_vocab/checkpoint_10999999
I0123 19:25:44.241592 139637719117824 training_loop.py:447] Only restoring trainable parameters.
I0123 19:25:44.242244 139637719117824 training_loop.py:724] parameter: decoder/embed/embedding, shape (1024, 1024), size 1048576
I0123 19:25:44.242324 139637719117824 training_loop.py:724] parameter: decoder/final_layernorm/scale, shape (1024,), size 1024
I0123 19:25:44.242375 139637719117824 training_loop.py:724] parameter: decoder/transformer0/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 19:25:44.242419 139637719117824 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 19:25:44.242460 139637719117824 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:25:44.242500 139637719117824 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 19:25:44.242538 139637719117824 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:25:44.242576 139637719117824 training_loop.py:724] parameter: decoder/transformer0/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:25:44.242612 139637719117824 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 19:25:44.242649 139637719117824 training_loop.py:724] parameter: decoder/transformer0/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 19:25:44.242687 139637719117824 training_loop.py:724] parameter: decoder/transformer0/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:25:44.242726 139637719117824 training_loop.py:724] parameter: decoder/transformer0/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 19:25:44.242764 139637719117824 training_loop.py:724] parameter: decoder/transformer1/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 19:25:44.242801 139637719117824 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 19:25:44.242836 139637719117824 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:25:44.242872 139637719117824 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 19:25:44.242908 139637719117824 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:25:44.242943 139637719117824 training_loop.py:724] parameter: decoder/transformer1/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:25:44.242981 139637719117824 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 19:25:44.243016 139637719117824 training_loop.py:724] parameter: decoder/transformer1/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 19:25:44.243064 139637719117824 training_loop.py:724] parameter: decoder/transformer1/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:25:44.243101 139637719117824 training_loop.py:724] parameter: decoder/transformer1/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 19:25:44.243138 139637719117824 training_loop.py:724] parameter: decoder/transformer10/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 19:25:44.243175 139637719117824 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 19:25:44.243211 139637719117824 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:25:44.243248 139637719117824 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 19:25:44.243285 139637719117824 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:25:44.243321 139637719117824 training_loop.py:724] parameter: decoder/transformer10/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:25:44.243357 139637719117824 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 19:25:44.243393 139637719117824 training_loop.py:724] parameter: decoder/transformer10/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 19:25:44.243429 139637719117824 training_loop.py:724] parameter: decoder/transformer10/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:25:44.243465 139637719117824 training_loop.py:724] parameter: decoder/transformer10/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 19:25:44.243501 139637719117824 training_loop.py:724] parameter: decoder/transformer11/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 19:25:44.243537 139637719117824 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 19:25:44.243572 139637719117824 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:25:44.243608 139637719117824 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 19:25:44.243644 139637719117824 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:25:44.243680 139637719117824 training_loop.py:724] parameter: decoder/transformer11/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:25:44.243716 139637719117824 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 19:25:44.243753 139637719117824 training_loop.py:724] parameter: decoder/transformer11/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 19:25:44.243788 139637719117824 training_loop.py:724] parameter: decoder/transformer11/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:25:44.243824 139637719117824 training_loop.py:724] parameter: decoder/transformer11/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 19:25:44.243860 139637719117824 training_loop.py:724] parameter: decoder/transformer2/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 19:25:44.243895 139637719117824 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 19:25:44.243932 139637719117824 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:25:44.243968 139637719117824 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 19:25:44.244010 139637719117824 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:25:44.244047 139637719117824 training_loop.py:724] parameter: decoder/transformer2/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:25:44.244083 139637719117824 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 19:25:44.244119 139637719117824 training_loop.py:724] parameter: decoder/transformer2/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 19:25:44.244155 139637719117824 training_loop.py:724] parameter: decoder/transformer2/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:25:44.244191 139637719117824 training_loop.py:724] parameter: decoder/transformer2/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 19:25:44.244226 139637719117824 training_loop.py:724] parameter: decoder/transformer3/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 19:25:44.244261 139637719117824 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 19:25:44.244296 139637719117824 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:25:44.244331 139637719117824 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 19:25:44.244366 139637719117824 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:25:44.244401 139637719117824 training_loop.py:724] parameter: decoder/transformer3/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:25:44.244436 139637719117824 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 19:25:44.244472 139637719117824 training_loop.py:724] parameter: decoder/transformer3/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 19:25:44.244507 139637719117824 training_loop.py:724] parameter: decoder/transformer3/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:25:44.244541 139637719117824 training_loop.py:724] parameter: decoder/transformer3/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 19:25:44.244575 139637719117824 training_loop.py:724] parameter: decoder/transformer4/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 19:25:44.244610 139637719117824 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 19:25:44.244644 139637719117824 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:25:44.244679 139637719117824 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 19:25:44.244714 139637719117824 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:25:44.244750 139637719117824 training_loop.py:724] parameter: decoder/transformer4/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:25:44.244785 139637719117824 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 19:25:44.244820 139637719117824 training_loop.py:724] parameter: decoder/transformer4/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 19:25:44.244855 139637719117824 training_loop.py:724] parameter: decoder/transformer4/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:25:44.244890 139637719117824 training_loop.py:724] parameter: decoder/transformer4/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 19:25:44.244925 139637719117824 training_loop.py:724] parameter: decoder/transformer5/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 19:25:44.244965 139637719117824 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 19:25:44.245002 139637719117824 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:25:44.245039 139637719117824 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 19:25:44.245074 139637719117824 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:25:44.245109 139637719117824 training_loop.py:724] parameter: decoder/transformer5/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:25:44.245143 139637719117824 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 19:25:44.245178 139637719117824 training_loop.py:724] parameter: decoder/transformer5/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 19:25:44.245214 139637719117824 training_loop.py:724] parameter: decoder/transformer5/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:25:44.245251 139637719117824 training_loop.py:724] parameter: decoder/transformer5/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 19:25:44.245287 139637719117824 training_loop.py:724] parameter: decoder/transformer6/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 19:25:44.245323 139637719117824 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 19:25:44.245359 139637719117824 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:25:44.245395 139637719117824 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 19:25:44.245431 139637719117824 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:25:44.245466 139637719117824 training_loop.py:724] parameter: decoder/transformer6/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:25:44.245501 139637719117824 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 19:25:44.245537 139637719117824 training_loop.py:724] parameter: decoder/transformer6/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 19:25:44.245576 139637719117824 training_loop.py:724] parameter: decoder/transformer6/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:25:44.245613 139637719117824 training_loop.py:724] parameter: decoder/transformer6/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 19:25:44.245659 139637719117824 training_loop.py:724] parameter: decoder/transformer7/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 19:25:44.245699 139637719117824 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 19:25:44.245739 139637719117824 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:25:44.245775 139637719117824 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 19:25:44.245811 139637719117824 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:25:44.245847 139637719117824 training_loop.py:724] parameter: decoder/transformer7/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:25:44.245883 139637719117824 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 19:25:44.245918 139637719117824 training_loop.py:724] parameter: decoder/transformer7/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 19:25:44.245960 139637719117824 training_loop.py:724] parameter: decoder/transformer7/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:25:44.245997 139637719117824 training_loop.py:724] parameter: decoder/transformer7/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 19:25:44.246034 139637719117824 training_loop.py:724] parameter: decoder/transformer8/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 19:25:44.246070 139637719117824 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 19:25:44.246106 139637719117824 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:25:44.246142 139637719117824 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 19:25:44.246177 139637719117824 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:25:44.246213 139637719117824 training_loop.py:724] parameter: decoder/transformer8/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:25:44.246249 139637719117824 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 19:25:44.246284 139637719117824 training_loop.py:724] parameter: decoder/transformer8/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 19:25:44.246319 139637719117824 training_loop.py:724] parameter: decoder/transformer8/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:25:44.246354 139637719117824 training_loop.py:724] parameter: decoder/transformer8/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 19:25:44.246389 139637719117824 training_loop.py:724] parameter: decoder/transformer9/relative_positions/rel_embedding, shape (8, 32), size 256
I0123 19:25:44.246424 139637719117824 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/attention_scale, shape (8,), size 8
I0123 19:25:44.246458 139637719117824 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/keys_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:25:44.246492 139637719117824 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/pre_attn_layernorm/scale, shape (1024,), size 1024
I0123 19:25:44.246527 139637719117824 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/queries_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:25:44.246562 139637719117824 training_loop.py:724] parameter: decoder/transformer9/tbase/_kvq/values_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:25:44.246598 139637719117824 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/hidden0/kernel, shape (1024, 4096), size 4194304
I0123 19:25:44.246632 139637719117824 training_loop.py:724] parameter: decoder/transformer9/tbase/ffn/output_layer/kernel, shape (4096, 1024), size 4194304
I0123 19:25:44.246667 139637719117824 training_loop.py:724] parameter: decoder/transformer9/tbase/post_attn_mlp/output_layer/kernel, shape (1024, 1024), size 1048576
I0123 19:25:44.246702 139637719117824 training_loop.py:724] parameter: decoder/transformer9/tbase/pre_ffn_layernorm/scale, shape (1024,), size 1024
I0123 19:25:44.246728 139637719117824 training_loop.py:725] Total parameters: 152072288
I0123 19:25:44.246956 139637719117824 training_loop.py:739] Total state size: 0
I0123 19:25:44.271013 139637719117824 training_loop.py:492] Training loop: creating task for mode beam_search
I0123 19:25:44.271265 139637719117824 training_loop.py:685] Creating logging writer (train) for mode beam_search
I0123 19:25:44.271658 139637719117824 training_loop.py:652] Compiling mode beam_search with jit.
I0123 19:25:44.271997 139637719117824 training_loop.py:89] registering functions: dict_keys([])
I0123 19:25:44.288805 139637719117824 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k ? simtri b a c n o m
I0123 19:25:47.358028 139637719117824 ddar.py:60] Depth 1/1000 time = 3.0045199394226074
I0123 19:25:58.721838 139637719117824 ddar.py:60] Depth 2/1000 time = 11.363595485687256
I0123 19:26:11.242411 139637719117824 ddar.py:60] Depth 3/1000 time = 12.520255327224731
I0123 19:26:24.030944 139637719117824 ddar.py:60] Depth 4/1000 time = 12.788063287734985
I0123 19:26:36.762696 139637719117824 ddar.py:60] Depth 5/1000 time = 12.73058009147644
I0123 19:26:49.842092 139637719117824 ddar.py:60] Depth 6/1000 time = 13.01222276687622
I0123 19:26:49.842725 139637719117824 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 19:26:49.842853 139637719117824 alphageometry.py:540] Depth 0. There are 1 nodes to expand:
I0123 19:26:49.842895 139637719117824 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C c d e 02 D c d d e 03 ; f : C b d f 04 D b d d f 05 ; g : C a d g 06 D a d d g 07 ; h : C a b h 08 ; i : C b c i 09 T b h h i 10 ; j : C h i j 11 ; k : C b c k 12 T b c j k 13 ; l : C a c l 14 T a c j l 15 ; m : C e h m 16 D e h h m 17 ; n : C f l n 18 D f l l n 19 ; o : C g k o 20 D g k k o 21 ? S b a c n o m {F1} x00
I0123 19:26:49.842927 139637719117824 alphageometry.py:549] Decoding from {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C c d e 02 D c d d e 03 ; f : C b d f 04 D b d d f 05 ; g : C a d g 06 D a d d g 07 ; h : C a b h 08 ; i : C b c i 09 T b h h i 10 ; j : C h i j 11 ; k : C b c k 12 T b c j k 13 ; l : C a c l 14 T a c j l 15 ; m : C e h m 16 D e h h m 17 ; n : C f l n 18 D f l l n 19 ; o : C g k o 20 D g k k o 21 ? S b a c n o m {F1} x00
I0123 19:26:49.988272 139637719117824 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:49.988453 139637719117824 decoder_stack.py:316] dstack: scanning over 1 windows.
I0123 19:26:49.988553 139637719117824 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 19:26:49.988628 139637719117824 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 19:26:49.988699 139637719117824 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 19:26:49.988768 139637719117824 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 19:26:49.988836 139637719117824 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 19:26:49.988903 139637719117824 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 19:26:49.988970 139637719117824 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 19:26:49.989036 139637719117824 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 19:26:49.989101 139637719117824 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 19:26:49.989168 139637719117824 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 19:26:49.989236 139637719117824 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 19:26:49.989301 139637719117824 transformer_layer.py:657] tlayer: Skipping XL cache for mode beam_search.
I0123 19:26:49.989341 139637719117824 attention.py:418] Single window, no scan.
I0123 19:26:49.989386 139637719117824 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 19:26:49.989493 139637719117824 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:26:49.989531 139637719117824 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:26:49.989561 139637719117824 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:26:49.991461 139637719117824 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:49.994037 139637719117824 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:26:49.999692 139637719117824 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:49.999963 139637719117824 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:26:50.002549 139637719117824 transformer_layer.py:213] tlayer: windowed attention.
I0123 19:26:50.006434 139637719117824 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:26:50.006489 139637719117824 attention.py:418] Single window, no scan.
I0123 19:26:50.006524 139637719117824 transformer_layer.py:389] tlayer: self-attention.
I0123 19:26:50.006557 139637719117824 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.006619 139637719117824 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.007220 139637719117824 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.007292 139637719117824 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.007641 139637719117824 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.008387 139637719117824 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.010861 139637719117824 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.011533 139637719117824 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.011608 139637719117824 transformer_layer.py:468] tlayer: End windows.
I0123 19:26:50.011640 139637719117824 transformer_layer.py:472] tlayer: final FFN.
I0123 19:26:50.011697 139637719117824 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.011822 139637719117824 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:26:50.012140 139637719117824 nn_components.py:325] mlp: activation = None
I0123 19:26:50.012181 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:26:50.014063 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.014156 139637719117824 nn_components.py:261] mlp: residual
I0123 19:26:50.016572 139637719117824 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.016649 139637719117824 transformer_base.py:443] tbase: final FFN
I0123 19:26:50.017062 139637719117824 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:26:50.019425 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:26:50.021330 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.021422 139637719117824 nn_components.py:261] mlp: residual
I0123 19:26:50.021715 139637719117824 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.021795 139637719117824 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 19:26:50.021899 139637719117824 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:26:50.021936 139637719117824 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:26:50.021965 139637719117824 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:26:50.023728 139637719117824 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.026001 139637719117824 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:26:50.032001 139637719117824 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.032250 139637719117824 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:26:50.034783 139637719117824 transformer_layer.py:213] tlayer: windowed attention.
I0123 19:26:50.038402 139637719117824 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:26:50.038457 139637719117824 attention.py:418] Single window, no scan.
I0123 19:26:50.038491 139637719117824 transformer_layer.py:389] tlayer: self-attention.
I0123 19:26:50.038521 139637719117824 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.038582 139637719117824 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.039190 139637719117824 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.039263 139637719117824 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.039613 139637719117824 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.040378 139637719117824 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.042818 139637719117824 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.043438 139637719117824 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.043512 139637719117824 transformer_layer.py:468] tlayer: End windows.
I0123 19:26:50.043545 139637719117824 transformer_layer.py:472] tlayer: final FFN.
I0123 19:26:50.043600 139637719117824 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.043728 139637719117824 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:26:50.044039 139637719117824 nn_components.py:325] mlp: activation = None
I0123 19:26:50.044080 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:26:50.046036 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.046128 139637719117824 nn_components.py:261] mlp: residual
I0123 19:26:50.048528 139637719117824 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.048606 139637719117824 transformer_base.py:443] tbase: final FFN
I0123 19:26:50.049024 139637719117824 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:26:50.051251 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:26:50.053118 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.053209 139637719117824 nn_components.py:261] mlp: residual
I0123 19:26:50.053490 139637719117824 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.053568 139637719117824 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 19:26:50.053679 139637719117824 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:26:50.053717 139637719117824 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:26:50.053746 139637719117824 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:26:50.055589 139637719117824 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.057872 139637719117824 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:26:50.063333 139637719117824 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.063582 139637719117824 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:26:50.066128 139637719117824 transformer_layer.py:213] tlayer: windowed attention.
I0123 19:26:50.069748 139637719117824 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:26:50.069801 139637719117824 attention.py:418] Single window, no scan.
I0123 19:26:50.069835 139637719117824 transformer_layer.py:389] tlayer: self-attention.
I0123 19:26:50.069864 139637719117824 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.069924 139637719117824 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.070479 139637719117824 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.070551 139637719117824 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.070902 139637719117824 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.071652 139637719117824 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.074089 139637719117824 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.074753 139637719117824 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.074828 139637719117824 transformer_layer.py:468] tlayer: End windows.
I0123 19:26:50.074862 139637719117824 transformer_layer.py:472] tlayer: final FFN.
I0123 19:26:50.074917 139637719117824 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.075042 139637719117824 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:26:50.075349 139637719117824 nn_components.py:325] mlp: activation = None
I0123 19:26:50.075390 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:26:50.077257 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.077346 139637719117824 nn_components.py:261] mlp: residual
I0123 19:26:50.079769 139637719117824 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.079848 139637719117824 transformer_base.py:443] tbase: final FFN
I0123 19:26:50.080263 139637719117824 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:26:50.082578 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:26:50.084456 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.084547 139637719117824 nn_components.py:261] mlp: residual
I0123 19:26:50.084832 139637719117824 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.084911 139637719117824 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 19:26:50.085016 139637719117824 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:26:50.085053 139637719117824 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:26:50.085082 139637719117824 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:26:50.086848 139637719117824 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.089131 139637719117824 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:26:50.094705 139637719117824 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.094954 139637719117824 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:26:50.097460 139637719117824 transformer_layer.py:213] tlayer: windowed attention.
I0123 19:26:50.101056 139637719117824 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:26:50.101109 139637719117824 attention.py:418] Single window, no scan.
I0123 19:26:50.101142 139637719117824 transformer_layer.py:389] tlayer: self-attention.
I0123 19:26:50.101171 139637719117824 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.101229 139637719117824 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.101832 139637719117824 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.101906 139637719117824 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.102252 139637719117824 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.102998 139637719117824 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.105393 139637719117824 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.106009 139637719117824 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.106085 139637719117824 transformer_layer.py:468] tlayer: End windows.
I0123 19:26:50.106118 139637719117824 transformer_layer.py:472] tlayer: final FFN.
I0123 19:26:50.106173 139637719117824 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.106295 139637719117824 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:26:50.106602 139637719117824 nn_components.py:325] mlp: activation = None
I0123 19:26:50.106643 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:26:50.108586 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.108677 139637719117824 nn_components.py:261] mlp: residual
I0123 19:26:50.111090 139637719117824 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.111167 139637719117824 transformer_base.py:443] tbase: final FFN
I0123 19:26:50.111580 139637719117824 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:26:50.113784 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:26:50.115649 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.115740 139637719117824 nn_components.py:261] mlp: residual
I0123 19:26:50.116021 139637719117824 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.116098 139637719117824 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 19:26:50.116203 139637719117824 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:26:50.116239 139637719117824 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:26:50.116267 139637719117824 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:26:50.118096 139637719117824 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.120358 139637719117824 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:26:50.125804 139637719117824 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.126060 139637719117824 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:26:50.128601 139637719117824 transformer_layer.py:213] tlayer: windowed attention.
I0123 19:26:50.132198 139637719117824 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:26:50.132252 139637719117824 attention.py:418] Single window, no scan.
I0123 19:26:50.132286 139637719117824 transformer_layer.py:389] tlayer: self-attention.
I0123 19:26:50.132315 139637719117824 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.132376 139637719117824 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.132926 139637719117824 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.132999 139637719117824 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.133348 139637719117824 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.134109 139637719117824 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.136503 139637719117824 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.137502 139637719117824 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.137578 139637719117824 transformer_layer.py:468] tlayer: End windows.
I0123 19:26:50.137611 139637719117824 transformer_layer.py:472] tlayer: final FFN.
I0123 19:26:50.137672 139637719117824 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.137800 139637719117824 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:26:50.138111 139637719117824 nn_components.py:325] mlp: activation = None
I0123 19:26:50.138152 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:26:50.140044 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.140133 139637719117824 nn_components.py:261] mlp: residual
I0123 19:26:50.142530 139637719117824 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.142607 139637719117824 transformer_base.py:443] tbase: final FFN
I0123 19:26:50.143022 139637719117824 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:26:50.145296 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:26:50.147169 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.147262 139637719117824 nn_components.py:261] mlp: residual
I0123 19:26:50.147545 139637719117824 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.147624 139637719117824 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 19:26:50.147728 139637719117824 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:26:50.147764 139637719117824 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:26:50.147793 139637719117824 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:26:50.149537 139637719117824 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.151814 139637719117824 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:26:50.157407 139637719117824 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.157662 139637719117824 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:26:50.160171 139637719117824 transformer_layer.py:213] tlayer: windowed attention.
I0123 19:26:50.163733 139637719117824 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:26:50.163785 139637719117824 attention.py:418] Single window, no scan.
I0123 19:26:50.163819 139637719117824 transformer_layer.py:389] tlayer: self-attention.
I0123 19:26:50.163848 139637719117824 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.163954 139637719117824 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.164508 139637719117824 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.164581 139637719117824 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.164927 139637719117824 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.165679 139637719117824 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.168077 139637719117824 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.168680 139637719117824 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.168754 139637719117824 transformer_layer.py:468] tlayer: End windows.
I0123 19:26:50.168787 139637719117824 transformer_layer.py:472] tlayer: final FFN.
I0123 19:26:50.168843 139637719117824 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.168998 139637719117824 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:26:50.169311 139637719117824 nn_components.py:325] mlp: activation = None
I0123 19:26:50.169353 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:26:50.171316 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.171408 139637719117824 nn_components.py:261] mlp: residual
I0123 19:26:50.173836 139637719117824 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.173914 139637719117824 transformer_base.py:443] tbase: final FFN
I0123 19:26:50.174330 139637719117824 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:26:50.176541 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:26:50.178403 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.178495 139637719117824 nn_components.py:261] mlp: residual
I0123 19:26:50.178780 139637719117824 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.178857 139637719117824 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 19:26:50.178961 139637719117824 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:26:50.178997 139637719117824 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:26:50.179026 139637719117824 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:26:50.180829 139637719117824 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.183076 139637719117824 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:26:50.188534 139637719117824 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.188783 139637719117824 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:26:50.191347 139637719117824 transformer_layer.py:213] tlayer: windowed attention.
I0123 19:26:50.194932 139637719117824 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:26:50.194985 139637719117824 attention.py:418] Single window, no scan.
I0123 19:26:50.195019 139637719117824 transformer_layer.py:389] tlayer: self-attention.
I0123 19:26:50.195048 139637719117824 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.195108 139637719117824 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.195650 139637719117824 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.195723 139637719117824 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.196065 139637719117824 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.196797 139637719117824 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.199183 139637719117824 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.199835 139637719117824 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.199909 139637719117824 transformer_layer.py:468] tlayer: End windows.
I0123 19:26:50.199941 139637719117824 transformer_layer.py:472] tlayer: final FFN.
I0123 19:26:50.199997 139637719117824 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.200121 139637719117824 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:26:50.200429 139637719117824 nn_components.py:325] mlp: activation = None
I0123 19:26:50.200469 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:26:50.202334 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.202427 139637719117824 nn_components.py:261] mlp: residual
I0123 19:26:50.204832 139637719117824 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.204909 139637719117824 transformer_base.py:443] tbase: final FFN
I0123 19:26:50.205320 139637719117824 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:26:50.207589 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:26:50.209447 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.209540 139637719117824 nn_components.py:261] mlp: residual
I0123 19:26:50.209829 139637719117824 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.209908 139637719117824 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 19:26:50.210013 139637719117824 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:26:50.210050 139637719117824 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:26:50.210078 139637719117824 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:26:50.211814 139637719117824 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.214054 139637719117824 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:26:50.219577 139637719117824 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.219822 139637719117824 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:26:50.222316 139637719117824 transformer_layer.py:213] tlayer: windowed attention.
I0123 19:26:50.225862 139637719117824 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:26:50.225921 139637719117824 attention.py:418] Single window, no scan.
I0123 19:26:50.225955 139637719117824 transformer_layer.py:389] tlayer: self-attention.
I0123 19:26:50.225984 139637719117824 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.226093 139637719117824 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.226646 139637719117824 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.226719 139637719117824 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.227062 139637719117824 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.227800 139637719117824 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.230194 139637719117824 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.230803 139637719117824 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.230876 139637719117824 transformer_layer.py:468] tlayer: End windows.
I0123 19:26:50.230908 139637719117824 transformer_layer.py:472] tlayer: final FFN.
I0123 19:26:50.230962 139637719117824 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.231085 139637719117824 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:26:50.231392 139637719117824 nn_components.py:325] mlp: activation = None
I0123 19:26:50.231432 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:26:50.233356 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.233447 139637719117824 nn_components.py:261] mlp: residual
I0123 19:26:50.235840 139637719117824 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.235917 139637719117824 transformer_base.py:443] tbase: final FFN
I0123 19:26:50.236327 139637719117824 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:26:50.238501 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:26:50.240350 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.240441 139637719117824 nn_components.py:261] mlp: residual
I0123 19:26:50.240717 139637719117824 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.240794 139637719117824 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 19:26:50.240897 139637719117824 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:26:50.240933 139637719117824 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:26:50.240962 139637719117824 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:26:50.243093 139637719117824 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.245365 139637719117824 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:26:50.250815 139637719117824 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.251063 139637719117824 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:26:50.253592 139637719117824 transformer_layer.py:213] tlayer: windowed attention.
I0123 19:26:50.257173 139637719117824 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:26:50.257227 139637719117824 attention.py:418] Single window, no scan.
I0123 19:26:50.257266 139637719117824 transformer_layer.py:389] tlayer: self-attention.
I0123 19:26:50.257298 139637719117824 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.257358 139637719117824 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.257911 139637719117824 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.257985 139637719117824 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.258333 139637719117824 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.259076 139637719117824 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.261448 139637719117824 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.262058 139637719117824 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.262131 139637719117824 transformer_layer.py:468] tlayer: End windows.
I0123 19:26:50.262163 139637719117824 transformer_layer.py:472] tlayer: final FFN.
I0123 19:26:50.262218 139637719117824 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.262341 139637719117824 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:26:50.262696 139637719117824 nn_components.py:325] mlp: activation = None
I0123 19:26:50.262737 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:26:50.264592 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.264682 139637719117824 nn_components.py:261] mlp: residual
I0123 19:26:50.267056 139637719117824 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.267133 139637719117824 transformer_base.py:443] tbase: final FFN
I0123 19:26:50.267543 139637719117824 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:26:50.269768 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:26:50.271688 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.271779 139637719117824 nn_components.py:261] mlp: residual
I0123 19:26:50.272063 139637719117824 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.272141 139637719117824 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 19:26:50.272245 139637719117824 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:26:50.272282 139637719117824 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:26:50.272310 139637719117824 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:26:50.274067 139637719117824 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.276331 139637719117824 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:26:50.281869 139637719117824 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.282113 139637719117824 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:26:50.284623 139637719117824 transformer_layer.py:213] tlayer: windowed attention.
I0123 19:26:50.288203 139637719117824 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:26:50.288255 139637719117824 attention.py:418] Single window, no scan.
I0123 19:26:50.288288 139637719117824 transformer_layer.py:389] tlayer: self-attention.
I0123 19:26:50.288326 139637719117824 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.288385 139637719117824 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.288985 139637719117824 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.289058 139637719117824 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.289406 139637719117824 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.290150 139637719117824 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.292546 139637719117824 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.293152 139637719117824 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.293226 139637719117824 transformer_layer.py:468] tlayer: End windows.
I0123 19:26:50.293258 139637719117824 transformer_layer.py:472] tlayer: final FFN.
I0123 19:26:50.293312 139637719117824 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.293436 139637719117824 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:26:50.293752 139637719117824 nn_components.py:325] mlp: activation = None
I0123 19:26:50.293794 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:26:50.295677 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.295766 139637719117824 nn_components.py:261] mlp: residual
I0123 19:26:50.298225 139637719117824 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.298304 139637719117824 transformer_base.py:443] tbase: final FFN
I0123 19:26:50.298712 139637719117824 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:26:50.300927 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:26:50.302814 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.302906 139637719117824 nn_components.py:261] mlp: residual
I0123 19:26:50.303191 139637719117824 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.303270 139637719117824 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 19:26:50.303374 139637719117824 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:26:50.303411 139637719117824 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:26:50.303440 139637719117824 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:26:50.305186 139637719117824 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.307532 139637719117824 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:26:50.313013 139637719117824 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.313262 139637719117824 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:26:50.315765 139637719117824 transformer_layer.py:213] tlayer: windowed attention.
I0123 19:26:50.319352 139637719117824 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:26:50.319406 139637719117824 attention.py:418] Single window, no scan.
I0123 19:26:50.319438 139637719117824 transformer_layer.py:389] tlayer: self-attention.
I0123 19:26:50.319467 139637719117824 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.319581 139637719117824 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.320138 139637719117824 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.320213 139637719117824 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.320564 139637719117824 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.321311 139637719117824 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.323731 139637719117824 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.324342 139637719117824 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.324417 139637719117824 transformer_layer.py:468] tlayer: End windows.
I0123 19:26:50.324450 139637719117824 transformer_layer.py:472] tlayer: final FFN.
I0123 19:26:50.324505 139637719117824 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.324632 139637719117824 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:26:50.324945 139637719117824 nn_components.py:325] mlp: activation = None
I0123 19:26:50.324986 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:26:50.326932 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.327023 139637719117824 nn_components.py:261] mlp: residual
I0123 19:26:50.329459 139637719117824 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.329535 139637719117824 transformer_base.py:443] tbase: final FFN
I0123 19:26:50.329960 139637719117824 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:26:50.332176 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:26:50.334072 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.334164 139637719117824 nn_components.py:261] mlp: residual
I0123 19:26:50.334450 139637719117824 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.334528 139637719117824 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 19:26:50.334632 139637719117824 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:26:50.334669 139637719117824 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:26:50.334697 139637719117824 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:26:50.336531 139637719117824 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.338807 139637719117824 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:26:50.344308 139637719117824 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.344558 139637719117824 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:26:50.347084 139637719117824 transformer_layer.py:213] tlayer: windowed attention.
I0123 19:26:50.350752 139637719117824 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:26:50.350805 139637719117824 attention.py:418] Single window, no scan.
I0123 19:26:50.350839 139637719117824 transformer_layer.py:389] tlayer: self-attention.
I0123 19:26:50.350868 139637719117824 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.350928 139637719117824 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.351479 139637719117824 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.351552 139637719117824 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[1,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.351901 139637719117824 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.352649 139637719117824 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.355055 139637719117824 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.355664 139637719117824 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.355738 139637719117824 transformer_layer.py:468] tlayer: End windows.
I0123 19:26:50.355771 139637719117824 transformer_layer.py:472] tlayer: final FFN.
I0123 19:26:50.355825 139637719117824 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.355948 139637719117824 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:26:50.356257 139637719117824 nn_components.py:325] mlp: activation = None
I0123 19:26:50.356298 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:26:50.358586 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.358677 139637719117824 nn_components.py:261] mlp: residual
I0123 19:26:50.361071 139637719117824 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.361147 139637719117824 transformer_base.py:443] tbase: final FFN
I0123 19:26:50.361567 139637719117824 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:26:50.363776 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:26:50.365637 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.365739 139637719117824 nn_components.py:261] mlp: residual
I0123 19:26:50.366020 139637719117824 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.366260 139637719117824 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 19:26:50.366325 139637719117824 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 19:26:50.366380 139637719117824 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 19:26:50.366432 139637719117824 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 19:26:50.366484 139637719117824 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 19:26:50.366535 139637719117824 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 19:26:50.366586 139637719117824 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 19:26:50.366635 139637719117824 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 19:26:50.366685 139637719117824 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 19:26:50.366735 139637719117824 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 19:26:50.366785 139637719117824 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 19:26:50.366835 139637719117824 transformer_layer.py:673] tlayer: Skipping XL cache update for mode beam_search.
I0123 19:26:50.366869 139637719117824 decoder_stack.py:344] dstack: Final layernorm.
I0123 19:26:50.369713 139637719117824 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=1/0)>
I0123 19:26:50.413465 139637719117824 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.413547 139637719117824 decoder_stack.py:333] dstack: autoregressive generator.
I0123 19:26:50.413598 139637719117824 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 19:26:50.413704 139637719117824 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:26:50.413741 139637719117824 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:26:50.413769 139637719117824 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:26:50.413828 139637719117824 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.416438 139637719117824 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:26:50.421881 139637719117824 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.422137 139637719117824 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:26:50.424690 139637719117824 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:26:50.437245 139637719117824 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:26:50.437297 139637719117824 attention.py:418] Single window, no scan.
I0123 19:26:50.437331 139637719117824 transformer_layer.py:389] tlayer: self-attention.
I0123 19:26:50.437360 139637719117824 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.437420 139637719117824 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.437984 139637719117824 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.438059 139637719117824 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.438415 139637719117824 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.439097 139637719117824 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.441600 139637719117824 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.442213 139637719117824 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.442287 139637719117824 transformer_layer.py:468] tlayer: End windows.
I0123 19:26:50.442319 139637719117824 transformer_layer.py:472] tlayer: final FFN.
I0123 19:26:50.442374 139637719117824 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.442498 139637719117824 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:26:50.442603 139637719117824 nn_components.py:325] mlp: activation = None
I0123 19:26:50.442639 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:26:50.444627 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.444716 139637719117824 nn_components.py:261] mlp: residual
I0123 19:26:50.447100 139637719117824 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.447178 139637719117824 transformer_base.py:443] tbase: final FFN
I0123 19:26:50.447285 139637719117824 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:26:50.449503 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:26:50.451327 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.451426 139637719117824 nn_components.py:261] mlp: residual
I0123 19:26:50.451714 139637719117824 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.451792 139637719117824 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 19:26:50.451897 139637719117824 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:26:50.451933 139637719117824 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:26:50.451962 139637719117824 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:26:50.452022 139637719117824 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.454221 139637719117824 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:26:50.459557 139637719117824 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.459812 139637719117824 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:26:50.462417 139637719117824 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:26:50.474587 139637719117824 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:26:50.474640 139637719117824 attention.py:418] Single window, no scan.
I0123 19:26:50.474674 139637719117824 transformer_layer.py:389] tlayer: self-attention.
I0123 19:26:50.474703 139637719117824 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.474763 139637719117824 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.475359 139637719117824 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.475433 139637719117824 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.475783 139637719117824 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.476454 139637719117824 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.478872 139637719117824 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.479474 139637719117824 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.479548 139637719117824 transformer_layer.py:468] tlayer: End windows.
I0123 19:26:50.479581 139637719117824 transformer_layer.py:472] tlayer: final FFN.
I0123 19:26:50.479638 139637719117824 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.479762 139637719117824 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:26:50.479866 139637719117824 nn_components.py:325] mlp: activation = None
I0123 19:26:50.479902 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:26:50.481742 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.481832 139637719117824 nn_components.py:261] mlp: residual
I0123 19:26:50.484255 139637719117824 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.484332 139637719117824 transformer_base.py:443] tbase: final FFN
I0123 19:26:50.484437 139637719117824 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:26:50.486619 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:26:50.488441 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.488539 139637719117824 nn_components.py:261] mlp: residual
I0123 19:26:50.488825 139637719117824 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.488902 139637719117824 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 19:26:50.489007 139637719117824 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:26:50.489044 139637719117824 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:26:50.489072 139637719117824 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:26:50.489131 139637719117824 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.491320 139637719117824 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:26:50.496675 139637719117824 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.496933 139637719117824 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:26:50.499480 139637719117824 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:26:50.511771 139637719117824 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:26:50.511825 139637719117824 attention.py:418] Single window, no scan.
I0123 19:26:50.511858 139637719117824 transformer_layer.py:389] tlayer: self-attention.
I0123 19:26:50.511887 139637719117824 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.511945 139637719117824 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.512493 139637719117824 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.512566 139637719117824 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.512921 139637719117824 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.513593 139637719117824 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.516003 139637719117824 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.517031 139637719117824 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.517107 139637719117824 transformer_layer.py:468] tlayer: End windows.
I0123 19:26:50.517140 139637719117824 transformer_layer.py:472] tlayer: final FFN.
I0123 19:26:50.517197 139637719117824 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.517327 139637719117824 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:26:50.517434 139637719117824 nn_components.py:325] mlp: activation = None
I0123 19:26:50.517471 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:26:50.519307 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.519399 139637719117824 nn_components.py:261] mlp: residual
I0123 19:26:50.521785 139637719117824 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.521862 139637719117824 transformer_base.py:443] tbase: final FFN
I0123 19:26:50.521967 139637719117824 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:26:50.524141 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:26:50.526041 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.526134 139637719117824 nn_components.py:261] mlp: residual
I0123 19:26:50.526429 139637719117824 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.526507 139637719117824 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 19:26:50.526612 139637719117824 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:26:50.526648 139637719117824 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:26:50.526677 139637719117824 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:26:50.526736 139637719117824 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.528936 139637719117824 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:26:50.534264 139637719117824 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.534518 139637719117824 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:26:50.537039 139637719117824 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:26:50.549533 139637719117824 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:26:50.549586 139637719117824 attention.py:418] Single window, no scan.
I0123 19:26:50.549619 139637719117824 transformer_layer.py:389] tlayer: self-attention.
I0123 19:26:50.549653 139637719117824 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.549717 139637719117824 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.550279 139637719117824 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.550355 139637719117824 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.550720 139637719117824 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.551416 139637719117824 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.553889 139637719117824 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.554497 139637719117824 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.554570 139637719117824 transformer_layer.py:468] tlayer: End windows.
I0123 19:26:50.554602 139637719117824 transformer_layer.py:472] tlayer: final FFN.
I0123 19:26:50.554659 139637719117824 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.554783 139637719117824 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:26:50.554888 139637719117824 nn_components.py:325] mlp: activation = None
I0123 19:26:50.554924 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:26:50.556735 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.556825 139637719117824 nn_components.py:261] mlp: residual
I0123 19:26:50.559183 139637719117824 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.559259 139637719117824 transformer_base.py:443] tbase: final FFN
I0123 19:26:50.559365 139637719117824 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:26:50.561596 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:26:50.563415 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.563507 139637719117824 nn_components.py:261] mlp: residual
I0123 19:26:50.563789 139637719117824 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.563872 139637719117824 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 19:26:50.563978 139637719117824 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:26:50.564015 139637719117824 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:26:50.564044 139637719117824 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:26:50.564103 139637719117824 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.566313 139637719117824 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:26:50.571629 139637719117824 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.571886 139637719117824 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:26:50.574508 139637719117824 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:26:50.586671 139637719117824 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:26:50.586725 139637719117824 attention.py:418] Single window, no scan.
I0123 19:26:50.586758 139637719117824 transformer_layer.py:389] tlayer: self-attention.
I0123 19:26:50.586787 139637719117824 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.586845 139637719117824 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.587443 139637719117824 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.587516 139637719117824 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.587863 139637719117824 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.588525 139637719117824 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.590921 139637719117824 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.591527 139637719117824 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.591600 139637719117824 transformer_layer.py:468] tlayer: End windows.
I0123 19:26:50.591633 139637719117824 transformer_layer.py:472] tlayer: final FFN.
I0123 19:26:50.591689 139637719117824 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.591815 139637719117824 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:26:50.591921 139637719117824 nn_components.py:325] mlp: activation = None
I0123 19:26:50.591958 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:26:50.593797 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.593891 139637719117824 nn_components.py:261] mlp: residual
I0123 19:26:50.596304 139637719117824 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.596381 139637719117824 transformer_base.py:443] tbase: final FFN
I0123 19:26:50.596488 139637719117824 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:26:50.598670 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:26:50.600486 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.600578 139637719117824 nn_components.py:261] mlp: residual
I0123 19:26:50.600861 139637719117824 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.600946 139637719117824 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 19:26:50.601052 139637719117824 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:26:50.601089 139637719117824 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:26:50.601119 139637719117824 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:26:50.601179 139637719117824 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.603381 139637719117824 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:26:50.608761 139637719117824 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.609015 139637719117824 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:26:50.611594 139637719117824 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:26:50.624153 139637719117824 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:26:50.624206 139637719117824 attention.py:418] Single window, no scan.
I0123 19:26:50.624240 139637719117824 transformer_layer.py:389] tlayer: self-attention.
I0123 19:26:50.624269 139637719117824 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.624326 139637719117824 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.624871 139637719117824 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.624944 139637719117824 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.625291 139637719117824 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.625961 139637719117824 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.628331 139637719117824 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.628984 139637719117824 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.629057 139637719117824 transformer_layer.py:468] tlayer: End windows.
I0123 19:26:50.629090 139637719117824 transformer_layer.py:472] tlayer: final FFN.
I0123 19:26:50.629145 139637719117824 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.629268 139637719117824 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:26:50.629374 139637719117824 nn_components.py:325] mlp: activation = None
I0123 19:26:50.629411 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:26:50.631242 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.631332 139637719117824 nn_components.py:261] mlp: residual
I0123 19:26:50.633682 139637719117824 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.633760 139637719117824 transformer_base.py:443] tbase: final FFN
I0123 19:26:50.633865 139637719117824 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:26:50.636030 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:26:50.637913 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.638006 139637719117824 nn_components.py:261] mlp: residual
I0123 19:26:50.638293 139637719117824 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.638371 139637719117824 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 19:26:50.638483 139637719117824 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:26:50.638520 139637719117824 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:26:50.638548 139637719117824 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:26:50.638607 139637719117824 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.640790 139637719117824 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:26:50.646086 139637719117824 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.646335 139637719117824 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:26:50.648849 139637719117824 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:26:50.660924 139637719117824 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:26:50.660977 139637719117824 attention.py:418] Single window, no scan.
I0123 19:26:50.661011 139637719117824 transformer_layer.py:389] tlayer: self-attention.
I0123 19:26:50.661040 139637719117824 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.661098 139637719117824 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.661647 139637719117824 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.661721 139637719117824 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.662066 139637719117824 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.662732 139637719117824 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.665169 139637719117824 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.665779 139637719117824 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.665853 139637719117824 transformer_layer.py:468] tlayer: End windows.
I0123 19:26:50.665885 139637719117824 transformer_layer.py:472] tlayer: final FFN.
I0123 19:26:50.665939 139637719117824 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.666062 139637719117824 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:26:50.666166 139637719117824 nn_components.py:325] mlp: activation = None
I0123 19:26:50.666202 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:26:50.668007 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.668097 139637719117824 nn_components.py:261] mlp: residual
I0123 19:26:50.670457 139637719117824 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.670534 139637719117824 transformer_base.py:443] tbase: final FFN
I0123 19:26:50.670640 139637719117824 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:26:50.672836 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:26:50.674668 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.674761 139637719117824 nn_components.py:261] mlp: residual
I0123 19:26:50.675048 139637719117824 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.675127 139637719117824 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 19:26:50.675239 139637719117824 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:26:50.675277 139637719117824 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:26:50.675306 139637719117824 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:26:50.675365 139637719117824 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.677531 139637719117824 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:26:50.682894 139637719117824 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.683154 139637719117824 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:26:50.685767 139637719117824 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:26:50.697932 139637719117824 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:26:50.697984 139637719117824 attention.py:418] Single window, no scan.
I0123 19:26:50.698018 139637719117824 transformer_layer.py:389] tlayer: self-attention.
I0123 19:26:50.698046 139637719117824 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.698105 139637719117824 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.698705 139637719117824 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.698778 139637719117824 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.699127 139637719117824 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.699795 139637719117824 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.702196 139637719117824 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.702801 139637719117824 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.702875 139637719117824 transformer_layer.py:468] tlayer: End windows.
I0123 19:26:50.702907 139637719117824 transformer_layer.py:472] tlayer: final FFN.
I0123 19:26:50.702963 139637719117824 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.703087 139637719117824 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:26:50.703192 139637719117824 nn_components.py:325] mlp: activation = None
I0123 19:26:50.703228 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:26:50.705034 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.705124 139637719117824 nn_components.py:261] mlp: residual
I0123 19:26:50.707549 139637719117824 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.707626 139637719117824 transformer_base.py:443] tbase: final FFN
I0123 19:26:50.707731 139637719117824 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:26:50.709911 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:26:50.711740 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.711831 139637719117824 nn_components.py:261] mlp: residual
I0123 19:26:50.712120 139637719117824 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.712198 139637719117824 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 19:26:50.712305 139637719117824 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:26:50.712347 139637719117824 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:26:50.712378 139637719117824 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:26:50.712438 139637719117824 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.714638 139637719117824 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:26:50.720042 139637719117824 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.720295 139637719117824 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:26:50.722842 139637719117824 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:26:50.735368 139637719117824 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:26:50.735421 139637719117824 attention.py:418] Single window, no scan.
I0123 19:26:50.735455 139637719117824 transformer_layer.py:389] tlayer: self-attention.
I0123 19:26:50.735483 139637719117824 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.735542 139637719117824 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.736088 139637719117824 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.736160 139637719117824 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.736509 139637719117824 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.737353 139637719117824 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.739735 139637719117824 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.740335 139637719117824 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.740408 139637719117824 transformer_layer.py:468] tlayer: End windows.
I0123 19:26:50.740441 139637719117824 transformer_layer.py:472] tlayer: final FFN.
I0123 19:26:50.740496 139637719117824 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.740620 139637719117824 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:26:50.740723 139637719117824 nn_components.py:325] mlp: activation = None
I0123 19:26:50.740759 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:26:50.742646 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.742736 139637719117824 nn_components.py:261] mlp: residual
I0123 19:26:50.745096 139637719117824 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.745172 139637719117824 transformer_base.py:443] tbase: final FFN
I0123 19:26:50.745276 139637719117824 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:26:50.747444 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:26:50.749256 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.749348 139637719117824 nn_components.py:261] mlp: residual
I0123 19:26:50.749634 139637719117824 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.749718 139637719117824 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 19:26:50.749824 139637719117824 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:26:50.749861 139637719117824 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:26:50.749896 139637719117824 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:26:50.749959 139637719117824 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.752163 139637719117824 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:26:50.757544 139637719117824 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.757808 139637719117824 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:26:50.760369 139637719117824 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:26:50.772566 139637719117824 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:26:50.772619 139637719117824 attention.py:418] Single window, no scan.
I0123 19:26:50.772653 139637719117824 transformer_layer.py:389] tlayer: self-attention.
I0123 19:26:50.772681 139637719117824 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.772740 139637719117824 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.773285 139637719117824 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.773357 139637719117824 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.773712 139637719117824 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.774378 139637719117824 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.776757 139637719117824 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.777364 139637719117824 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.777439 139637719117824 transformer_layer.py:468] tlayer: End windows.
I0123 19:26:50.777471 139637719117824 transformer_layer.py:472] tlayer: final FFN.
I0123 19:26:50.777526 139637719117824 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.777656 139637719117824 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:26:50.777762 139637719117824 nn_components.py:325] mlp: activation = None
I0123 19:26:50.777799 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:26:50.779665 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.779754 139637719117824 nn_components.py:261] mlp: residual
I0123 19:26:50.782104 139637719117824 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.782180 139637719117824 transformer_base.py:443] tbase: final FFN
I0123 19:26:50.782285 139637719117824 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:26:50.784437 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:26:50.786265 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.786358 139637719117824 nn_components.py:261] mlp: residual
I0123 19:26:50.786645 139637719117824 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.786723 139637719117824 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 19:26:50.786828 139637719117824 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:26:50.786863 139637719117824 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:26:50.786891 139637719117824 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:26:50.786956 139637719117824 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.789143 139637719117824 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:26:50.794522 139637719117824 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.794773 139637719117824 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:26:50.797311 139637719117824 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:26:50.809519 139637719117824 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:26:50.809573 139637719117824 attention.py:418] Single window, no scan.
I0123 19:26:50.809607 139637719117824 transformer_layer.py:389] tlayer: self-attention.
I0123 19:26:50.809636 139637719117824 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.809702 139637719117824 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.810245 139637719117824 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.810317 139637719117824 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.810666 139637719117824 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.811340 139637719117824 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.813718 139637719117824 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.814326 139637719117824 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.814400 139637719117824 transformer_layer.py:468] tlayer: End windows.
I0123 19:26:50.814434 139637719117824 transformer_layer.py:472] tlayer: final FFN.
I0123 19:26:50.814489 139637719117824 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.814611 139637719117824 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:26:50.814714 139637719117824 nn_components.py:325] mlp: activation = None
I0123 19:26:50.814750 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:26:50.816618 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.816708 139637719117824 nn_components.py:261] mlp: residual
I0123 19:26:50.819065 139637719117824 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.819141 139637719117824 transformer_base.py:443] tbase: final FFN
I0123 19:26:50.819246 139637719117824 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:26:50.821388 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:26:50.823203 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.823295 139637719117824 nn_components.py:261] mlp: residual
I0123 19:26:50.823581 139637719117824 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.823660 139637719117824 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 19:26:50.823766 139637719117824 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:26:50.823803 139637719117824 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:26:50.823832 139637719117824 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:26:50.823898 139637719117824 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.826089 139637719117824 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:26:50.831520 139637719117824 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.831773 139637719117824 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:26:50.834353 139637719117824 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:26:50.847372 139637719117824 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:26:50.847426 139637719117824 attention.py:418] Single window, no scan.
I0123 19:26:50.847460 139637719117824 transformer_layer.py:389] tlayer: self-attention.
I0123 19:26:50.847488 139637719117824 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.847548 139637719117824 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.848095 139637719117824 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.848167 139637719117824 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.848515 139637719117824 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.849180 139637719117824 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.851580 139637719117824 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.852181 139637719117824 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.852255 139637719117824 transformer_layer.py:468] tlayer: End windows.
I0123 19:26:50.852288 139637719117824 transformer_layer.py:472] tlayer: final FFN.
I0123 19:26:50.852343 139637719117824 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.852469 139637719117824 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:26:50.852575 139637719117824 nn_components.py:325] mlp: activation = None
I0123 19:26:50.852612 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:26:50.854511 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.854603 139637719117824 nn_components.py:261] mlp: residual
I0123 19:26:50.856954 139637719117824 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.857031 139637719117824 transformer_base.py:443] tbase: final FFN
I0123 19:26:50.857134 139637719117824 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:26:50.859287 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:26:50.861099 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.861190 139637719117824 nn_components.py:261] mlp: residual
I0123 19:26:50.861477 139637719117824 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.861560 139637719117824 decoder_stack.py:344] dstack: Final layernorm.
I0123 19:26:50.864326 139637719117824 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=3/0)>
I0123 19:26:50.913908 139637719117824 decoder_stack.py:275] dstack: embeddings = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:50.913991 139637719117824 decoder_stack.py:333] dstack: autoregressive generator.
I0123 19:26:50.914052 139637719117824 decoder_stack.py:224] dstack: ---- Layer 0 ----
I0123 19:26:50.914156 139637719117824 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:26:50.914193 139637719117824 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:26:50.914222 139637719117824 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:26:50.914283 139637719117824 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:50.916531 139637719117824 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:26:50.921933 139637719117824 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:50.922184 139637719117824 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:26:50.924736 139637719117824 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:26:50.937033 139637719117824 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:26:50.937087 139637719117824 attention.py:418] Single window, no scan.
I0123 19:26:50.937121 139637719117824 transformer_layer.py:389] tlayer: self-attention.
I0123 19:26:50.937151 139637719117824 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:50.937210 139637719117824 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:50.937770 139637719117824 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:50.937844 139637719117824 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:50.938195 139637719117824 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:50.938983 139637719117824 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:50.941548 139637719117824 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:50.942163 139637719117824 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:50.942239 139637719117824 transformer_layer.py:468] tlayer: End windows.
I0123 19:26:50.942272 139637719117824 transformer_layer.py:472] tlayer: final FFN.
I0123 19:26:50.942328 139637719117824 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:50.942456 139637719117824 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:26:50.942564 139637719117824 nn_components.py:325] mlp: activation = None
I0123 19:26:50.942600 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:26:50.944478 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:50.944568 139637719117824 nn_components.py:261] mlp: residual
I0123 19:26:50.946922 139637719117824 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:50.947000 139637719117824 transformer_base.py:443] tbase: final FFN
I0123 19:26:50.947106 139637719117824 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:26:50.949239 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:26:50.951056 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:50.951147 139637719117824 nn_components.py:261] mlp: residual
I0123 19:26:50.951432 139637719117824 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:50.951511 139637719117824 decoder_stack.py:224] dstack: ---- Layer 1 ----
I0123 19:26:50.951622 139637719117824 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:26:50.951659 139637719117824 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:26:50.951688 139637719117824 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:26:50.951749 139637719117824 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:50.953947 139637719117824 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:26:50.959345 139637719117824 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:50.959599 139637719117824 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:26:50.962140 139637719117824 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:26:50.974396 139637719117824 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:26:50.974450 139637719117824 attention.py:418] Single window, no scan.
I0123 19:26:50.974484 139637719117824 transformer_layer.py:389] tlayer: self-attention.
I0123 19:26:50.974514 139637719117824 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:50.974575 139637719117824 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:50.975125 139637719117824 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:50.975198 139637719117824 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:50.975546 139637719117824 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:50.976205 139637719117824 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:50.978604 139637719117824 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:50.979216 139637719117824 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:50.979291 139637719117824 transformer_layer.py:468] tlayer: End windows.
I0123 19:26:50.979324 139637719117824 transformer_layer.py:472] tlayer: final FFN.
I0123 19:26:50.979380 139637719117824 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:50.979503 139637719117824 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:26:50.979607 139637719117824 nn_components.py:325] mlp: activation = None
I0123 19:26:50.979643 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:26:50.981520 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:50.981612 139637719117824 nn_components.py:261] mlp: residual
I0123 19:26:50.983970 139637719117824 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:50.984047 139637719117824 transformer_base.py:443] tbase: final FFN
I0123 19:26:50.984151 139637719117824 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:26:50.986305 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:26:50.988148 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:50.988240 139637719117824 nn_components.py:261] mlp: residual
I0123 19:26:50.988523 139637719117824 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:50.988600 139637719117824 decoder_stack.py:224] dstack: ---- Layer 2 ----
I0123 19:26:50.988704 139637719117824 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:26:50.988747 139637719117824 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:26:50.988777 139637719117824 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:26:50.988837 139637719117824 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:50.991040 139637719117824 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:26:50.996400 139637719117824 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:50.996654 139637719117824 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:26:50.999202 139637719117824 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:26:51.011422 139637719117824 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:26:51.011476 139637719117824 attention.py:418] Single window, no scan.
I0123 19:26:51.011510 139637719117824 transformer_layer.py:389] tlayer: self-attention.
I0123 19:26:51.011540 139637719117824 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.011599 139637719117824 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.012146 139637719117824 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.012220 139637719117824 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.012571 139637719117824 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.013242 139637719117824 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.015657 139637719117824 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.016269 139637719117824 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.016344 139637719117824 transformer_layer.py:468] tlayer: End windows.
I0123 19:26:51.016378 139637719117824 transformer_layer.py:472] tlayer: final FFN.
I0123 19:26:51.016434 139637719117824 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.016559 139637719117824 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:26:51.016665 139637719117824 nn_components.py:325] mlp: activation = None
I0123 19:26:51.016702 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:26:51.019062 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.019155 139637719117824 nn_components.py:261] mlp: residual
I0123 19:26:51.021537 139637719117824 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.021614 139637719117824 transformer_base.py:443] tbase: final FFN
I0123 19:26:51.021728 139637719117824 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:26:51.023893 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:26:51.025744 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.025838 139637719117824 nn_components.py:261] mlp: residual
I0123 19:26:51.026130 139637719117824 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.026210 139637719117824 decoder_stack.py:224] dstack: ---- Layer 3 ----
I0123 19:26:51.026315 139637719117824 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:26:51.026359 139637719117824 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:26:51.026389 139637719117824 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:26:51.026449 139637719117824 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.028661 139637719117824 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:26:51.034069 139637719117824 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.034324 139637719117824 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:26:51.036867 139637719117824 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:26:51.049226 139637719117824 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:26:51.049280 139637719117824 attention.py:418] Single window, no scan.
I0123 19:26:51.049314 139637719117824 transformer_layer.py:389] tlayer: self-attention.
I0123 19:26:51.049343 139637719117824 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.049403 139637719117824 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.049956 139637719117824 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.050030 139637719117824 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.050379 139637719117824 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.051049 139637719117824 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.053459 139637719117824 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.054069 139637719117824 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.054145 139637719117824 transformer_layer.py:468] tlayer: End windows.
I0123 19:26:51.054178 139637719117824 transformer_layer.py:472] tlayer: final FFN.
I0123 19:26:51.054234 139637719117824 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.054358 139637719117824 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:26:51.054464 139637719117824 nn_components.py:325] mlp: activation = None
I0123 19:26:51.054501 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:26:51.056397 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.056487 139637719117824 nn_components.py:261] mlp: residual
I0123 19:26:51.058849 139637719117824 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.058926 139637719117824 transformer_base.py:443] tbase: final FFN
I0123 19:26:51.059031 139637719117824 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:26:51.061172 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:26:51.062998 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.063091 139637719117824 nn_components.py:261] mlp: residual
I0123 19:26:51.063377 139637719117824 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.063455 139637719117824 decoder_stack.py:224] dstack: ---- Layer 4 ----
I0123 19:26:51.063560 139637719117824 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:26:51.063597 139637719117824 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:26:51.063631 139637719117824 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:26:51.063694 139637719117824 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.065881 139637719117824 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:26:51.071279 139637719117824 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.071535 139637719117824 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:26:51.074105 139637719117824 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:26:51.086269 139637719117824 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:26:51.086323 139637719117824 attention.py:418] Single window, no scan.
I0123 19:26:51.086356 139637719117824 transformer_layer.py:389] tlayer: self-attention.
I0123 19:26:51.086386 139637719117824 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.086447 139637719117824 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.086994 139637719117824 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.087067 139637719117824 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.087415 139637719117824 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.088080 139637719117824 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.090465 139637719117824 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.091065 139637719117824 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.091139 139637719117824 transformer_layer.py:468] tlayer: End windows.
I0123 19:26:51.091171 139637719117824 transformer_layer.py:472] tlayer: final FFN.
I0123 19:26:51.091227 139637719117824 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.091351 139637719117824 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:26:51.091455 139637719117824 nn_components.py:325] mlp: activation = None
I0123 19:26:51.091492 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:26:51.093371 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.093462 139637719117824 nn_components.py:261] mlp: residual
I0123 19:26:51.095815 139637719117824 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.095892 139637719117824 transformer_base.py:443] tbase: final FFN
I0123 19:26:51.095997 139637719117824 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:26:51.098186 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:26:51.099998 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.100090 139637719117824 nn_components.py:261] mlp: residual
I0123 19:26:51.100378 139637719117824 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.100457 139637719117824 decoder_stack.py:224] dstack: ---- Layer 5 ----
I0123 19:26:51.100561 139637719117824 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:26:51.100598 139637719117824 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:26:51.100626 139637719117824 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:26:51.100693 139637719117824 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.102889 139637719117824 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:26:51.108231 139637719117824 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.108485 139637719117824 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:26:51.111024 139637719117824 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:26:51.123227 139637719117824 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:26:51.123281 139637719117824 attention.py:418] Single window, no scan.
I0123 19:26:51.123314 139637719117824 transformer_layer.py:389] tlayer: self-attention.
I0123 19:26:51.123343 139637719117824 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.123404 139637719117824 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.123952 139637719117824 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.124026 139637719117824 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.124376 139637719117824 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.125040 139637719117824 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.127449 139637719117824 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.128053 139637719117824 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.128126 139637719117824 transformer_layer.py:468] tlayer: End windows.
I0123 19:26:51.128158 139637719117824 transformer_layer.py:472] tlayer: final FFN.
I0123 19:26:51.128216 139637719117824 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.128339 139637719117824 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:26:51.128444 139637719117824 nn_components.py:325] mlp: activation = None
I0123 19:26:51.128481 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:26:51.130799 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.130891 139637719117824 nn_components.py:261] mlp: residual
I0123 19:26:51.133245 139637719117824 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.133322 139637719117824 transformer_base.py:443] tbase: final FFN
I0123 19:26:51.133429 139637719117824 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:26:51.135602 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:26:51.137428 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.137520 139637719117824 nn_components.py:261] mlp: residual
I0123 19:26:51.137812 139637719117824 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.137892 139637719117824 decoder_stack.py:224] dstack: ---- Layer 6 ----
I0123 19:26:51.137997 139637719117824 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:26:51.138033 139637719117824 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:26:51.138061 139637719117824 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:26:51.138122 139637719117824 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.140309 139637719117824 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:26:51.145729 139637719117824 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.145986 139637719117824 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:26:51.148537 139637719117824 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:26:51.160741 139637719117824 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:26:51.160794 139637719117824 attention.py:418] Single window, no scan.
I0123 19:26:51.160827 139637719117824 transformer_layer.py:389] tlayer: self-attention.
I0123 19:26:51.160857 139637719117824 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.160915 139637719117824 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.161461 139637719117824 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.161534 139637719117824 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.161889 139637719117824 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.162552 139637719117824 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.164934 139637719117824 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.165539 139637719117824 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.165612 139637719117824 transformer_layer.py:468] tlayer: End windows.
I0123 19:26:51.165649 139637719117824 transformer_layer.py:472] tlayer: final FFN.
I0123 19:26:51.165708 139637719117824 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.165830 139637719117824 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:26:51.165935 139637719117824 nn_components.py:325] mlp: activation = None
I0123 19:26:51.165971 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:26:51.167864 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.167953 139637719117824 nn_components.py:261] mlp: residual
I0123 19:26:51.170303 139637719117824 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.170380 139637719117824 transformer_base.py:443] tbase: final FFN
I0123 19:26:51.170485 139637719117824 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:26:51.172623 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:26:51.174449 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.174542 139637719117824 nn_components.py:261] mlp: residual
I0123 19:26:51.174828 139637719117824 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.174907 139637719117824 decoder_stack.py:224] dstack: ---- Layer 7 ----
I0123 19:26:51.175012 139637719117824 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:26:51.175049 139637719117824 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:26:51.175077 139637719117824 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:26:51.175136 139637719117824 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.177321 139637719117824 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:26:51.182701 139637719117824 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.182952 139637719117824 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:26:51.185461 139637719117824 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:26:51.197626 139637719117824 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:26:51.197684 139637719117824 attention.py:418] Single window, no scan.
I0123 19:26:51.197717 139637719117824 transformer_layer.py:389] tlayer: self-attention.
I0123 19:26:51.197746 139637719117824 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.197806 139637719117824 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.198354 139637719117824 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.198428 139637719117824 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.198776 139637719117824 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.199447 139637719117824 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.201864 139637719117824 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.202470 139637719117824 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.202543 139637719117824 transformer_layer.py:468] tlayer: End windows.
I0123 19:26:51.202575 139637719117824 transformer_layer.py:472] tlayer: final FFN.
I0123 19:26:51.202631 139637719117824 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.202754 139637719117824 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:26:51.202859 139637719117824 nn_components.py:325] mlp: activation = None
I0123 19:26:51.202896 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:26:51.204795 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.204885 139637719117824 nn_components.py:261] mlp: residual
I0123 19:26:51.207264 139637719117824 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.207339 139637719117824 transformer_base.py:443] tbase: final FFN
I0123 19:26:51.207446 139637719117824 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:26:51.209589 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:26:51.211429 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.211520 139637719117824 nn_components.py:261] mlp: residual
I0123 19:26:51.211802 139637719117824 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.211880 139637719117824 decoder_stack.py:224] dstack: ---- Layer 8 ----
I0123 19:26:51.211984 139637719117824 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:26:51.212020 139637719117824 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:26:51.212050 139637719117824 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:26:51.212109 139637719117824 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.214312 139637719117824 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:26:51.219689 139637719117824 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.219939 139637719117824 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:26:51.222471 139637719117824 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:26:51.234604 139637719117824 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:26:51.234657 139637719117824 attention.py:418] Single window, no scan.
I0123 19:26:51.234689 139637719117824 transformer_layer.py:389] tlayer: self-attention.
I0123 19:26:51.234718 139637719117824 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.234777 139637719117824 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.235328 139637719117824 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.235400 139637719117824 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.235747 139637719117824 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.236412 139637719117824 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.238804 139637719117824 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.239406 139637719117824 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.239479 139637719117824 transformer_layer.py:468] tlayer: End windows.
I0123 19:26:51.239511 139637719117824 transformer_layer.py:472] tlayer: final FFN.
I0123 19:26:51.239568 139637719117824 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.239691 139637719117824 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:26:51.239797 139637719117824 nn_components.py:325] mlp: activation = None
I0123 19:26:51.239833 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:26:51.242151 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.242242 139637719117824 nn_components.py:261] mlp: residual
I0123 19:26:51.244598 139637719117824 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.244673 139637719117824 transformer_base.py:443] tbase: final FFN
I0123 19:26:51.244779 139637719117824 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:26:51.246913 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:26:51.248708 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.248799 139637719117824 nn_components.py:261] mlp: residual
I0123 19:26:51.249078 139637719117824 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.249155 139637719117824 decoder_stack.py:224] dstack: ---- Layer 9 ----
I0123 19:26:51.249260 139637719117824 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:26:51.249297 139637719117824 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:26:51.249326 139637719117824 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:26:51.249386 139637719117824 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.251571 139637719117824 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:26:51.257070 139637719117824 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.257324 139637719117824 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:26:51.259861 139637719117824 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:26:51.271988 139637719117824 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:26:51.272041 139637719117824 attention.py:418] Single window, no scan.
I0123 19:26:51.272075 139637719117824 transformer_layer.py:389] tlayer: self-attention.
I0123 19:26:51.272104 139637719117824 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.272161 139637719117824 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.272699 139637719117824 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.272770 139637719117824 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.273114 139637719117824 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.273778 139637719117824 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.276146 139637719117824 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.276739 139637719117824 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.276811 139637719117824 transformer_layer.py:468] tlayer: End windows.
I0123 19:26:51.276844 139637719117824 transformer_layer.py:472] tlayer: final FFN.
I0123 19:26:51.276899 139637719117824 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.277020 139637719117824 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:26:51.277123 139637719117824 nn_components.py:325] mlp: activation = None
I0123 19:26:51.277159 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:26:51.279047 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.279137 139637719117824 nn_components.py:261] mlp: residual
I0123 19:26:51.281477 139637719117824 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.281552 139637719117824 transformer_base.py:443] tbase: final FFN
I0123 19:26:51.281663 139637719117824 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:26:51.283814 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:26:51.285622 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.285722 139637719117824 nn_components.py:261] mlp: residual
I0123 19:26:51.286007 139637719117824 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.286084 139637719117824 decoder_stack.py:224] dstack: ---- Layer 10 ----
I0123 19:26:51.286189 139637719117824 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:26:51.286226 139637719117824 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:26:51.286256 139637719117824 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:26:51.286317 139637719117824 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.288479 139637719117824 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:26:51.293837 139637719117824 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.294097 139637719117824 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:26:51.296610 139637719117824 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:26:51.308683 139637719117824 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:26:51.308737 139637719117824 attention.py:418] Single window, no scan.
I0123 19:26:51.308771 139637719117824 transformer_layer.py:389] tlayer: self-attention.
I0123 19:26:51.308799 139637719117824 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.308858 139637719117824 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.309398 139637719117824 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.309470 139637719117824 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.309828 139637719117824 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.310493 139637719117824 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.312896 139637719117824 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.313496 139637719117824 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.313569 139637719117824 transformer_layer.py:468] tlayer: End windows.
I0123 19:26:51.313601 139637719117824 transformer_layer.py:472] tlayer: final FFN.
I0123 19:26:51.313663 139637719117824 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.313791 139637719117824 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:26:51.313896 139637719117824 nn_components.py:325] mlp: activation = None
I0123 19:26:51.313933 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:26:51.315815 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.315906 139637719117824 nn_components.py:261] mlp: residual
I0123 19:26:51.318264 139637719117824 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.318339 139637719117824 transformer_base.py:443] tbase: final FFN
I0123 19:26:51.318445 139637719117824 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:26:51.320578 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:26:51.322391 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.322483 139637719117824 nn_components.py:261] mlp: residual
I0123 19:26:51.322768 139637719117824 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.322845 139637719117824 decoder_stack.py:224] dstack: ---- Layer 11 ----
I0123 19:26:51.322951 139637719117824 transformer_layer.py:154] tlayer: recurrent = False
I0123 19:26:51.322988 139637719117824 transformer_layer.py:155] tlayer: compute_importance = False
I0123 19:26:51.323016 139637719117824 transformer_layer.py:161] tlayer: compute keys,values,queries.
I0123 19:26:51.323077 139637719117824 transformer_base.py:146] kvq: pre_attn xs = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.325244 139637719117824 transformer_base.py:161] kvq: pre_attn dropout.
I0123 19:26:51.330639 139637719117824 transformer_base.py:173] kvq: queries = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.330899 139637719117824 transformer_base.py:194] kvq: normalize keys, queries.
I0123 19:26:51.333407 139637719117824 transformer_layer.py:169] tlayer: using autoregressive decoder.
I0123 19:26:51.345522 139637719117824 transformer_layer.py:299] tlayer: num_windows = 1.
I0123 19:26:51.345576 139637719117824 attention.py:418] Single window, no scan.
I0123 19:26:51.345609 139637719117824 transformer_layer.py:389] tlayer: self-attention.
I0123 19:26:51.345638 139637719117824 attention.py:133] attn: keys = Traced<ShapedArray(bfloat16[32,1024,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.345704 139637719117824 attention.py:134] attn: queries = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.346256 139637719117824 attention.py:139] attn: content attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.346329 139637719117824 attention.py:143] attn: pbias = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.346673 139637719117824 attention.py:150] attn: learned attention scale: Traced<ShapedArray(bfloat16[8])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.347334 139637719117824 attention.py:161] attn: pre-softmax attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.349716 139637719117824 attention.py:177] attn: final attn = Traced<ShapedArray(bfloat16[32,8,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.350320 139637719117824 attention.py:182] attn: y = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.350392 139637719117824 transformer_layer.py:468] tlayer: End windows.
I0123 19:26:51.350424 139637719117824 transformer_layer.py:472] tlayer: final FFN.
I0123 19:26:51.350479 139637719117824 transformer_base.py:399] tbase: attn_ys = Traced<ShapedArray(bfloat16[32,1,8,128])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.350601 139637719117824 transformer_base.py:410] tbase: post-attention MLP.
I0123 19:26:51.350704 139637719117824 nn_components.py:325] mlp: activation = None
I0123 19:26:51.350741 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:26:51.353013 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.353103 139637719117824 nn_components.py:261] mlp: residual
I0123 19:26:51.355450 139637719117824 transformer_base.py:431] tbase: pre-FFN layernorm = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.355526 139637719117824 transformer_base.py:443] tbase: final FFN
I0123 19:26:51.355633 139637719117824 nn_components.py:320] mlp: hidden 4096, relu
I0123 19:26:51.357766 139637719117824 nn_components.py:329] mlp: final activation = None
I0123 19:26:51.359554 139637719117824 nn_components.py:332] mlp: final = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.359645 139637719117824 nn_components.py:261] mlp: residual
I0123 19:26:51.359925 139637719117824 transformer_base.py:450] tbase: ys = Traced<ShapedArray(bfloat16[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:26:51.360005 139637719117824 decoder_stack.py:344] dstack: Final layernorm.
I0123 19:26:51.362780 139637719117824 decoder_stack.py:365] dstack: logits = Traced<ShapedArray(float32[32,1,1024])>with<DynamicJaxprTrace(level=2/0)>
I0123 19:27:04.513269 139637719117824 alphageometry.py:566] LM output (score=-2.477531): "p : C a e p 22 D a p e p 23 ;"
I0123 19:27:04.513432 139637719117824 alphageometry.py:567] Translation: "p = on_line p a e, on_bline p e a"

I0123 19:27:04.513480 139637719117824 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_line p a e, on_bline p e a ? simtri b a c n o m"
I0123 19:27:04.513663 139637719117824 graph.py:498] 
I0123 19:27:04.513724 139637719117824 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_line p a e, on_bline p e a ? simtri b a c n o m
I0123 19:27:08.450932 139637719117824 ddar.py:60] Depth 1/1000 time = 3.871760368347168
I0123 19:27:22.408936 139637719117824 ddar.py:60] Depth 2/1000 time = 13.957791566848755
I0123 19:27:38.639497 139637719117824 ddar.py:60] Depth 3/1000 time = 16.23026704788208
I0123 19:27:55.038040 139637719117824 ddar.py:60] Depth 4/1000 time = 16.398077487945557
I0123 19:28:11.192287 139637719117824 ddar.py:60] Depth 5/1000 time = 16.15398073196411
I0123 19:28:27.423707 139637719117824 ddar.py:60] Depth 6/1000 time = 16.229990243911743
I0123 19:28:44.250173 139637719117824 ddar.py:60] Depth 7/1000 time = 16.724871397018433
I0123 19:28:44.250635 139637719117824 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 19:28:44.250764 139637719117824 alphageometry.py:566] LM output (score=-2.509944): "p : C f m p 22 D f p m p 23 ;"
I0123 19:28:44.250805 139637719117824 alphageometry.py:567] Translation: "p = on_line p f m, on_bline p m f"

I0123 19:28:44.250846 139637719117824 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_line p f m, on_bline p m f ? simtri b a c n o m"
I0123 19:28:44.251039 139637719117824 graph.py:498] 
I0123 19:28:44.251101 139637719117824 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_line p f m, on_bline p m f ? simtri b a c n o m
I0123 19:28:47.876012 139637719117824 ddar.py:60] Depth 1/1000 time = 3.5541739463806152
I0123 19:29:00.308019 139637719117824 ddar.py:60] Depth 2/1000 time = 12.431695938110352
I0123 19:29:14.915232 139637719117824 ddar.py:60] Depth 3/1000 time = 14.606858491897583
I0123 19:29:29.955029 139637719117824 ddar.py:60] Depth 4/1000 time = 15.039371252059937
I0123 19:29:44.637251 139637719117824 ddar.py:60] Depth 5/1000 time = 14.68179965019226
I0123 19:29:59.731224 139637719117824 ddar.py:60] Depth 6/1000 time = 15.092631340026855
I0123 19:30:15.267271 139637719117824 ddar.py:60] Depth 7/1000 time = 15.434521436691284
I0123 19:30:15.267825 139637719117824 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 19:30:15.267973 139637719117824 alphageometry.py:566] LM output (score=-2.578736): "p : C f g p 22 D f p g p 23 ;"
I0123 19:30:15.268016 139637719117824 alphageometry.py:567] Translation: "p = on_line p f g, on_bline p g f"

I0123 19:30:15.268072 139637719117824 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_line p f g, on_bline p g f ? simtri b a c n o m"
I0123 19:30:15.268297 139637719117824 graph.py:498] 
I0123 19:30:15.268362 139637719117824 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_line p f g, on_bline p g f ? simtri b a c n o m
I0123 19:30:19.053811 139637719117824 ddar.py:60] Depth 1/1000 time = 3.714130401611328
I0123 19:30:32.611598 139637719117824 ddar.py:60] Depth 2/1000 time = 13.557568311691284
I0123 19:30:49.099553 139637719117824 ddar.py:60] Depth 3/1000 time = 16.487635850906372
I0123 19:31:06.194990 139637719117824 ddar.py:60] Depth 4/1000 time = 17.09512758255005
I0123 19:31:22.960721 139637719117824 ddar.py:60] Depth 5/1000 time = 16.765413522720337
I0123 19:31:39.869662 139637719117824 ddar.py:60] Depth 6/1000 time = 16.90750765800476
I0123 19:31:57.278092 139637719117824 ddar.py:60] Depth 7/1000 time = 17.300395250320435
I0123 19:31:57.278658 139637719117824 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 19:31:57.278768 139637719117824 alphageometry.py:566] LM output (score=-2.632152): "p : P a b c p 22 T a b a p 23 ;"
I0123 19:31:57.278805 139637719117824 alphageometry.py:567] Translation: "p = on_pline p c a b, on_tline p a a b"

I0123 19:31:57.278848 139637719117824 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_pline p c a b, on_tline p a a b ? simtri b a c n o m"
I0123 19:31:57.279047 139637719117824 graph.py:498] 
I0123 19:31:57.279110 139637719117824 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_pline p c a b, on_tline p a a b ? simtri b a c n o m
I0123 19:32:00.847095 139637719117824 ddar.py:60] Depth 1/1000 time = 3.5042595863342285
I0123 19:32:12.999107 139637719117824 ddar.py:60] Depth 2/1000 time = 12.151681184768677
I0123 19:32:28.113806 139637719117824 ddar.py:60] Depth 3/1000 time = 15.114379644393921
I0123 19:32:42.958853 139637719117824 ddar.py:60] Depth 4/1000 time = 14.844642162322998
I0123 19:32:58.118080 139637719117824 ddar.py:60] Depth 5/1000 time = 15.158812284469604
I0123 19:33:13.380514 139637719117824 ddar.py:60] Depth 6/1000 time = 15.261016607284546
I0123 19:33:28.942719 139637719117824 ddar.py:60] Depth 7/1000 time = 15.482206583023071
I0123 19:33:28.947993 139637719117824 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 19:33:28.948111 139637719117824 alphageometry.py:566] LM output (score=-2.652616): "p : T e f f p 22 ;"
I0123 19:33:28.948151 139637719117824 alphageometry.py:567] Translation: "p = on_tline p f e f"

I0123 19:33:28.948199 139637719117824 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_tline p f e f ? simtri b a c n o m"
I0123 19:33:28.948390 139637719117824 graph.py:498] 
I0123 19:33:28.948452 139637719117824 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_tline p f e f ? simtri b a c n o m
I0123 19:33:32.226933 139637719117824 ddar.py:60] Depth 1/1000 time = 3.2203762531280518
I0123 19:33:44.601736 139637719117824 ddar.py:60] Depth 2/1000 time = 12.374603271484375
I0123 19:33:58.983217 139637719117824 ddar.py:60] Depth 3/1000 time = 14.381238222122192
I0123 19:34:13.768209 139637719117824 ddar.py:60] Depth 4/1000 time = 14.784719944000244
I0123 19:34:27.946046 139637719117824 ddar.py:60] Depth 5/1000 time = 14.177561044692993
I0123 19:34:42.534980 139637719117824 ddar.py:60] Depth 6/1000 time = 14.587570667266846
I0123 19:34:57.352127 139637719117824 ddar.py:60] Depth 7/1000 time = 14.744532108306885
I0123 19:34:57.354392 139637719117824 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 19:34:57.354539 139637719117824 alphageometry.py:566] LM output (score=-2.724478): "p : P f p h i 22 ;"
I0123 19:34:57.354581 139637719117824 alphageometry.py:567] Translation: "p = on_pline p f h i"

I0123 19:34:57.354633 139637719117824 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_pline p f h i ? simtri b a c n o m"
I0123 19:34:57.354837 139637719117824 graph.py:498] 
I0123 19:34:57.354900 139637719117824 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_pline p f h i ? simtri b a c n o m
I0123 19:35:00.597405 139637719117824 ddar.py:60] Depth 1/1000 time = 3.181527853012085
I0123 19:35:11.880136 139637719117824 ddar.py:60] Depth 2/1000 time = 11.28251576423645
I0123 19:35:26.104769 139637719117824 ddar.py:60] Depth 3/1000 time = 14.224365234375
I0123 19:35:40.220699 139637719117824 ddar.py:60] Depth 4/1000 time = 14.115558385848999
I0123 19:35:54.491686 139637719117824 ddar.py:60] Depth 5/1000 time = 14.270563125610352
I0123 19:36:08.448848 139637719117824 ddar.py:60] Depth 6/1000 time = 13.955996036529541
I0123 19:36:23.117174 139637719117824 ddar.py:60] Depth 7/1000 time = 14.600115776062012
I0123 19:36:23.119573 139637719117824 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 19:36:23.119683 139637719117824 alphageometry.py:566] LM output (score=-2.759774): "p : C b f p 22 D b f b p 23 ;"
I0123 19:36:23.119720 139637719117824 alphageometry.py:567] Translation: "p = on_line p b f, on_circle p b f"

I0123 19:36:23.119762 139637719117824 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_line p b f, on_circle p b f ? simtri b a c n o m"
I0123 19:36:23.119953 139637719117824 graph.py:498] 
I0123 19:36:23.120015 139637719117824 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_line p b f, on_circle p b f ? simtri b a c n o m
I0123 19:36:26.846646 139637719117824 ddar.py:60] Depth 1/1000 time = 3.65217924118042
I0123 19:36:40.557192 139637719117824 ddar.py:60] Depth 2/1000 time = 13.710278511047363
I0123 19:36:55.821756 139637719117824 ddar.py:60] Depth 3/1000 time = 15.264198064804077
I0123 19:37:11.234438 139637719117824 ddar.py:60] Depth 4/1000 time = 15.41242527961731
I0123 19:37:26.746082 139637719117824 ddar.py:60] Depth 5/1000 time = 15.511256217956543
I0123 19:37:41.938196 139637719117824 ddar.py:60] Depth 6/1000 time = 15.189679622650146
I0123 19:37:57.716464 139637719117824 ddar.py:60] Depth 7/1000 time = 15.69633960723877
I0123 19:37:57.719292 139637719117824 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 19:37:57.719439 139637719117824 alphageometry.py:566] LM output (score=-2.764223): "p : T f p h i 22 ;"
I0123 19:37:57.719483 139637719117824 alphageometry.py:567] Translation: "p = on_tline p f h i"

I0123 19:37:57.719525 139637719117824 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_tline p f h i ? simtri b a c n o m"
I0123 19:37:57.719718 139637719117824 graph.py:498] 
I0123 19:37:57.719783 139637719117824 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_tline p f h i ? simtri b a c n o m
I0123 19:38:01.156999 139637719117824 ddar.py:60] Depth 1/1000 time = 3.377842903137207
I0123 19:38:13.539804 139637719117824 ddar.py:60] Depth 2/1000 time = 12.382568359375
I0123 19:38:27.593116 139637719117824 ddar.py:60] Depth 3/1000 time = 14.052935361862183
I0123 19:38:41.873265 139637719117824 ddar.py:60] Depth 4/1000 time = 14.279741525650024
I0123 19:38:55.848887 139637719117824 ddar.py:60] Depth 5/1000 time = 13.974292039871216
I0123 19:39:10.402803 139637719117824 ddar.py:60] Depth 6/1000 time = 14.477981090545654
I0123 19:39:10.403399 139637719117824 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 19:39:10.403501 139637719117824 alphageometry.py:566] LM output (score=-2.823726): "p : D d f d p 22 ;"
I0123 19:39:10.403538 139637719117824 alphageometry.py:567] Translation: "p = on_circle p d f"

I0123 19:39:10.403585 139637719117824 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_circle p d f ? simtri b a c n o m"
I0123 19:39:10.403768 139637719117824 graph.py:498] 
I0123 19:39:10.403827 139637719117824 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_circle p d f ? simtri b a c n o m
I0123 19:39:16.411111 139637719117824 ddar.py:60] Depth 1/1000 time = 5.915010929107666
I0123 19:39:34.356344 139637719117824 ddar.py:60] Depth 2/1000 time = 17.94496989250183
I0123 19:39:56.019155 139637719117824 ddar.py:60] Depth 3/1000 time = 21.662424564361572
I0123 19:40:17.784422 139637719117824 ddar.py:60] Depth 4/1000 time = 21.764891624450684
I0123 19:40:39.607650 139637719117824 ddar.py:60] Depth 5/1000 time = 21.82272171974182
I0123 19:41:01.334223 139637719117824 ddar.py:60] Depth 6/1000 time = 21.725119590759277
I0123 19:41:23.956327 139637719117824 ddar.py:60] Depth 7/1000 time = 22.502548217773438
I0123 19:41:23.958684 139637719117824 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 19:41:23.958827 139637719117824 alphageometry.py:566] LM output (score=-2.887521): "p : C k l p 22 D k p l p 23 ;"
I0123 19:41:23.958868 139637719117824 alphageometry.py:567] Translation: "p = on_line p k l, on_bline p l k"

I0123 19:41:23.958920 139637719117824 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_line p k l, on_bline p l k ? simtri b a c n o m"
I0123 19:41:23.959127 139637719117824 graph.py:498] 
I0123 19:41:23.959187 139637719117824 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_line p k l, on_bline p l k ? simtri b a c n o m
I0123 19:41:27.810842 139637719117824 ddar.py:60] Depth 1/1000 time = 3.774683713912964
I0123 19:41:40.162765 139637719117824 ddar.py:60] Depth 2/1000 time = 12.35170602798462
I0123 19:41:53.773156 139637719117824 ddar.py:60] Depth 3/1000 time = 13.61009931564331
I0123 19:42:07.211363 139637719117824 ddar.py:60] Depth 4/1000 time = 13.437885284423828
I0123 19:42:20.861439 139637719117824 ddar.py:60] Depth 5/1000 time = 13.648509740829468
I0123 19:42:34.581013 139637719117824 ddar.py:60] Depth 6/1000 time = 13.64200735092163
I0123 19:42:34.581467 139637719117824 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 19:42:34.581589 139637719117824 alphageometry.py:566] LM output (score=-2.891377): "p : D d e d p 22 ;"
I0123 19:42:34.581629 139637719117824 alphageometry.py:567] Translation: "p = on_circle p d e"

I0123 19:42:34.581678 139637719117824 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_circle p d e ? simtri b a c n o m"
I0123 19:42:34.581871 139637719117824 graph.py:498] 
I0123 19:42:34.581937 139637719117824 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_circle p d e ? simtri b a c n o m
I0123 19:42:39.869343 139637719117824 ddar.py:60] Depth 1/1000 time = 5.207207679748535
I0123 19:42:59.159382 139637719117824 ddar.py:60] Depth 2/1000 time = 19.28981637954712
I0123 19:43:20.207302 139637719117824 ddar.py:60] Depth 3/1000 time = 21.047666311264038
I0123 19:43:41.629191 139637719117824 ddar.py:60] Depth 4/1000 time = 21.421623945236206
I0123 19:44:03.104009 139637719117824 ddar.py:60] Depth 5/1000 time = 21.473607301712036
I0123 19:44:25.225735 139637719117824 ddar.py:60] Depth 6/1000 time = 21.994248628616333
I0123 19:44:25.226192 139637719117824 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 19:44:25.226313 139637719117824 alphageometry.py:566] LM output (score=-2.984303): "p : T h i i p 22 ;"
I0123 19:44:25.226354 139637719117824 alphageometry.py:567] Translation: "p = on_tline p i h i"

I0123 19:44:25.226398 139637719117824 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_tline p i h i ? simtri b a c n o m"
I0123 19:44:25.226586 139637719117824 graph.py:498] 
I0123 19:44:25.226652 139637719117824 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_tline p i h i ? simtri b a c n o m
I0123 19:44:28.506720 139637719117824 ddar.py:60] Depth 1/1000 time = 3.2034859657287598
I0123 19:44:40.982089 139637719117824 ddar.py:60] Depth 2/1000 time = 12.47515344619751
I0123 19:44:54.633099 139637719117824 ddar.py:60] Depth 3/1000 time = 13.650721549987793
I0123 19:45:08.706948 139637719117824 ddar.py:60] Depth 4/1000 time = 14.073499917984009
I0123 19:45:22.518420 139637719117824 ddar.py:60] Depth 5/1000 time = 13.811148166656494
I0123 19:45:36.638519 139637719117824 ddar.py:60] Depth 6/1000 time = 14.118762969970703
I0123 19:45:50.679835 139637719117824 ddar.py:60] Depth 7/1000 time = 13.968690872192383
I0123 19:45:50.682072 139637719117824 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 19:45:50.682180 139637719117824 alphageometry.py:566] LM output (score=-3.065201): "p : P f p j k 22 ;"
I0123 19:45:50.682218 139637719117824 alphageometry.py:567] Translation: "p = on_pline p f j k"

I0123 19:45:50.682257 139637719117824 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_pline p f j k ? simtri b a c n o m"
I0123 19:45:50.682440 139637719117824 graph.py:498] 
I0123 19:45:50.682500 139637719117824 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_pline p f j k ? simtri b a c n o m
I0123 19:45:54.243366 139637719117824 ddar.py:60] Depth 1/1000 time = 3.5004537105560303
I0123 19:46:06.917423 139637719117824 ddar.py:60] Depth 2/1000 time = 12.673814535140991
I0123 19:46:21.180198 139637719117824 ddar.py:60] Depth 3/1000 time = 14.262468576431274
I0123 19:46:35.530594 139637719117824 ddar.py:60] Depth 4/1000 time = 14.350030660629272
I0123 19:46:49.790281 139637719117824 ddar.py:60] Depth 5/1000 time = 14.25939393043518
I0123 19:47:04.430832 139637719117824 ddar.py:60] Depth 6/1000 time = 14.63924503326416
I0123 19:47:19.309527 139637719117824 ddar.py:60] Depth 7/1000 time = 14.811575412750244
I0123 19:47:19.311843 139637719117824 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 19:47:19.311980 139637719117824 alphageometry.py:566] LM output (score=-3.067575): "p : C k l p 22 D k l l p 23 ;"
I0123 19:47:19.312023 139637719117824 alphageometry.py:567] Translation: "p = on_line p k l, on_circle p l k"

I0123 19:47:19.312066 139637719117824 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_line p k l, on_circle p l k ? simtri b a c n o m"
I0123 19:47:19.312257 139637719117824 graph.py:498] 
I0123 19:47:19.312320 139637719117824 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_line p k l, on_circle p l k ? simtri b a c n o m
I0123 19:47:22.994837 139637719117824 ddar.py:60] Depth 1/1000 time = 3.6087419986724854
I0123 19:47:35.391324 139637719117824 ddar.py:60] Depth 2/1000 time = 12.39618182182312
I0123 19:47:50.417157 139637719117824 ddar.py:60] Depth 3/1000 time = 15.025583028793335
I0123 19:48:05.149821 139637719117824 ddar.py:60] Depth 4/1000 time = 14.732401132583618
I0123 19:48:19.895208 139637719117824 ddar.py:60] Depth 5/1000 time = 14.745109796524048
I0123 19:48:34.668293 139637719117824 ddar.py:60] Depth 6/1000 time = 14.771377563476562
I0123 19:48:50.110666 139637719117824 ddar.py:60] Depth 7/1000 time = 15.368365287780762
I0123 19:48:50.113443 139637719117824 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 19:48:50.113600 139637719117824 alphageometry.py:566] LM output (score=-3.121219): "p : T e p h i 22 ;"
I0123 19:48:50.113647 139637719117824 alphageometry.py:567] Translation: "p = on_tline p e h i"

I0123 19:48:50.113703 139637719117824 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_tline p e h i ? simtri b a c n o m"
I0123 19:48:50.113911 139637719117824 graph.py:498] 
I0123 19:48:50.113975 139637719117824 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_tline p e h i ? simtri b a c n o m
I0123 19:48:53.579196 139637719117824 ddar.py:60] Depth 1/1000 time = 3.4013733863830566
I0123 19:49:06.155251 139637719117824 ddar.py:60] Depth 2/1000 time = 12.575845956802368
I0123 19:49:20.011187 139637719117824 ddar.py:60] Depth 3/1000 time = 13.855695962905884
I0123 19:49:33.896122 139637719117824 ddar.py:60] Depth 4/1000 time = 13.88465929031372
I0123 19:49:47.783763 139637719117824 ddar.py:60] Depth 5/1000 time = 13.887329578399658
I0123 19:50:01.894272 139637719117824 ddar.py:60] Depth 6/1000 time = 14.109232902526855
I0123 19:50:15.901370 139637719117824 ddar.py:60] Depth 7/1000 time = 13.931956768035889
I0123 19:50:15.903697 139637719117824 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 19:50:15.903856 139637719117824 alphageometry.py:566] LM output (score=-3.123465): "p : D a d d p 22 ;"
I0123 19:50:15.903898 139637719117824 alphageometry.py:567] Translation: "p = on_circle p d a"

I0123 19:50:15.903951 139637719117824 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_circle p d a ? simtri b a c n o m"
I0123 19:50:15.904156 139637719117824 graph.py:498] 
I0123 19:50:15.904216 139637719117824 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_circle p d a ? simtri b a c n o m
I0123 19:50:21.638177 139637719117824 ddar.py:60] Depth 1/1000 time = 5.666526794433594
I0123 19:50:39.441952 139637719117824 ddar.py:60] Depth 2/1000 time = 17.8035147190094
I0123 19:51:01.012989 139637719117824 ddar.py:60] Depth 3/1000 time = 21.57065773010254
I0123 19:51:22.196194 139637719117824 ddar.py:60] Depth 4/1000 time = 21.182854890823364
I0123 19:51:43.728783 139637719117824 ddar.py:60] Depth 5/1000 time = 21.531139612197876
I0123 19:52:05.852647 139637719117824 ddar.py:60] Depth 6/1000 time = 21.999438047409058
I0123 19:52:05.853277 139637719117824 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 19:52:05.853451 139637719117824 alphageometry.py:566] LM output (score=-3.166678): "p : C b f p 22 D b f f p 23 ;"
I0123 19:52:05.853495 139637719117824 alphageometry.py:567] Translation: "p = on_line p b f, on_circle p f b"

I0123 19:52:05.853568 139637719117824 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_line p b f, on_circle p f b ? simtri b a c n o m"
I0123 19:52:05.853808 139637719117824 graph.py:498] 
I0123 19:52:05.853879 139637719117824 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_line p b f, on_circle p f b ? simtri b a c n o m
I0123 19:52:10.003460 139637719117824 ddar.py:60] Depth 1/1000 time = 4.07023811340332
I0123 19:52:23.867762 139637719117824 ddar.py:60] Depth 2/1000 time = 13.864084720611572
I0123 19:52:38.637383 139637719117824 ddar.py:60] Depth 3/1000 time = 14.769399881362915
I0123 19:52:54.064515 139637719117824 ddar.py:60] Depth 4/1000 time = 15.42687201499939
I0123 19:53:09.475460 139637719117824 ddar.py:60] Depth 5/1000 time = 15.410661458969116
I0123 19:53:24.563155 139637719117824 ddar.py:60] Depth 6/1000 time = 15.085247039794922
I0123 19:53:40.326552 139637719117824 ddar.py:60] Depth 7/1000 time = 15.6911039352417
I0123 19:53:40.329083 139637719117824 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 19:53:40.329236 139637719117824 alphageometry.py:566] LM output (score=-3.178807): "p : D f m f p 22 D g m g p 23 ;"
I0123 19:53:40.329281 139637719117824 alphageometry.py:567] Translation: "p = on_circle p f m, on_circle p g m"

I0123 19:53:40.329324 139637719117824 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_circle p f m, on_circle p g m ? simtri b a c n o m"
I0123 19:53:40.329528 139637719117824 graph.py:498] 
I0123 19:53:40.329591 139637719117824 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_circle p f m, on_circle p g m ? simtri b a c n o m
I0123 19:53:44.100314 139637719117824 ddar.py:60] Depth 1/1000 time = 3.677945852279663
I0123 19:53:56.994195 139637719117824 ddar.py:60] Depth 2/1000 time = 12.893661975860596
I0123 19:54:11.764703 139637719117824 ddar.py:60] Depth 3/1000 time = 14.77019476890564
I0123 19:54:26.619348 139637719117824 ddar.py:60] Depth 4/1000 time = 14.854274272918701
I0123 19:54:41.570010 139637719117824 ddar.py:60] Depth 5/1000 time = 14.9503812789917
I0123 19:54:56.880582 139637719117824 ddar.py:60] Depth 6/1000 time = 15.30897569656372
I0123 19:55:12.368281 139637719117824 ddar.py:60] Depth 7/1000 time = 15.434776544570923
I0123 19:55:27.906319 139637719117824 ddar.py:60] Depth 8/1000 time = 15.504960060119629
I0123 19:55:27.908739 139637719117824 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 19:55:27.908870 139637719117824 alphageometry.py:566] LM output (score=-3.293813): "p : D f m f p 22 D m n n p 23 ;"
I0123 19:55:27.908912 139637719117824 alphageometry.py:567] Translation: "p = on_circle p f m, on_circle p n m"

I0123 19:55:27.908951 139637719117824 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_circle p f m, on_circle p n m ? simtri b a c n o m"
I0123 19:55:27.909150 139637719117824 graph.py:498] 
I0123 19:55:27.909213 139637719117824 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_circle p f m, on_circle p n m ? simtri b a c n o m
I0123 19:55:31.642576 139637719117824 ddar.py:60] Depth 1/1000 time = 3.6528077125549316
I0123 19:55:44.751779 139637719117824 ddar.py:60] Depth 2/1000 time = 13.108945608139038
I0123 19:56:00.424195 139637719117824 ddar.py:60] Depth 3/1000 time = 15.672067642211914
I0123 19:56:15.978013 139637719117824 ddar.py:60] Depth 4/1000 time = 15.553391933441162
I0123 19:56:31.543502 139637719117824 ddar.py:60] Depth 5/1000 time = 15.564157485961914
I0123 19:56:47.402694 139637719117824 ddar.py:60] Depth 6/1000 time = 15.763012647628784
I0123 19:56:47.403355 139637719117824 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 19:56:47.403489 139637719117824 alphageometry.py:566] LM output (score=-3.300741): "p : T i k k p 22 ;"
I0123 19:56:47.403526 139637719117824 alphageometry.py:567] Translation: "p = on_tline p k i k"

I0123 19:56:47.403584 139637719117824 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_tline p k i k ? simtri b a c n o m"
I0123 19:56:47.403796 139637719117824 graph.py:498] 
I0123 19:56:47.403857 139637719117824 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_tline p k i k ? simtri b a c n o m
I0123 19:56:51.252499 139637719117824 ddar.py:60] Depth 1/1000 time = 3.7945096492767334
I0123 19:57:04.164907 139637719117824 ddar.py:60] Depth 2/1000 time = 12.912137746810913
I0123 19:57:18.709896 139637719117824 ddar.py:60] Depth 3/1000 time = 14.54462742805481
I0123 19:57:33.395859 139637719117824 ddar.py:60] Depth 4/1000 time = 14.685726642608643
I0123 19:57:48.107474 139637719117824 ddar.py:60] Depth 5/1000 time = 14.71133542060852
I0123 19:58:02.874920 139637719117824 ddar.py:60] Depth 6/1000 time = 14.766145467758179
I0123 19:58:17.461116 139637719117824 ddar.py:60] Depth 7/1000 time = 14.521252632141113
I0123 19:58:17.463267 139637719117824 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 19:58:17.463390 139637719117824 alphageometry.py:566] LM output (score=-3.320926): "p : P i k l p 22 ;"
I0123 19:58:17.463429 139637719117824 alphageometry.py:567] Translation: "p = on_pline p l i k"

I0123 19:58:17.463468 139637719117824 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_pline p l i k ? simtri b a c n o m"
I0123 19:58:17.463645 139637719117824 graph.py:498] 
I0123 19:58:17.463710 139637719117824 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_pline p l i k ? simtri b a c n o m
I0123 19:58:20.970776 139637719117824 ddar.py:60] Depth 1/1000 time = 3.4459056854248047
I0123 19:58:33.898097 139637719117824 ddar.py:60] Depth 2/1000 time = 12.927093029022217
I0123 19:58:47.963550 139637719117824 ddar.py:60] Depth 3/1000 time = 14.065179586410522
I0123 19:59:01.679948 139637719117824 ddar.py:60] Depth 4/1000 time = 13.716091632843018
I0123 19:59:15.999087 139637719117824 ddar.py:60] Depth 5/1000 time = 14.3177490234375
I0123 19:59:30.157623 139637719117824 ddar.py:60] Depth 6/1000 time = 14.090764284133911
I0123 19:59:30.158303 139637719117824 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 19:59:30.158436 139637719117824 alphageometry.py:566] LM output (score=-3.336631): "p : D f n f p 22 D l n l p 23 ;"
I0123 19:59:30.158474 139637719117824 alphageometry.py:567] Translation: "ERROR: Traceback (most recent call last):
  File "/home/chi/alphageometry-test/alphageometry.py", line 438, in try_translate_constrained_to_construct
    g.copy().add_clause(clause, 0, DEFINITIONS)
  File "/home/chi/alphageometry-test/graph.py", line 2635, in add_clause
    raise PointTooCloseError()
graph.PointTooCloseError
"

I0123 19:59:30.158526 139637719117824 alphageometry.py:566] LM output (score=-3.347604): "p : P a b c p 22 ;"
I0123 19:59:30.158556 139637719117824 alphageometry.py:567] Translation: "p = on_pline p c a b"

I0123 19:59:30.158590 139637719117824 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_pline p c a b ? simtri b a c n o m"
I0123 19:59:30.158800 139637719117824 graph.py:498] 
I0123 19:59:30.158861 139637719117824 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_pline p c a b ? simtri b a c n o m
I0123 19:59:33.683088 139637719117824 ddar.py:60] Depth 1/1000 time = 3.463059663772583
I0123 19:59:45.719696 139637719117824 ddar.py:60] Depth 2/1000 time = 12.036388635635376
I0123 19:59:59.567179 139637719117824 ddar.py:60] Depth 3/1000 time = 13.847179651260376
I0123 20:00:13.591447 139637719117824 ddar.py:60] Depth 4/1000 time = 14.023969173431396
I0123 20:00:27.906684 139637719117824 ddar.py:60] Depth 5/1000 time = 14.313762664794922
I0123 20:00:42.155987 139637719117824 ddar.py:60] Depth 6/1000 time = 14.179059982299805
I0123 20:00:42.156487 139637719117824 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 20:00:42.156625 139637719117824 alphageometry.py:566] LM output (score=-3.360558): "p : T f p i k 22 ;"
I0123 20:00:42.156667 139637719117824 alphageometry.py:567] Translation: "p = on_tline p f i k"

I0123 20:00:42.156718 139637719117824 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_tline p f i k ? simtri b a c n o m"
I0123 20:00:42.156925 139637719117824 graph.py:498] 
I0123 20:00:42.156990 139637719117824 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_tline p f i k ? simtri b a c n o m
I0123 20:00:45.998439 139637719117824 ddar.py:60] Depth 1/1000 time = 3.78235125541687
I0123 20:00:58.678813 139637719117824 ddar.py:60] Depth 2/1000 time = 12.680139541625977
I0123 20:01:13.217792 139637719117824 ddar.py:60] Depth 3/1000 time = 14.538666248321533
I0123 20:01:28.233818 139637719117824 ddar.py:60] Depth 4/1000 time = 15.015723705291748
I0123 20:01:42.713416 139637719117824 ddar.py:60] Depth 5/1000 time = 14.478115320205688
I0123 20:01:57.473820 139637719117824 ddar.py:60] Depth 6/1000 time = 14.690864562988281
I0123 20:01:57.474243 139637719117824 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 20:01:57.474373 139637719117824 alphageometry.py:566] LM output (score=-3.411824): "p : P f p j l 22 ;"
I0123 20:01:57.474414 139637719117824 alphageometry.py:567] Translation: "p = on_pline p f j l"

I0123 20:01:57.474459 139637719117824 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_pline p f j l ? simtri b a c n o m"
I0123 20:01:57.474652 139637719117824 graph.py:498] 
I0123 20:01:57.474717 139637719117824 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_pline p f j l ? simtri b a c n o m
I0123 20:02:00.544841 139637719117824 ddar.py:60] Depth 1/1000 time = 3.002640724182129
I0123 20:02:13.254335 139637719117824 ddar.py:60] Depth 2/1000 time = 12.709208011627197
I0123 20:02:27.536380 139637719117824 ddar.py:60] Depth 3/1000 time = 14.281803607940674
I0123 20:02:41.341689 139637719117824 ddar.py:60] Depth 4/1000 time = 13.805087089538574
I0123 20:02:55.288418 139637719117824 ddar.py:60] Depth 5/1000 time = 13.946478605270386
I0123 20:03:09.718531 139637719117824 ddar.py:60] Depth 6/1000 time = 14.428725242614746
I0123 20:03:23.988632 139637719117824 ddar.py:60] Depth 7/1000 time = 14.204431295394897
I0123 20:03:23.990994 139637719117824 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 20:03:23.991144 139637719117824 alphageometry.py:566] LM output (score=-3.442244): "p : D f m f p 22 T f m f p 23 ;"
I0123 20:03:23.991186 139637719117824 alphageometry.py:567] Translation: "p = on_circle p f m, on_tline p f f m"

I0123 20:03:23.991238 139637719117824 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_circle p f m, on_tline p f f m ? simtri b a c n o m"
I0123 20:03:23.991445 139637719117824 graph.py:498] 
I0123 20:03:23.991508 139637719117824 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_circle p f m, on_tline p f f m ? simtri b a c n o m
I0123 20:03:27.151034 139637719117824 ddar.py:60] Depth 1/1000 time = 3.0827739238739014
I0123 20:03:39.226938 139637719117824 ddar.py:60] Depth 2/1000 time = 12.075681924819946
I0123 20:03:53.622263 139637719117824 ddar.py:60] Depth 3/1000 time = 14.39506220817566
I0123 20:04:08.183433 139637719117824 ddar.py:60] Depth 4/1000 time = 14.560810089111328
I0123 20:04:22.598668 139637719117824 ddar.py:60] Depth 5/1000 time = 14.414809465408325
I0123 20:04:37.220224 139637719117824 ddar.py:60] Depth 6/1000 time = 14.620071649551392
I0123 20:04:51.758214 139637719117824 ddar.py:60] Depth 7/1000 time = 14.536466836929321
I0123 20:05:06.470756 139637719117824 ddar.py:60] Depth 8/1000 time = 14.636296272277832
I0123 20:05:06.473296 139637719117824 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 20:05:06.473415 139637719117824 alphageometry.py:566] LM output (score=-3.456932): "p : T e p i k 22 ;"
I0123 20:05:06.473452 139637719117824 alphageometry.py:567] Translation: "p = on_tline p e i k"

I0123 20:05:06.473493 139637719117824 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_tline p e i k ? simtri b a c n o m"
I0123 20:05:06.473686 139637719117824 graph.py:498] 
I0123 20:05:06.473747 139637719117824 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_tline p e i k ? simtri b a c n o m
I0123 20:05:09.997387 139637719117824 ddar.py:60] Depth 1/1000 time = 3.4665350914001465
I0123 20:05:22.957177 139637719117824 ddar.py:60] Depth 2/1000 time = 12.95945429801941
I0123 20:05:37.698223 139637719117824 ddar.py:60] Depth 3/1000 time = 14.740779638290405
I0123 20:05:52.802290 139637719117824 ddar.py:60] Depth 4/1000 time = 15.10377025604248
I0123 20:06:07.466290 139637719117824 ddar.py:60] Depth 5/1000 time = 14.662643909454346
I0123 20:06:22.419569 139637719117824 ddar.py:60] Depth 6/1000 time = 14.8867506980896
I0123 20:06:22.419980 139637719117824 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 20:06:22.420091 139637719117824 alphageometry.py:566] LM output (score=-3.495645): "p : D f n f p 22 D g n g p 23 ;"
I0123 20:06:22.420129 139637719117824 alphageometry.py:567] Translation: "p = on_circle p f n, on_circle p g n"

I0123 20:06:22.420168 139637719117824 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_circle p f n, on_circle p g n ? simtri b a c n o m"
I0123 20:06:22.420350 139637719117824 graph.py:498] 
I0123 20:06:22.420411 139637719117824 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_circle p f n, on_circle p g n ? simtri b a c n o m
I0123 20:06:26.176505 139637719117824 ddar.py:60] Depth 1/1000 time = 3.6754817962646484
I0123 20:06:39.477807 139637719117824 ddar.py:60] Depth 2/1000 time = 13.301072359085083
I0123 20:06:54.579408 139637719117824 ddar.py:60] Depth 3/1000 time = 15.101293802261353
I0123 20:07:09.737049 139637719117824 ddar.py:60] Depth 4/1000 time = 15.157274961471558
I0123 20:07:24.917842 139637719117824 ddar.py:60] Depth 5/1000 time = 15.180519819259644
I0123 20:07:40.133282 139637719117824 ddar.py:60] Depth 6/1000 time = 15.213852882385254
I0123 20:07:55.594692 139637719117824 ddar.py:60] Depth 7/1000 time = 15.40918779373169
I0123 20:08:11.108349 139637719117824 ddar.py:60] Depth 8/1000 time = 15.480210065841675
I0123 20:08:11.110781 139637719117824 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 20:08:11.110913 139637719117824 alphageometry.py:566] LM output (score=-3.503585): "p : D f p g p 22 ^ g f g p f p f g 23 ;"
I0123 20:08:11.110965 139637719117824 alphageometry.py:567] Translation: "ERROR: Invalid predicate ^ g f g p f p f g"

I0123 20:08:11.111003 139637719117824 alphageometry.py:566] LM output (score=-3.505233): "p : T h i p s 22 ;"
I0123 20:08:11.111030 139637719117824 alphageometry.py:567] Translation: "ERROR: point s does not exist."

I0123 20:08:11.111057 139637719117824 alphageometry.py:566] LM output (score=-3.511684): "p : T f p j l 22 ;"
I0123 20:08:11.111081 139637719117824 alphageometry.py:567] Translation: "p = on_tline p f j l"

I0123 20:08:11.111108 139637719117824 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_tline p f j l ? simtri b a c n o m"
I0123 20:08:11.111286 139637719117824 graph.py:498] 
I0123 20:08:11.111346 139637719117824 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_tline p f j l ? simtri b a c n o m
I0123 20:08:14.568852 139637719117824 ddar.py:60] Depth 1/1000 time = 3.397408962249756
I0123 20:08:27.324883 139637719117824 ddar.py:60] Depth 2/1000 time = 12.755810499191284
I0123 20:08:41.324112 139637719117824 ddar.py:60] Depth 3/1000 time = 13.99889349937439
I0123 20:08:55.393530 139637719117824 ddar.py:60] Depth 4/1000 time = 14.069010496139526
I0123 20:09:09.591090 139637719117824 ddar.py:60] Depth 5/1000 time = 14.197262525558472
I0123 20:09:23.819419 139637719117824 ddar.py:60] Depth 6/1000 time = 14.226918458938599
I0123 20:09:37.649522 139637719117824 ddar.py:60] Depth 7/1000 time = 13.761982917785645
I0123 20:09:37.651648 139637719117824 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 20:09:37.651783 139637719117824 alphageometry.py:566] LM output (score=-3.544177): "p : P a p h i 22 ;"
I0123 20:09:37.651824 139637719117824 alphageometry.py:567] Translation: "p = on_pline p a h i"

I0123 20:09:37.651864 139637719117824 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_pline p a h i ? simtri b a c n o m"
I0123 20:09:37.652054 139637719117824 graph.py:498] 
I0123 20:09:37.652116 139637719117824 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_pline p a h i ? simtri b a c n o m
I0123 20:09:41.212803 139637719117824 ddar.py:60] Depth 1/1000 time = 3.4990391731262207
I0123 20:09:53.360419 139637719117824 ddar.py:60] Depth 2/1000 time = 12.147377967834473
I0123 20:10:07.977873 139637719117824 ddar.py:60] Depth 3/1000 time = 14.61717700958252
I0123 20:10:22.559491 139637719117824 ddar.py:60] Depth 4/1000 time = 14.581265211105347
I0123 20:10:37.833175 139637719117824 ddar.py:60] Depth 5/1000 time = 15.273277759552002
I0123 20:10:52.360396 139637719117824 ddar.py:60] Depth 6/1000 time = 14.525928020477295
I0123 20:11:07.301309 139637719117824 ddar.py:60] Depth 7/1000 time = 14.86858081817627
I0123 20:11:07.303629 139637719117824 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 20:11:07.303827 139637719117824 alphageometry.py:540] Depth 1. There are 29 nodes to expand:
I0123 20:11:07.303885 139637719117824 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C c d e 02 D c d d e 03 ; f : C b d f 04 D b d d f 05 ; g : C a d g 06 D a d d g 07 ; h : C a b h 08 ; i : C b c i 09 T b h h i 10 ; j : C h i j 11 ; k : C b c k 12 T b c j k 13 ; l : C a c l 14 T a c j l 15 ; m : C e h m 16 D e h h m 17 ; n : C f l n 18 D f l l n 19 ; o : C g k o 20 D g k k o 21 ? S b a c n o m {F1} x00 p : C a e p 22 D a p e p 23 ; x00
I0123 20:11:07.303922 139637719117824 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C c d e 02 D c d d e 03 ; f : C b d f 04 D b d d f 05 ; g : C a d g 06 D a d d g 07 ; h : C a b h 08 ; i : C b c i 09 T b h h i 10 ; j : C h i j 11 ; k : C b c k 12 T b c j k 13 ; l : C a c l 14 T a c j l 15 ; m : C e h m 16 D e h h m 17 ; n : C f l n 18 D f l l n 19 ; o : C g k o 20 D g k k o 21 ? S b a c n o m {F1} x00 p : C f m p 22 D f p m p 23 ; x00
I0123 20:11:07.303954 139637719117824 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C c d e 02 D c d d e 03 ; f : C b d f 04 D b d d f 05 ; g : C a d g 06 D a d d g 07 ; h : C a b h 08 ; i : C b c i 09 T b h h i 10 ; j : C h i j 11 ; k : C b c k 12 T b c j k 13 ; l : C a c l 14 T a c j l 15 ; m : C e h m 16 D e h h m 17 ; n : C f l n 18 D f l l n 19 ; o : C g k o 20 D g k k o 21 ? S b a c n o m {F1} x00 p : C f g p 22 D f p g p 23 ; x00
I0123 20:11:07.303980 139637719117824 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C c d e 02 D c d d e 03 ; f : C b d f 04 D b d d f 05 ; g : C a d g 06 D a d d g 07 ; h : C a b h 08 ; i : C b c i 09 T b h h i 10 ; j : C h i j 11 ; k : C b c k 12 T b c j k 13 ; l : C a c l 14 T a c j l 15 ; m : C e h m 16 D e h h m 17 ; n : C f l n 18 D f l l n 19 ; o : C g k o 20 D g k k o 21 ? S b a c n o m {F1} x00 p : P a b c p 22 T a b a p 23 ; x00
I0123 20:11:07.304005 139637719117824 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C c d e 02 D c d d e 03 ; f : C b d f 04 D b d d f 05 ; g : C a d g 06 D a d d g 07 ; h : C a b h 08 ; i : C b c i 09 T b h h i 10 ; j : C h i j 11 ; k : C b c k 12 T b c j k 13 ; l : C a c l 14 T a c j l 15 ; m : C e h m 16 D e h h m 17 ; n : C f l n 18 D f l l n 19 ; o : C g k o 20 D g k k o 21 ? S b a c n o m {F1} x00 p : T e f f p 22 ; x00
I0123 20:11:07.304033 139637719117824 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C c d e 02 D c d d e 03 ; f : C b d f 04 D b d d f 05 ; g : C a d g 06 D a d d g 07 ; h : C a b h 08 ; i : C b c i 09 T b h h i 10 ; j : C h i j 11 ; k : C b c k 12 T b c j k 13 ; l : C a c l 14 T a c j l 15 ; m : C e h m 16 D e h h m 17 ; n : C f l n 18 D f l l n 19 ; o : C g k o 20 D g k k o 21 ? S b a c n o m {F1} x00 p : P f p h i 22 ; x00
I0123 20:11:07.304057 139637719117824 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C c d e 02 D c d d e 03 ; f : C b d f 04 D b d d f 05 ; g : C a d g 06 D a d d g 07 ; h : C a b h 08 ; i : C b c i 09 T b h h i 10 ; j : C h i j 11 ; k : C b c k 12 T b c j k 13 ; l : C a c l 14 T a c j l 15 ; m : C e h m 16 D e h h m 17 ; n : C f l n 18 D f l l n 19 ; o : C g k o 20 D g k k o 21 ? S b a c n o m {F1} x00 p : C b f p 22 D b f b p 23 ; x00
I0123 20:11:07.304081 139637719117824 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C c d e 02 D c d d e 03 ; f : C b d f 04 D b d d f 05 ; g : C a d g 06 D a d d g 07 ; h : C a b h 08 ; i : C b c i 09 T b h h i 10 ; j : C h i j 11 ; k : C b c k 12 T b c j k 13 ; l : C a c l 14 T a c j l 15 ; m : C e h m 16 D e h h m 17 ; n : C f l n 18 D f l l n 19 ; o : C g k o 20 D g k k o 21 ? S b a c n o m {F1} x00 p : T f p h i 22 ; x00
I0123 20:11:07.304104 139637719117824 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C c d e 02 D c d d e 03 ; f : C b d f 04 D b d d f 05 ; g : C a d g 06 D a d d g 07 ; h : C a b h 08 ; i : C b c i 09 T b h h i 10 ; j : C h i j 11 ; k : C b c k 12 T b c j k 13 ; l : C a c l 14 T a c j l 15 ; m : C e h m 16 D e h h m 17 ; n : C f l n 18 D f l l n 19 ; o : C g k o 20 D g k k o 21 ? S b a c n o m {F1} x00 p : D d f d p 22 ; x00
I0123 20:11:07.304131 139637719117824 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C c d e 02 D c d d e 03 ; f : C b d f 04 D b d d f 05 ; g : C a d g 06 D a d d g 07 ; h : C a b h 08 ; i : C b c i 09 T b h h i 10 ; j : C h i j 11 ; k : C b c k 12 T b c j k 13 ; l : C a c l 14 T a c j l 15 ; m : C e h m 16 D e h h m 17 ; n : C f l n 18 D f l l n 19 ; o : C g k o 20 D g k k o 21 ? S b a c n o m {F1} x00 p : C k l p 22 D k p l p 23 ; x00
I0123 20:11:07.304156 139637719117824 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C c d e 02 D c d d e 03 ; f : C b d f 04 D b d d f 05 ; g : C a d g 06 D a d d g 07 ; h : C a b h 08 ; i : C b c i 09 T b h h i 10 ; j : C h i j 11 ; k : C b c k 12 T b c j k 13 ; l : C a c l 14 T a c j l 15 ; m : C e h m 16 D e h h m 17 ; n : C f l n 18 D f l l n 19 ; o : C g k o 20 D g k k o 21 ? S b a c n o m {F1} x00 p : D d e d p 22 ; x00
I0123 20:11:07.304179 139637719117824 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C c d e 02 D c d d e 03 ; f : C b d f 04 D b d d f 05 ; g : C a d g 06 D a d d g 07 ; h : C a b h 08 ; i : C b c i 09 T b h h i 10 ; j : C h i j 11 ; k : C b c k 12 T b c j k 13 ; l : C a c l 14 T a c j l 15 ; m : C e h m 16 D e h h m 17 ; n : C f l n 18 D f l l n 19 ; o : C g k o 20 D g k k o 21 ? S b a c n o m {F1} x00 p : T h i i p 22 ; x00
I0123 20:11:07.304204 139637719117824 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C c d e 02 D c d d e 03 ; f : C b d f 04 D b d d f 05 ; g : C a d g 06 D a d d g 07 ; h : C a b h 08 ; i : C b c i 09 T b h h i 10 ; j : C h i j 11 ; k : C b c k 12 T b c j k 13 ; l : C a c l 14 T a c j l 15 ; m : C e h m 16 D e h h m 17 ; n : C f l n 18 D f l l n 19 ; o : C g k o 20 D g k k o 21 ? S b a c n o m {F1} x00 p : P f p j k 22 ; x00
I0123 20:11:07.304228 139637719117824 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C c d e 02 D c d d e 03 ; f : C b d f 04 D b d d f 05 ; g : C a d g 06 D a d d g 07 ; h : C a b h 08 ; i : C b c i 09 T b h h i 10 ; j : C h i j 11 ; k : C b c k 12 T b c j k 13 ; l : C a c l 14 T a c j l 15 ; m : C e h m 16 D e h h m 17 ; n : C f l n 18 D f l l n 19 ; o : C g k o 20 D g k k o 21 ? S b a c n o m {F1} x00 p : C k l p 22 D k l l p 23 ; x00
I0123 20:11:07.304250 139637719117824 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C c d e 02 D c d d e 03 ; f : C b d f 04 D b d d f 05 ; g : C a d g 06 D a d d g 07 ; h : C a b h 08 ; i : C b c i 09 T b h h i 10 ; j : C h i j 11 ; k : C b c k 12 T b c j k 13 ; l : C a c l 14 T a c j l 15 ; m : C e h m 16 D e h h m 17 ; n : C f l n 18 D f l l n 19 ; o : C g k o 20 D g k k o 21 ? S b a c n o m {F1} x00 p : T e p h i 22 ; x00
I0123 20:11:07.304273 139637719117824 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C c d e 02 D c d d e 03 ; f : C b d f 04 D b d d f 05 ; g : C a d g 06 D a d d g 07 ; h : C a b h 08 ; i : C b c i 09 T b h h i 10 ; j : C h i j 11 ; k : C b c k 12 T b c j k 13 ; l : C a c l 14 T a c j l 15 ; m : C e h m 16 D e h h m 17 ; n : C f l n 18 D f l l n 19 ; o : C g k o 20 D g k k o 21 ? S b a c n o m {F1} x00 p : D a d d p 22 ; x00
I0123 20:11:07.304297 139637719117824 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C c d e 02 D c d d e 03 ; f : C b d f 04 D b d d f 05 ; g : C a d g 06 D a d d g 07 ; h : C a b h 08 ; i : C b c i 09 T b h h i 10 ; j : C h i j 11 ; k : C b c k 12 T b c j k 13 ; l : C a c l 14 T a c j l 15 ; m : C e h m 16 D e h h m 17 ; n : C f l n 18 D f l l n 19 ; o : C g k o 20 D g k k o 21 ? S b a c n o m {F1} x00 p : C b f p 22 D b f f p 23 ; x00
I0123 20:11:07.304320 139637719117824 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C c d e 02 D c d d e 03 ; f : C b d f 04 D b d d f 05 ; g : C a d g 06 D a d d g 07 ; h : C a b h 08 ; i : C b c i 09 T b h h i 10 ; j : C h i j 11 ; k : C b c k 12 T b c j k 13 ; l : C a c l 14 T a c j l 15 ; m : C e h m 16 D e h h m 17 ; n : C f l n 18 D f l l n 19 ; o : C g k o 20 D g k k o 21 ? S b a c n o m {F1} x00 p : D f m f p 22 D g m g p 23 ; x00
I0123 20:11:07.304346 139637719117824 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C c d e 02 D c d d e 03 ; f : C b d f 04 D b d d f 05 ; g : C a d g 06 D a d d g 07 ; h : C a b h 08 ; i : C b c i 09 T b h h i 10 ; j : C h i j 11 ; k : C b c k 12 T b c j k 13 ; l : C a c l 14 T a c j l 15 ; m : C e h m 16 D e h h m 17 ; n : C f l n 18 D f l l n 19 ; o : C g k o 20 D g k k o 21 ? S b a c n o m {F1} x00 p : D f m f p 22 D m n n p 23 ; x00
I0123 20:11:07.304370 139637719117824 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C c d e 02 D c d d e 03 ; f : C b d f 04 D b d d f 05 ; g : C a d g 06 D a d d g 07 ; h : C a b h 08 ; i : C b c i 09 T b h h i 10 ; j : C h i j 11 ; k : C b c k 12 T b c j k 13 ; l : C a c l 14 T a c j l 15 ; m : C e h m 16 D e h h m 17 ; n : C f l n 18 D f l l n 19 ; o : C g k o 20 D g k k o 21 ? S b a c n o m {F1} x00 p : T i k k p 22 ; x00
I0123 20:11:07.304392 139637719117824 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C c d e 02 D c d d e 03 ; f : C b d f 04 D b d d f 05 ; g : C a d g 06 D a d d g 07 ; h : C a b h 08 ; i : C b c i 09 T b h h i 10 ; j : C h i j 11 ; k : C b c k 12 T b c j k 13 ; l : C a c l 14 T a c j l 15 ; m : C e h m 16 D e h h m 17 ; n : C f l n 18 D f l l n 19 ; o : C g k o 20 D g k k o 21 ? S b a c n o m {F1} x00 p : P i k l p 22 ; x00
I0123 20:11:07.304414 139637719117824 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C c d e 02 D c d d e 03 ; f : C b d f 04 D b d d f 05 ; g : C a d g 06 D a d d g 07 ; h : C a b h 08 ; i : C b c i 09 T b h h i 10 ; j : C h i j 11 ; k : C b c k 12 T b c j k 13 ; l : C a c l 14 T a c j l 15 ; m : C e h m 16 D e h h m 17 ; n : C f l n 18 D f l l n 19 ; o : C g k o 20 D g k k o 21 ? S b a c n o m {F1} x00 p : P a b c p 22 ; x00
I0123 20:11:07.304436 139637719117824 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C c d e 02 D c d d e 03 ; f : C b d f 04 D b d d f 05 ; g : C a d g 06 D a d d g 07 ; h : C a b h 08 ; i : C b c i 09 T b h h i 10 ; j : C h i j 11 ; k : C b c k 12 T b c j k 13 ; l : C a c l 14 T a c j l 15 ; m : C e h m 16 D e h h m 17 ; n : C f l n 18 D f l l n 19 ; o : C g k o 20 D g k k o 21 ? S b a c n o m {F1} x00 p : T f p i k 22 ; x00
I0123 20:11:07.304458 139637719117824 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C c d e 02 D c d d e 03 ; f : C b d f 04 D b d d f 05 ; g : C a d g 06 D a d d g 07 ; h : C a b h 08 ; i : C b c i 09 T b h h i 10 ; j : C h i j 11 ; k : C b c k 12 T b c j k 13 ; l : C a c l 14 T a c j l 15 ; m : C e h m 16 D e h h m 17 ; n : C f l n 18 D f l l n 19 ; o : C g k o 20 D g k k o 21 ? S b a c n o m {F1} x00 p : P f p j l 22 ; x00
I0123 20:11:07.304481 139637719117824 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C c d e 02 D c d d e 03 ; f : C b d f 04 D b d d f 05 ; g : C a d g 06 D a d d g 07 ; h : C a b h 08 ; i : C b c i 09 T b h h i 10 ; j : C h i j 11 ; k : C b c k 12 T b c j k 13 ; l : C a c l 14 T a c j l 15 ; m : C e h m 16 D e h h m 17 ; n : C f l n 18 D f l l n 19 ; o : C g k o 20 D g k k o 21 ? S b a c n o m {F1} x00 p : D f m f p 22 T f m f p 23 ; x00
I0123 20:11:07.304503 139637719117824 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C c d e 02 D c d d e 03 ; f : C b d f 04 D b d d f 05 ; g : C a d g 06 D a d d g 07 ; h : C a b h 08 ; i : C b c i 09 T b h h i 10 ; j : C h i j 11 ; k : C b c k 12 T b c j k 13 ; l : C a c l 14 T a c j l 15 ; m : C e h m 16 D e h h m 17 ; n : C f l n 18 D f l l n 19 ; o : C g k o 20 D g k k o 21 ? S b a c n o m {F1} x00 p : T e p i k 22 ; x00
I0123 20:11:07.304525 139637719117824 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C c d e 02 D c d d e 03 ; f : C b d f 04 D b d d f 05 ; g : C a d g 06 D a d d g 07 ; h : C a b h 08 ; i : C b c i 09 T b h h i 10 ; j : C h i j 11 ; k : C b c k 12 T b c j k 13 ; l : C a c l 14 T a c j l 15 ; m : C e h m 16 D e h h m 17 ; n : C f l n 18 D f l l n 19 ; o : C g k o 20 D g k k o 21 ? S b a c n o m {F1} x00 p : D f n f p 22 D g n g p 23 ; x00
I0123 20:11:07.304551 139637719117824 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C c d e 02 D c d d e 03 ; f : C b d f 04 D b d d f 05 ; g : C a d g 06 D a d d g 07 ; h : C a b h 08 ; i : C b c i 09 T b h h i 10 ; j : C h i j 11 ; k : C b c k 12 T b c j k 13 ; l : C a c l 14 T a c j l 15 ; m : C e h m 16 D e h h m 17 ; n : C f l n 18 D f l l n 19 ; o : C g k o 20 D g k k o 21 ? S b a c n o m {F1} x00 p : T f p j l 22 ; x00
I0123 20:11:07.304574 139637719117824 alphageometry.py:544] {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C c d e 02 D c d d e 03 ; f : C b d f 04 D b d d f 05 ; g : C a d g 06 D a d d g 07 ; h : C a b h 08 ; i : C b c i 09 T b h h i 10 ; j : C h i j 11 ; k : C b c k 12 T b c j k 13 ; l : C a c l 14 T a c j l 15 ; m : C e h m 16 D e h h m 17 ; n : C f l n 18 D f l l n 19 ; o : C g k o 20 D g k k o 21 ? S b a c n o m {F1} x00 p : P a p h i 22 ; x00
I0123 20:11:07.304606 139637719117824 alphageometry.py:549] Decoding from {S} a : ; b : ; c : ; d : D a d b d 00 D b d c d 01 ; e : C c d e 02 D c d d e 03 ; f : C b d f 04 D b d d f 05 ; g : C a d g 06 D a d d g 07 ; h : C a b h 08 ; i : C b c i 09 T b h h i 10 ; j : C h i j 11 ; k : C b c k 12 T b c j k 13 ; l : C a c l 14 T a c j l 15 ; m : C e h m 16 D e h h m 17 ; n : C f l n 18 D f l l n 19 ; o : C g k o 20 D g k k o 21 ? S b a c n o m {F1} x00 p : C a e p 22 D a p e p 23 ; x00
I0123 20:11:13.240345 139637719117824 alphageometry.py:566] LM output (score=-2.214270): "q : T e p m q 24 ;"
I0123 20:11:13.240499 139637719117824 alphageometry.py:567] Translation: "q = on_tline q m e p"

I0123 20:11:13.240545 139637719117824 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_line p a e, on_bline p e a; q = on_tline q m e p ? simtri b a c n o m"
I0123 20:11:13.240743 139637719117824 graph.py:498] 
I0123 20:11:13.240811 139637719117824 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_line p a e, on_bline p e a; q = on_tline q m e p ? simtri b a c n o m
I0123 20:11:17.772283 139637719117824 ddar.py:60] Depth 1/1000 time = 4.459682464599609
I0123 20:11:34.392064 139637719117824 ddar.py:60] Depth 2/1000 time = 16.619497537612915
I0123 20:11:52.994136 139637719117824 ddar.py:60] Depth 3/1000 time = 18.601640939712524
I0123 20:12:12.371829 139637719117824 ddar.py:60] Depth 4/1000 time = 19.3773672580719
I0123 20:12:31.196238 139637719117824 ddar.py:60] Depth 5/1000 time = 18.823939561843872
I0123 20:12:50.527614 139637719117824 ddar.py:60] Depth 6/1000 time = 19.329707860946655
I0123 20:13:09.625961 139637719117824 ddar.py:60] Depth 7/1000 time = 18.986856698989868
I0123 20:13:09.626682 139637719117824 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 20:13:09.626781 139637719117824 alphageometry.py:566] LM output (score=-2.301364): "q : T h i i q 24 ;"
I0123 20:13:09.626817 139637719117824 alphageometry.py:567] Translation: "ERROR: Traceback (most recent call last):
  File "/home/chi/alphageometry-test/alphageometry.py", line 438, in try_translate_constrained_to_construct
    g.copy().add_clause(clause, 0, DEFINITIONS)
  File "/home/chi/alphageometry-test/graph.py", line 2622, in add_clause
    nums = draw_fn()
  File "/home/chi/alphageometry-test/graph.py", line 2608, in draw_fn
    return nm.reduce(to_be_intersected, existing_points)
  File "/home/chi/alphageometry-test/numericals.py", line 1306, in reduce
    return objs[0].sample_within(existing_points)
  File "/home/chi/alphageometry-test/numericals.py", line 299, in sample_within
    a, b = line_circle_intersection(self, Circle(center.foot(self), radius))
  File "/home/chi/alphageometry-test/numericals.py", line 544, in line_circle_intersection
    raise InvalidQuadSolveError()
numericals.InvalidQuadSolveError
"

I0123 20:13:09.628041 139637719117824 alphageometry.py:566] LM output (score=-2.390293): "q : C f h q 24 D f q h q 25 ;"
I0123 20:13:09.628073 139637719117824 alphageometry.py:567] Translation: "q = on_line q f h, on_bline q h f"

I0123 20:13:09.628111 139637719117824 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_line p a e, on_bline p e a; q = on_line q f h, on_bline q h f ? simtri b a c n o m"
I0123 20:13:09.628341 139637719117824 graph.py:498] 
I0123 20:13:09.628406 139637719117824 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_line p a e, on_bline p e a; q = on_line q f h, on_bline q h f ? simtri b a c n o m
I0123 20:13:14.289904 139637719117824 ddar.py:60] Depth 1/1000 time = 4.5797905921936035
I0123 20:13:31.441151 139637719117824 ddar.py:60] Depth 2/1000 time = 17.15096616744995
I0123 20:13:52.706192 139637719117824 ddar.py:60] Depth 3/1000 time = 21.26454496383667
I0123 20:14:14.511946 139637719117824 ddar.py:60] Depth 4/1000 time = 21.805278301239014
I0123 20:14:36.857948 139637719117824 ddar.py:60] Depth 5/1000 time = 22.34557271003723
I0123 20:14:58.994180 139637719117824 ddar.py:60] Depth 6/1000 time = 22.1344473361969
I0123 20:15:20.964737 139637719117824 ddar.py:60] Depth 7/1000 time = 21.79946208000183
I0123 20:15:20.965374 139637719117824 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 20:15:20.965467 139637719117824 alphageometry.py:566] LM output (score=-2.475484): "q : T f q h i 24 ;"
I0123 20:15:20.965503 139637719117824 alphageometry.py:567] Translation: "ERROR: Traceback (most recent call last):
  File "/home/chi/alphageometry-test/alphageometry.py", line 438, in try_translate_constrained_to_construct
    g.copy().add_clause(clause, 0, DEFINITIONS)
  File "/home/chi/alphageometry-test/graph.py", line 2622, in add_clause
    nums = draw_fn()
  File "/home/chi/alphageometry-test/graph.py", line 2608, in draw_fn
    return nm.reduce(to_be_intersected, existing_points)
  File "/home/chi/alphageometry-test/numericals.py", line 1306, in reduce
    return objs[0].sample_within(existing_points)
  File "/home/chi/alphageometry-test/numericals.py", line 299, in sample_within
    a, b = line_circle_intersection(self, Circle(center.foot(self), radius))
  File "/home/chi/alphageometry-test/numericals.py", line 544, in line_circle_intersection
    raise InvalidQuadSolveError()
numericals.InvalidQuadSolveError
"

I0123 20:15:20.966029 139637719117824 alphageometry.py:566] LM output (score=-2.502189): "q : C f p q 24 D f q p q 25 ;"
I0123 20:15:20.966062 139637719117824 alphageometry.py:567] Translation: "q = on_line q f p, on_bline q p f"

I0123 20:15:20.966098 139637719117824 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_line p a e, on_bline p e a; q = on_line q f p, on_bline q p f ? simtri b a c n o m"
I0123 20:15:20.966338 139637719117824 graph.py:498] 
I0123 20:15:20.966401 139637719117824 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_line p a e, on_bline p e a; q = on_line q f p, on_bline q p f ? simtri b a c n o m
I0123 20:15:25.568216 139637719117824 ddar.py:60] Depth 1/1000 time = 4.518549919128418
I0123 20:15:41.397985 139637719117824 ddar.py:60] Depth 2/1000 time = 15.829558372497559
I0123 20:16:00.897465 139637719117824 ddar.py:60] Depth 3/1000 time = 19.49921703338623
I0123 20:16:20.144819 139637719117824 ddar.py:60] Depth 4/1000 time = 19.24698519706726
I0123 20:16:40.247554 139637719117824 ddar.py:60] Depth 5/1000 time = 20.1022686958313
I0123 20:16:59.508242 139637719117824 ddar.py:60] Depth 6/1000 time = 19.25910472869873
I0123 20:17:19.571805 139637719117824 ddar.py:60] Depth 7/1000 time = 19.930307626724243
I0123 20:17:19.575841 139637719117824 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 20:17:19.575927 139637719117824 alphageometry.py:566] LM output (score=-2.502948): "q : T e p e q 24 ;"
I0123 20:17:19.575965 139637719117824 alphageometry.py:567] Translation: "q = on_tline q e e p"

I0123 20:17:19.576006 139637719117824 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_line p a e, on_bline p e a; q = on_tline q e e p ? simtri b a c n o m"
I0123 20:17:19.576203 139637719117824 graph.py:498] 
I0123 20:17:19.576268 139637719117824 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_line p a e, on_bline p e a; q = on_tline q e e p ? simtri b a c n o m
I0123 20:17:24.150946 139637719117824 ddar.py:60] Depth 1/1000 time = 4.503632545471191
I0123 20:17:40.716559 139637719117824 ddar.py:60] Depth 2/1000 time = 16.565378427505493
I0123 20:18:00.060242 139637719117824 ddar.py:60] Depth 3/1000 time = 19.34341311454773
I0123 20:18:19.957568 139637719117824 ddar.py:60] Depth 4/1000 time = 19.896989345550537
I0123 20:18:39.158022 139637719117824 ddar.py:60] Depth 5/1000 time = 19.20010542869568
I0123 20:18:58.196827 139637719117824 ddar.py:60] Depth 6/1000 time = 19.03736186027527
I0123 20:19:18.847593 139637719117824 ddar.py:60] Depth 7/1000 time = 20.545542240142822
I0123 20:19:18.851235 139637719117824 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 20:19:18.851324 139637719117824 alphageometry.py:566] LM output (score=-2.531825): "q : C a p q 24 D a q p q 25 ;"
I0123 20:19:18.851361 139637719117824 alphageometry.py:567] Translation: "q = on_line q a p, on_bline q p a"

I0123 20:19:18.851413 139637719117824 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_line p a e, on_bline p e a; q = on_line q a p, on_bline q p a ? simtri b a c n o m"
I0123 20:19:18.851652 139637719117824 graph.py:498] 
I0123 20:19:18.851716 139637719117824 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_line p a e, on_bline p e a; q = on_line q a p, on_bline q p a ? simtri b a c n o m
I0123 20:19:23.255362 139637719117824 ddar.py:60] Depth 1/1000 time = 4.313850402832031
I0123 20:19:41.623977 139637719117824 ddar.py:60] Depth 2/1000 time = 18.368353605270386
I0123 20:20:02.364080 139637719117824 ddar.py:60] Depth 3/1000 time = 20.739754676818848
I0123 20:20:22.786742 139637719117824 ddar.py:60] Depth 4/1000 time = 20.42228412628174
I0123 20:20:43.185832 139637719117824 ddar.py:60] Depth 5/1000 time = 20.3986074924469
I0123 20:21:04.187319 139637719117824 ddar.py:60] Depth 6/1000 time = 20.99851632118225
I0123 20:21:25.159013 139637719117824 ddar.py:60] Depth 7/1000 time = 20.85059952735901
I0123 20:21:25.162845 139637719117824 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 20:21:25.162913 139637719117824 alphageometry.py:566] LM output (score=-2.578386): "q : C f i q 24 D f q i q 25 ;"
I0123 20:21:25.162948 139637719117824 alphageometry.py:567] Translation: "q = on_line q f i, on_bline q i f"

I0123 20:21:25.162986 139637719117824 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_line p a e, on_bline p e a; q = on_line q f i, on_bline q i f ? simtri b a c n o m"
I0123 20:21:25.163184 139637719117824 graph.py:498] 
I0123 20:21:25.163250 139637719117824 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_line p a e, on_bline p e a; q = on_line q f i, on_bline q i f ? simtri b a c n o m
I0123 20:21:29.581370 139637719117824 ddar.py:60] Depth 1/1000 time = 4.333649396896362
I0123 20:21:46.595103 139637719117824 ddar.py:60] Depth 2/1000 time = 17.013324737548828
I0123 20:22:07.351174 139637719117824 ddar.py:60] Depth 3/1000 time = 20.755549907684326
I0123 20:22:29.171754 139637719117824 ddar.py:60] Depth 4/1000 time = 21.820080518722534
I0123 20:22:50.120562 139637719117824 ddar.py:60] Depth 5/1000 time = 20.948339700698853
I0123 20:23:11.773349 139637719117824 ddar.py:60] Depth 6/1000 time = 21.651179790496826
I0123 20:23:32.965080 139637719117824 ddar.py:60] Depth 7/1000 time = 21.0154230594635
I0123 20:23:32.965419 139637719117824 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 20:23:32.965479 139637719117824 alphageometry.py:566] LM output (score=-2.583643): "q : C d e q 24 D d q e q 25 ;"
I0123 20:23:32.965513 139637719117824 alphageometry.py:567] Translation: "q = on_line q d e, on_bline q e d"

I0123 20:23:32.965557 139637719117824 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_line p a e, on_bline p e a; q = on_line q d e, on_bline q e d ? simtri b a c n o m"
I0123 20:23:32.965769 139637719117824 graph.py:498] 
I0123 20:23:32.965838 139637719117824 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_line p a e, on_bline p e a; q = on_line q d e, on_bline q e d ? simtri b a c n o m
I0123 20:23:38.110853 139637719117824 ddar.py:60] Depth 1/1000 time = 5.058557033538818
I0123 20:23:55.876742 139637719117824 ddar.py:60] Depth 2/1000 time = 17.76556706428528
I0123 20:24:16.434100 139637719117824 ddar.py:60] Depth 3/1000 time = 20.556995630264282
I0123 20:24:37.654719 139637719117824 ddar.py:60] Depth 4/1000 time = 21.2201509475708
I0123 20:24:58.216778 139637719117824 ddar.py:60] Depth 5/1000 time = 20.561668872833252
I0123 20:25:19.096255 139637719117824 ddar.py:60] Depth 6/1000 time = 20.876271724700928
I0123 20:25:39.302595 139637719117824 ddar.py:60] Depth 7/1000 time = 20.07042360305786
I0123 20:25:39.303110 139637719117824 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 20:25:39.303200 139637719117824 alphageometry.py:566] LM output (score=-2.620992): "q : C f m q 24 D f q m q 25 ;"
I0123 20:25:39.303237 139637719117824 alphageometry.py:567] Translation: "q = on_line q f m, on_bline q m f"

I0123 20:25:39.303292 139637719117824 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_line p a e, on_bline p e a; q = on_line q f m, on_bline q m f ? simtri b a c n o m"
I0123 20:25:39.303527 139637719117824 graph.py:498] 
I0123 20:25:39.303591 139637719117824 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_line p a e, on_bline p e a; q = on_line q f m, on_bline q m f ? simtri b a c n o m
I0123 20:25:44.667653 139637719117824 ddar.py:60] Depth 1/1000 time = 4.625366449356079
I0123 20:26:00.680367 139637719117824 ddar.py:60] Depth 2/1000 time = 16.012479066848755
I0123 20:26:20.453868 139637719117824 ddar.py:60] Depth 3/1000 time = 19.77317214012146
I0123 20:26:40.555901 139637719117824 ddar.py:60] Depth 4/1000 time = 20.1016628742218
I0123 20:27:01.268435 139637719117824 ddar.py:60] Depth 5/1000 time = 20.712196588516235
I0123 20:27:21.530600 139637719117824 ddar.py:60] Depth 6/1000 time = 20.260445833206177
I0123 20:27:42.119200 139637719117824 ddar.py:60] Depth 7/1000 time = 20.432343244552612
I0123 20:27:42.119807 139637719117824 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 20:27:42.119898 139637719117824 alphageometry.py:566] LM output (score=-2.624590): "q : C m l q 24 D m q l q 25 ;"
I0123 20:27:42.119934 139637719117824 alphageometry.py:567] Translation: "q = on_line q m l, on_bline q l m"

I0123 20:27:42.119987 139637719117824 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_line p a e, on_bline p e a; q = on_line q m l, on_bline q l m ? simtri b a c n o m"
I0123 20:27:42.120217 139637719117824 graph.py:498] 
I0123 20:27:42.120282 139637719117824 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_line p a e, on_bline p e a; q = on_line q m l, on_bline q l m ? simtri b a c n o m
I0123 20:27:46.730635 139637719117824 ddar.py:60] Depth 1/1000 time = 4.528081655502319
I0123 20:28:02.862619 139637719117824 ddar.py:60] Depth 2/1000 time = 16.13169527053833
I0123 20:28:21.634933 139637719117824 ddar.py:60] Depth 3/1000 time = 18.771851778030396
I0123 20:28:39.903851 139637719117824 ddar.py:60] Depth 4/1000 time = 18.268399238586426
I0123 20:28:59.417869 139637719117824 ddar.py:60] Depth 5/1000 time = 19.513532638549805
I0123 20:29:18.308226 139637719117824 ddar.py:60] Depth 6/1000 time = 18.888777017593384
I0123 20:29:37.706423 139637719117824 ddar.py:60] Depth 7/1000 time = 19.276060342788696
I0123 20:29:37.710561 139637719117824 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 20:29:37.710684 139637719117824 alphageometry.py:566] LM output (score=-2.663153): "q : ^ d a d q d q d f 24 ^ f a f q f q f d 25 ;"
I0123 20:29:37.710725 139637719117824 alphageometry.py:567] Translation: "ERROR: Invalid predicate ^ d a d q d q d f"

I0123 20:29:37.710780 139637719117824 alphageometry.py:566] LM output (score=-2.721222): "q : T e p l q 24 ;"
I0123 20:29:37.710809 139637719117824 alphageometry.py:567] Translation: "q = on_tline q l e p"

I0123 20:29:37.710844 139637719117824 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_line p a e, on_bline p e a; q = on_tline q l e p ? simtri b a c n o m"
I0123 20:29:37.711070 139637719117824 graph.py:498] 
I0123 20:29:37.711135 139637719117824 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_line p a e, on_bline p e a; q = on_tline q l e p ? simtri b a c n o m
I0123 20:29:42.347189 139637719117824 ddar.py:60] Depth 1/1000 time = 4.5610034465789795
I0123 20:29:58.606775 139637719117824 ddar.py:60] Depth 2/1000 time = 16.259371995925903
I0123 20:30:19.240346 139637719117824 ddar.py:60] Depth 3/1000 time = 20.633257627487183
I0123 20:30:39.980442 139637719117824 ddar.py:60] Depth 4/1000 time = 20.73975110054016
I0123 20:30:59.994667 139637719117824 ddar.py:60] Depth 5/1000 time = 20.013904094696045
I0123 20:31:21.451252 139637719117824 ddar.py:60] Depth 6/1000 time = 21.455028772354126
I0123 20:31:42.010149 139637719117824 ddar.py:60] Depth 7/1000 time = 20.44546890258789
I0123 20:31:42.013653 139637719117824 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 20:31:42.013745 139637719117824 alphageometry.py:566] LM output (score=-2.786265): "q : T f q j k 24 ;"
I0123 20:31:42.013783 139637719117824 alphageometry.py:567] Translation: "q = on_tline q f j k"

I0123 20:31:42.013825 139637719117824 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_line p a e, on_bline p e a; q = on_tline q f j k ? simtri b a c n o m"
I0123 20:31:42.014024 139637719117824 graph.py:498] 
I0123 20:31:42.014091 139637719117824 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_line p a e, on_bline p e a; q = on_tline q f j k ? simtri b a c n o m
I0123 20:31:46.194411 139637719117824 ddar.py:60] Depth 1/1000 time = 4.102743625640869
I0123 20:32:02.217540 139637719117824 ddar.py:60] Depth 2/1000 time = 16.022833108901978
I0123 20:32:21.574682 139637719117824 ddar.py:60] Depth 3/1000 time = 19.356730461120605
I0123 20:32:40.440286 139637719117824 ddar.py:60] Depth 4/1000 time = 18.865179300308228
I0123 20:32:59.391378 139637719117824 ddar.py:60] Depth 5/1000 time = 18.950567722320557
I0123 20:33:18.270058 139637719117824 ddar.py:60] Depth 6/1000 time = 18.876999616622925
I0123 20:33:37.481321 139637719117824 ddar.py:60] Depth 7/1000 time = 19.10922622680664
I0123 20:33:37.484568 139637719117824 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 20:33:37.484677 139637719117824 alphageometry.py:566] LM output (score=-2.810111): "q : T d p e q 24 ;"
I0123 20:33:37.484715 139637719117824 alphageometry.py:567] Translation: "q = on_tline q e d p"

I0123 20:33:37.484771 139637719117824 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_line p a e, on_bline p e a; q = on_tline q e d p ? simtri b a c n o m"
I0123 20:33:37.484991 139637719117824 graph.py:498] 
I0123 20:33:37.485055 139637719117824 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_line p a e, on_bline p e a; q = on_tline q e d p ? simtri b a c n o m
I0123 20:33:41.714656 139637719117824 ddar.py:60] Depth 1/1000 time = 4.152796030044556
I0123 20:33:57.615912 139637719117824 ddar.py:60] Depth 2/1000 time = 15.900960206985474
I0123 20:34:18.464396 139637719117824 ddar.py:60] Depth 3/1000 time = 20.848024129867554
I0123 20:34:38.907080 139637719117824 ddar.py:60] Depth 4/1000 time = 20.44236660003662
I0123 20:34:58.590161 139637719117824 ddar.py:60] Depth 5/1000 time = 19.682772397994995
I0123 20:35:19.559754 139637719117824 ddar.py:60] Depth 6/1000 time = 20.968208074569702
I0123 20:35:40.385323 139637719117824 ddar.py:60] Depth 7/1000 time = 20.717575550079346
I0123 20:35:40.388710 139637719117824 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 20:35:40.388784 139637719117824 alphageometry.py:566] LM output (score=-2.812548): "q : T f q j l 24 ;"
I0123 20:35:40.388820 139637719117824 alphageometry.py:567] Translation: "q = on_tline q f j l"

I0123 20:35:40.388867 139637719117824 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_line p a e, on_bline p e a; q = on_tline q f j l ? simtri b a c n o m"
I0123 20:35:40.389072 139637719117824 graph.py:498] 
I0123 20:35:40.389135 139637719117824 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_line p a e, on_bline p e a; q = on_tline q f j l ? simtri b a c n o m
I0123 20:35:44.152713 139637719117824 ddar.py:60] Depth 1/1000 time = 3.6860110759735107
I0123 20:36:00.234084 139637719117824 ddar.py:60] Depth 2/1000 time = 16.08098530769348
I0123 20:36:17.883758 139637719117824 ddar.py:60] Depth 3/1000 time = 17.64926505088806
I0123 20:36:36.325772 139637719117824 ddar.py:60] Depth 4/1000 time = 18.441606044769287
I0123 20:36:54.807101 139637719117824 ddar.py:60] Depth 5/1000 time = 18.48099374771118
I0123 20:37:13.349666 139637719117824 ddar.py:60] Depth 6/1000 time = 18.541085720062256
I0123 20:37:32.414029 139637719117824 ddar.py:60] Depth 7/1000 time = 18.95327401161194
I0123 20:37:32.417841 139637719117824 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 20:37:32.417949 139637719117824 alphageometry.py:566] LM output (score=-2.928767): "q : T e p i q 24 ;"
I0123 20:37:32.417988 139637719117824 alphageometry.py:567] Translation: "q = on_tline q i e p"

I0123 20:37:32.418046 139637719117824 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_line p a e, on_bline p e a; q = on_tline q i e p ? simtri b a c n o m"
I0123 20:37:32.418287 139637719117824 graph.py:498] 
I0123 20:37:32.418354 139637719117824 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_line p a e, on_bline p e a; q = on_tline q i e p ? simtri b a c n o m
I0123 20:37:36.564212 139637719117824 ddar.py:60] Depth 1/1000 time = 4.070997476577759
I0123 20:37:53.148619 139637719117824 ddar.py:60] Depth 2/1000 time = 16.58410358428955
I0123 20:38:12.085154 139637719117824 ddar.py:60] Depth 3/1000 time = 18.936052322387695
I0123 20:38:31.979527 139637719117824 ddar.py:60] Depth 4/1000 time = 19.8938889503479
I0123 20:38:51.047910 139637719117824 ddar.py:60] Depth 5/1000 time = 19.068068742752075
I0123 20:39:10.192552 139637719117824 ddar.py:60] Depth 6/1000 time = 19.143116235733032
I0123 20:39:29.930391 139637719117824 ddar.py:60] Depth 7/1000 time = 19.62873125076294
I0123 20:39:29.930982 139637719117824 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 20:39:29.931051 139637719117824 alphageometry.py:566] LM output (score=-2.936729): "q : T e f f q 24 ;"
I0123 20:39:29.931088 139637719117824 alphageometry.py:567] Translation: "q = on_tline q f e f"

I0123 20:39:29.931137 139637719117824 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_line p a e, on_bline p e a; q = on_tline q f e f ? simtri b a c n o m"
I0123 20:39:29.931342 139637719117824 graph.py:498] 
I0123 20:39:29.931407 139637719117824 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_line p a e, on_bline p e a; q = on_tline q f e f ? simtri b a c n o m
I0123 20:39:33.556930 139637719117824 ddar.py:60] Depth 1/1000 time = 3.5542774200439453
I0123 20:39:49.804912 139637719117824 ddar.py:60] Depth 2/1000 time = 16.247734546661377
I0123 20:40:09.157310 139637719117824 ddar.py:60] Depth 3/1000 time = 19.352038860321045
I0123 20:40:28.581175 139637719117824 ddar.py:60] Depth 4/1000 time = 19.423479557037354
I0123 20:40:48.007866 139637719117824 ddar.py:60] Depth 5/1000 time = 19.42636275291443
I0123 20:41:08.118748 139637719117824 ddar.py:60] Depth 6/1000 time = 20.109464406967163
I0123 20:41:27.939263 139637719117824 ddar.py:60] Depth 7/1000 time = 19.714020013809204
I0123 20:41:27.942643 139637719117824 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 20:41:27.942716 139637719117824 alphageometry.py:566] LM output (score=-2.969387): "q : P f q j l 24 ;"
I0123 20:41:27.942753 139637719117824 alphageometry.py:567] Translation: "q = on_pline q f j l"

I0123 20:41:27.942800 139637719117824 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_line p a e, on_bline p e a; q = on_pline q f j l ? simtri b a c n o m"
I0123 20:41:27.943003 139637719117824 graph.py:498] 
I0123 20:41:27.943067 139637719117824 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_line p a e, on_bline p e a; q = on_pline q f j l ? simtri b a c n o m
I0123 20:41:31.781951 139637719117824 ddar.py:60] Depth 1/1000 time = 3.7664833068847656
I0123 20:41:47.026566 139637719117824 ddar.py:60] Depth 2/1000 time = 15.24428915977478
I0123 20:42:04.761385 139637719117824 ddar.py:60] Depth 3/1000 time = 17.734506845474243
I0123 20:42:23.460722 139637719117824 ddar.py:60] Depth 4/1000 time = 18.69898295402527
I0123 20:42:41.376081 139637719117824 ddar.py:60] Depth 5/1000 time = 17.915026903152466
I0123 20:42:59.819858 139637719117824 ddar.py:60] Depth 6/1000 time = 18.442281246185303
I0123 20:43:19.344197 139637719117824 ddar.py:60] Depth 7/1000 time = 19.41274094581604
I0123 20:43:19.347592 139637719117824 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 20:43:19.347660 139637719117824 alphageometry.py:566] LM output (score=-2.983619): "q : T e p h q 24 ;"
I0123 20:43:19.347696 139637719117824 alphageometry.py:567] Translation: "q = on_tline q h e p"

I0123 20:43:19.347736 139637719117824 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_line p a e, on_bline p e a; q = on_tline q h e p ? simtri b a c n o m"
I0123 20:43:19.347935 139637719117824 graph.py:498] 
I0123 20:43:19.348001 139637719117824 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_line p a e, on_bline p e a; q = on_tline q h e p ? simtri b a c n o m
I0123 20:43:23.469650 139637719117824 ddar.py:60] Depth 1/1000 time = 4.048287868499756
I0123 20:43:39.095428 139637719117824 ddar.py:60] Depth 2/1000 time = 15.625484466552734
I0123 20:43:58.162919 139637719117824 ddar.py:60] Depth 3/1000 time = 19.067212343215942
I0123 20:44:17.457326 139637719117824 ddar.py:60] Depth 4/1000 time = 19.294090747833252
I0123 20:44:36.756786 139637719117824 ddar.py:60] Depth 5/1000 time = 19.299097299575806
I0123 20:44:56.071213 139637719117824 ddar.py:60] Depth 6/1000 time = 19.31290602684021
I0123 20:45:16.537693 139637719117824 ddar.py:60] Depth 7/1000 time = 20.353161096572876
I0123 20:45:16.541313 139637719117824 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 20:45:16.541382 139637719117824 alphageometry.py:566] LM output (score=-2.993467): "q : C i l q 24 D i l l q 25 ;"
I0123 20:45:16.541418 139637719117824 alphageometry.py:567] Translation: "q = on_line q i l, on_circle q l i"

I0123 20:45:16.541471 139637719117824 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_line p a e, on_bline p e a; q = on_line q i l, on_circle q l i ? simtri b a c n o m"
I0123 20:45:16.541680 139637719117824 graph.py:498] 
I0123 20:45:16.541749 139637719117824 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_line p a e, on_bline p e a; q = on_line q i l, on_circle q l i ? simtri b a c n o m
I0123 20:45:21.072660 139637719117824 ddar.py:60] Depth 1/1000 time = 4.442119598388672
I0123 20:45:37.142973 139637719117824 ddar.py:60] Depth 2/1000 time = 16.070029973983765
I0123 20:45:56.523558 139637719117824 ddar.py:60] Depth 3/1000 time = 19.380131244659424
I0123 20:46:16.002183 139637719117824 ddar.py:60] Depth 4/1000 time = 19.478195428848267
I0123 20:46:35.496884 139637719117824 ddar.py:60] Depth 5/1000 time = 19.49436068534851
I0123 20:46:54.157472 139637719117824 ddar.py:60] Depth 6/1000 time = 18.658939123153687
I0123 20:47:14.022828 139637719117824 ddar.py:60] Depth 7/1000 time = 19.745455503463745
I0123 20:47:14.026773 139637719117824 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 20:47:14.026866 139637719117824 alphageometry.py:566] LM output (score=-3.038229): "q : T a p a q 24 ;"
I0123 20:47:14.026904 139637719117824 alphageometry.py:567] Translation: "q = on_tline q a a p"

I0123 20:47:14.026956 139637719117824 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_line p a e, on_bline p e a; q = on_tline q a a p ? simtri b a c n o m"
I0123 20:47:14.027179 139637719117824 graph.py:498] 
I0123 20:47:14.027242 139637719117824 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_line p a e, on_bline p e a; q = on_tline q a a p ? simtri b a c n o m
I0123 20:47:18.966049 139637719117824 ddar.py:60] Depth 1/1000 time = 4.867931127548218
I0123 20:47:35.822831 139637719117824 ddar.py:60] Depth 2/1000 time = 16.856494903564453
I0123 20:47:56.176268 139637719117824 ddar.py:60] Depth 3/1000 time = 20.352958917617798
I0123 20:48:17.549319 139637719117824 ddar.py:60] Depth 4/1000 time = 21.37254810333252
I0123 20:48:38.993277 139637719117824 ddar.py:60] Depth 5/1000 time = 21.443507194519043
I0123 20:48:59.564628 139637719117824 ddar.py:60] Depth 6/1000 time = 20.56999158859253
I0123 20:49:21.302999 139637719117824 ddar.py:60] Depth 7/1000 time = 21.62967085838318
I0123 20:49:21.303727 139637719117824 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 20:49:21.303854 139637719117824 alphageometry.py:566] LM output (score=-3.075047): "q : ^ d a d q d q d e 24 ^ e a e q e q e d 25 ;"
I0123 20:49:21.303895 139637719117824 alphageometry.py:567] Translation: "ERROR: Invalid predicate ^ d a d q d q d e"

I0123 20:49:21.303949 139637719117824 alphageometry.py:566] LM output (score=-3.088834): "q : T f q i k 24 ;"
I0123 20:49:21.303978 139637719117824 alphageometry.py:567] Translation: "q = on_tline q f i k"

I0123 20:49:21.304017 139637719117824 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_line p a e, on_bline p e a; q = on_tline q f i k ? simtri b a c n o m"
I0123 20:49:21.304262 139637719117824 graph.py:498] 
I0123 20:49:21.304327 139637719117824 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_line p a e, on_bline p e a; q = on_tline q f i k ? simtri b a c n o m
I0123 20:49:24.912319 139637719117824 ddar.py:60] Depth 1/1000 time = 3.532992362976074
I0123 20:49:41.402382 139637719117824 ddar.py:60] Depth 2/1000 time = 16.489787817001343
I0123 20:50:00.455957 139637719117824 ddar.py:60] Depth 3/1000 time = 19.0531268119812
I0123 20:50:19.660466 139637719117824 ddar.py:60] Depth 4/1000 time = 19.204069137573242
I0123 20:50:37.953350 139637719117824 ddar.py:60] Depth 5/1000 time = 18.292466402053833
I0123 20:50:57.027932 139637719117824 ddar.py:60] Depth 6/1000 time = 19.072994709014893
I0123 20:51:16.259032 139637719117824 ddar.py:60] Depth 7/1000 time = 19.22431182861328
I0123 20:51:35.548520 139637719117824 ddar.py:60] Depth 8/1000 time = 19.193080186843872
I0123 20:51:35.548873 139637719117824 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 20:51:35.548942 139637719117824 alphageometry.py:566] LM output (score=-3.103292): "q : C i l q 24 D i p p q 25 ;"
I0123 20:51:35.548978 139637719117824 alphageometry.py:567] Translation: "q = on_line q i l, on_circle q p i"

I0123 20:51:35.549023 139637719117824 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_line p a e, on_bline p e a; q = on_line q i l, on_circle q p i ? simtri b a c n o m"
I0123 20:51:35.549230 139637719117824 graph.py:498] 
I0123 20:51:35.549298 139637719117824 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_line p a e, on_bline p e a; q = on_line q i l, on_circle q p i ? simtri b a c n o m
I0123 20:51:40.257863 139637719117824 ddar.py:60] Depth 1/1000 time = 4.628276824951172
I0123 20:51:55.765982 139637719117824 ddar.py:60] Depth 2/1000 time = 15.507828712463379
I0123 20:52:14.302726 139637719117824 ddar.py:60] Depth 3/1000 time = 18.53629994392395
I0123 20:52:32.357095 139637719117824 ddar.py:60] Depth 4/1000 time = 18.053991556167603
I0123 20:52:51.274682 139637719117824 ddar.py:60] Depth 5/1000 time = 18.917104244232178
I0123 20:53:09.359709 139637719117824 ddar.py:60] Depth 6/1000 time = 18.083516597747803
I0123 20:53:28.684628 139637719117824 ddar.py:60] Depth 7/1000 time = 19.215117692947388
I0123 20:53:28.688086 139637719117824 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 20:53:28.688156 139637719117824 alphageometry.py:566] LM output (score=-3.104266): "q : C i l q 24 D i q l q 25 ;"
I0123 20:53:28.688191 139637719117824 alphageometry.py:567] Translation: "q = on_line q i l, on_bline q l i"

I0123 20:53:28.688235 139637719117824 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_line p a e, on_bline p e a; q = on_line q i l, on_bline q l i ? simtri b a c n o m"
I0123 20:53:28.688453 139637719117824 graph.py:498] 
I0123 20:53:28.688519 139637719117824 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_line p a e, on_bline p e a; q = on_line q i l, on_bline q l i ? simtri b a c n o m
I0123 20:53:32.439685 139637719117824 ddar.py:60] Depth 1/1000 time = 3.668353319168091
I0123 20:53:49.005730 139637719117824 ddar.py:60] Depth 2/1000 time = 16.565746068954468
I0123 20:54:06.847825 139637719117824 ddar.py:60] Depth 3/1000 time = 17.841598987579346
I0123 20:54:24.875976 139637719117824 ddar.py:60] Depth 4/1000 time = 18.02765965461731
I0123 20:54:43.796545 139637719117824 ddar.py:60] Depth 5/1000 time = 18.920093774795532
I0123 20:55:01.877609 139637719117824 ddar.py:60] Depth 6/1000 time = 18.079434871673584
I0123 20:55:20.384841 139637719117824 ddar.py:60] Depth 7/1000 time = 18.395074129104614
I0123 20:55:20.385304 139637719117824 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 20:55:20.385388 139637719117824 alphageometry.py:566] LM output (score=-3.105944): "q : C k l q 24 D k l l q 25 ;"
I0123 20:55:20.385423 139637719117824 alphageometry.py:567] Translation: "q = on_line q k l, on_circle q l k"

I0123 20:55:20.385481 139637719117824 alphageometry.py:576] Solving: "a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_line p a e, on_bline p e a; q = on_line q k l, on_circle q l k ? simtri b a c n o m"
I0123 20:55:20.385717 139637719117824 graph.py:498] 
I0123 20:55:20.385782 139637719117824 graph.py:499] a b c = triangle a b c; d = circle d c b a; e = on_circle e d c, on_line e d c; f = on_circle f d b, on_line f d b; g = on_circle g d a, on_line g d a; h = on_line h b a; i = lc_tangent i h b, on_line i c b; j = on_line j h i; k = foot k j c b; l = foot l j c a; m = mirror m e h; n = mirror n f l; o = mirror o g k; p = on_line p a e, on_bline p e a; q = on_line q k l, on_circle q l k ? simtri b a c n o m
I0123 20:55:24.455936 139637719117824 ddar.py:60] Depth 1/1000 time = 3.988987684249878
I0123 20:55:41.866684 139637719117824 ddar.py:60] Depth 2/1000 time = 17.410468578338623
I0123 20:56:00.713993 139637719117824 ddar.py:60] Depth 3/1000 time = 18.846924543380737
I0123 20:56:20.597464 139637719117824 ddar.py:60] Depth 4/1000 time = 19.88301968574524
I0123 20:56:39.557094 139637719117824 ddar.py:60] Depth 5/1000 time = 18.95922040939331
I0123 20:56:39.559377 139637719117824 alphageometry.py:221] DD+AR failed to solve the problem.
I0123 20:56:39.559429 139637719117824 alphageometry.py:585] Timeout.
